{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\\n\\n    Yapilmasi gerekenler:\\n        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\\n        - Farkli grafikler cikartmaya calis.\\n        - ML.\\n        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\\n        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\\n\\n    Sorunlar:\\n        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\\n        ortalamasi az olur. Burada farkli bir yontem bul.\\n        Gunluk maks min alinabilir\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1-) Notlar\n",
    "\"\"\"\n",
    "    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\n",
    "\n",
    "    Yapilmasi gerekenler:\n",
    "        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\n",
    "        - Farkli grafikler cikartmaya calis.\n",
    "        - ML.\n",
    "        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\n",
    "        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\n",
    "\n",
    "    Sorunlar:\n",
    "        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\n",
    "        ortalamasi az olur. Burada farkli bir yontem bul.\n",
    "        Gunluk maks min alinabilir\n",
    "\"\"\"\n",
    "# 1-) read and preproccess train.csv\n",
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "# 3-) read and preprocess weather.csv\n",
    "# 4-) read and preprocess holidays.csv\n",
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi\n",
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği\n",
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli ve gerekli degil)\n",
    "# 10-) test icin birlestirme islemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-) Import required moduls and libraries\n",
    "\n",
    "# bildirimisiz_sum tahmin edilecek\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import os\n",
    "from unidecode import unidecode # to convert Turkish characters to English\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sm\n",
    "import statsmodels.api as sa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tarih         ilce  bildirimsiz_sum  bildirimli_sum\n",
      "19236 2021-01-01  izmir-konak                9               0\n",
      "19237 2021-01-02  izmir-konak               20               0\n",
      "19238 2021-01-03  izmir-konak                7               1\n",
      "19239 2021-01-04  izmir-konak               16               1\n",
      "19240 2021-01-05  izmir-konak                3               0\n",
      "...          ...          ...              ...             ...\n",
      "20355 2024-01-27  izmir-konak               12               3\n",
      "20356 2024-01-28  izmir-konak               13               1\n",
      "20357 2024-01-29  izmir-konak               22               0\n",
      "20358 2024-01-30  izmir-konak               28               1\n",
      "20359 2024-01-31  izmir-konak               16               0\n",
      "\n",
      "[1124 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-) read and preproccess train.csv\n",
    "train = pd.read_csv(\"./train.csv\", low_memory=False) # 46.944 satir, 4 kolon\n",
    "\n",
    "#print(train[\"tarih\"]) # 1.098 farkli tarih var, 47 farkli ilce var\n",
    "\n",
    "tarihler = []\n",
    "for i in train[\"tarih\"]:\n",
    "    tarihler.append(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "train[\"tarih\"] = tarihler\n",
    "\n",
    "# print(train.dtypes)\n",
    "\n",
    "dict :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak o ilcenin verisi (1096 gun) df olarak tutar\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "print(dict[\"izmir-konak\"])\n",
    "ilceler = (list(dict.keys()))\n",
    "#print(dict.keys()) # keys olarak her ilceyi, values olarak o ilcelerin bulundugu satirlari icerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'izmir-aliaga': 1106, 'izmir-balcova': 698, 'izmir-bayindir': 1105, 'izmir-bayrakli': 1086, 'izmir-bergama': 1120, 'izmir-beydag': 673, 'izmir-bornova': 1124, 'izmir-buca': 1115, 'izmir-cesme': 1125, 'izmir-cigli': 1071, 'izmir-dikili': 1119, 'izmir-foca': 1086, 'izmir-gaziemir': 920, 'izmir-guzelbahce': 856, 'izmir-karabaglar': 1100, 'izmir-karaburun': 1089, 'izmir-karsiyaka': 1085, 'izmir-kemalpasa': 1118, 'izmir-kinik': 914, 'izmir-kiraz': 1097, 'izmir-konak': 1124, 'izmir-menderes': 1125, 'izmir-menemen': 1119, 'izmir-narlidere': 783, 'izmir-odemis': 1124, 'izmir-seferihisar': 1111, 'izmir-selcuk': 872, 'izmir-tire': 1107, 'izmir-torbali': 1124, 'izmir-urla': 1122, 'manisa-ahmetli': 622, 'manisa-akhisar': 1126, 'manisa-alasehir': 1119, 'manisa-demirci': 938, 'manisa-golmarmara': 566, 'manisa-gordes': 1059, 'manisa-kirkagac': 950, 'manisa-koprubasi': 805, 'manisa-kula': 1039, 'manisa-salihli': 1126, 'manisa-sarigol': 1027, 'manisa-saruhanli': 1105, 'manisa-sehzadeler': 1123, 'manisa-selendi': 993, 'manisa-soma': 1086, 'manisa-turgutlu': 1121, 'manisa-yunusemre': 1125}\n"
     ]
    }
   ],
   "source": [
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "\"\"\"\n",
    "for label in dict.keys(): # her ilce icin bildirimsiz ve bildirimli olarak grafiklerini cikart\n",
    "    print(dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.bar(dict[label][\"tarih\"],dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.title(label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(dict[\"izmir-konak\"][\"tarih\"],dict[\"izmir-konak\"][\"bildirimsiz_sum\"])\n",
    "plt.title(label)\n",
    "plt.margins(0.01)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# ilce tarih sayilarini al hepsinde esit veri yok\n",
    "ilce_tarih_sayilari = {}\n",
    "for name in dict.keys():\n",
    "    ilce_tarih_sayilari[name] = len(list(dict[name][\"tarih\"].to_dict().values()))\n",
    "\n",
    "print(ilce_tarih_sayilari)\n",
    "for name in dict.keys():\n",
    "    dict[name].set_index(\"tarih\", inplace=True)\n",
    "\n",
    "# train.set_index(\"tarih\", inplace=True) # train'in tarih kolonunu indexe cevir\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'lat', 'lon', 't_2m:C', 'effective_cloud_cover:p',\n",
      "       'global_rad:W', 'relative_humidity_2m:p', 'wind_dir_10m:d',\n",
      "       'wind_speed_10m:ms', 'prob_precip_1h:p', 't_apparent:C', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3-) read and preprocess weather.csv\n",
    "\n",
    "weather = pd.read_csv(\"./weather.csv\", low_memory=False)\n",
    "print(weather.columns) # onemli kolonlar: date, t_apparent:C (hissedilen sicaklik), wind_dir_10m:d (ruzgar yonu),\n",
    "# wind_speed_10m:ms (ruzgar hizi), prob_precip_1h:p (yagis), ilce\n",
    "\n",
    "# ilceleri ayir\n",
    "ilce_weather = {} # keys olarak ilceleri, values olarak o ilcelerin saatlik (1165 gun) hava durumlarini tutar\n",
    "for label, group in weather.groupby(\"name\"):\n",
    "    ilce_weather[label.lower()] = group\n",
    "\n",
    "# tarihleri tarih formatina cevir\n",
    "#print(ilce_weather[\"izmir-konak\"].dtypes)\n",
    "for name in ilce_weather.keys():\n",
    "\n",
    "    tarihler = [] # duzenli tarihleri burada tut\n",
    "    for date in ilce_weather[name][\"date\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ilce_weather[name][\"date\"] = tarihler # duzenli tarihleri date kolonuna ata\n",
    "    ilce_weather[name].set_index(\"date\", inplace=True) # tarihleri indexe cevir\n",
    "    ilce_weather[name][\"tarih\"] = ilce_weather[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "ilce_weather_day = {} # ilce hava durumu verilerini gunluk olarak tut (ortalama ile)\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_day[name] = ilce_weather[name].resample(\"D\").mean(numeric_only=True)# index'teki tarihleri gune cevir\n",
    "    ilce_weather_day[name][\"tarih\"] = ilce_weather_day[name].index\n",
    "\n",
    "#print(ilce_weather[\"izmir-konak\"][\"tarih\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lon', 't_2m:C', 'effective_cloud_cover:p', 'global_rad:W',\n",
      "       'relative_humidity_2m:p', 'wind_dir_10m:d', 'wind_speed_10m:ms',\n",
      "       'prob_precip_1h:p', 't_apparent:C', 'tarih'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ilce_weather_day[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "lat                               float64\n",
      "lon                               float64\n",
      "t_2m:C                            float64\n",
      "effective_cloud_cover:p           float64\n",
      "global_rad:W                      float64\n",
      "relative_humidity_2m:p            float64\n",
      "wind_dir_10m:d                    float64\n",
      "wind_speed_10m:ms                 float64\n",
      "prob_precip_1h:p                  float64\n",
      "t_apparent:C                      float64\n",
      "name                               object\n",
      "tarih                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3.1-) her ilcenin hava durumunda her gununu ayri ayri df lere koyup dict te tut (runtime: 6m 35s)\n",
    "\n",
    "ilce_weather_detailed = {} \n",
    "# {izmir-konak: {2021-01-01 : df , 2021-01-02 : df ,...} , manisa-akhisar: {2021-01-01 : df , 2021-01-02 : df ,...} }\n",
    "\n",
    "\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_detailed[name] = {}\n",
    "    for label,group in ilce_weather[name].groupby(\"date\"):\n",
    "\n",
    "        gun = label.strftime('%Y-%m-%d')\n",
    "        if gun in ilce_weather_detailed[name]:\n",
    "            ilce_weather_detailed[name][gun] = pd.concat([ilce_weather_detailed[name][gun], group], ignore_index=True)\n",
    "        else:\n",
    "            ilce_weather_detailed[name][gun] = group.copy()\n",
    "\n",
    "\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].keys())[0]))\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].values())[0]).dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat  lot  Sicaklik_max  Sicaklik  Sicaklik_min  \\\n",
      "Tarih                                                            \n",
      "2021-01-01  38.4177  NaN          15.3       NaN          11.9   \n",
      "2021-01-02  38.4177  NaN          17.4       NaN          11.0   \n",
      "2021-01-03  38.4177  NaN          15.3       NaN          11.2   \n",
      "2021-01-04  38.4177  NaN          17.7       NaN          10.5   \n",
      "2021-01-05  38.4177  NaN          16.7       NaN          11.2   \n",
      "\n",
      "            Bulutluluk_max  BUlutluluk  Bulutluluk_min  Guneslilik_max  \\\n",
      "Tarih                                                                    \n",
      "2021-01-01            90.0         NaN            28.2           275.4   \n",
      "2021-01-02            57.5         NaN            10.4           374.0   \n",
      "2021-01-03            99.8         NaN            12.4           151.9   \n",
      "2021-01-04            97.4         NaN             9.2           357.0   \n",
      "2021-01-05            99.7         NaN             5.4           362.3   \n",
      "\n",
      "            Guneslilik  ...  Ruzgar_yonu_min  Ruzgar_hizi_max  Ruzgar_hizi  \\\n",
      "Tarih                   ...                                                  \n",
      "2021-01-01         NaN  ...            128.1              4.0          NaN   \n",
      "2021-01-02         NaN  ...            113.6              3.3          NaN   \n",
      "2021-01-03         NaN  ...            125.8              3.3          NaN   \n",
      "2021-01-04         NaN  ...            125.4              6.6          NaN   \n",
      "2021-01-05         NaN  ...            122.0              5.9          NaN   \n",
      "\n",
      "            Ruzgar_hizi_minYagis_max  Yagis  Yagis_min  \\\n",
      "Tarih                                                    \n",
      "2021-01-01                       NaN    NaN        1.0   \n",
      "2021-01-02                       NaN    NaN        1.0   \n",
      "2021-01-03                       NaN    NaN        1.0   \n",
      "2021-01-04                       NaN    NaN        1.0   \n",
      "2021-01-05                       NaN    NaN        1.0   \n",
      "\n",
      "            Hissedilen_sicaklik_max  Hissedilen_sicaklik  \\\n",
      "Tarih                                                      \n",
      "2021-01-01                     18.0                  NaN   \n",
      "2021-01-02                     20.0                  NaN   \n",
      "2021-01-03                     16.8                  NaN   \n",
      "2021-01-04                     18.4                  NaN   \n",
      "2021-01-05                     20.1                  NaN   \n",
      "\n",
      "            Hissedilen_sicaklik_min         Ilce  \n",
      "Tarih                                             \n",
      "2021-01-01                     12.0  izmir-konak  \n",
      "2021-01-02                     11.1  izmir-konak  \n",
      "2021-01-03                     11.1  izmir-konak  \n",
      "2021-01-04                     10.5  izmir-konak  \n",
      "2021-01-05                     11.3  izmir-konak  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3.2-) 3.1'de ayrilan ilce gunlerini simdi her gun icin degerlerin min max'ini bulup ilce df'lerini tekrar olustur\n",
    "\n",
    "# runtime: 1m 10s\n",
    "weather_last = {} # key olarak ilceleri, value olarak da o ilcelerin hava durumu degerlerini min-max ile tutar\n",
    "for name in ilce_weather_detailed.keys():\n",
    "    weather_last[name] = pd.DataFrame()\n",
    "\n",
    "    for date, day_df in ilce_weather_detailed[name].items():\n",
    "        \n",
    "        # max min leri al\n",
    "        max_values = day_df.max()\n",
    "        min_values = day_df.min()\n",
    "\n",
    "        # satır oluştur\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Tarih\": [datetime.strptime(date, \"%Y-%m-%d\")],\n",
    "            \"lat\": [day_df[\"lat\"].iloc[0]],\n",
    "            \"lon\": [day_df[\"lon\"].iloc[0]],\n",
    "            \"Sicaklik_max\": [max_values[\"t_2m:C\"]],\n",
    "            \"Sicaklik_min\": [min_values[\"t_2m:C\"]],\n",
    "            \"Bulutluluk_max\": [max_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Bulutluluk_min\": [min_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Guneslilik_max\": [max_values.get(\"global_rad:W\", None)],  \n",
    "            \"Guneslilik_min\": [min_values.get(\"global_rad:W\", None)],  \n",
    "            \"Bagil_nem_max\": [max_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Bagil_nem_min\": [min_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Ruzgar_yonu_max\": [max_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_yonu_min\": [min_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_hizi_max\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Ruzgar_hizi_min\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Yagis_max\": [max_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Yagis_min\": [min_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Hissedilen_sicaklik_max\": [max_values.get(\"t_apparent:C\", None)],\n",
    "            \"Hissedilen_sicaklik_min\": [min_values.get(\"t_apparent:C\", None)],\n",
    "            \"Ilce\": [day_df[\"name\"].iloc[0].lower()]  # Ilce ekle\n",
    "        })\n",
    "\n",
    "        weather_last[name] = pd.concat([weather_last[name], new_row], ignore_index=True)\n",
    "\n",
    "    # mean degerleri ekle\n",
    "    weather_last[name] = pd.concat([weather_last[name], ilce_weather_day[name][[\"t_2m:C\",\"effective_cloud_cover:p\",\n",
    "    \"global_rad:W\", \"relative_humidity_2m:p\",\"wind_dir_10m:d\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\",\n",
    "    \"t_apparent:C\"]]], axis=1)\n",
    "    \n",
    "new_column_names = {\n",
    "    \"lat\" : \"lat\",\n",
    "    \"lot\" : \"lot\",\n",
    "    \"Sicaklik_max\" : \"Sicaklik_max\",\n",
    "    \"Sicaklik_min\" : \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\" : \"Bulutluluk_max\",\n",
    "    \"Bulutluluk_min\" : \"Bulutluluk_min\",\n",
    "    \"Guneslilik_max\" : \"Guneslilik_max\",\n",
    "    \"Guneslilik_min\" : \"Guneslilik_min\",\n",
    "    \"Bagil_nem_max\" : \"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\" : \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\" : \"Ruzgar_yonu_max\",\n",
    "    \"Ruzgar_yonu_min\" : \"Ruzgar_yonu_min\",\n",
    "    \"Ruzgar_hizi_max\" : \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi_min\" : \"Ruzgar_hizi_min\",\n",
    "    \"Yagis_max\" : \"Yagis_max\",\n",
    "    \"Yagis_min\" : \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\" : \"Hissedilen_sicaklik_max\",\n",
    "    \"Hissedilen_sicaklik_min\" : \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\" : \"Ilce\",\n",
    "    \"t_2m:C\" : \"Sicaklik\",\n",
    "    \"effective_cloud_cover:p\" : \"Bulutluluk\",\n",
    "    \"global_rad:W\" : \"Guneslilik\",\n",
    "    \"relative_humidity_2m:p\" : \"Bagil_nem\",\n",
    "    \"wind_dir_10m:d\" : \"Ruzgar_yonu\",\n",
    "    \"wind_speed_10m:ms\" : \"Ruzgar_hizi\",\n",
    "    \"prob_precip_1h:p\" : \"Yagis\",\n",
    "    \"t_apparent:C\" : \"Hissedilen_sicaklik\",\n",
    "    \"Tarih\" : \"Tarih\"\n",
    "}\n",
    "new_column_order = [\n",
    "    \"lat\",\n",
    "    \"lot\",\n",
    "    \"Sicaklik_max\",\n",
    "    \"Sicaklik\",\n",
    "    \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\",\n",
    "    \"BUlutluluk\",\n",
    "    \"Bulutluluk_min\",\n",
    "    \"Guneslilik_max\",\n",
    "    \"Guneslilik\",\n",
    "    \"Guneslilik_min\",\n",
    "    \"Bagil_nem_max\",\n",
    "    \"Bagil_nem\",\n",
    "    \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\",\n",
    "    \"Ruzgar_yonu\",\n",
    "    \"Ruzgar_yonu_min\",\n",
    "    \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi\",\n",
    "    \"Ruzgar_hizi_min\"\n",
    "    \"Yagis_max\",\n",
    "    \"Yagis\",\n",
    "    \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\",\n",
    "    \"Hissedilen_sicaklik\",\n",
    "    \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\"\n",
    "\n",
    "]\n",
    "for name in weather_last.keys():\n",
    "    weather_last[name].set_index(\"Tarih\", inplace=True) # tarih kolonunu indexe ata\n",
    "    weather_last[name][\"Tarih\"] = weather_last[name].index # tarih kolonunu tekrardan olustur\n",
    "    weather_last[name] = weather_last[name].rename(columns=new_column_names)\n",
    "    weather_last[name] = weather_last[name].reindex(columns=new_column_order)\n",
    "\n",
    "\n",
    "print(weather_last[\"izmir-konak\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lot', 'Sicaklik_max', 'Sicaklik', 'Sicaklik_min',\n",
      "       'Bulutluluk_max', 'BUlutluluk', 'Bulutluluk_min', 'Guneslilik_max',\n",
      "       'Guneslilik', 'Guneslilik_min', 'Bagil_nem_max', 'Bagil_nem',\n",
      "       'Bagil_nem_min', 'Ruzgar_yonu_max', 'Ruzgar_yonu', 'Ruzgar_yonu_min',\n",
      "       'Ruzgar_hizi_max', 'Ruzgar_hizi', 'Ruzgar_hizi_minYagis_max', 'Yagis',\n",
      "       'Yagis_min', 'Hissedilen_sicaklik_max', 'Hissedilen_sicaklik',\n",
      "       'Hissedilen_sicaklik_min', 'Ilce'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(weather_last[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tatil Adı'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4-) read and preprocess holidays.csv\n",
    "\n",
    "holiday = pd.read_csv(\"./holidays.csv\", low_memory=False)\n",
    "\n",
    "# print(holiday.head())\n",
    "\n",
    "holiday[\"tarih\"] = holiday['Yıl'].astype(str) + '-' + holiday['Ay'].astype(str) + '-' + holiday['Gün'].astype(str)\n",
    "holiday['tarih'] = pd.to_datetime(holiday['tarih'], format='%Y-%m-%d')\n",
    "holiday.set_index(\"tarih\", inplace=True)\n",
    "holiday = holiday.drop(columns=[\"Yıl\", \"Ay\", \"Gün\"])\n",
    "\n",
    "print(holiday.columns) # index olarak tarihi (YY-AA-GG), Bayram_Flag olarak da bayram ismini tutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tarih         ilce  bildirimsiz_sum  bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "                 Tatil Adı  \n",
      "tarih                       \n",
      "2021-01-01  New Year's Day  \n",
      "2021-01-02             NaN  \n",
      "2021-01-03             NaN  \n",
      "2021-01-04             NaN  \n",
      "2021-01-05             NaN  \n",
      "...                    ...  \n",
      "2024-01-27             NaN  \n",
      "2024-01-28             NaN  \n",
      "2024-01-29             NaN  \n",
      "2024-01-30             NaN  \n",
      "2024-01-31             NaN  \n",
      "\n",
      "[1124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "\n",
    "def merge_holiday(df1, df2=holiday):\n",
    "    merged_df = pd.merge(df1, df2[\"Tatil Adı\"], left_index=True, right_index=True, how=\"left\")\n",
    "    #df1[\"Bayramlar\"] = df2[\"Bayram_Flag\"]\n",
    "    return merged_df\n",
    "\n",
    "dict_holiday = {}\n",
    "for name in dict.keys():\n",
    "    dict_holiday[name] = merge_holiday(dict[name],holiday)\n",
    "    dict_holiday[name]['tarih'] = dict_holiday[name].index\n",
    "    dict_holiday[name] = dict_holiday[name].reindex(columns=[\"tarih\", \"ilce\", \"bildirimsiz_sum\", \"bildirimli_sum\", \"Tatil Adı\"])\n",
    "    \n",
    "print(dict_holiday[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ruzgar_hizi_min', 'Yagis_max'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m merged_all \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m dict_holiday\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m---> 10\u001b[0m     merged_all[name] \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_weather\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_holiday\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweather_last\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     merged_all[name]\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarih\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIlce\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBildirimsiz_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBildirimli_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# tekrar isimlendir\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBayram_Flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSicaklik_max\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSicaklik_min\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBagil_nem_max\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBagil_nem_min\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuzgar_hizi_max\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRuzgar_hizi_min\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYagis_max\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYagis_min\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     16\u001b[0m     merged_all[name][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGün\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(merged_all[name]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m, in \u001b[0;36mmerge_weather\u001b[1;34m(df1, df2)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_weather\u001b[39m(df1, df2):\n\u001b[1;32m----> 4\u001b[0m     merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df1, \u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSicaklik_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSicaklik_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBagil_nem_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBagil_nem_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRuzgar_hizi_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRuzgar_hizi_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYagis_max\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYagis_min\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_df\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Ruzgar_hizi_min', 'Yagis_max'] not in index\""
     ]
    }
   ],
   "source": [
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "\n",
    "def merge_weather(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "merged_all = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all[name] = merge_weather(dict_holiday[name], weather_last[name])\n",
    "\n",
    "    merged_all[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "    \n",
    "    merged_all[name]['Gün'] = range(1, len(merged_all[name]) + 1)\n",
    "\n",
    "\n",
    "def merge_weather_mean(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"t_2m:C\",\"relative_humidity_2m:p\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "merged_all_mean = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all_mean[name] = merge_weather_mean(dict_holiday[name], ilce_weather_day[name])\n",
    "    merged_all_mean[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik\", \"Bagil_Nem\", \"Ruzgar_Hizi\", \"Yagis\"]\n",
    "    merged_all_mean[name]['Gün'] = range(1, len(merged_all_mean[name]) + 1)\n",
    "\n",
    "all_in_one_mean = pd.concat(merged_all_mean.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "\n",
    "\n",
    "print(merged_all[\"izmir-konak\"].head())\n",
    "\n",
    "all_in_one = pd.concat(merged_all.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "print(\"\\nall_in_one: \\n\\n\",all_in_one.dtypes)\n",
    "\n",
    "merged_all_week = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_week[name] = merged_all[name].resample(\"W\").sum(numeric_only=True)\n",
    "#print(merged_all_week[\"izmir-konak\"])\n",
    "\n",
    "merged_all_month = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n",
    "print(merged_all_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi (runtime: 19s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimli_siz\"):\n",
    "    os.makedirs(\"./graphs/bildirimli_siz\")\n",
    "    print(\"bildirimli_siz klasoru olustu\")\n",
    "\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "#     plt.figure(figsize=(17,8))\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "#     plt.xlabel(\"Tarih\", fontsize=13)\n",
    "#     plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "#     plt.margins(0.01)\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.subplots_adjust(bottom=0.15)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\\n\\nanaliz = pd.concat([\\n    ayristirma2.observed,\\n    ayristirma2.trend,\\n    ayristirma2.seasonal,\\n    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\\n], axis=1)\\nanaliz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\\n\\nindeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\\n\\n\\nX = sa.add_constant(indeks)\\nmodel = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\\nsonuc = model.fit()\\nrsquared_value = sonuc.rsquared\\ny = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\\n\\nyeni_satirlar = pd.DataFrame(index=y)\\nanaliz = pd.concat([analiz, yeni_satirlar])\\n\\n# not: bu degerleri ayarla\\nmev = [\\n    1.038656,\\n    0.973940,\\n    0.987404,\\n    1.038656\\n]\\n\\nnan_indices = analiz.index[analiz[\\'Mevsimsellik\\'].isna()]\\nfor i, index in enumerate(nan_indices):\\n    if i < len(mev):\\n        analiz.at[index, \\'Mevsimsellik\\'] = mev[i]\\n#print(analiz[\"Mevsimsellik\"])\\n\\ngirdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\\nregmodel = sonuc.predict(sa.add_constant(girdi))\\n\\nanaliz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\\n\\n\\nprint(analiz.head())\\n\\nplt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\')\\n#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\', horizontalalignment=\\'left\\')\\nplt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\\n#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\\nplt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği (runtime: 18s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimsiz_detailed\"):\n",
    "    os.makedirs(\"./graphs/bildirimsiz_detailed\")\n",
    "    print(\"bildirimsiz_detailed klasoru olustu\")\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "\n",
    "    # plt.figure(figsize=(17,8))\n",
    "    # plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    # plt.title(\"{} - Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    # plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    # window_size = 3  # Hareketli ortalama penceresi\n",
    "    # merged_all_week[name]['Moving_Average'] = merged_all_week[name][\"Bildirimsiz_sum\"].rolling(window=window_size, center=True).mean()\n",
    "    # #plt.plot(merged_all_week[name]['Moving_Average'], label=\"MHO\", color=\"black\")\n",
    "\n",
    "    # ortalama = merged_all_week[name][\"Bildirimsiz_sum\"].mean()\n",
    "    # plt.axhline(y=ortalama, color='orange', linestyle='--', label='Ortalama %{:.1f}'.format(ortalama),linewidth=2.2)\n",
    "    # alpha = 0.2  # Yumuşatma parametresi \n",
    "    # # formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "    # merged_all_week[name]['EWMA'] = merged_all_week[name][\"Bildirimsiz_sum\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    # plt.plot(merged_all_week[name]['EWMA'], label=\"EWMA\", color=\"red\", lw=2.9)\n",
    "\n",
    "\n",
    "    # plt.margins(0.01)\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.subplots_adjust(bottom=0.15)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"./graphs/bildirimsiz_detailed/{}.png\".format(name))\n",
    "    #plt.show()\n",
    "\n",
    "\"\"\"\n",
    "ayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\n",
    "\n",
    "analiz = pd.concat([\n",
    "    ayristirma2.observed,\n",
    "    ayristirma2.trend,\n",
    "    ayristirma2.seasonal,\n",
    "    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\n",
    "], axis=1)\n",
    "analiz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\n",
    "\n",
    "indeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\n",
    "\n",
    "\n",
    "X = sa.add_constant(indeks)\n",
    "model = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\n",
    "sonuc = model.fit()\n",
    "rsquared_value = sonuc.rsquared\n",
    "y = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\n",
    "\n",
    "yeni_satirlar = pd.DataFrame(index=y)\n",
    "analiz = pd.concat([analiz, yeni_satirlar])\n",
    "\n",
    "# not: bu degerleri ayarla\n",
    "mev = [\n",
    "    1.038656,\n",
    "    0.973940,\n",
    "    0.987404,\n",
    "    1.038656\n",
    "]\n",
    "\n",
    "nan_indices = analiz.index[analiz['Mevsimsellik'].isna()]\n",
    "for i, index in enumerate(nan_indices):\n",
    "    if i < len(mev):\n",
    "        analiz.at[index, 'Mevsimsellik'] = mev[i]\n",
    "#print(analiz[\"Mevsimsellik\"])\n",
    "\n",
    "girdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\n",
    "regmodel = sonuc.predict(sa.add_constant(girdi))\n",
    "\n",
    "analiz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\n",
    "\n",
    "\n",
    "print(analiz.head())\n",
    "\n",
    "plt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top')\n",
    "#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "plt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\n",
    "#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\n",
    "plt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.0: 30.0, 6.0: 26.285714285714285, 7.0: 34.170212765957444, 8.2: 31.0, 8.5: 36.0, 8.6: 34.0, 8.9: 37.0, 10.3: 26.0, 11.6: 26.0, 12.7: 35.0, 16.6: 37.0, 18.2: 44.0, 18.9: 28.0, 20.7: 35.0, 22.1: 26.0, 23.7: 33.0, 27.7: 25.0, 28.8: 52.5, 29.0: 32.0, 41.3: 35.0, 44.0: 69.0, 44.3: 27.0, 52.6: 22.0, 54.3: 32.0, 54.5: 13.0, 55.0: 20.0, 56.5: 18.0, 57.2: 43.0, 58.5: 37.0, 60.0: 44.0, 60.1: 28.0, 61.6: 29.0, 67.1: 32.0, 69.3: 45.0, 73.1: 22.0, 73.7: 32.0, 77.0: 61.0, 80.7: 41.0, 85.5: 23.0, 88.3: 38.0, 88.4: 37.0, 91.3: 49.0, 98.6: 72.0, 99.9: 41.0, 100.8: 54.0, 101.0: 31.0, 101.3: 27.0, 102.4: 28.0, 102.9: 18.0, 104.2: 40.0, 105.8: 53.0, 106.6: 30.0, 107.8: 20.0, 108.7: 35.0, 120.2: 40.0, 120.4: 28.0, 122.3: 31.0, 125.6: 54.0, 127.6: 39.0, 139.2: 53.0, 141.70000000000002: 35.0, 143.8: 68.0, 154.6: 27.0, 157.2: 41.0, 157.60000000000002: 31.0, 159.3: 47.0, 162.0: 58.0, 164.8: 32.0, 174.4: 29.0, 175.5: 37.0, 175.7: 33.0, 178.0: 27.0, 178.5: 53.0, 178.6: 60.0, 179.1: 53.0, 180.0: 30.0, 180.6: 28.0, 184.0: 43.0, 186.3: 68.0, 190.4: 35.0, 191.2: 84.0, 191.5: 33.0, 191.6: 32.0, 197.8: 66.0, 202.89999999999998: 35.0, 209.8: 45.0, 212.5: 40.0, 215.6: 59.0, 216.9: 37.0, 219.0: 30.0, 223.6: 25.0, 233.6: 35.0, 237.3: 64.0, 241.2: 35.0, 244.1: 39.0, 246.5: 62.0, 252.1: 52.0, 263.8: 64.0, 272.5: 27.0, 280.1: 60.0, 300.6: 35.0, 308.1: 40.0, 323.4: 76.0, 327.6: 65.0, 335.0: 35.0, 352.9: 85.0, 393.6: 48.0, 413.7: 40.0}\n",
      "     Ort. Yagis  Ort. Kesinti\n",
      "0           5.0     30.000000\n",
      "1           6.0     26.285714\n",
      "2           7.0     34.170213\n",
      "3           8.2     31.000000\n",
      "4           8.5     36.000000\n",
      "..          ...           ...\n",
      "103       327.6     65.000000\n",
      "104       335.0     35.000000\n",
      "105       352.9     85.000000\n",
      "106       393.6     48.000000\n",
      "107       413.7     40.000000\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli, gerekli degil)\n",
    "\n",
    "yagis_dict = {}\n",
    "for label,group in merged_all_week[\"izmir-aliaga\"].groupby(\"Yagis_max\"):\n",
    "    yagis_dict[label] = group\n",
    "\n",
    "yagis_dict_toplamlari = {}\n",
    "for deger in yagis_dict.keys():\n",
    "    yagis_dict_toplamlari[deger] = yagis_dict[deger][\"Bildirimsiz_sum\"].mean()\n",
    "print(yagis_dict_toplamlari)\n",
    "\n",
    "hesaplamalar = pd.DataFrame(list(yagis_dict_toplamlari.items()), columns=['Ort. Yagis', 'Ort. Kesinti'])\n",
    "print(hesaplamalar)\n",
    "window_size = 3  # Hareketli ortalama penceresi\n",
    "hesaplamalar['Moving_Average'] = hesaplamalar[\"Ort. Kesinti\"].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "alpha = 0.3  # Yumuşatma parametresi \n",
    "# formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "hesaplamalar['EWMA'] = hesaplamalar[\"Ort. Kesinti\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# #plt.bar(merged_all_week[\"izmir-aliaga\"][\"Yagis\"],merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], width=0.05, label=\"Bildirimsiz\")\n",
    "# plt.scatter(yagis_dict_toplamlari.keys(), yagis_dict_toplamlari.values(), label=\"Ort. Kesinti\")\n",
    "# #plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['Moving_Average'], label=\"MHO\", color=\"red\")\n",
    "# plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['EWMA'], label=\"EWMA\", color=\"red\")\n",
    "# plt.margins(0.01)\n",
    "# plt.title(\"Izmir_Aliaga Yagis - Bildirimsiz (Haftalik)\")\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.xlabel(\"Ortalama Yagis Miktari\")\n",
    "# plt.ylabel(\"Ortalama Elektrik Kesintisi\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tarih         ilce  bildirimli_sum\n",
      "18   2024-02-01  izmir-konak               4\n",
      "65   2024-02-02  izmir-konak               1\n",
      "112  2024-02-03  izmir-konak               1\n",
      "159  2024-02-04  izmir-konak               0\n",
      "206  2024-02-05  izmir-konak               2\n",
      "Index(['Tarih', 'Ilce', 'Bildirimli_sum'], dtype='object')\n",
      "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
      "tarih                                                                          \n",
      "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
      "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
      "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
      "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
      "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
      "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
      "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
      "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
      "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
      "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
      "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
      "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
      "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
      "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
      "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
      "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
      "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
      "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
      "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
      "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
      "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
      "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
      "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
      "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
      "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
      "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
      "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
      "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
      "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
      "\n",
      "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
      "tarih                                                                     \n",
      "2024-02-01           4.6           89.5           50.8              2.9   \n",
      "2024-02-02           3.7           92.1           48.5              3.3   \n",
      "2024-02-03           5.6           88.7           49.0              4.7   \n",
      "2024-02-04           5.9           86.1           56.0              1.6   \n",
      "2024-02-05           7.1           95.7           59.0              2.7   \n",
      "2024-02-06           9.3           95.3           60.8              3.0   \n",
      "2024-02-07          12.0           95.7           67.3              6.9   \n",
      "2024-02-08          12.7           89.8           62.3              6.5   \n",
      "2024-02-09          10.8           94.2           60.1              3.5   \n",
      "2024-02-10          11.0           93.2           66.3              6.2   \n",
      "2024-02-11          13.8           82.0           54.5              8.0   \n",
      "2024-02-12          12.1           90.0           65.2              7.9   \n",
      "2024-02-13           8.9           96.1           52.5              3.9   \n",
      "2024-02-14           8.0           99.9           47.6              5.6   \n",
      "2024-02-15           8.2           80.6           59.9              5.3   \n",
      "2024-02-16           7.0           84.0           53.4              4.2   \n",
      "2024-02-17           7.4           87.0           53.2              4.0   \n",
      "2024-02-18           6.8           87.2           53.2              4.0   \n",
      "2024-02-19           6.3           84.7           49.8              3.7   \n",
      "2024-02-20           5.1           89.9           52.6              2.9   \n",
      "2024-02-21           6.1           88.8           44.0              3.4   \n",
      "2024-02-22           8.0           75.4           40.9              2.5   \n",
      "2024-02-23           8.6           95.3           62.4              4.3   \n",
      "2024-02-24           9.7           97.4           55.6              2.8   \n",
      "2024-02-25          10.6           90.8           48.1              3.1   \n",
      "2024-02-26          10.9           84.1           47.7              5.4   \n",
      "2024-02-27          11.8           87.6           50.0              2.6   \n",
      "2024-02-28          10.7           91.7           44.1              2.4   \n",
      "2024-02-29           9.9           92.8           41.2              2.6   \n",
      "\n",
      "            Ruzgar_hizi_min  Yagis_max  Yagis_min  Gün  \n",
      "tarih                                                   \n",
      "2024-02-01              2.9        1.0        1.0    1  \n",
      "2024-02-02              3.3        1.0        1.0    2  \n",
      "2024-02-03              4.7        1.0        1.0    3  \n",
      "2024-02-04              1.6        8.7        1.0    4  \n",
      "2024-02-05              2.7        1.0        1.0    5  \n",
      "2024-02-06              3.0        1.0        1.0    6  \n",
      "2024-02-07              6.9        4.6        1.0    7  \n",
      "2024-02-08              6.5        1.0        1.0    8  \n",
      "2024-02-09              3.5       57.1        1.0    9  \n",
      "2024-02-10              6.2       14.1        1.0   10  \n",
      "2024-02-11              8.0       94.2        1.0   11  \n",
      "2024-02-12              7.9       95.0        1.0   12  \n",
      "2024-02-13              3.9       31.7        1.0   13  \n",
      "2024-02-14              5.6        1.0        1.0   14  \n",
      "2024-02-15              5.3       68.7        1.0   15  \n",
      "2024-02-16              4.2        1.0        1.0   16  \n",
      "2024-02-17              4.0        1.0        1.0   17  \n",
      "2024-02-18              4.0        1.0        1.0   18  \n",
      "2024-02-19              3.7        1.0        1.0   19  \n",
      "2024-02-20              2.9        1.0        1.0   20  \n",
      "2024-02-21              3.4        8.3        1.0   21  \n",
      "2024-02-22              2.5        1.0        1.0   22  \n",
      "2024-02-23              4.3        1.0        1.0   23  \n",
      "2024-02-24              2.8        1.0        1.0   24  \n",
      "2024-02-25              3.1       25.7        1.0   25  \n",
      "2024-02-26              5.4       31.1        1.0   26  \n",
      "2024-02-27              2.6        1.0        1.0   27  \n",
      "2024-02-28              2.4        1.0        1.0   28  \n",
      "2024-02-29              2.6       85.1        1.0   29  \n"
     ]
    }
   ],
   "source": [
    "# 10-) test icin birlestirme islemleri\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\", low_memory=False) # 47 ilce icin 28 gunluk veriler var. (tarih, ilce, bildirimli_sum)\n",
    "#print(test)\n",
    "\n",
    "dict_test :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak ilcelerin 4 ocak - 31 ocak arasi verilerini df olarak tutar\n",
    "for label, group in test.groupby(\"ilce\"):\n",
    "    dict_test[label] = group\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].head())\n",
    "\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    tarihler = [] # duzgun tarihleri tutacak\n",
    "    for date in dict_test[name][\"tarih\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d\")) \n",
    "\n",
    "    dict_test[name][\"tarih\"] = tarihler # duzeltilmis tarihleri ata\n",
    "\n",
    "    dict_test[name].set_index(\"tarih\", inplace=True) # tarih kolonunu index'e ata\n",
    "    dict_test[name][\"Tarih\"] = dict_test[name].index # tarih kolonunu yeniden olustur\n",
    "    dict_test[name] = dict_test[name].iloc[:, [2, 0, 1]] # kolon siralarini duzenle\n",
    "    dict_test[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\"] # kolon isimlerini duzenle\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].columns)\n",
    "\n",
    "\n",
    "dict_test_merged = {} # birlestirilenleri tutacak dict\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    gecici = merge_holiday(dict_test[name], holiday) # test'e holiday ekle\n",
    "    dict_test_merged[name] = merge_weather(gecici, weather_last[name]) # sonra weather'i ekle\n",
    "\n",
    "    dict_test_merged[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "\n",
    "    dict_test_merged[name]['Gün'] = range(1, len(dict_test_merged[name]) + 1) # gun kolonu ekle (1-28 arasi oluyor)\n",
    "\n",
    "print(dict_test_merged[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKINE OGRENMESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "...                    ...           ...           ...            ...   \n",
      "2024-01-27             NaN          12.7           4.6           89.1   \n",
      "2024-01-28             NaN          10.8           4.9           91.6   \n",
      "2024-01-29             NaN           8.9           3.9           83.9   \n",
      "2024-01-30             NaN           9.0           4.4           76.3   \n",
      "2024-01-31             NaN          11.4           4.5           77.6   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "...                   ...              ...              ...        ...   \n",
      "2024-01-27           45.5              2.1              2.1        1.0   \n",
      "2024-01-28           43.5              4.8              4.8        1.0   \n",
      "2024-01-29           52.2              6.9              6.9        1.0   \n",
      "2024-01-30           50.2              6.5              6.5       53.5   \n",
      "2024-01-31           47.7              5.6              5.6        1.0   \n",
      "\n",
      "            Yagis_min   Gün  \n",
      "tarih                        \n",
      "2021-01-01        1.0     1  \n",
      "2021-01-02        1.0     2  \n",
      "2021-01-03        1.0     3  \n",
      "2021-01-04        1.0     4  \n",
      "2021-01-05        1.0     5  \n",
      "...               ...   ...  \n",
      "2024-01-27        1.0  1120  \n",
      "2024-01-28        1.0  1121  \n",
      "2024-01-29        1.0  1122  \n",
      "2024-01-30        1.0  1123  \n",
      "2024-01-31        1.0  1124  \n",
      "\n",
      "[1124 rows x 14 columns]\n",
      "['izmir-aliaga', 'izmir-balcova', 'izmir-bayindir', 'izmir-bayrakli', 'izmir-bergama', 'izmir-beydag', 'izmir-bornova', 'izmir-buca', 'izmir-cesme', 'izmir-cigli', 'izmir-dikili', 'izmir-foca', 'izmir-gaziemir', 'izmir-guzelbahce', 'izmir-karabaglar', 'izmir-karaburun', 'izmir-karsiyaka', 'izmir-kemalpasa', 'izmir-kinik', 'izmir-kiraz', 'izmir-konak', 'izmir-menderes', 'izmir-menemen', 'izmir-narlidere', 'izmir-odemis', 'izmir-seferihisar', 'izmir-selcuk', 'izmir-tire', 'izmir-torbali', 'izmir-urla', 'manisa-ahmetli', 'manisa-akhisar', 'manisa-alasehir', 'manisa-demirci', 'manisa-golmarmara', 'manisa-gordes', 'manisa-kirkagac', 'manisa-koprubasi', 'manisa-kula', 'manisa-salihli', 'manisa-sarigol', 'manisa-saruhanli', 'manisa-sehzadeler', 'manisa-selendi', 'manisa-soma', 'manisa-turgutlu', 'manisa-yunusemre']\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all['izmir-konak']\n",
    "df_test = dict_test_merged[\"izmir-konak\"]\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "output_var = df['Bildirimsiz_sum']\n",
    "target = 'Bildirimsiz_sum'\n",
    "ilceler = []\n",
    "\n",
    "dict = {}\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "ilceler = list(dict.keys())\n",
    "\n",
    "print(df)\n",
    "print(ilceler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tarih</th>\n",
       "      <th>Ilce</th>\n",
       "      <th>Bildirimli_sum</th>\n",
       "      <th>Bayram_Flag</th>\n",
       "      <th>Sicaklik_max</th>\n",
       "      <th>Sicaklik_min</th>\n",
       "      <th>Bagil_nem_max</th>\n",
       "      <th>Bagil_nem_min</th>\n",
       "      <th>Ruzgar_hizi_max</th>\n",
       "      <th>Ruzgar_hizi_min</th>\n",
       "      <th>Yagis_max</th>\n",
       "      <th>Yagis_min</th>\n",
       "      <th>Gün</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarih</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-01</th>\n",
       "      <td>2024-02-01</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>89.5</td>\n",
       "      <td>50.8</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>2024-02-02</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>92.1</td>\n",
       "      <td>48.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-03</th>\n",
       "      <td>2024-02-03</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>5.6</td>\n",
       "      <td>88.7</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-04</th>\n",
       "      <td>2024-02-04</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.4</td>\n",
       "      <td>5.9</td>\n",
       "      <td>86.1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>2024-02-05</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.6</td>\n",
       "      <td>7.1</td>\n",
       "      <td>95.7</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>2024-02-06</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>95.3</td>\n",
       "      <td>60.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>2024-02-07</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.3</td>\n",
       "      <td>12.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>67.3</td>\n",
       "      <td>6.9</td>\n",
       "      <td>6.9</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-08</th>\n",
       "      <td>2024-02-08</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.4</td>\n",
       "      <td>12.7</td>\n",
       "      <td>89.8</td>\n",
       "      <td>62.3</td>\n",
       "      <td>6.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-09</th>\n",
       "      <td>2024-02-09</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.4</td>\n",
       "      <td>10.8</td>\n",
       "      <td>94.2</td>\n",
       "      <td>60.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>57.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-10</th>\n",
       "      <td>2024-02-10</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.2</td>\n",
       "      <td>11.0</td>\n",
       "      <td>93.2</td>\n",
       "      <td>66.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.2</td>\n",
       "      <td>14.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-11</th>\n",
       "      <td>2024-02-11</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.5</td>\n",
       "      <td>13.8</td>\n",
       "      <td>82.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>94.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-12</th>\n",
       "      <td>2024-02-12</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.2</td>\n",
       "      <td>12.1</td>\n",
       "      <td>90.0</td>\n",
       "      <td>65.2</td>\n",
       "      <td>7.9</td>\n",
       "      <td>7.9</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-13</th>\n",
       "      <td>2024-02-13</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>96.1</td>\n",
       "      <td>52.5</td>\n",
       "      <td>3.9</td>\n",
       "      <td>3.9</td>\n",
       "      <td>31.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>2024-02-14</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.8</td>\n",
       "      <td>8.0</td>\n",
       "      <td>99.9</td>\n",
       "      <td>47.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-15</th>\n",
       "      <td>2024-02-15</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>80.6</td>\n",
       "      <td>59.9</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>68.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-16</th>\n",
       "      <td>2024-02-16</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.5</td>\n",
       "      <td>7.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>53.4</td>\n",
       "      <td>4.2</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-17</th>\n",
       "      <td>2024-02-17</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.4</td>\n",
       "      <td>7.4</td>\n",
       "      <td>87.0</td>\n",
       "      <td>53.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-18</th>\n",
       "      <td>2024-02-18</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>87.2</td>\n",
       "      <td>53.2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-19</th>\n",
       "      <td>2024-02-19</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>84.7</td>\n",
       "      <td>49.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-20</th>\n",
       "      <td>2024-02-20</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.9</td>\n",
       "      <td>5.1</td>\n",
       "      <td>89.9</td>\n",
       "      <td>52.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-21</th>\n",
       "      <td>2024-02-21</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.6</td>\n",
       "      <td>6.1</td>\n",
       "      <td>88.8</td>\n",
       "      <td>44.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-22</th>\n",
       "      <td>2024-02-22</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>75.4</td>\n",
       "      <td>40.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>2024-02-23</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.9</td>\n",
       "      <td>8.6</td>\n",
       "      <td>95.3</td>\n",
       "      <td>62.4</td>\n",
       "      <td>4.3</td>\n",
       "      <td>4.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-24</th>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>97.4</td>\n",
       "      <td>55.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-25</th>\n",
       "      <td>2024-02-25</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>90.8</td>\n",
       "      <td>48.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>25.7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-26</th>\n",
       "      <td>2024-02-26</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.5</td>\n",
       "      <td>10.9</td>\n",
       "      <td>84.1</td>\n",
       "      <td>47.7</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>31.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>2024-02-27</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.5</td>\n",
       "      <td>11.8</td>\n",
       "      <td>87.6</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-28</th>\n",
       "      <td>2024-02-28</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>91.7</td>\n",
       "      <td>44.1</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.1</td>\n",
       "      <td>9.9</td>\n",
       "      <td>92.8</td>\n",
       "      <td>41.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>85.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
       "tarih                                                                          \n",
       "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
       "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
       "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
       "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
       "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
       "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
       "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
       "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
       "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
       "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
       "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
       "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
       "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
       "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
       "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
       "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
       "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
       "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
       "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
       "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
       "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
       "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
       "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
       "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
       "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
       "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
       "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
       "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
       "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
       "\n",
       "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
       "tarih                                                                     \n",
       "2024-02-01           4.6           89.5           50.8              2.9   \n",
       "2024-02-02           3.7           92.1           48.5              3.3   \n",
       "2024-02-03           5.6           88.7           49.0              4.7   \n",
       "2024-02-04           5.9           86.1           56.0              1.6   \n",
       "2024-02-05           7.1           95.7           59.0              2.7   \n",
       "2024-02-06           9.3           95.3           60.8              3.0   \n",
       "2024-02-07          12.0           95.7           67.3              6.9   \n",
       "2024-02-08          12.7           89.8           62.3              6.5   \n",
       "2024-02-09          10.8           94.2           60.1              3.5   \n",
       "2024-02-10          11.0           93.2           66.3              6.2   \n",
       "2024-02-11          13.8           82.0           54.5              8.0   \n",
       "2024-02-12          12.1           90.0           65.2              7.9   \n",
       "2024-02-13           8.9           96.1           52.5              3.9   \n",
       "2024-02-14           8.0           99.9           47.6              5.6   \n",
       "2024-02-15           8.2           80.6           59.9              5.3   \n",
       "2024-02-16           7.0           84.0           53.4              4.2   \n",
       "2024-02-17           7.4           87.0           53.2              4.0   \n",
       "2024-02-18           6.8           87.2           53.2              4.0   \n",
       "2024-02-19           6.3           84.7           49.8              3.7   \n",
       "2024-02-20           5.1           89.9           52.6              2.9   \n",
       "2024-02-21           6.1           88.8           44.0              3.4   \n",
       "2024-02-22           8.0           75.4           40.9              2.5   \n",
       "2024-02-23           8.6           95.3           62.4              4.3   \n",
       "2024-02-24           9.7           97.4           55.6              2.8   \n",
       "2024-02-25          10.6           90.8           48.1              3.1   \n",
       "2024-02-26          10.9           84.1           47.7              5.4   \n",
       "2024-02-27          11.8           87.6           50.0              2.6   \n",
       "2024-02-28          10.7           91.7           44.1              2.4   \n",
       "2024-02-29           9.9           92.8           41.2              2.6   \n",
       "\n",
       "            Ruzgar_hizi_min  Yagis_max  Yagis_min  Gün  \n",
       "tarih                                                   \n",
       "2024-02-01              2.9        1.0        1.0    1  \n",
       "2024-02-02              3.3        1.0        1.0    2  \n",
       "2024-02-03              4.7        1.0        1.0    3  \n",
       "2024-02-04              1.6        8.7        1.0    4  \n",
       "2024-02-05              2.7        1.0        1.0    5  \n",
       "2024-02-06              3.0        1.0        1.0    6  \n",
       "2024-02-07              6.9        4.6        1.0    7  \n",
       "2024-02-08              6.5        1.0        1.0    8  \n",
       "2024-02-09              3.5       57.1        1.0    9  \n",
       "2024-02-10              6.2       14.1        1.0   10  \n",
       "2024-02-11              8.0       94.2        1.0   11  \n",
       "2024-02-12              7.9       95.0        1.0   12  \n",
       "2024-02-13              3.9       31.7        1.0   13  \n",
       "2024-02-14              5.6        1.0        1.0   14  \n",
       "2024-02-15              5.3       68.7        1.0   15  \n",
       "2024-02-16              4.2        1.0        1.0   16  \n",
       "2024-02-17              4.0        1.0        1.0   17  \n",
       "2024-02-18              4.0        1.0        1.0   18  \n",
       "2024-02-19              3.7        1.0        1.0   19  \n",
       "2024-02-20              2.9        1.0        1.0   20  \n",
       "2024-02-21              3.4        8.3        1.0   21  \n",
       "2024-02-22              2.5        1.0        1.0   22  \n",
       "2024-02-23              4.3        1.0        1.0   23  \n",
       "2024-02-24              2.8        1.0        1.0   24  \n",
       "2024-02-25              3.1       25.7        1.0   25  \n",
       "2024-02-26              5.4       31.1        1.0   26  \n",
       "2024-02-27              2.6        1.0        1.0   27  \n",
       "2024-02-28              2.4        1.0        1.0   28  \n",
       "2024-02-29              2.6       85.1        1.0   29  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Sicaklik', 'Bagil_Nem', 'Ruzgar_Hizi', 'Yagis'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 17\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# indexi gun yapmak gerek!!!!!\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# gunu scale etmemek gerek!!!\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# bayrami da scale etmesek olur!\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# #Scaling\u001b[39;00m\n\u001b[0;32m     16\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler()\n\u001b[1;32m---> 17\u001b[0m feature_transform \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     18\u001b[0m feature_transform \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mfeatures, data\u001b[38;5;241m=\u001b[39mfeature_transform, index\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     19\u001b[0m feature_transform_gun \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(df[features_gun])\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Xesth\\anaconda3\\envs\\tf_env\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Sicaklik', 'Bagil_Nem', 'Ruzgar_Hizi', 'Yagis'] not in index\""
     ]
    }
   ],
   "source": [
    "# ilcelerin numerizasyonu\n",
    "columns_tonumerate = ['Bayram_Flag']\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "# test csv dosyasi numerizasyon\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "# indexi gun yapmak gerek!!!!!\n",
    "# gunu scale etmemek gerek!!!\n",
    "# bayrami da scale etmesek olur!\n",
    "# #Scaling\n",
    "scaler = MinMaxScaler()\n",
    "feature_transform = scaler.fit_transform(df[features])\n",
    "feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tarih</th>\n",
       "      <th>Ilce</th>\n",
       "      <th>Bildirimli_sum</th>\n",
       "      <th>Bayram_Flag</th>\n",
       "      <th>Sicaklik</th>\n",
       "      <th>Bagil_Nem</th>\n",
       "      <th>Ruzgar_Hizi</th>\n",
       "      <th>Yagis</th>\n",
       "      <th>Gün</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarih</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>2024-01-04</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.104167</td>\n",
       "      <td>88.675000</td>\n",
       "      <td>2.758333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>2024-01-05</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.183333</td>\n",
       "      <td>79.950000</td>\n",
       "      <td>1.520833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-06</th>\n",
       "      <td>2024-01-06</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13.729167</td>\n",
       "      <td>72.937500</td>\n",
       "      <td>3.920833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-07</th>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.950000</td>\n",
       "      <td>81.804167</td>\n",
       "      <td>5.770833</td>\n",
       "      <td>2.116667</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08</th>\n",
       "      <td>2024-01-08</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.958333</td>\n",
       "      <td>77.908333</td>\n",
       "      <td>3.637500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-09</th>\n",
       "      <td>2024-01-09</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>11.216667</td>\n",
       "      <td>84.845833</td>\n",
       "      <td>2.470833</td>\n",
       "      <td>33.691667</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-10</th>\n",
       "      <td>2024-01-10</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.875000</td>\n",
       "      <td>71.445833</td>\n",
       "      <td>4.812500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-11</th>\n",
       "      <td>2024-01-11</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6.716667</td>\n",
       "      <td>76.941667</td>\n",
       "      <td>2.495833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-12</th>\n",
       "      <td>2024-01-12</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.845833</td>\n",
       "      <td>70.991667</td>\n",
       "      <td>1.825000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-13</th>\n",
       "      <td>2024-01-13</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4.995833</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>3.254167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-14</th>\n",
       "      <td>2024-01-14</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.129167</td>\n",
       "      <td>72.108333</td>\n",
       "      <td>1.579167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-15</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9.370833</td>\n",
       "      <td>77.429167</td>\n",
       "      <td>3.625000</td>\n",
       "      <td>2.670833</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-16</th>\n",
       "      <td>2024-01-16</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14.379167</td>\n",
       "      <td>84.654167</td>\n",
       "      <td>3.041667</td>\n",
       "      <td>7.970833</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17</th>\n",
       "      <td>2024-01-17</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.975000</td>\n",
       "      <td>83.925000</td>\n",
       "      <td>1.316667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-18</th>\n",
       "      <td>2024-01-18</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13.850000</td>\n",
       "      <td>81.091667</td>\n",
       "      <td>4.070833</td>\n",
       "      <td>2.375000</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-19</th>\n",
       "      <td>2024-01-19</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.120833</td>\n",
       "      <td>81.504167</td>\n",
       "      <td>4.408333</td>\n",
       "      <td>1.608333</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-20</th>\n",
       "      <td>2024-01-20</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.575000</td>\n",
       "      <td>82.479167</td>\n",
       "      <td>3.295833</td>\n",
       "      <td>3.395833</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-21</th>\n",
       "      <td>2024-01-21</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9.162500</td>\n",
       "      <td>76.416667</td>\n",
       "      <td>3.658333</td>\n",
       "      <td>45.666667</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-22</th>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.604167</td>\n",
       "      <td>59.995833</td>\n",
       "      <td>4.033333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-23</th>\n",
       "      <td>2024-01-23</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6.745833</td>\n",
       "      <td>66.812500</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24</th>\n",
       "      <td>2024-01-24</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6.908333</td>\n",
       "      <td>71.075000</td>\n",
       "      <td>2.562500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-25</th>\n",
       "      <td>2024-01-25</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.458333</td>\n",
       "      <td>77.387500</td>\n",
       "      <td>1.383333</td>\n",
       "      <td>7.908333</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-26</th>\n",
       "      <td>2024-01-26</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>77.233333</td>\n",
       "      <td>1.491667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-27</th>\n",
       "      <td>2024-01-27</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8.379167</td>\n",
       "      <td>72.575000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-28</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.587500</td>\n",
       "      <td>70.383333</td>\n",
       "      <td>2.925000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-29</th>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.970833</td>\n",
       "      <td>69.762500</td>\n",
       "      <td>4.650000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-30</th>\n",
       "      <td>2024-01-30</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6.475000</td>\n",
       "      <td>64.350000</td>\n",
       "      <td>4.837500</td>\n",
       "      <td>6.612500</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-31</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>izmir-konak</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.191667</td>\n",
       "      <td>65.329167</td>\n",
       "      <td>3.695833</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Tarih         Ilce  Bildirimli_sum  Bayram_Flag   Sicaklik  \\\n",
       "tarih                                                                        \n",
       "2024-01-04 2024-01-04  izmir-konak               0            0  14.104167   \n",
       "2024-01-05 2024-01-05  izmir-konak               1            0  14.183333   \n",
       "2024-01-06 2024-01-06  izmir-konak               4            0  13.729167   \n",
       "2024-01-07 2024-01-07  izmir-konak               0            0  14.950000   \n",
       "2024-01-08 2024-01-08  izmir-konak               1            0  13.958333   \n",
       "2024-01-09 2024-01-09  izmir-konak               1            0  11.216667   \n",
       "2024-01-10 2024-01-10  izmir-konak               1            0   7.875000   \n",
       "2024-01-11 2024-01-11  izmir-konak               3            0   6.716667   \n",
       "2024-01-12 2024-01-12  izmir-konak               1            0   6.845833   \n",
       "2024-01-13 2024-01-13  izmir-konak               2            0   4.995833   \n",
       "2024-01-14 2024-01-14  izmir-konak               0            0   5.129167   \n",
       "2024-01-15 2024-01-15  izmir-konak               3            0   9.370833   \n",
       "2024-01-16 2024-01-16  izmir-konak               1            0  14.379167   \n",
       "2024-01-17 2024-01-17  izmir-konak               0            0  12.975000   \n",
       "2024-01-18 2024-01-18  izmir-konak               1            0  13.850000   \n",
       "2024-01-19 2024-01-19  izmir-konak               0            0  16.120833   \n",
       "2024-01-20 2024-01-20  izmir-konak               0            0  15.575000   \n",
       "2024-01-21 2024-01-21  izmir-konak               1            0   9.162500   \n",
       "2024-01-22 2024-01-22  izmir-konak               1            0   7.604167   \n",
       "2024-01-23 2024-01-23  izmir-konak               6            0   6.745833   \n",
       "2024-01-24 2024-01-24  izmir-konak               2            0   6.908333   \n",
       "2024-01-25 2024-01-25  izmir-konak               1            0   7.458333   \n",
       "2024-01-26 2024-01-26  izmir-konak               4            0   8.600000   \n",
       "2024-01-27 2024-01-27  izmir-konak               3            0   8.379167   \n",
       "2024-01-28 2024-01-28  izmir-konak               1            0   7.587500   \n",
       "2024-01-29 2024-01-29  izmir-konak               0            0   5.970833   \n",
       "2024-01-30 2024-01-30  izmir-konak               1            0   6.475000   \n",
       "2024-01-31 2024-01-31  izmir-konak               0            0   7.191667   \n",
       "\n",
       "            Bagil_Nem  Ruzgar_Hizi      Yagis  Gün  \n",
       "tarih                                               \n",
       "2024-01-04  88.675000     2.758333   1.000000    1  \n",
       "2024-01-05  79.950000     1.520833   1.000000    2  \n",
       "2024-01-06  72.937500     3.920833   1.000000    3  \n",
       "2024-01-07  81.804167     5.770833   2.116667    4  \n",
       "2024-01-08  77.908333     3.637500   1.000000    5  \n",
       "2024-01-09  84.845833     2.470833  33.691667    6  \n",
       "2024-01-10  71.445833     4.812500   1.000000    7  \n",
       "2024-01-11  76.941667     2.495833   1.000000    8  \n",
       "2024-01-12  70.991667     1.825000   1.000000    9  \n",
       "2024-01-13  64.000000     3.254167   1.000000   10  \n",
       "2024-01-14  72.108333     1.579167   1.000000   11  \n",
       "2024-01-15  77.429167     3.625000   2.670833   12  \n",
       "2024-01-16  84.654167     3.041667   7.970833   13  \n",
       "2024-01-17  83.925000     1.316667   1.000000   14  \n",
       "2024-01-18  81.091667     4.070833   2.375000   15  \n",
       "2024-01-19  81.504167     4.408333   1.608333   16  \n",
       "2024-01-20  82.479167     3.295833   3.395833   17  \n",
       "2024-01-21  76.416667     3.658333  45.666667   18  \n",
       "2024-01-22  59.995833     4.033333   1.000000   19  \n",
       "2024-01-23  66.812500     2.166667   1.000000   20  \n",
       "2024-01-24  71.075000     2.562500   1.000000   21  \n",
       "2024-01-25  77.387500     1.383333   7.908333   22  \n",
       "2024-01-26  77.233333     1.491667   1.000000   23  \n",
       "2024-01-27  72.575000     1.100000   1.000000   24  \n",
       "2024-01-28  70.383333     2.925000   1.000000   25  \n",
       "2024-01-29  69.762500     4.650000   1.000000   26  \n",
       "2024-01-30  64.350000     4.837500   6.612500   27  \n",
       "2024-01-31  65.329167     3.695833   1.000000   28  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "feature_test = scaler2.fit_transform(df_test[features])\n",
    "feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bildirimli_sum</th>\n",
       "      <th>Sicaklik</th>\n",
       "      <th>Bayram_Flag</th>\n",
       "      <th>Bagil_Nem</th>\n",
       "      <th>Ruzgar_Hizi</th>\n",
       "      <th>Yagis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarih</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-01-04</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.818727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.355040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-05</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.825843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695772</td>\n",
       "      <td>0.090098</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-06</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.785019</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451257</td>\n",
       "      <td>0.603925</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-07</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894757</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.760424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-08</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.805618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624582</td>\n",
       "      <td>0.543265</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-09</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.559176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.866483</td>\n",
       "      <td>0.293488</td>\n",
       "      <td>0.731903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-10</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.258801</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.399245</td>\n",
       "      <td>0.794826</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-11</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.154682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590876</td>\n",
       "      <td>0.298840</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-12</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.383408</td>\n",
       "      <td>0.155219</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-13</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139619</td>\n",
       "      <td>0.461195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.422345</td>\n",
       "      <td>0.102587</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-15</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.393258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.607874</td>\n",
       "      <td>0.540589</td>\n",
       "      <td>0.037407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-16</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.843446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.859800</td>\n",
       "      <td>0.415700</td>\n",
       "      <td>0.156063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.717228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.834375</td>\n",
       "      <td>0.046387</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-18</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.795880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.735580</td>\n",
       "      <td>0.636039</td>\n",
       "      <td>0.030784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-19</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749964</td>\n",
       "      <td>0.708296</td>\n",
       "      <td>0.013619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.950936</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.783960</td>\n",
       "      <td>0.470116</td>\n",
       "      <td>0.053638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-21</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.374532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.572570</td>\n",
       "      <td>0.547725</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-22</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.234457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628011</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-23</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237687</td>\n",
       "      <td>0.228368</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-24</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.171910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.386314</td>\n",
       "      <td>0.313113</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-25</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.221348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.606422</td>\n",
       "      <td>0.060660</td>\n",
       "      <td>0.154664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-26</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.323970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.601046</td>\n",
       "      <td>0.083854</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-27</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.304120</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.438617</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-28</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.232959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.362197</td>\n",
       "      <td>0.390723</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-29</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.087640</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340549</td>\n",
       "      <td>0.760036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-30</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.132959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151823</td>\n",
       "      <td>0.800178</td>\n",
       "      <td>0.125653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197378</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185965</td>\n",
       "      <td>0.555754</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bildirimli_sum  Sicaklik  Bayram_Flag  Bagil_Nem  Ruzgar_Hizi  \\\n",
       "tarih                                                                       \n",
       "2024-01-04        0.000000  0.818727          0.0   1.000000     0.355040   \n",
       "2024-01-05        0.166667  0.825843          0.0   0.695772     0.090098   \n",
       "2024-01-06        0.666667  0.785019          0.0   0.451257     0.603925   \n",
       "2024-01-07        0.000000  0.894757          0.0   0.760424     1.000000   \n",
       "2024-01-08        0.166667  0.805618          0.0   0.624582     0.543265   \n",
       "2024-01-09        0.166667  0.559176          0.0   0.866483     0.293488   \n",
       "2024-01-10        0.166667  0.258801          0.0   0.399245     0.794826   \n",
       "2024-01-11        0.500000  0.154682          0.0   0.590876     0.298840   \n",
       "2024-01-12        0.166667  0.166292          0.0   0.383408     0.155219   \n",
       "2024-01-13        0.333333  0.000000          0.0   0.139619     0.461195   \n",
       "2024-01-14        0.000000  0.011985          0.0   0.422345     0.102587   \n",
       "2024-01-15        0.500000  0.393258          0.0   0.607874     0.540589   \n",
       "2024-01-16        0.166667  0.843446          0.0   0.859800     0.415700   \n",
       "2024-01-17        0.000000  0.717228          0.0   0.834375     0.046387   \n",
       "2024-01-18        0.166667  0.795880          0.0   0.735580     0.636039   \n",
       "2024-01-19        0.000000  1.000000          0.0   0.749964     0.708296   \n",
       "2024-01-20        0.000000  0.950936          0.0   0.783960     0.470116   \n",
       "2024-01-21        0.166667  0.374532          0.0   0.572570     0.547725   \n",
       "2024-01-22        0.166667  0.234457          0.0   0.000000     0.628011   \n",
       "2024-01-23        1.000000  0.157303          0.0   0.237687     0.228368   \n",
       "2024-01-24        0.333333  0.171910          0.0   0.386314     0.313113   \n",
       "2024-01-25        0.166667  0.221348          0.0   0.606422     0.060660   \n",
       "2024-01-26        0.666667  0.323970          0.0   0.601046     0.083854   \n",
       "2024-01-27        0.500000  0.304120          0.0   0.438617     0.000000   \n",
       "2024-01-28        0.166667  0.232959          0.0   0.362197     0.390723   \n",
       "2024-01-29        0.000000  0.087640          0.0   0.340549     0.760036   \n",
       "2024-01-30        0.166667  0.132959          0.0   0.151823     0.800178   \n",
       "2024-01-31        0.000000  0.197378          0.0   0.185965     0.555754   \n",
       "\n",
       "               Yagis  \n",
       "tarih                 \n",
       "2024-01-04  0.000000  \n",
       "2024-01-05  0.000000  \n",
       "2024-01-06  0.000000  \n",
       "2024-01-07  0.025000  \n",
       "2024-01-08  0.000000  \n",
       "2024-01-09  0.731903  \n",
       "2024-01-10  0.000000  \n",
       "2024-01-11  0.000000  \n",
       "2024-01-12  0.000000  \n",
       "2024-01-13  0.000000  \n",
       "2024-01-14  0.000000  \n",
       "2024-01-15  0.037407  \n",
       "2024-01-16  0.156063  \n",
       "2024-01-17  0.000000  \n",
       "2024-01-18  0.030784  \n",
       "2024-01-19  0.013619  \n",
       "2024-01-20  0.053638  \n",
       "2024-01-21  1.000000  \n",
       "2024-01-22  0.000000  \n",
       "2024-01-23  0.000000  \n",
       "2024-01-24  0.000000  \n",
       "2024-01-25  0.154664  \n",
       "2024-01-26  0.000000  \n",
       "2024-01-27  0.000000  \n",
       "2024-01-28  0.000000  \n",
       "2024-01-29  0.000000  \n",
       "2024-01-30  0.125653  \n",
       "2024-01-31  0.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test\n",
    "# output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y test-train elde edimi\n",
    "x = df[features]\n",
    "y = output_var # = df[\"target_var\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "#Splitting to Training set and Test set --- burasi timeseries icin split\n",
    "timesplit = TimeSeriesSplit(n_splits=15)\n",
    "for train_index, test_index in timesplit.split(feature_transform):\n",
    "        X_tr, X_te = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_tr, y_te = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "129/129 [==============================] - 1s 1ms/step - loss: 119.2844\n",
      "Epoch 2/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 91.8044\n",
      "Epoch 3/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 57.5516\n",
      "Epoch 4/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 47.4861\n",
      "Epoch 5/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 45.7311\n",
      "Epoch 6/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 44.9020\n",
      "Epoch 7/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 44.2442\n",
      "Epoch 8/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 43.6629\n",
      "Epoch 9/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 43.1340\n",
      "Epoch 10/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 42.6475\n",
      "Epoch 11/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 42.1975\n",
      "Epoch 12/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 41.7796\n",
      "Epoch 13/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 41.3898\n",
      "Epoch 14/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 41.0248\n",
      "Epoch 15/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 40.6815\n",
      "Epoch 16/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 40.3580\n",
      "Epoch 17/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 40.0540\n",
      "Epoch 18/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 39.7679\n",
      "Epoch 19/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 39.5011\n",
      "Epoch 20/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 39.2500\n",
      "Epoch 21/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 39.0197\n",
      "Epoch 22/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 38.8042\n",
      "Epoch 23/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 38.6051\n",
      "Epoch 24/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 38.4214\n",
      "Epoch 25/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 38.2523\n",
      "Epoch 26/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 38.0956\n",
      "Epoch 27/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 37.9514\n",
      "Epoch 28/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.8175\n",
      "Epoch 29/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.6928\n",
      "Epoch 30/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.5775\n",
      "Epoch 31/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.4699\n",
      "Epoch 32/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.3690\n",
      "Epoch 33/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.2726\n",
      "Epoch 34/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.1838\n",
      "Epoch 35/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.0978\n",
      "Epoch 36/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 37.0184\n",
      "Epoch 37/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.9397\n",
      "Epoch 38/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.8648\n",
      "Epoch 39/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.7942\n",
      "Epoch 40/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.7246\n",
      "Epoch 41/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.6573\n",
      "Epoch 42/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.5927\n",
      "Epoch 43/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.5298\n",
      "Epoch 44/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.4676\n",
      "Epoch 45/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.4080\n",
      "Epoch 46/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 36.3478\n",
      "Epoch 47/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.2908\n",
      "Epoch 48/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.2347\n",
      "Epoch 49/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.1779\n",
      "Epoch 50/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.1224\n",
      "Epoch 51/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.0684\n",
      "Epoch 52/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 36.0150\n",
      "Epoch 53/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.9612\n",
      "Epoch 54/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.9092\n",
      "Epoch 55/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.8561\n",
      "Epoch 56/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.8040\n",
      "Epoch 57/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.7518\n",
      "Epoch 58/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 35.7014\n",
      "Epoch 59/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.6498\n",
      "Epoch 60/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.5991\n",
      "Epoch 61/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.5487\n",
      "Epoch 62/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.4973\n",
      "Epoch 63/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.4473\n",
      "Epoch 64/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.3966\n",
      "Epoch 65/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.3462\n",
      "Epoch 66/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.2956\n",
      "Epoch 67/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.2453\n",
      "Epoch 68/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.1942\n",
      "Epoch 69/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.1446\n",
      "Epoch 70/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.0936\n",
      "Epoch 71/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 35.0429\n",
      "Epoch 72/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.9922\n",
      "Epoch 73/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.9413\n",
      "Epoch 74/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.8902\n",
      "Epoch 75/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.8388\n",
      "Epoch 76/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.7875\n",
      "Epoch 77/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.7357\n",
      "Epoch 78/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.6840\n",
      "Epoch 79/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.6319\n",
      "Epoch 80/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 34.5800\n",
      "Epoch 81/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.5279\n",
      "Epoch 82/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 34.4744\n",
      "Epoch 83/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.4213\n",
      "Epoch 84/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.3686\n",
      "Epoch 85/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.3147\n",
      "Epoch 86/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.2615\n",
      "Epoch 87/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 34.2066\n",
      "Epoch 88/100\n",
      "129/129 [==============================] - 0s 2ms/step - loss: 34.1532\n",
      "Epoch 89/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.0979\n",
      "Epoch 90/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 34.0438\n",
      "Epoch 91/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.9879\n",
      "Epoch 92/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.9331\n",
      "Epoch 93/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.8775\n",
      "Epoch 94/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.8220\n",
      "Epoch 95/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.7659\n",
      "Epoch 96/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.7089\n",
      "Epoch 97/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.6534\n",
      "Epoch 98/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.5975\n",
      "Epoch 99/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.5385\n",
      "Epoch 100/100\n",
      "129/129 [==============================] - 0s 1ms/step - loss: 33.4809\n",
      "3/3 [==============================] - 1s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "#Process the data for LSTM\n",
    "trainX = np.array(X_tr)\n",
    "testX = np.array(X_te)\n",
    "X_tr = trainX.reshape(X_tr.shape[0], 1, X_tr.shape[1])\n",
    "X_te = testX.reshape(X_te.shape[0], 1, X_te.shape[1])\n",
    "\n",
    "#Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Model Training\n",
    "history=lstm.fit(X_tr, y_tr, epochs=100, batch_size=8, verbose=1, shuffle=False)\n",
    "\n",
    "#LSTM Prediction\n",
    "y_pr= lstm.predict(X_te)\n",
    "\n",
    "# Predicted vs True Adj Close Value – LSTM  --burasi copy paste\n",
    "plt.plot(y_te, label='True Value')\n",
    "plt.plot(y_pr, label='LSTM Value')\n",
    "plt.title(\"Prediction by LSTM\")\n",
    "plt.xlabel('Time Scale')\n",
    "plt.ylabel('Scaled USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# test_pred = lstm.predict(gercek test)\n",
    "# csv ye yazdir vs vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with class weights: 0.10454545454545454\n",
      "egitim verisi dogrulugu  1.0\n",
      "test verisi dogrulugu  0.10454545454545454\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "k=8\n",
    "neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_hat = neigh.predict(X_test)\n",
    "\n",
    "test_accuracy = neigh.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy with class weights:\", test_accuracy)\n",
    "print(\"egitim verisi dogrulugu \", metrics.accuracy_score(y_train,neigh.predict(X_train)))\n",
    "print(\"test verisi dogrulugu \", metrics.accuracy_score(y_test,y_hat))\n",
    "\n",
    "# test tahmin --tahmini yazmada sikinti cikiyor tek bir ilceden tahmin yapinca iste\n",
    "y_hat = neigh.predict(df_test[features])\n",
    "submission = pd.read_csv(\"sample_submission.csv\", low_memory=False)\n",
    "y_hat = np.round(y_hat).astype(int)\n",
    "df_test['bildirimsiz_sum'] = y_hat\n",
    "df_test.to_csv('knnprediction.csv', index=False)\n",
    "\n",
    "# optimal k degeri\n",
    "\n",
    "# # Define the range of k values to try\n",
    "# k_values = range(1, 21)\n",
    "\n",
    "# # Perform cross-validation for each value of k\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "\n",
    "# # Find the optimal value of k with the highest cross-validation score\n",
    "# optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "# print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "# # Train the model with the optimal k value\n",
    "# neigh = KNeighborsClassifier(n_neighbors=optimal_k).fit(X_train, y_train)\n",
    "# test_accuracy = neigh.score(X_test, y_test)\n",
    "# print(\"Test accuracy with optimal k:\", test_accuracy)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() specifies that the network is a linear stack of layers\n",
    "\n",
    "model.add() adds the hidden layer.\n",
    "\n",
    "Dense means that neurons between layers are fully connected\n",
    "\n",
    "input_dim defines the number of features in the training dataset\n",
    "\n",
    "activation defines the activation function\n",
    "\n",
    "loss selects the cost function\n",
    "\n",
    "optimizer selects the learning algorithm\n",
    "\n",
    "metrics selects the performance metrics to be saved for further analysis\n",
    "\n",
    "model.fit() initialize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 40.6147 - mae: 5.0409 - val_loss: 34.0724 - val_mae: 4.3407\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.9873 - mae: 3.6419 - val_loss: 21.5782 - val_mae: 3.1329\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7866 - mae: 2.6853 - val_loss: 15.1946 - val_mae: 2.8608\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5604 - mae: 2.7175 - val_loss: 15.0648 - val_mae: 2.9242\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4320 - mae: 2.6546 - val_loss: 14.9761 - val_mae: 2.8611\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2573 - mae: 2.6407 - val_loss: 14.8589 - val_mae: 2.8742\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1859 - mae: 2.6655 - val_loss: 14.7349 - val_mae: 2.8647\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0891 - mae: 2.6116 - val_loss: 14.6446 - val_mae: 2.8518\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9857 - mae: 2.6593 - val_loss: 14.5353 - val_mae: 2.8779\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8727 - mae: 2.6348 - val_loss: 14.4327 - val_mae: 2.8387\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8182 - mae: 2.6047 - val_loss: 14.3492 - val_mae: 2.8393\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8230 - mae: 2.6651 - val_loss: 14.2653 - val_mae: 2.8419\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7038 - mae: 2.5956 - val_loss: 14.2058 - val_mae: 2.8270\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6830 - mae: 2.6041 - val_loss: 14.1365 - val_mae: 2.8540\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6017 - mae: 2.6165 - val_loss: 14.0774 - val_mae: 2.8214\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5697 - mae: 2.6075 - val_loss: 14.0189 - val_mae: 2.8123\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5230 - mae: 2.6399 - val_loss: 13.9206 - val_mae: 2.8346\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5265 - mae: 2.5757 - val_loss: 13.8835 - val_mae: 2.8068\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4682 - mae: 2.6335 - val_loss: 13.8272 - val_mae: 2.8093\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4818 - mae: 2.6075 - val_loss: 13.7999 - val_mae: 2.7932\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.4283 - mae: 2.6033 - val_loss: 13.7544 - val_mae: 2.7890\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3451 - mae: 2.5838 - val_loss: 13.6748 - val_mae: 2.8065\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3306 - mae: 2.6111 - val_loss: 13.6304 - val_mae: 2.8012\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3595 - mae: 2.5917 - val_loss: 13.5967 - val_mae: 2.8105\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3400 - mae: 2.5851 - val_loss: 13.5631 - val_mae: 2.8060\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2654 - mae: 2.6027 - val_loss: 13.5334 - val_mae: 2.7788\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2423 - mae: 2.5789 - val_loss: 13.4907 - val_mae: 2.7992\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2625 - mae: 2.6397 - val_loss: 13.4993 - val_mae: 2.7656\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2134 - mae: 2.5609 - val_loss: 13.4165 - val_mae: 2.7783\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1820 - mae: 2.6082 - val_loss: 13.4198 - val_mae: 2.7625\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1746 - mae: 2.5575 - val_loss: 13.3535 - val_mae: 2.7764\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1333 - mae: 2.6002 - val_loss: 13.3265 - val_mae: 2.7612\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0678 - mae: 2.5819 - val_loss: 13.2768 - val_mae: 2.7655\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0648 - mae: 2.5941 - val_loss: 13.2331 - val_mae: 2.7673\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1291 - mae: 2.6309 - val_loss: 13.3915 - val_mae: 2.7205\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1911 - mae: 2.5687 - val_loss: 13.1650 - val_mae: 2.7532\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0001 - mae: 2.5725 - val_loss: 13.1240 - val_mae: 2.7488\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9911 - mae: 2.5638 - val_loss: 13.0946 - val_mae: 2.7553\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0431 - mae: 2.6176 - val_loss: 13.1273 - val_mae: 2.7299\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9664 - mae: 2.5703 - val_loss: 13.0111 - val_mae: 2.7468\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8977 - mae: 2.5658 - val_loss: 12.9942 - val_mae: 2.7274\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9131 - mae: 2.5822 - val_loss: 12.9294 - val_mae: 2.7407\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8655 - mae: 2.5657 - val_loss: 12.9006 - val_mae: 2.7369\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8662 - mae: 2.5659 - val_loss: 12.8666 - val_mae: 2.7405\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8405 - mae: 2.5920 - val_loss: 12.8798 - val_mae: 2.7081\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8165 - mae: 2.5559 - val_loss: 12.8086 - val_mae: 2.7516\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7712 - mae: 2.5791 - val_loss: 12.8034 - val_mae: 2.7139\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7829 - mae: 2.5661 - val_loss: 12.7358 - val_mae: 2.7102\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6791 - mae: 2.5546 - val_loss: 12.6773 - val_mae: 2.7267\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7478 - mae: 2.5600 - val_loss: 12.6408 - val_mae: 2.7231\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 11.6396 - mae: 2.4473\n",
      "Mean Absolute Error on Test Data: 2.447287082672119\n",
      "7/7 [==============================] - 0s 919us/step\n",
      "R-squared: 0.14812716665452974\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 19ms/step - loss: 3.6411 - mae: 1.3942 - val_loss: 2.0551 - val_mae: 1.0115\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.2364 - mae: 0.9978 - val_loss: 1.3635 - val_mae: 0.9179\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8765 - mae: 1.0364 - val_loss: 1.3940 - val_mae: 0.9342\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8779 - mae: 1.0524 - val_loss: 1.3494 - val_mae: 0.9142\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8414 - mae: 1.0230 - val_loss: 1.3261 - val_mae: 0.9029\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8334 - mae: 1.0154 - val_loss: 1.3182 - val_mae: 0.9007\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8163 - mae: 1.0215 - val_loss: 1.3149 - val_mae: 0.9003\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7997 - mae: 1.0161 - val_loss: 1.3018 - val_mae: 0.8934\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7893 - mae: 1.0157 - val_loss: 1.2953 - val_mae: 0.8913\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7764 - mae: 1.0012 - val_loss: 1.2860 - val_mae: 0.8845\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7632 - mae: 0.9965 - val_loss: 1.2817 - val_mae: 0.8829\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7659 - mae: 1.0229 - val_loss: 1.2861 - val_mae: 0.8911\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7395 - mae: 0.9934 - val_loss: 1.2711 - val_mae: 0.8694\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7285 - mae: 0.9927 - val_loss: 1.2778 - val_mae: 0.8858\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7205 - mae: 1.0045 - val_loss: 1.2720 - val_mae: 0.8811\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7123 - mae: 0.9972 - val_loss: 1.2597 - val_mae: 0.8705\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7019 - mae: 0.9799 - val_loss: 1.2576 - val_mae: 0.8691\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6967 - mae: 0.9888 - val_loss: 1.2695 - val_mae: 0.8827\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6903 - mae: 0.9969 - val_loss: 1.2545 - val_mae: 0.8660\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6793 - mae: 0.9768 - val_loss: 1.2550 - val_mae: 0.8716\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6774 - mae: 0.9973 - val_loss: 1.2520 - val_mae: 0.8680\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6651 - mae: 0.9741 - val_loss: 1.2559 - val_mae: 0.8729\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6661 - mae: 0.9987 - val_loss: 1.2535 - val_mae: 0.8693\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6614 - mae: 0.9808 - val_loss: 1.2532 - val_mae: 0.8697\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6493 - mae: 0.9760 - val_loss: 1.2491 - val_mae: 0.8635\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6551 - mae: 0.9838 - val_loss: 1.2521 - val_mae: 0.8673\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6408 - mae: 0.9787 - val_loss: 1.2472 - val_mae: 0.8613\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6378 - mae: 0.9689 - val_loss: 1.2501 - val_mae: 0.8639\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6351 - mae: 0.9704 - val_loss: 1.2485 - val_mae: 0.8630\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6385 - mae: 0.9859 - val_loss: 1.2492 - val_mae: 0.8610\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6285 - mae: 0.9661 - val_loss: 1.2518 - val_mae: 0.8641\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6196 - mae: 0.9722 - val_loss: 1.2506 - val_mae: 0.8614\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6137 - mae: 0.9669 - val_loss: 1.2490 - val_mae: 0.8597\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6161 - mae: 0.9686 - val_loss: 1.2490 - val_mae: 0.8593\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6151 - mae: 0.9630 - val_loss: 1.2580 - val_mae: 0.8665\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6129 - mae: 0.9728 - val_loss: 1.2566 - val_mae: 0.8644\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1.6116 - mae: 0.9704 - val_loss: 1.2503 - val_mae: 0.8563\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6103 - mae: 0.9634 - val_loss: 1.2662 - val_mae: 0.8692\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.6107 - mae: 0.9693 - val_loss: 1.2484 - val_mae: 0.8556\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5934 - mae: 0.9584 - val_loss: 1.2498 - val_mae: 0.8572\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5995 - mae: 0.9731 - val_loss: 1.2501 - val_mae: 0.8549\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5948 - mae: 0.9598 - val_loss: 1.2514 - val_mae: 0.8559\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5888 - mae: 0.9577 - val_loss: 1.2504 - val_mae: 0.8554\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5888 - mae: 0.9612 - val_loss: 1.2570 - val_mae: 0.8574\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5906 - mae: 0.9732 - val_loss: 1.2498 - val_mae: 0.8515\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5857 - mae: 0.9523 - val_loss: 1.2528 - val_mae: 0.8545\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5832 - mae: 0.9540 - val_loss: 1.2600 - val_mae: 0.8590\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5791 - mae: 0.9699 - val_loss: 1.2495 - val_mae: 0.8527\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5882 - mae: 0.9485 - val_loss: 1.2528 - val_mae: 0.8538\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.5999 - mae: 0.9699 - val_loss: 1.2490 - val_mae: 0.8507\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.6075 - mae: 0.9391\n",
      "Mean Absolute Error on Test Data: 0.93913334608078\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.014006933614155948\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 38.6146 - mae: 4.7783 - val_loss: 30.9609 - val_mae: 4.3060\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.9365 - mae: 3.6761 - val_loss: 18.7429 - val_mae: 2.9467\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.5016 - mae: 2.8382 - val_loss: 12.8904 - val_mae: 2.6100\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4561 - mae: 2.8933 - val_loss: 12.5394 - val_mae: 2.5668\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0995 - mae: 2.8186 - val_loss: 12.2318 - val_mae: 2.5186\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8192 - mae: 2.7477 - val_loss: 11.9681 - val_mae: 2.5009\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5664 - mae: 2.7409 - val_loss: 11.7195 - val_mae: 2.4930\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3538 - mae: 2.7062 - val_loss: 11.5148 - val_mae: 2.4739\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2881 - mae: 2.7239 - val_loss: 11.4010 - val_mae: 2.4542\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0812 - mae: 2.6654 - val_loss: 11.2549 - val_mae: 2.4694\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0795 - mae: 2.7064 - val_loss: 11.2042 - val_mae: 2.4408\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9896 - mae: 2.6633 - val_loss: 11.0977 - val_mae: 2.4802\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8775 - mae: 2.6728 - val_loss: 11.0196 - val_mae: 2.4431\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8488 - mae: 2.6332 - val_loss: 10.9196 - val_mae: 2.4718\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8185 - mae: 2.6710 - val_loss: 10.8638 - val_mae: 2.4634\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7567 - mae: 2.6500 - val_loss: 10.8103 - val_mae: 2.4663\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8390 - mae: 2.6659 - val_loss: 10.8370 - val_mae: 2.4265\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7684 - mae: 2.6603 - val_loss: 10.7435 - val_mae: 2.4668\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7030 - mae: 2.6277 - val_loss: 10.6718 - val_mae: 2.4466\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6511 - mae: 2.6351 - val_loss: 10.6812 - val_mae: 2.4727\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6979 - mae: 2.6658 - val_loss: 10.6607 - val_mae: 2.4437\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6858 - mae: 2.6453 - val_loss: 10.6162 - val_mae: 2.4723\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6274 - mae: 2.6281 - val_loss: 10.5844 - val_mae: 2.4536\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5896 - mae: 2.6379 - val_loss: 10.5676 - val_mae: 2.4516\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5716 - mae: 2.6239 - val_loss: 10.5680 - val_mae: 2.4555\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5842 - mae: 2.6314 - val_loss: 10.5247 - val_mae: 2.4628\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5526 - mae: 2.6541 - val_loss: 10.5163 - val_mae: 2.4350\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6192 - mae: 2.6331 - val_loss: 10.4852 - val_mae: 2.4657\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5117 - mae: 2.6301 - val_loss: 10.5021 - val_mae: 2.4343\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5562 - mae: 2.6272 - val_loss: 10.4737 - val_mae: 2.4325\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4701 - mae: 2.6144 - val_loss: 10.4335 - val_mae: 2.4376\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4947 - mae: 2.6312 - val_loss: 10.4233 - val_mae: 2.4430\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4699 - mae: 2.6467 - val_loss: 10.4212 - val_mae: 2.4275\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4676 - mae: 2.5980 - val_loss: 10.4192 - val_mae: 2.4312\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5900 - mae: 2.6786 - val_loss: 10.4276 - val_mae: 2.4182\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4173 - mae: 2.5890 - val_loss: 10.3747 - val_mae: 2.4403\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4542 - mae: 2.6625 - val_loss: 10.3275 - val_mae: 2.4374\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3969 - mae: 2.5975 - val_loss: 10.3173 - val_mae: 2.4389\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3935 - mae: 2.6476 - val_loss: 10.3517 - val_mae: 2.4243\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.3648 - mae: 2.6022 - val_loss: 10.3138 - val_mae: 2.4299\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3783 - mae: 2.6036 - val_loss: 10.2716 - val_mae: 2.4412\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3346 - mae: 2.6548 - val_loss: 10.2738 - val_mae: 2.4297\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3723 - mae: 2.5880 - val_loss: 10.2991 - val_mae: 2.4289\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3029 - mae: 2.6343 - val_loss: 10.2415 - val_mae: 2.4355\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3404 - mae: 2.5938 - val_loss: 10.2413 - val_mae: 2.4587\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2684 - mae: 2.6399 - val_loss: 10.2212 - val_mae: 2.4244\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2984 - mae: 2.5930 - val_loss: 10.1925 - val_mae: 2.4344\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2858 - mae: 2.6072 - val_loss: 10.1926 - val_mae: 2.4410\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2349 - mae: 2.6531 - val_loss: 10.2187 - val_mae: 2.4115\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3313 - mae: 2.6028 - val_loss: 10.1773 - val_mae: 2.4367\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 13.0034 - mae: 2.5754\n",
      "Mean Absolute Error on Test Data: 2.5753984451293945\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.07516097562634783\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 29.0280 - mae: 4.0282 - val_loss: 50.8924 - val_mae: 4.2411\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.6266 - mae: 3.2576 - val_loss: 42.4051 - val_mae: 3.3690\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7949 - mae: 2.5430 - val_loss: 34.7033 - val_mae: 2.9416\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0798 - mae: 2.5150 - val_loss: 33.2569 - val_mae: 3.0512\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9354 - mae: 2.5233 - val_loss: 33.2599 - val_mae: 3.0063\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8460 - mae: 2.4765 - val_loss: 33.1151 - val_mae: 3.0015\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7953 - mae: 2.5023 - val_loss: 32.8676 - val_mae: 3.0056\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7007 - mae: 2.4401 - val_loss: 32.9988 - val_mae: 2.9691\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6326 - mae: 2.4497 - val_loss: 32.5315 - val_mae: 2.9943\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.5884 - mae: 2.4458 - val_loss: 32.4288 - val_mae: 2.9770\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.5455 - mae: 2.4104 - val_loss: 32.3904 - val_mae: 2.9592\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4740 - mae: 2.4238 - val_loss: 32.0452 - val_mae: 2.9668\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4221 - mae: 2.4205 - val_loss: 31.8332 - val_mae: 2.9669\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.3845 - mae: 2.3973 - val_loss: 31.7806 - val_mae: 2.9534\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.3255 - mae: 2.4061 - val_loss: 31.4695 - val_mae: 2.9649\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.2793 - mae: 2.3963 - val_loss: 31.4514 - val_mae: 2.9447\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.2493 - mae: 2.3864 - val_loss: 31.0647 - val_mae: 2.9566\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.2077 - mae: 2.3955 - val_loss: 30.9560 - val_mae: 2.9317\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.1558 - mae: 2.4316 - val_loss: 30.7490 - val_mae: 2.9275\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.0984 - mae: 2.3622 - val_loss: 30.6447 - val_mae: 2.9159\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.0417 - mae: 2.3567 - val_loss: 30.2888 - val_mae: 2.9297\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.0280 - mae: 2.4119 - val_loss: 30.3354 - val_mae: 2.9020\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.9997 - mae: 2.4046 - val_loss: 30.1993 - val_mae: 2.8967\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.9452 - mae: 2.3275 - val_loss: 30.1340 - val_mae: 2.8775\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.9180 - mae: 2.3945 - val_loss: 29.7077 - val_mae: 2.8914\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.8725 - mae: 2.3276 - val_loss: 29.5346 - val_mae: 2.8710\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.9428 - mae: 2.4406 - val_loss: 29.4118 - val_mae: 2.8822\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.7601 - mae: 2.3603 - val_loss: 29.3407 - val_mae: 2.8609\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.6840 - mae: 2.3464 - val_loss: 29.2889 - val_mae: 2.8414\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.6658 - mae: 2.3266 - val_loss: 29.0158 - val_mae: 2.8437\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.6133 - mae: 2.3535 - val_loss: 28.9495 - val_mae: 2.8348\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.5760 - mae: 2.3415 - val_loss: 28.8352 - val_mae: 2.8183\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.5371 - mae: 2.3005 - val_loss: 28.5960 - val_mae: 2.8158\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.5446 - mae: 2.3565 - val_loss: 28.7112 - val_mae: 2.8080\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.5967 - mae: 2.3197 - val_loss: 28.5364 - val_mae: 2.7990\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.4886 - mae: 2.2801 - val_loss: 28.4420 - val_mae: 2.8015\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.4655 - mae: 2.3627 - val_loss: 28.2353 - val_mae: 2.8058\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.4456 - mae: 2.3003 - val_loss: 28.2034 - val_mae: 2.7935\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3995 - mae: 2.3445 - val_loss: 28.0102 - val_mae: 2.7943\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.5752 - mae: 2.2675 - val_loss: 28.3145 - val_mae: 2.7628\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3682 - mae: 2.3509 - val_loss: 27.9196 - val_mae: 2.7818\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3080 - mae: 2.2957 - val_loss: 27.9351 - val_mae: 2.7580\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3187 - mae: 2.3256 - val_loss: 27.7577 - val_mae: 2.7665\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.2564 - mae: 2.2892 - val_loss: 27.8302 - val_mae: 2.7473\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.2537 - mae: 2.3005 - val_loss: 27.2751 - val_mae: 2.7668\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.2240 - mae: 2.3171 - val_loss: 27.5575 - val_mae: 2.7357\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.2050 - mae: 2.2886 - val_loss: 27.2326 - val_mae: 2.7384\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1685 - mae: 2.3137 - val_loss: 27.0950 - val_mae: 2.7279\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1477 - mae: 2.2955 - val_loss: 27.1474 - val_mae: 2.7047\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1363 - mae: 2.2830 - val_loss: 26.8409 - val_mae: 2.7155\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 23.9886 - mae: 2.5717\n",
      "Mean Absolute Error on Test Data: 2.5716731548309326\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1890296896121304\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 13ms/step - loss: 62.9751 - mae: 6.5616 - val_loss: 72.8841 - val_mae: 6.6263\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 49.0264 - mae: 5.4550 - val_loss: 54.7179 - val_mae: 5.2655\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5728 - mae: 3.9161 - val_loss: 34.2632 - val_mae: 3.7847\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1532 - mae: 3.1118 - val_loss: 28.6094 - val_mae: 3.6923\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4296 - mae: 3.2078 - val_loss: 28.6574 - val_mae: 3.6455\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3434 - mae: 3.1403 - val_loss: 28.8365 - val_mae: 3.6091\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2154 - mae: 3.1717 - val_loss: 28.3198 - val_mae: 3.6485\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1443 - mae: 3.1477 - val_loss: 28.7100 - val_mae: 3.5933\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0938 - mae: 3.1532 - val_loss: 28.1649 - val_mae: 3.6300\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9711 - mae: 3.1351 - val_loss: 28.2969 - val_mae: 3.5925\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9326 - mae: 3.1252 - val_loss: 28.1305 - val_mae: 3.5936\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9359 - mae: 3.1473 - val_loss: 27.7926 - val_mae: 3.6191\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8841 - mae: 3.1248 - val_loss: 28.2015 - val_mae: 3.5682\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7804 - mae: 3.1223 - val_loss: 27.7196 - val_mae: 3.5983\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7786 - mae: 3.1157 - val_loss: 27.8659 - val_mae: 3.5679\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7559 - mae: 3.1361 - val_loss: 27.7077 - val_mae: 3.5734\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7447 - mae: 3.1255 - val_loss: 27.8185 - val_mae: 3.5548\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.7438 - mae: 3.1486 - val_loss: 27.5412 - val_mae: 3.5696\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6491 - mae: 3.0916 - val_loss: 27.5743 - val_mae: 3.5539\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6376 - mae: 3.1067 - val_loss: 27.3978 - val_mae: 3.5508\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6043 - mae: 3.1095 - val_loss: 27.3669 - val_mae: 3.5495\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.5735 - mae: 3.1269 - val_loss: 27.3434 - val_mae: 3.5427\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.5270 - mae: 3.0986 - val_loss: 27.0997 - val_mae: 3.5508\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4681 - mae: 3.1106 - val_loss: 27.2277 - val_mae: 3.5359\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4858 - mae: 3.0857 - val_loss: 27.0339 - val_mae: 3.5454\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4670 - mae: 3.1037 - val_loss: 26.8541 - val_mae: 3.5507\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4328 - mae: 3.1205 - val_loss: 27.0973 - val_mae: 3.5271\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4256 - mae: 3.0619 - val_loss: 26.9132 - val_mae: 3.5293\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3704 - mae: 3.1151 - val_loss: 26.8269 - val_mae: 3.5273\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3301 - mae: 3.0802 - val_loss: 26.8481 - val_mae: 3.5197\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3277 - mae: 3.1066 - val_loss: 26.7624 - val_mae: 3.5342\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3420 - mae: 3.0728 - val_loss: 26.7922 - val_mae: 3.5118\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3067 - mae: 3.1214 - val_loss: 26.7362 - val_mae: 3.5218\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3095 - mae: 3.0530 - val_loss: 26.7546 - val_mae: 3.5098\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3307 - mae: 3.1117 - val_loss: 26.7869 - val_mae: 3.5069\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1872 - mae: 3.0861 - val_loss: 26.5074 - val_mae: 3.5179\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2519 - mae: 3.0639 - val_loss: 26.3607 - val_mae: 3.5271\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2178 - mae: 3.0924 - val_loss: 26.8057 - val_mae: 3.5026\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1136 - mae: 3.0768 - val_loss: 26.2843 - val_mae: 3.5284\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1251 - mae: 3.0758 - val_loss: 26.7619 - val_mae: 3.5025\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.0495 - mae: 3.0583 - val_loss: 26.3042 - val_mae: 3.5118\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1196 - mae: 3.0917 - val_loss: 26.7145 - val_mae: 3.5018\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1216 - mae: 3.0840 - val_loss: 26.2101 - val_mae: 3.5117\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.0415 - mae: 3.0536 - val_loss: 26.3404 - val_mae: 3.5111\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.9926 - mae: 3.0985 - val_loss: 26.5210 - val_mae: 3.5028\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.9555 - mae: 3.0520 - val_loss: 26.3032 - val_mae: 3.4908\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.0173 - mae: 3.0899 - val_loss: 26.6769 - val_mae: 3.4995\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.9498 - mae: 3.0522 - val_loss: 26.1373 - val_mae: 3.4980\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.8813 - mae: 3.0556 - val_loss: 26.1262 - val_mae: 3.5058\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.8719 - mae: 3.0597 - val_loss: 26.0841 - val_mae: 3.5039\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 27.0145 - mae: 3.3189\n",
      "Mean Absolute Error on Test Data: 3.31890869140625\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1736218058391541\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 18ms/step - loss: 7.2547 - mae: 1.7968 - val_loss: 6.9998 - val_mae: 1.1936\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.1651 - mae: 1.3130 - val_loss: 5.7929 - val_mae: 1.1507\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.2465 - mae: 1.3230 - val_loss: 5.7421 - val_mae: 1.4100\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1637 - mae: 1.4174 - val_loss: 5.6954 - val_mae: 1.3996\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1047 - mae: 1.3671 - val_loss: 5.6054 - val_mae: 1.3221\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.0915 - mae: 1.3313 - val_loss: 5.5603 - val_mae: 1.3001\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.0829 - mae: 1.3697 - val_loss: 5.5601 - val_mae: 1.3646\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.0426 - mae: 1.3652 - val_loss: 5.5000 - val_mae: 1.3358\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.0241 - mae: 1.3743 - val_loss: 5.4699 - val_mae: 1.3464\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.9680 - mae: 1.3276 - val_loss: 5.3897 - val_mae: 1.2827\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.9429 - mae: 1.3169 - val_loss: 5.3322 - val_mae: 1.2791\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.9155 - mae: 1.3050 - val_loss: 5.2777 - val_mae: 1.2723\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.9032 - mae: 1.2938 - val_loss: 5.2583 - val_mae: 1.2934\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.8875 - mae: 1.3889 - val_loss: 5.3148 - val_mae: 1.3847\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.8539 - mae: 1.3621 - val_loss: 5.1837 - val_mae: 1.2824\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.8186 - mae: 1.2949 - val_loss: 5.1283 - val_mae: 1.2460\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7955 - mae: 1.3023 - val_loss: 5.1383 - val_mae: 1.2989\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7934 - mae: 1.3376 - val_loss: 5.1120 - val_mae: 1.3003\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.8024 - mae: 1.3710 - val_loss: 5.0802 - val_mae: 1.2990\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7522 - mae: 1.2896 - val_loss: 5.0182 - val_mae: 1.2198\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7471 - mae: 1.2833 - val_loss: 5.0482 - val_mae: 1.2972\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7834 - mae: 1.3830 - val_loss: 5.1190 - val_mae: 1.3726\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7513 - mae: 1.3698 - val_loss: 4.9880 - val_mae: 1.2638\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7099 - mae: 1.2875 - val_loss: 4.9670 - val_mae: 1.2420\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7015 - mae: 1.2699 - val_loss: 4.9389 - val_mae: 1.2127\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7033 - mae: 1.3020 - val_loss: 4.9815 - val_mae: 1.2903\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.6844 - mae: 1.2972 - val_loss: 4.9197 - val_mae: 1.2156\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6781 - mae: 1.2788 - val_loss: 4.9400 - val_mae: 1.2516\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6705 - mae: 1.2870 - val_loss: 4.9335 - val_mae: 1.2663\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6684 - mae: 1.2916 - val_loss: 4.9398 - val_mae: 1.2675\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6760 - mae: 1.3260 - val_loss: 4.9177 - val_mae: 1.2409\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7346 - mae: 1.2367 - val_loss: 4.9196 - val_mae: 1.2231\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6945 - mae: 1.3384 - val_loss: 4.9665 - val_mae: 1.3030\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6511 - mae: 1.3077 - val_loss: 4.9082 - val_mae: 1.2408\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6356 - mae: 1.2613 - val_loss: 4.9081 - val_mae: 1.2372\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6954 - mae: 1.3529 - val_loss: 4.9612 - val_mae: 1.2946\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6703 - mae: 1.2416 - val_loss: 4.9042 - val_mae: 1.1769\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6749 - mae: 1.2981 - val_loss: 4.9532 - val_mae: 1.2812\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6275 - mae: 1.2817 - val_loss: 4.9040 - val_mae: 1.2166\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6278 - mae: 1.2748 - val_loss: 4.9640 - val_mae: 1.2912\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6358 - mae: 1.2608 - val_loss: 4.9120 - val_mae: 1.2231\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6905 - mae: 1.3476 - val_loss: 4.9728 - val_mae: 1.2941\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6128 - mae: 1.2688 - val_loss: 4.9050 - val_mae: 1.2303\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6036 - mae: 1.2705 - val_loss: 4.9326 - val_mae: 1.2696\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6337 - mae: 1.3404 - val_loss: 4.9858 - val_mae: 1.3267\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6213 - mae: 1.2575 - val_loss: 4.8702 - val_mae: 1.2120\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6069 - mae: 1.2781 - val_loss: 4.8977 - val_mae: 1.2517\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6873 - mae: 1.2271 - val_loss: 4.9031 - val_mae: 1.1970\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.5959 - mae: 1.2749 - val_loss: 4.9701 - val_mae: 1.2965\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.5810 - mae: 1.2813 - val_loss: 4.9048 - val_mae: 1.2194\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 3.8420 - mae: 1.3045\n",
      "Mean Absolute Error on Test Data: 1.3044828176498413\n",
      "5/5 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.13156389903296395\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 121.7631 - mae: 8.9356 - val_loss: 119.9872 - val_mae: 8.7432\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 102.5060 - mae: 7.7902 - val_loss: 94.7952 - val_mae: 7.2387\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 72.7415 - mae: 5.7720 - val_loss: 58.9693 - val_mae: 4.8835\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.5718 - mae: 4.1657 - val_loss: 42.8683 - val_mae: 4.2386\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.3998 - mae: 4.3938 - val_loss: 42.5284 - val_mae: 4.2718\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.3462 - mae: 4.2201 - val_loss: 42.7493 - val_mae: 4.1874\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.7787 - mae: 4.2695 - val_loss: 42.0999 - val_mae: 4.2287\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4852 - mae: 4.2437 - val_loss: 42.0552 - val_mae: 4.1966\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.2780 - mae: 4.2225 - val_loss: 41.7153 - val_mae: 4.2069\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.0962 - mae: 4.2045 - val_loss: 41.6657 - val_mae: 4.1827\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.8930 - mae: 4.2050 - val_loss: 41.2895 - val_mae: 4.1997\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.7759 - mae: 4.1990 - val_loss: 41.2398 - val_mae: 4.1844\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.5217 - mae: 4.1924 - val_loss: 40.9632 - val_mae: 4.1933\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.2992 - mae: 4.1787 - val_loss: 40.8837 - val_mae: 4.1857\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.1840 - mae: 4.1752 - val_loss: 40.6313 - val_mae: 4.1991\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.0060 - mae: 4.1947 - val_loss: 40.5639 - val_mae: 4.1808\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.9260 - mae: 4.1538 - val_loss: 40.4495 - val_mae: 4.1800\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.8771 - mae: 4.0954 - val_loss: 40.2443 - val_mae: 4.1916\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.6920 - mae: 4.2193 - val_loss: 40.0295 - val_mae: 4.2016\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.4661 - mae: 4.1448 - val_loss: 40.1369 - val_mae: 4.1728\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.3673 - mae: 4.1132 - val_loss: 40.0988 - val_mae: 4.1665\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.2449 - mae: 4.1536 - val_loss: 39.8588 - val_mae: 4.1822\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.1320 - mae: 4.1204 - val_loss: 39.7348 - val_mae: 4.1846\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.1009 - mae: 4.1720 - val_loss: 39.7219 - val_mae: 4.1714\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.9558 - mae: 4.1254 - val_loss: 39.6166 - val_mae: 4.1745\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.1031 - mae: 4.1592 - val_loss: 39.3850 - val_mae: 4.2147\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.8494 - mae: 4.1203 - val_loss: 39.5148 - val_mae: 4.1670\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.6807 - mae: 4.1137 - val_loss: 39.2871 - val_mae: 4.1767\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.6566 - mae: 4.1606 - val_loss: 39.2732 - val_mae: 4.1667\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.5809 - mae: 4.1415 - val_loss: 39.0736 - val_mae: 4.1856\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.5863 - mae: 4.0982 - val_loss: 38.9355 - val_mae: 4.1846\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.5399 - mae: 4.0887 - val_loss: 38.8622 - val_mae: 4.1958\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.5940 - mae: 4.1904 - val_loss: 39.2712 - val_mae: 4.1516\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.1879 - mae: 4.0531 - val_loss: 38.6631 - val_mae: 4.1940\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.1663 - mae: 4.1600 - val_loss: 38.7955 - val_mae: 4.1587\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.0388 - mae: 4.0751 - val_loss: 38.5033 - val_mae: 4.1666\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.9764 - mae: 4.0625 - val_loss: 38.3316 - val_mae: 4.1785\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.0369 - mae: 4.1698 - val_loss: 38.5023 - val_mae: 4.1616\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.8559 - mae: 4.0488 - val_loss: 38.0030 - val_mae: 4.2027\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.6769 - mae: 4.1124 - val_loss: 38.1396 - val_mae: 4.1578\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.5470 - mae: 4.0865 - val_loss: 37.9191 - val_mae: 4.1499\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.4113 - mae: 4.0530 - val_loss: 37.7513 - val_mae: 4.1781\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.3595 - mae: 4.1003 - val_loss: 37.6273 - val_mae: 4.1699\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.2212 - mae: 4.0457 - val_loss: 37.5104 - val_mae: 4.1513\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.1431 - mae: 4.0905 - val_loss: 37.3588 - val_mae: 4.1741\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.9833 - mae: 4.0480 - val_loss: 37.2512 - val_mae: 4.1475\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.0734 - mae: 4.0548 - val_loss: 37.3700 - val_mae: 4.1299\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.6696 - mae: 4.0638 - val_loss: 36.8767 - val_mae: 4.1742\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.6969 - mae: 4.1114 - val_loss: 37.0298 - val_mae: 4.1320\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.6781 - mae: 3.9917 - val_loss: 36.6288 - val_mae: 4.1582\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 32.0317 - mae: 4.3541\n",
      "Mean Absolute Error on Test Data: 4.354104995727539\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.22089670987684507\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 70.9390 - mae: 6.7785 - val_loss: 64.0786 - val_mae: 6.4547\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.5697 - mae: 5.2971 - val_loss: 40.8853 - val_mae: 4.5233\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5642 - mae: 3.5449 - val_loss: 23.0348 - val_mae: 3.1021\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.8560 - mae: 3.3611 - val_loss: 22.3742 - val_mae: 3.2164\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 25.1303 - mae: 3.3140 - val_loss: 22.3566 - val_mae: 3.1407\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.7630 - mae: 3.3458 - val_loss: 22.1581 - val_mae: 3.1442\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.6164 - mae: 3.2838 - val_loss: 22.0930 - val_mae: 3.1191\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.5318 - mae: 3.2807 - val_loss: 21.9917 - val_mae: 3.1144\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.4742 - mae: 3.3200 - val_loss: 21.7883 - val_mae: 3.1199\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.3667 - mae: 3.2810 - val_loss: 21.6251 - val_mae: 3.1204\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.2762 - mae: 3.2606 - val_loss: 21.7160 - val_mae: 3.0824\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.2569 - mae: 3.3151 - val_loss: 21.3655 - val_mae: 3.1110\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.1885 - mae: 3.2423 - val_loss: 21.4435 - val_mae: 3.0879\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.0507 - mae: 3.3117 - val_loss: 21.1349 - val_mae: 3.1298\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.0874 - mae: 3.2579 - val_loss: 21.1038 - val_mae: 3.0995\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.9170 - mae: 3.2791 - val_loss: 21.0913 - val_mae: 3.0845\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.9112 - mae: 3.2412 - val_loss: 20.8778 - val_mae: 3.0752\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.8378 - mae: 3.3205 - val_loss: 20.7481 - val_mae: 3.0676\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.7208 - mae: 3.2517 - val_loss: 20.7305 - val_mae: 3.0632\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.6499 - mae: 3.2501 - val_loss: 20.5292 - val_mae: 3.0704\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.6765 - mae: 3.2895 - val_loss: 20.8260 - val_mae: 3.0174\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.6191 - mae: 3.2528 - val_loss: 20.6047 - val_mae: 3.0243\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4649 - mae: 3.2014 - val_loss: 20.2337 - val_mae: 3.0462\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4220 - mae: 3.3032 - val_loss: 20.2923 - val_mae: 3.0155\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.3278 - mae: 3.2264 - val_loss: 20.0850 - val_mae: 3.0208\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.2133 - mae: 3.2407 - val_loss: 20.1364 - val_mae: 3.0154\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.2034 - mae: 3.2446 - val_loss: 19.8932 - val_mae: 3.0135\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.1388 - mae: 3.2084 - val_loss: 19.8925 - val_mae: 2.9948\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 23.1150 - mae: 3.2917 - val_loss: 19.9086 - val_mae: 2.9878\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.0025 - mae: 3.2231 - val_loss: 19.6715 - val_mae: 2.9925\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9144 - mae: 3.2274 - val_loss: 19.8272 - val_mae: 2.9824\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9103 - mae: 3.2439 - val_loss: 19.7346 - val_mae: 2.9629\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.7701 - mae: 3.2031 - val_loss: 19.5989 - val_mae: 2.9743\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.7040 - mae: 3.2135 - val_loss: 19.5967 - val_mae: 2.9702\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.6456 - mae: 3.2211 - val_loss: 19.4298 - val_mae: 2.9586\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.5440 - mae: 3.1940 - val_loss: 19.3297 - val_mae: 2.9808\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.5312 - mae: 3.2601 - val_loss: 19.1834 - val_mae: 2.9613\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.4458 - mae: 3.1949 - val_loss: 19.1305 - val_mae: 2.9470\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.4264 - mae: 3.2083 - val_loss: 18.9734 - val_mae: 2.9870\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.3479 - mae: 3.2118 - val_loss: 19.0678 - val_mae: 2.9339\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.2395 - mae: 3.2041 - val_loss: 18.8897 - val_mae: 2.9380\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.2746 - mae: 3.2081 - val_loss: 18.7105 - val_mae: 2.9571\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.1229 - mae: 3.2138 - val_loss: 18.8521 - val_mae: 2.9255\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.1025 - mae: 3.1786 - val_loss: 18.7129 - val_mae: 2.9415\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.0755 - mae: 3.2271 - val_loss: 18.7395 - val_mae: 2.9076\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.9817 - mae: 3.1446 - val_loss: 18.4840 - val_mae: 2.9357\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.0309 - mae: 3.2797 - val_loss: 18.5301 - val_mae: 2.8921\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.8979 - mae: 3.1346 - val_loss: 18.3218 - val_mae: 2.9220\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.8034 - mae: 3.2093 - val_loss: 18.2027 - val_mae: 2.9287\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.6919 - mae: 3.2097 - val_loss: 18.2363 - val_mae: 2.8973\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 19.0113 - mae: 3.2336\n",
      "Mean Absolute Error on Test Data: 3.233628273010254\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09194353822728951\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 199.8243 - mae: 12.4601 - val_loss: 196.1312 - val_mae: 11.6704\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 166.1619 - mae: 11.0269 - val_loss: 151.7409 - val_mae: 9.6457\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 111.3540 - mae: 8.3771 - val_loss: 90.5094 - val_mae: 6.6510\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.8794 - mae: 5.6439 - val_loss: 60.0384 - val_mae: 5.4362\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.6272 - mae: 5.3579 - val_loss: 60.2106 - val_mae: 5.5801\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.1118 - mae: 5.2829 - val_loss: 59.3615 - val_mae: 5.4155\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.5924 - mae: 5.2324 - val_loss: 59.0459 - val_mae: 5.4497\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.1818 - mae: 5.2108 - val_loss: 58.6522 - val_mae: 5.4069\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.7696 - mae: 5.2074 - val_loss: 58.3829 - val_mae: 5.4236\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.3943 - mae: 5.1843 - val_loss: 58.0012 - val_mae: 5.3902\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 42.9544 - mae: 5.1296 - val_loss: 57.7012 - val_mae: 5.3583\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 42.6831 - mae: 5.1310 - val_loss: 57.4802 - val_mae: 5.3735\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 42.2830 - mae: 5.1203 - val_loss: 57.0821 - val_mae: 5.3250\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 42.2045 - mae: 5.0423 - val_loss: 56.8334 - val_mae: 5.3247\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.9834 - mae: 5.1327 - val_loss: 56.6026 - val_mae: 5.3204\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.5829 - mae: 5.0106 - val_loss: 56.3289 - val_mae: 5.2802\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.2545 - mae: 5.0710 - val_loss: 56.1209 - val_mae: 5.2904\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.9600 - mae: 5.0002 - val_loss: 55.8189 - val_mae: 5.2771\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.6761 - mae: 4.9894 - val_loss: 55.5927 - val_mae: 5.2736\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4425 - mae: 4.9850 - val_loss: 55.3560 - val_mae: 5.2660\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.2275 - mae: 4.9954 - val_loss: 55.0405 - val_mae: 5.2362\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.1007 - mae: 4.9482 - val_loss: 54.7515 - val_mae: 5.2302\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.7614 - mae: 4.9450 - val_loss: 54.4610 - val_mae: 5.1984\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.7510 - mae: 4.9291 - val_loss: 54.1962 - val_mae: 5.2112\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 39.4601 - mae: 4.8949 - val_loss: 53.9543 - val_mae: 5.1970\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.5362 - mae: 4.9471 - val_loss: 53.7542 - val_mae: 5.1587\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.2006 - mae: 4.8696 - val_loss: 53.5345 - val_mae: 5.1918\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.1052 - mae: 4.8853 - val_loss: 53.3381 - val_mae: 5.1663\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.9301 - mae: 4.8591 - val_loss: 53.2982 - val_mae: 5.2265\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.8827 - mae: 4.8684 - val_loss: 52.9605 - val_mae: 5.1847\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.6649 - mae: 4.8558 - val_loss: 52.7605 - val_mae: 5.1819\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.5878 - mae: 4.8279 - val_loss: 52.6248 - val_mae: 5.1940\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.5365 - mae: 4.8486 - val_loss: 52.4016 - val_mae: 5.1689\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.4240 - mae: 4.8145 - val_loss: 52.2798 - val_mae: 5.1505\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.5177 - mae: 4.8595 - val_loss: 52.1249 - val_mae: 5.1680\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.7823 - mae: 4.7866 - val_loss: 52.1885 - val_mae: 5.2091\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.2204 - mae: 4.8228 - val_loss: 51.7676 - val_mae: 5.1419\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.1447 - mae: 4.8019 - val_loss: 51.6918 - val_mae: 5.1621\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.0312 - mae: 4.7961 - val_loss: 51.4903 - val_mae: 5.1047\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.9890 - mae: 4.7763 - val_loss: 51.4481 - val_mae: 5.1145\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.9306 - mae: 4.7954 - val_loss: 51.2087 - val_mae: 5.1167\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.9017 - mae: 4.7704 - val_loss: 51.0915 - val_mae: 5.1216\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.8293 - mae: 4.7690 - val_loss: 51.1067 - val_mae: 5.1758\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.8293 - mae: 4.7372 - val_loss: 50.8682 - val_mae: 5.1491\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.6019 - mae: 4.7831 - val_loss: 50.7892 - val_mae: 5.1487\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.8626 - mae: 4.7422 - val_loss: 50.8524 - val_mae: 5.1762\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.6595 - mae: 4.7977 - val_loss: 50.4505 - val_mae: 5.0669\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.4254 - mae: 4.7196 - val_loss: 50.3635 - val_mae: 5.1157\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.3653 - mae: 4.7653 - val_loss: 50.2254 - val_mae: 5.0735\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.4173 - mae: 4.7201 - val_loss: 50.2310 - val_mae: 5.1304\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 43.0856 - mae: 4.6834\n",
      "Mean Absolute Error on Test Data: 4.683351993560791\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.19846178164919315\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 12ms/step - loss: 26.5096 - mae: 4.0219 - val_loss: 21.2418 - val_mae: 3.5813\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 18.1964 - mae: 3.0399 - val_loss: 12.9602 - val_mae: 2.4826\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5842 - mae: 2.3879 - val_loss: 8.7789 - val_mae: 2.0729\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.2463 - mae: 2.4424 - val_loss: 8.6610 - val_mae: 2.1143\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.1560 - mae: 2.4128 - val_loss: 8.6016 - val_mae: 2.0709\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.1246 - mae: 2.3764 - val_loss: 8.5557 - val_mae: 2.0579\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.0833 - mae: 2.3897 - val_loss: 8.4931 - val_mae: 2.0558\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.0592 - mae: 2.3767 - val_loss: 8.4294 - val_mae: 2.0586\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.0349 - mae: 2.3927 - val_loss: 8.3813 - val_mae: 2.0516\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.0070 - mae: 2.3905 - val_loss: 8.3658 - val_mae: 2.0296\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.9676 - mae: 2.3655 - val_loss: 8.2931 - val_mae: 2.0410\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.9560 - mae: 2.3924 - val_loss: 8.2810 - val_mae: 2.0207\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.9255 - mae: 2.3519 - val_loss: 8.1905 - val_mae: 2.0349\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.8977 - mae: 2.3816 - val_loss: 8.1429 - val_mae: 2.0408\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 9.9155 - mae: 2.3458 - val_loss: 8.1448 - val_mae: 2.0084\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.8383 - mae: 2.3692 - val_loss: 8.0577 - val_mae: 2.0321\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.8311 - mae: 2.3691 - val_loss: 7.9958 - val_mae: 2.0200\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7927 - mae: 2.3424 - val_loss: 8.0156 - val_mae: 1.9916\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7556 - mae: 2.3485 - val_loss: 7.9196 - val_mae: 2.0047\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7388 - mae: 2.3587 - val_loss: 7.9172 - val_mae: 1.9806\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7615 - mae: 2.3682 - val_loss: 7.8389 - val_mae: 1.9944\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.6948 - mae: 2.3284 - val_loss: 7.8489 - val_mae: 1.9709\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7161 - mae: 2.3360 - val_loss: 7.7958 - val_mae: 1.9684\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.6385 - mae: 2.3286 - val_loss: 7.6935 - val_mae: 1.9942\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7004 - mae: 2.3462 - val_loss: 7.6665 - val_mae: 1.9723\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9.6240 - mae: 2.3571 - val_loss: 7.6685 - val_mae: 1.9663\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5918 - mae: 2.3162 - val_loss: 7.6209 - val_mae: 1.9740\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5755 - mae: 2.3439 - val_loss: 7.6095 - val_mae: 1.9538\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5324 - mae: 2.3254 - val_loss: 7.5726 - val_mae: 1.9549\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.4971 - mae: 2.3304 - val_loss: 7.5249 - val_mae: 1.9530\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5018 - mae: 2.3022 - val_loss: 7.4639 - val_mae: 1.9562\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 9.4857 - mae: 2.3289 - val_loss: 7.4677 - val_mae: 1.9554\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.4458 - mae: 2.3258 - val_loss: 7.4069 - val_mae: 1.9484\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.4826 - mae: 2.2960 - val_loss: 7.4020 - val_mae: 1.9333\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.6980 - mae: 2.3958 - val_loss: 7.4775 - val_mae: 1.9185\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.4418 - mae: 2.2707 - val_loss: 7.3326 - val_mae: 1.9431\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.3842 - mae: 2.3381 - val_loss: 7.3591 - val_mae: 1.9358\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.3312 - mae: 2.2900 - val_loss: 7.2496 - val_mae: 1.9392\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2913 - mae: 2.3013 - val_loss: 7.2378 - val_mae: 1.9226\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2728 - mae: 2.2932 - val_loss: 7.1629 - val_mae: 1.9337\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2606 - mae: 2.3054 - val_loss: 7.2387 - val_mae: 1.9178\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2598 - mae: 2.2688 - val_loss: 7.0796 - val_mae: 1.9351\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2364 - mae: 2.3067 - val_loss: 7.0963 - val_mae: 1.9206\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2237 - mae: 2.2701 - val_loss: 7.0954 - val_mae: 1.9285\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1977 - mae: 2.2963 - val_loss: 7.0096 - val_mae: 1.9173\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1812 - mae: 2.2754 - val_loss: 7.0357 - val_mae: 1.9346\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1411 - mae: 2.2952 - val_loss: 7.0130 - val_mae: 1.8944\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1358 - mae: 2.2599 - val_loss: 7.0102 - val_mae: 1.9339\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1673 - mae: 2.3019 - val_loss: 7.0255 - val_mae: 1.8901\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1523 - mae: 2.2887 - val_loss: 6.9659 - val_mae: 1.9013\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.5278 - mae: 2.3218\n",
      "Mean Absolute Error on Test Data: 2.3218228816986084\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.020247681233753312\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 44.8575 - mae: 5.4544 - val_loss: 39.9656 - val_mae: 5.0836\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4514 - mae: 4.1397 - val_loss: 23.9953 - val_mae: 3.5201\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2291 - mae: 2.9019 - val_loss: 13.8642 - val_mae: 2.6498\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5622 - mae: 2.7833 - val_loss: 13.4722 - val_mae: 2.6586\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3520 - mae: 2.7423 - val_loss: 13.3474 - val_mae: 2.6397\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2345 - mae: 2.7114 - val_loss: 13.2456 - val_mae: 2.6247\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1209 - mae: 2.7153 - val_loss: 13.0723 - val_mae: 2.6291\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0279 - mae: 2.7017 - val_loss: 13.0026 - val_mae: 2.6116\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9937 - mae: 2.7292 - val_loss: 12.9372 - val_mae: 2.5973\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8965 - mae: 2.7058 - val_loss: 12.8222 - val_mae: 2.5924\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8280 - mae: 2.6730 - val_loss: 12.6740 - val_mae: 2.5986\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7386 - mae: 2.6950 - val_loss: 12.6158 - val_mae: 2.5884\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7068 - mae: 2.6871 - val_loss: 12.5191 - val_mae: 2.5819\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6667 - mae: 2.6600 - val_loss: 12.4107 - val_mae: 2.5896\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6819 - mae: 2.7352 - val_loss: 12.4978 - val_mae: 2.5691\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5975 - mae: 2.6713 - val_loss: 12.3257 - val_mae: 2.5825\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5620 - mae: 2.6666 - val_loss: 12.3592 - val_mae: 2.5759\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5891 - mae: 2.6637 - val_loss: 12.2440 - val_mae: 2.5975\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5793 - mae: 2.7310 - val_loss: 12.3577 - val_mae: 2.5607\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5116 - mae: 2.6569 - val_loss: 12.2114 - val_mae: 2.5788\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5201 - mae: 2.6712 - val_loss: 12.2494 - val_mae: 2.5638\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.4886 - mae: 2.7106 - val_loss: 12.3357 - val_mae: 2.5512\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4690 - mae: 2.6655 - val_loss: 12.2275 - val_mae: 2.5524\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4513 - mae: 2.6785 - val_loss: 12.1818 - val_mae: 2.5563\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4231 - mae: 2.6460 - val_loss: 12.0875 - val_mae: 2.5756\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4190 - mae: 2.6924 - val_loss: 12.0990 - val_mae: 2.5738\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3642 - mae: 2.6738 - val_loss: 12.1283 - val_mae: 2.5546\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3522 - mae: 2.6673 - val_loss: 12.0609 - val_mae: 2.5713\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3655 - mae: 2.6653 - val_loss: 12.0829 - val_mae: 2.5638\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3886 - mae: 2.6903 - val_loss: 12.3166 - val_mae: 2.5464\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3203 - mae: 2.6525 - val_loss: 12.0791 - val_mae: 2.5599\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3386 - mae: 2.6795 - val_loss: 12.0190 - val_mae: 2.5591\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3959 - mae: 2.6783 - val_loss: 12.4253 - val_mae: 2.5429\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3665 - mae: 2.6643 - val_loss: 12.1476 - val_mae: 2.5487\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2687 - mae: 2.6617 - val_loss: 12.0696 - val_mae: 2.5567\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2523 - mae: 2.6543 - val_loss: 12.1590 - val_mae: 2.5552\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2594 - mae: 2.6691 - val_loss: 12.0109 - val_mae: 2.5643\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2183 - mae: 2.6512 - val_loss: 12.0905 - val_mae: 2.5538\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.2546 - mae: 2.6577 - val_loss: 11.9496 - val_mae: 2.5713\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2009 - mae: 2.6546 - val_loss: 12.1116 - val_mae: 2.5555\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1897 - mae: 2.6545 - val_loss: 12.1268 - val_mae: 2.5497\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1796 - mae: 2.6425 - val_loss: 11.9631 - val_mae: 2.5657\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2104 - mae: 2.6737 - val_loss: 12.1299 - val_mae: 2.5432\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1544 - mae: 2.6472 - val_loss: 12.0211 - val_mae: 2.5535\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2440 - mae: 2.6654 - val_loss: 11.9259 - val_mae: 2.5893\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2823 - mae: 2.6387 - val_loss: 12.0388 - val_mae: 2.5708\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1788 - mae: 2.7017 - val_loss: 12.0717 - val_mae: 2.5483\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1559 - mae: 2.6441 - val_loss: 11.9533 - val_mae: 2.5494\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1094 - mae: 2.6326 - val_loss: 12.0665 - val_mae: 2.5551\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1216 - mae: 2.6533 - val_loss: 11.9783 - val_mae: 2.5601\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12.5118 - mae: 2.6433\n",
      "Mean Absolute Error on Test Data: 2.6433215141296387\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.030025921918095277\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 24.3889 - mae: 3.9452 - val_loss: 19.7990 - val_mae: 3.3399\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7158 - mae: 2.8870 - val_loss: 11.4484 - val_mae: 2.3333\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.3708 - mae: 2.2222 - val_loss: 9.0601 - val_mae: 2.2912\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 8.6652 - mae: 2.2286 - val_loss: 8.9350 - val_mae: 2.2284\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5049 - mae: 2.1927 - val_loss: 8.9411 - val_mae: 2.2509\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4546 - mae: 2.1830 - val_loss: 8.8882 - val_mae: 2.2206\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4395 - mae: 2.1715 - val_loss: 8.8808 - val_mae: 2.2157\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4408 - mae: 2.1497 - val_loss: 8.8876 - val_mae: 2.2441\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3980 - mae: 2.1844 - val_loss: 8.8644 - val_mae: 2.2373\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3603 - mae: 2.1498 - val_loss: 8.8554 - val_mae: 2.2272\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3586 - mae: 2.1782 - val_loss: 8.8780 - val_mae: 2.2450\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3660 - mae: 2.1372 - val_loss: 8.8622 - val_mae: 2.2315\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3090 - mae: 2.1585 - val_loss: 8.8679 - val_mae: 2.2408\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.3069 - mae: 2.1334 - val_loss: 8.8681 - val_mae: 2.2327\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2903 - mae: 2.1611 - val_loss: 8.8752 - val_mae: 2.2394\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2752 - mae: 2.1429 - val_loss: 8.8688 - val_mae: 2.2249\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2554 - mae: 2.1421 - val_loss: 8.8696 - val_mae: 2.2259\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2606 - mae: 2.1330 - val_loss: 8.8728 - val_mae: 2.2256\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.2376 - mae: 2.1275 - val_loss: 8.8849 - val_mae: 2.2333\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2367 - mae: 2.1462 - val_loss: 8.8857 - val_mae: 2.2312\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2274 - mae: 2.1332 - val_loss: 8.8982 - val_mae: 2.2471\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2520 - mae: 2.1613 - val_loss: 8.8884 - val_mae: 2.2393\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2398 - mae: 2.1266 - val_loss: 8.8979 - val_mae: 2.2408\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2416 - mae: 2.1504 - val_loss: 8.8912 - val_mae: 2.2321\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2169 - mae: 2.1457 - val_loss: 8.8916 - val_mae: 2.2288\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2201 - mae: 2.1150 - val_loss: 8.9214 - val_mae: 2.2552\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2148 - mae: 2.1500 - val_loss: 8.8924 - val_mae: 2.2365\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2183 - mae: 2.1437 - val_loss: 8.8940 - val_mae: 2.2268\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2318 - mae: 2.1197 - val_loss: 8.9329 - val_mae: 2.2585\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2315 - mae: 2.1218 - val_loss: 8.9008 - val_mae: 2.2339\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2242 - mae: 2.1539 - val_loss: 8.9026 - val_mae: 2.2262\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2091 - mae: 2.1117 - val_loss: 8.9268 - val_mae: 2.2506\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1848 - mae: 2.1274 - val_loss: 8.9806 - val_mae: 2.2738\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1797 - mae: 2.1526 - val_loss: 8.9184 - val_mae: 2.2304\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1874 - mae: 2.1203 - val_loss: 8.9333 - val_mae: 2.2487\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2328 - mae: 2.1543 - val_loss: 8.9420 - val_mae: 2.2330\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1924 - mae: 2.1290 - val_loss: 8.9888 - val_mae: 2.2623\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2185 - mae: 2.1723 - val_loss: 8.9783 - val_mae: 2.2435\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2336 - mae: 2.1464 - val_loss: 8.9773 - val_mae: 2.2496\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.1767 - mae: 2.1189 - val_loss: 9.0189 - val_mae: 2.2755\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1989 - mae: 2.1683 - val_loss: 8.9787 - val_mae: 2.2488\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1953 - mae: 2.1357 - val_loss: 8.9772 - val_mae: 2.2419\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1711 - mae: 2.1097 - val_loss: 9.0306 - val_mae: 2.2749\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4097 - mae: 2.1982 - val_loss: 8.9685 - val_mae: 2.2314\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1329 - mae: 2.1237 - val_loss: 9.0006 - val_mae: 2.2678\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1488 - mae: 2.1220 - val_loss: 8.9761 - val_mae: 2.2296\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1672 - mae: 2.1166 - val_loss: 8.9640 - val_mae: 2.2478\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1407 - mae: 2.1124 - val_loss: 8.9890 - val_mae: 2.2669\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1726 - mae: 2.1504 - val_loss: 8.9769 - val_mae: 2.2461\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1578 - mae: 2.1268 - val_loss: 8.9755 - val_mae: 2.2362\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.6781 - mae: 2.0472\n",
      "Mean Absolute Error on Test Data: 2.04715895652771\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.06217786024534\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 16ms/step - loss: 10.7111 - mae: 2.4885 - val_loss: 8.2740 - val_mae: 2.1288\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.4256 - mae: 1.8118 - val_loss: 5.1602 - val_mae: 1.5260\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0023 - mae: 1.4963 - val_loss: 3.8971 - val_mae: 1.4946\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5789 - mae: 1.5744 - val_loss: 3.9137 - val_mae: 1.5234\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5598 - mae: 1.5657 - val_loss: 3.9119 - val_mae: 1.4989\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5400 - mae: 1.5331 - val_loss: 3.9263 - val_mae: 1.4880\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5327 - mae: 1.5565 - val_loss: 3.9332 - val_mae: 1.5181\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.5031 - mae: 1.5451 - val_loss: 3.9419 - val_mae: 1.4914\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4931 - mae: 1.5221 - val_loss: 3.9542 - val_mae: 1.4966\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4872 - mae: 1.5269 - val_loss: 3.9601 - val_mae: 1.5099\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4687 - mae: 1.5457 - val_loss: 3.9689 - val_mae: 1.5241\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4548 - mae: 1.5292 - val_loss: 3.9758 - val_mae: 1.4984\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4812 - mae: 1.5571 - val_loss: 3.9845 - val_mae: 1.5045\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4371 - mae: 1.5130 - val_loss: 3.9910 - val_mae: 1.5015\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4197 - mae: 1.5219 - val_loss: 3.9983 - val_mae: 1.5167\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4283 - mae: 1.5172 - val_loss: 4.0138 - val_mae: 1.5115\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4148 - mae: 1.5463 - val_loss: 4.0233 - val_mae: 1.5465\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.4081 - mae: 1.5191 - val_loss: 4.0253 - val_mae: 1.5086\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3879 - mae: 1.5173 - val_loss: 4.0401 - val_mae: 1.5256\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3758 - mae: 1.5306 - val_loss: 4.0519 - val_mae: 1.5145\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4.3703 - mae: 1.5055 - val_loss: 4.0482 - val_mae: 1.5193\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3820 - mae: 1.5376 - val_loss: 4.0555 - val_mae: 1.5225\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3490 - mae: 1.5068 - val_loss: 4.0976 - val_mae: 1.4878\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3782 - mae: 1.5330 - val_loss: 4.0733 - val_mae: 1.5214\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3381 - mae: 1.4954 - val_loss: 4.0840 - val_mae: 1.5174\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3436 - mae: 1.5339 - val_loss: 4.0872 - val_mae: 1.5151\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3125 - mae: 1.4996 - val_loss: 4.1155 - val_mae: 1.5175\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3041 - mae: 1.4974 - val_loss: 4.1157 - val_mae: 1.5371\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2998 - mae: 1.5291 - val_loss: 4.1112 - val_mae: 1.5382\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2967 - mae: 1.5049 - val_loss: 4.1469 - val_mae: 1.5287\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2884 - mae: 1.4922 - val_loss: 4.1326 - val_mae: 1.5274\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2838 - mae: 1.5228 - val_loss: 4.1520 - val_mae: 1.5365\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2821 - mae: 1.4910 - val_loss: 4.1589 - val_mae: 1.5420\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2546 - mae: 1.5054 - val_loss: 4.1754 - val_mae: 1.5395\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2458 - mae: 1.5025 - val_loss: 4.1880 - val_mae: 1.5314\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2391 - mae: 1.4940 - val_loss: 4.1892 - val_mae: 1.5351\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2384 - mae: 1.5055 - val_loss: 4.1937 - val_mae: 1.5326\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2332 - mae: 1.4932 - val_loss: 4.2094 - val_mae: 1.5421\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2366 - mae: 1.5175 - val_loss: 4.2356 - val_mae: 1.5332\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4.2260 - mae: 1.4582 - val_loss: 4.2427 - val_mae: 1.5338\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2156 - mae: 1.4841 - val_loss: 4.2292 - val_mae: 1.5495\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2096 - mae: 1.5065 - val_loss: 4.2489 - val_mae: 1.5423\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1991 - mae: 1.4806 - val_loss: 4.2686 - val_mae: 1.5349\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2087 - mae: 1.5098 - val_loss: 4.2494 - val_mae: 1.5525\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2111 - mae: 1.4579 - val_loss: 4.2679 - val_mae: 1.5424\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1900 - mae: 1.4915 - val_loss: 4.2898 - val_mae: 1.5454\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1691 - mae: 1.4900 - val_loss: 4.2895 - val_mae: 1.5636\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1686 - mae: 1.4853 - val_loss: 4.2903 - val_mae: 1.5404\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1733 - mae: 1.4947 - val_loss: 4.3404 - val_mae: 1.5409\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.1637 - mae: 1.4564 - val_loss: 4.3382 - val_mae: 1.5402\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.4245 - mae: 1.3546\n",
      "Mean Absolute Error on Test Data: 1.3546416759490967\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.015412378653902215\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 15ms/step - loss: 9.0188 - mae: 2.2489 - val_loss: 7.9775 - val_mae: 2.0865\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 6.3538 - mae: 1.6664 - val_loss: 5.2789 - val_mae: 1.4969\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 4.3648 - mae: 1.4158 - val_loss: 3.8143 - val_mae: 1.2964\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.9035 - mae: 1.4781 - val_loss: 3.7179 - val_mae: 1.3384\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.8578 - mae: 1.4695 - val_loss: 3.7470 - val_mae: 1.2934\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.8017 - mae: 1.4303 - val_loss: 3.7176 - val_mae: 1.2946\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.7684 - mae: 1.4455 - val_loss: 3.6903 - val_mae: 1.2964\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.7395 - mae: 1.4418 - val_loss: 3.6887 - val_mae: 1.2850\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.7138 - mae: 1.4228 - val_loss: 3.6917 - val_mae: 1.2753\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.7028 - mae: 1.4097 - val_loss: 3.6798 - val_mae: 1.2732\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.6783 - mae: 1.4283 - val_loss: 3.6435 - val_mae: 1.2886\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.6689 - mae: 1.4212 - val_loss: 3.6761 - val_mae: 1.2650\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.6370 - mae: 1.4225 - val_loss: 3.6333 - val_mae: 1.2853\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.6349 - mae: 1.4174 - val_loss: 3.6631 - val_mae: 1.2622\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.6282 - mae: 1.4354 - val_loss: 3.6241 - val_mae: 1.2836\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5991 - mae: 1.4007 - val_loss: 3.6956 - val_mae: 1.2471\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5912 - mae: 1.4007 - val_loss: 3.6286 - val_mae: 1.2768\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 3.5771 - mae: 1.4232 - val_loss: 3.6503 - val_mae: 1.2611\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5691 - mae: 1.3940 - val_loss: 3.6750 - val_mae: 1.2501\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5663 - mae: 1.4071 - val_loss: 3.6347 - val_mae: 1.2700\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5494 - mae: 1.4103 - val_loss: 3.6571 - val_mae: 1.2558\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5408 - mae: 1.3965 - val_loss: 3.6587 - val_mae: 1.2541\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5381 - mae: 1.4002 - val_loss: 3.6601 - val_mae: 1.2540\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.5488 - mae: 1.4047 - val_loss: 3.6823 - val_mae: 1.2464\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5327 - mae: 1.4026 - val_loss: 3.6547 - val_mae: 1.2570\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5245 - mae: 1.3934 - val_loss: 3.6845 - val_mae: 1.2441\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5079 - mae: 1.3893 - val_loss: 3.6511 - val_mae: 1.2573\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5234 - mae: 1.4178 - val_loss: 3.6549 - val_mae: 1.2544\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5000 - mae: 1.3861 - val_loss: 3.6683 - val_mae: 1.2490\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4975 - mae: 1.3900 - val_loss: 3.6595 - val_mae: 1.2543\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4996 - mae: 1.4036 - val_loss: 3.6638 - val_mae: 1.2528\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4846 - mae: 1.3907 - val_loss: 3.6732 - val_mae: 1.2477\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4901 - mae: 1.3815 - val_loss: 3.6586 - val_mae: 1.2530\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4806 - mae: 1.4021 - val_loss: 3.6523 - val_mae: 1.2571\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4717 - mae: 1.3873 - val_loss: 3.6768 - val_mae: 1.2461\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4667 - mae: 1.3767 - val_loss: 3.6713 - val_mae: 1.2495\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4647 - mae: 1.3969 - val_loss: 3.6764 - val_mae: 1.2488\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4781 - mae: 1.3618 - val_loss: 3.6807 - val_mae: 1.2464\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 3.5347 - mae: 1.4409 - val_loss: 3.6794 - val_mae: 1.2475\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.5138 - mae: 1.3528 - val_loss: 3.7042 - val_mae: 1.2393\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4342 - mae: 1.3942 - val_loss: 3.6390 - val_mae: 1.2780\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4259 - mae: 1.3906 - val_loss: 3.6964 - val_mae: 1.2414\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4317 - mae: 1.3684 - val_loss: 3.6767 - val_mae: 1.2483\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4214 - mae: 1.3697 - val_loss: 3.6770 - val_mae: 1.2465\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4247 - mae: 1.3762 - val_loss: 3.6750 - val_mae: 1.2505\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4115 - mae: 1.3819 - val_loss: 3.6632 - val_mae: 1.2601\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4208 - mae: 1.3701 - val_loss: 3.6601 - val_mae: 1.2605\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.4085 - mae: 1.3796 - val_loss: 3.6962 - val_mae: 1.2423\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.3963 - mae: 1.3767 - val_loss: 3.6460 - val_mae: 1.2744\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.3901 - mae: 1.3703 - val_loss: 3.6889 - val_mae: 1.2444\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.4285 - mae: 1.3805\n",
      "Mean Absolute Error on Test Data: 1.3805080652236938\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.045014640107240544\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 47.6152 - mae: 5.1409 - val_loss: 35.3343 - val_mae: 4.3505\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.1298 - mae: 3.9584 - val_loss: 21.8779 - val_mae: 3.0856\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4455 - mae: 3.2809 - val_loss: 16.8296 - val_mae: 2.9974\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3035 - mae: 3.2725 - val_loss: 16.6251 - val_mae: 3.0038\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1466 - mae: 3.2573 - val_loss: 16.3810 - val_mae: 2.9420\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0118 - mae: 3.1784 - val_loss: 16.1761 - val_mae: 2.9030\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9096 - mae: 3.1829 - val_loss: 16.0117 - val_mae: 2.9119\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8225 - mae: 3.2202 - val_loss: 15.8509 - val_mae: 2.9223\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7584 - mae: 3.1479 - val_loss: 15.6635 - val_mae: 2.8624\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6478 - mae: 3.2128 - val_loss: 15.5155 - val_mae: 2.8693\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4937 - mae: 3.1345 - val_loss: 15.3346 - val_mae: 2.8125\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3754 - mae: 3.1508 - val_loss: 15.2310 - val_mae: 2.8802\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3117 - mae: 3.1545 - val_loss: 15.0150 - val_mae: 2.8122\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2152 - mae: 3.1335 - val_loss: 14.8632 - val_mae: 2.7861\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1915 - mae: 3.0929 - val_loss: 14.7830 - val_mae: 2.8066\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0520 - mae: 3.1341 - val_loss: 14.6432 - val_mae: 2.7662\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.9874 - mae: 3.0809 - val_loss: 14.4864 - val_mae: 2.7924\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8865 - mae: 3.0945 - val_loss: 14.3491 - val_mae: 2.7517\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7788 - mae: 3.0737 - val_loss: 14.2820 - val_mae: 2.7724\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7372 - mae: 3.1131 - val_loss: 14.1970 - val_mae: 2.7277\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6825 - mae: 3.0518 - val_loss: 14.0975 - val_mae: 2.7516\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6043 - mae: 3.0476 - val_loss: 14.0598 - val_mae: 2.7355\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5387 - mae: 3.0617 - val_loss: 13.9794 - val_mae: 2.7233\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4913 - mae: 3.0622 - val_loss: 13.9203 - val_mae: 2.7147\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4378 - mae: 3.0579 - val_loss: 13.8491 - val_mae: 2.6927\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3784 - mae: 3.0338 - val_loss: 13.7650 - val_mae: 2.6898\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3161 - mae: 3.0265 - val_loss: 13.7280 - val_mae: 2.7151\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3139 - mae: 3.0628 - val_loss: 13.6102 - val_mae: 2.7014\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2306 - mae: 3.0464 - val_loss: 13.7042 - val_mae: 2.7532\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1795 - mae: 3.0234 - val_loss: 13.6333 - val_mae: 2.6930\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1076 - mae: 3.0216 - val_loss: 13.5630 - val_mae: 2.6877\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0587 - mae: 3.0185 - val_loss: 13.5432 - val_mae: 2.6831\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.0055 - mae: 2.9982 - val_loss: 13.5322 - val_mae: 2.6961\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9642 - mae: 2.9880 - val_loss: 13.4940 - val_mae: 2.7060\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0311 - mae: 3.0378 - val_loss: 13.4944 - val_mae: 2.6699\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8907 - mae: 3.0077 - val_loss: 13.4360 - val_mae: 2.6745\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8406 - mae: 2.9722 - val_loss: 13.4632 - val_mae: 2.7225\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8118 - mae: 3.0234 - val_loss: 13.4186 - val_mae: 2.6830\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7964 - mae: 3.0412 - val_loss: 13.5406 - val_mae: 2.6560\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7354 - mae: 2.9708 - val_loss: 13.4641 - val_mae: 2.7136\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6366 - mae: 2.9855 - val_loss: 13.3670 - val_mae: 2.6958\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7181 - mae: 2.9896 - val_loss: 13.3982 - val_mae: 2.7457\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6723 - mae: 3.0010 - val_loss: 13.4489 - val_mae: 2.6810\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.5814 - mae: 3.0120 - val_loss: 13.2951 - val_mae: 2.6733\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6751 - mae: 2.9466 - val_loss: 13.3048 - val_mae: 2.7001\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.5141 - mae: 2.9801 - val_loss: 13.4011 - val_mae: 2.6887\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4724 - mae: 2.9402 - val_loss: 13.3015 - val_mae: 2.6926\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4876 - mae: 3.0173 - val_loss: 13.2901 - val_mae: 2.6946\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.3954 - mae: 2.9508 - val_loss: 13.3024 - val_mae: 2.6619\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3895 - mae: 2.9548 - val_loss: 13.3162 - val_mae: 2.6705\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 13.6244 - mae: 2.7910\n",
      "Mean Absolute Error on Test Data: 2.790997266769409\n",
      "7/7 [==============================] - 0s 920us/step\n",
      "R-squared: 0.26860576848121964\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 42.2051 - mae: 4.6262 - val_loss: 52.8440 - val_mae: 4.4455\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.0032 - mae: 3.7022 - val_loss: 40.7661 - val_mae: 3.4218\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.8557 - mae: 2.9621 - val_loss: 32.9393 - val_mae: 3.4397\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0137 - mae: 3.2172 - val_loss: 32.1909 - val_mae: 3.5135\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1616 - mae: 2.9972 - val_loss: 31.8381 - val_mae: 3.3544\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8935 - mae: 2.9583 - val_loss: 31.0941 - val_mae: 3.3814\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5994 - mae: 3.0036 - val_loss: 30.4942 - val_mae: 3.3421\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2538 - mae: 2.9173 - val_loss: 29.8709 - val_mae: 3.3093\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8998 - mae: 2.9775 - val_loss: 28.9537 - val_mae: 3.3367\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6180 - mae: 2.8982 - val_loss: 28.3721 - val_mae: 3.2715\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2921 - mae: 2.8953 - val_loss: 27.9075 - val_mae: 3.2005\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.0830 - mae: 2.9031 - val_loss: 27.1977 - val_mae: 3.2310\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.7145 - mae: 2.8537 - val_loss: 26.7900 - val_mae: 3.1390\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.6033 - mae: 2.8444 - val_loss: 26.0280 - val_mae: 3.1974\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.2661 - mae: 2.8272 - val_loss: 25.5717 - val_mae: 3.1317\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0957 - mae: 2.8330 - val_loss: 25.0733 - val_mae: 3.1010\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9101 - mae: 2.8495 - val_loss: 24.5279 - val_mae: 3.0721\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6807 - mae: 2.7735 - val_loss: 24.1256 - val_mae: 3.0619\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5694 - mae: 2.7800 - val_loss: 23.6462 - val_mae: 3.0817\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4190 - mae: 2.7862 - val_loss: 23.4538 - val_mae: 3.0296\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3578 - mae: 2.7570 - val_loss: 23.1191 - val_mae: 3.0779\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3095 - mae: 2.8375 - val_loss: 22.7821 - val_mae: 3.0116\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1079 - mae: 2.7605 - val_loss: 22.4866 - val_mae: 3.0097\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0389 - mae: 2.7478 - val_loss: 22.2067 - val_mae: 2.9929\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2199 - mae: 2.7010 - val_loss: 22.2022 - val_mae: 3.0345\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9658 - mae: 2.7860 - val_loss: 21.9749 - val_mae: 2.9785\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8635 - mae: 2.7206 - val_loss: 21.8241 - val_mae: 2.9900\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9221 - mae: 2.7972 - val_loss: 21.5580 - val_mae: 2.9685\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7510 - mae: 2.7362 - val_loss: 21.4434 - val_mae: 2.9371\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7350 - mae: 2.7002 - val_loss: 21.3163 - val_mae: 2.9243\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7507 - mae: 2.7495 - val_loss: 21.1532 - val_mae: 2.9681\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6442 - mae: 2.7561 - val_loss: 20.7580 - val_mae: 2.9554\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5745 - mae: 2.6918 - val_loss: 20.7286 - val_mae: 2.9445\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6108 - mae: 2.7500 - val_loss: 20.5091 - val_mae: 2.9324\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5030 - mae: 2.6843 - val_loss: 20.4470 - val_mae: 2.9053\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4726 - mae: 2.7541 - val_loss: 20.3368 - val_mae: 2.8881\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4210 - mae: 2.6724 - val_loss: 20.2567 - val_mae: 2.9282\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4403 - mae: 2.7146 - val_loss: 20.1114 - val_mae: 2.8849\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3442 - mae: 2.6993 - val_loss: 20.0170 - val_mae: 2.8769\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4341 - mae: 2.6907 - val_loss: 20.0816 - val_mae: 2.8583\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3265 - mae: 2.6795 - val_loss: 19.8851 - val_mae: 2.8795\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4398 - mae: 2.6877 - val_loss: 19.9059 - val_mae: 2.8626\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3628 - mae: 2.7406 - val_loss: 19.6870 - val_mae: 2.8506\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2324 - mae: 2.6500 - val_loss: 19.6847 - val_mae: 2.8756\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3626 - mae: 2.7443 - val_loss: 19.5863 - val_mae: 2.8102\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3639 - mae: 2.6225 - val_loss: 19.6058 - val_mae: 2.8816\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2044 - mae: 2.7046 - val_loss: 19.6745 - val_mae: 2.8105\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1390 - mae: 2.6309 - val_loss: 19.5165 - val_mae: 2.8726\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1488 - mae: 2.7145 - val_loss: 19.3757 - val_mae: 2.8326\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0930 - mae: 2.6491 - val_loss: 19.4624 - val_mae: 2.8175\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 15.5352 - mae: 2.6517\n",
      "Mean Absolute Error on Test Data: 2.6517021656036377\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.22809877930803546\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 40.8168 - mae: 4.9855 - val_loss: 53.1988 - val_mae: 5.3386\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0327 - mae: 4.0926 - val_loss: 40.1820 - val_mae: 4.1881\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8626 - mae: 3.0532 - val_loss: 26.5696 - val_mae: 3.1747\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8264 - mae: 2.8260 - val_loss: 24.1346 - val_mae: 3.2732\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.6926 - mae: 2.8290 - val_loss: 24.5987 - val_mae: 3.1892\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5237 - mae: 2.8079 - val_loss: 24.1553 - val_mae: 3.2170\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4637 - mae: 2.7953 - val_loss: 24.0884 - val_mae: 3.2023\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4096 - mae: 2.8179 - val_loss: 23.8926 - val_mae: 3.2053\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3472 - mae: 2.7774 - val_loss: 24.0352 - val_mae: 3.1651\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3178 - mae: 2.7464 - val_loss: 23.6936 - val_mae: 3.1816\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3134 - mae: 2.8242 - val_loss: 23.6422 - val_mae: 3.1646\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2318 - mae: 2.7356 - val_loss: 23.6808 - val_mae: 3.1404\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1741 - mae: 2.7599 - val_loss: 23.3632 - val_mae: 3.1489\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1565 - mae: 2.7948 - val_loss: 23.2826 - val_mae: 3.1410\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.1218 - mae: 2.7563 - val_loss: 23.1816 - val_mae: 3.1333\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0676 - mae: 2.7288 - val_loss: 23.3421 - val_mae: 3.0992\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9971 - mae: 2.7299 - val_loss: 22.8827 - val_mae: 3.1240\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9971 - mae: 2.7297 - val_loss: 22.7711 - val_mae: 3.1203\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9161 - mae: 2.7471 - val_loss: 22.7677 - val_mae: 3.1130\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8663 - mae: 2.7421 - val_loss: 22.7556 - val_mae: 3.0908\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9186 - mae: 2.7077 - val_loss: 22.3829 - val_mae: 3.1248\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8612 - mae: 2.7373 - val_loss: 22.4845 - val_mae: 3.0862\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7415 - mae: 2.7471 - val_loss: 22.2546 - val_mae: 3.0867\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7615 - mae: 2.7629 - val_loss: 22.6940 - val_mae: 3.0425\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7993 - mae: 2.6388 - val_loss: 22.2970 - val_mae: 3.0517\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6731 - mae: 2.7706 - val_loss: 21.9783 - val_mae: 3.0685\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5823 - mae: 2.7077 - val_loss: 22.1633 - val_mae: 3.0349\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5767 - mae: 2.7202 - val_loss: 21.8791 - val_mae: 3.0436\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5386 - mae: 2.6779 - val_loss: 21.9115 - val_mae: 3.0313\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4953 - mae: 2.7054 - val_loss: 21.8218 - val_mae: 3.0250\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4496 - mae: 2.7423 - val_loss: 21.7299 - val_mae: 3.0200\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4643 - mae: 2.6690 - val_loss: 21.6800 - val_mae: 3.0094\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3551 - mae: 2.6889 - val_loss: 21.7858 - val_mae: 2.9967\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3183 - mae: 2.6734 - val_loss: 21.4223 - val_mae: 3.0112\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.2904 - mae: 2.7007 - val_loss: 21.3435 - val_mae: 3.0078\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2088 - mae: 2.7048 - val_loss: 21.5923 - val_mae: 2.9838\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2390 - mae: 2.6384 - val_loss: 21.1829 - val_mae: 3.0076\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2119 - mae: 2.6984 - val_loss: 21.2437 - val_mae: 2.9864\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1272 - mae: 2.6896 - val_loss: 21.2788 - val_mae: 2.9730\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2035 - mae: 2.6700 - val_loss: 21.1120 - val_mae: 2.9684\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0351 - mae: 2.6837 - val_loss: 20.9991 - val_mae: 2.9782\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0041 - mae: 2.6377 - val_loss: 20.9715 - val_mae: 2.9744\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9687 - mae: 2.6645 - val_loss: 20.7048 - val_mae: 2.9859\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9213 - mae: 2.6629 - val_loss: 20.9806 - val_mae: 2.9512\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 14.0320 - mae: 2.6140 - val_loss: 20.5160 - val_mae: 2.9980\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9076 - mae: 2.6919 - val_loss: 21.0000 - val_mae: 2.9473\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8440 - mae: 2.6206 - val_loss: 20.6820 - val_mae: 2.9630\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8096 - mae: 2.6661 - val_loss: 20.9033 - val_mae: 2.9384\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7753 - mae: 2.6537 - val_loss: 20.5195 - val_mae: 2.9578\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7666 - mae: 2.6414 - val_loss: 20.3006 - val_mae: 2.9571\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 11.2068 - mae: 2.5828\n",
      "Mean Absolute Error on Test Data: 2.5827746391296387\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.18707325713205292\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 70.0707 - mae: 6.8440 - val_loss: 60.8667 - val_mae: 6.2968\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 58.7874 - mae: 5.9807 - val_loss: 46.5986 - val_mae: 5.1030\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.2003 - mae: 4.5061 - val_loss: 27.6409 - val_mae: 3.4309\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.6359 - mae: 3.3033 - val_loss: 20.5352 - val_mae: 3.2196\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.9433 - mae: 3.4239 - val_loss: 20.6334 - val_mae: 3.2962\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.6872 - mae: 3.3288 - val_loss: 20.4055 - val_mae: 3.2182\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4979 - mae: 3.3022 - val_loss: 20.3533 - val_mae: 3.2190\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3722 - mae: 3.3207 - val_loss: 20.3285 - val_mae: 3.2389\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2463 - mae: 3.2935 - val_loss: 20.2244 - val_mae: 3.2070\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0859 - mae: 3.2878 - val_loss: 20.1777 - val_mae: 3.2061\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.9928 - mae: 3.3128 - val_loss: 20.1401 - val_mae: 3.2074\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8998 - mae: 3.2630 - val_loss: 20.1164 - val_mae: 3.2102\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7514 - mae: 3.2804 - val_loss: 20.0400 - val_mae: 3.1792\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6884 - mae: 3.2411 - val_loss: 20.1567 - val_mae: 3.2519\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5517 - mae: 3.2932 - val_loss: 20.0440 - val_mae: 3.2173\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4163 - mae: 3.2625 - val_loss: 19.9493 - val_mae: 3.1784\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3458 - mae: 3.2293 - val_loss: 19.9341 - val_mae: 3.1846\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3309 - mae: 3.2363 - val_loss: 19.9376 - val_mae: 3.2021\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1895 - mae: 3.2612 - val_loss: 19.9401 - val_mae: 3.2147\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1691 - mae: 3.2395 - val_loss: 19.8576 - val_mae: 3.1829\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1324 - mae: 3.2603 - val_loss: 19.8595 - val_mae: 3.1903\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0435 - mae: 3.2350 - val_loss: 19.8248 - val_mae: 3.1737\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9677 - mae: 3.2301 - val_loss: 19.8290 - val_mae: 3.1872\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8311 - mae: 3.2260 - val_loss: 19.8474 - val_mae: 3.2142\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8760 - mae: 3.2747 - val_loss: 19.8071 - val_mae: 3.1666\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8111 - mae: 3.2011 - val_loss: 19.7800 - val_mae: 3.1895\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7906 - mae: 3.2462 - val_loss: 19.7854 - val_mae: 3.1696\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7078 - mae: 3.1833 - val_loss: 19.7865 - val_mae: 3.1665\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6118 - mae: 3.2106 - val_loss: 19.8338 - val_mae: 3.2252\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5724 - mae: 3.1891 - val_loss: 19.7622 - val_mae: 3.1996\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6365 - mae: 3.1946 - val_loss: 19.7698 - val_mae: 3.2040\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5807 - mae: 3.2743 - val_loss: 19.7685 - val_mae: 3.1772\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4472 - mae: 3.1520 - val_loss: 19.7825 - val_mae: 3.2276\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4645 - mae: 3.2669 - val_loss: 19.7285 - val_mae: 3.1938\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8043 - mae: 3.1192 - val_loss: 19.8268 - val_mae: 3.2595\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5586 - mae: 3.2365 - val_loss: 19.7605 - val_mae: 3.2323\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3678 - mae: 3.1936 - val_loss: 19.7886 - val_mae: 3.2418\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2957 - mae: 3.1819 - val_loss: 19.7555 - val_mae: 3.2277\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2059 - mae: 3.2017 - val_loss: 19.7168 - val_mae: 3.2022\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1324 - mae: 3.1670 - val_loss: 19.7547 - val_mae: 3.1737\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1210 - mae: 3.1733 - val_loss: 19.7180 - val_mae: 3.1955\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1152 - mae: 3.1229 - val_loss: 19.7607 - val_mae: 3.2403\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1321 - mae: 3.2058 - val_loss: 19.7516 - val_mae: 3.1817\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9916 - mae: 3.1424 - val_loss: 19.6962 - val_mae: 3.2034\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0253 - mae: 3.1312 - val_loss: 19.8019 - val_mae: 3.2536\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9926 - mae: 3.2204 - val_loss: 19.8047 - val_mae: 3.1813\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0986 - mae: 3.1295 - val_loss: 19.8831 - val_mae: 3.2689\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 18.8770 - mae: 3.1559 - val_loss: 19.7644 - val_mae: 3.2063\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8545 - mae: 3.1447 - val_loss: 19.7233 - val_mae: 3.1818\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8411 - mae: 3.1342 - val_loss: 19.7927 - val_mae: 3.2299\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 22.0695 - mae: 3.4596\n",
      "Mean Absolute Error on Test Data: 3.4595744609832764\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1529287361551549\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 14ms/step - loss: 6.8044 - mae: 1.9300 - val_loss: 4.0019 - val_mae: 1.4209\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.2209 - mae: 1.3721 - val_loss: 2.3488 - val_mae: 1.0847\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.1852 - mae: 1.3307 - val_loss: 2.4296 - val_mae: 1.2368\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.1290 - mae: 1.3534 - val_loss: 2.2961 - val_mae: 1.1538\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.1211 - mae: 1.2763 - val_loss: 2.2629 - val_mae: 1.1322\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0852 - mae: 1.3047 - val_loss: 2.2666 - val_mae: 1.1477\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0686 - mae: 1.2957 - val_loss: 2.2510 - val_mae: 1.1435\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0580 - mae: 1.2753 - val_loss: 2.2226 - val_mae: 1.1227\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0524 - mae: 1.3034 - val_loss: 2.2325 - val_mae: 1.1369\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0318 - mae: 1.2920 - val_loss: 2.2159 - val_mae: 1.1270\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0241 - mae: 1.2816 - val_loss: 2.2093 - val_mae: 1.1250\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0167 - mae: 1.2813 - val_loss: 2.1979 - val_mae: 1.1174\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0066 - mae: 1.2647 - val_loss: 2.1904 - val_mae: 1.1140\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.0033 - mae: 1.2912 - val_loss: 2.2038 - val_mae: 1.1324\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2.9864 - mae: 1.2680 - val_loss: 2.1801 - val_mae: 1.1099\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9804 - mae: 1.2764 - val_loss: 2.1833 - val_mae: 1.1182\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9786 - mae: 1.2652 - val_loss: 2.1989 - val_mae: 1.1350\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9687 - mae: 1.2799 - val_loss: 2.1826 - val_mae: 1.1246\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9584 - mae: 1.2807 - val_loss: 2.1680 - val_mae: 1.1133\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9583 - mae: 1.2738 - val_loss: 2.1625 - val_mae: 1.1106\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9513 - mae: 1.2608 - val_loss: 2.1641 - val_mae: 1.1107\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2.9347 - mae: 1.2726 - val_loss: 2.1722 - val_mae: 1.1230\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9429 - mae: 1.2568 - val_loss: 2.1674 - val_mae: 1.1215\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9404 - mae: 1.2864 - val_loss: 2.1479 - val_mae: 1.1048\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9278 - mae: 1.2634 - val_loss: 2.1782 - val_mae: 1.1281\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.9337 - mae: 1.2542 - val_loss: 2.1513 - val_mae: 1.1092\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 2.9207 - mae: 1.2861 - val_loss: 2.1866 - val_mae: 1.1354\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2.9038 - mae: 1.2531 - val_loss: 2.1427 - val_mae: 1.1045\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2.8986 - mae: 1.2674 - val_loss: 2.1691 - val_mae: 1.1267\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 2.9012 - mae: 1.2572 - val_loss: 2.1436 - val_mae: 1.1080\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8925 - mae: 1.2700 - val_loss: 2.1403 - val_mae: 1.1060\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8876 - mae: 1.2484 - val_loss: 2.1723 - val_mae: 1.1290\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8789 - mae: 1.2695 - val_loss: 2.1300 - val_mae: 1.0993\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8967 - mae: 1.2438 - val_loss: 2.2105 - val_mae: 1.1536\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8888 - mae: 1.2695 - val_loss: 2.1526 - val_mae: 1.1202\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8730 - mae: 1.2524 - val_loss: 2.1413 - val_mae: 1.1109\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8683 - mae: 1.2654 - val_loss: 2.1583 - val_mae: 1.1266\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8639 - mae: 1.2527 - val_loss: 2.1570 - val_mae: 1.1223\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8614 - mae: 1.2405 - val_loss: 2.1527 - val_mae: 1.1224\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8504 - mae: 1.2698 - val_loss: 2.1410 - val_mae: 1.1135\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8609 - mae: 1.2500 - val_loss: 2.1661 - val_mae: 1.1301\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8381 - mae: 1.2564 - val_loss: 2.1221 - val_mae: 1.0969\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 2.8338 - mae: 1.2315 - val_loss: 2.1469 - val_mae: 1.1204\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8452 - mae: 1.2762 - val_loss: 2.1257 - val_mae: 1.1067\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8300 - mae: 1.2272 - val_loss: 2.1335 - val_mae: 1.1088\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8418 - mae: 1.2696 - val_loss: 2.1150 - val_mae: 1.0929\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8346 - mae: 1.2280 - val_loss: 2.1724 - val_mae: 1.1347\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8255 - mae: 1.2617 - val_loss: 2.1151 - val_mae: 1.0945\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8133 - mae: 1.2328 - val_loss: 2.1442 - val_mae: 1.1215\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 2.8016 - mae: 1.2483 - val_loss: 2.1385 - val_mae: 1.1167\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.9943 - mae: 1.1350\n",
      "Mean Absolute Error on Test Data: 1.134957194328308\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1159850549057454\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 62.8712 - mae: 5.4108 - val_loss: 67.6547 - val_mae: 4.9195\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 52.5531 - mae: 4.5422 - val_loss: 56.4116 - val_mae: 4.0119\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.9774 - mae: 3.6844 - val_loss: 45.4313 - val_mae: 3.6414\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.3110 - mae: 3.6443 - val_loss: 43.3046 - val_mae: 4.0757\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.7264 - mae: 3.8980 - val_loss: 42.9849 - val_mae: 4.0670\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3882 - mae: 3.7519 - val_loss: 42.6563 - val_mae: 3.9589\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0265 - mae: 3.7293 - val_loss: 42.3317 - val_mae: 4.0254\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8219 - mae: 3.8297 - val_loss: 41.9880 - val_mae: 4.0082\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5184 - mae: 3.6590 - val_loss: 41.6501 - val_mae: 3.9191\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1966 - mae: 3.7277 - val_loss: 41.4002 - val_mae: 4.0081\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7771 - mae: 3.6150 - val_loss: 41.1136 - val_mae: 3.8200\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.5147 - mae: 3.6686 - val_loss: 40.7410 - val_mae: 3.9176\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.1322 - mae: 3.5891 - val_loss: 40.4100 - val_mae: 3.8763\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8206 - mae: 3.6016 - val_loss: 40.1156 - val_mae: 3.8508\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.3721 - mae: 3.6366 - val_loss: 39.9075 - val_mae: 3.9441\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.2036 - mae: 3.6003 - val_loss: 39.5550 - val_mae: 3.8249\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.8148 - mae: 3.4962 - val_loss: 39.3193 - val_mae: 3.8011\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.5883 - mae: 3.4534 - val_loss: 39.1387 - val_mae: 3.8299\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2578 - mae: 3.5138 - val_loss: 38.9668 - val_mae: 3.8484\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.0623 - mae: 3.4741 - val_loss: 38.7731 - val_mae: 3.7386\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.7623 - mae: 3.4072 - val_loss: 38.7183 - val_mae: 3.8436\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.6727 - mae: 3.5174 - val_loss: 38.4988 - val_mae: 3.6873\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.4074 - mae: 3.3400 - val_loss: 38.4486 - val_mae: 3.8075\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2198 - mae: 3.4501 - val_loss: 38.3060 - val_mae: 3.6481\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0533 - mae: 3.3653 - val_loss: 38.2816 - val_mae: 3.7184\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.9404 - mae: 3.3960 - val_loss: 38.2225 - val_mae: 3.6201\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0859 - mae: 3.3963 - val_loss: 38.1555 - val_mae: 3.6411\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.7534 - mae: 3.2923 - val_loss: 38.2066 - val_mae: 3.7636\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.6021 - mae: 3.3857 - val_loss: 38.0959 - val_mae: 3.6584\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.5179 - mae: 3.3344 - val_loss: 38.1506 - val_mae: 3.6826\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.5085 - mae: 3.2943 - val_loss: 38.2784 - val_mae: 3.7923\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.5102 - mae: 3.4826 - val_loss: 38.0678 - val_mae: 3.6177\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.3742 - mae: 3.2639 - val_loss: 38.1373 - val_mae: 3.7323\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.3212 - mae: 3.4060 - val_loss: 38.1823 - val_mae: 3.7511\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.2824 - mae: 3.3323 - val_loss: 38.0350 - val_mae: 3.6153\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.2306 - mae: 3.2666 - val_loss: 38.0513 - val_mae: 3.6877\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 26.4498 - mae: 3.3862 - val_loss: 38.0248 - val_mae: 3.6096\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.1040 - mae: 3.3430 - val_loss: 38.0282 - val_mae: 3.6397\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.0806 - mae: 3.2507 - val_loss: 38.0476 - val_mae: 3.7106\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.1340 - mae: 3.4341 - val_loss: 38.0478 - val_mae: 3.6182\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.9829 - mae: 3.2658 - val_loss: 38.1418 - val_mae: 3.7128\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.0159 - mae: 3.2832 - val_loss: 38.2416 - val_mae: 3.7510\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.1041 - mae: 3.4592 - val_loss: 38.0750 - val_mae: 3.5607\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.3334 - mae: 3.2489 - val_loss: 38.1965 - val_mae: 3.7683\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.9341 - mae: 3.3701 - val_loss: 37.9674 - val_mae: 3.5304\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.8541 - mae: 3.2599 - val_loss: 38.0062 - val_mae: 3.6916\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.8311 - mae: 3.3152 - val_loss: 37.8737 - val_mae: 3.5402\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.8096 - mae: 3.3055 - val_loss: 37.8074 - val_mae: 3.6230\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.9267 - mae: 3.2062 - val_loss: 38.1160 - val_mae: 3.7669\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 25.7832 - mae: 3.3759 - val_loss: 37.8299 - val_mae: 3.6205\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.6345 - mae: 3.0513\n",
      "Mean Absolute Error on Test Data: 3.051297426223755\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.16179782458339598\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 2s 79ms/step - loss: 151.9964 - mae: 9.1844 - val_loss: 105.2356 - val_mae: 8.1344\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 134.6904 - mae: 8.2010 - val_loss: 85.5675 - val_mae: 6.8871\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 107.4693 - mae: 6.5729 - val_loss: 55.4226 - val_mae: 4.8969\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 77.2665 - mae: 5.0428 - val_loss: 38.2461 - val_mae: 4.4623\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 67.1432 - mae: 5.1995 - val_loss: 38.4555 - val_mae: 4.7536\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 65.8488 - mae: 5.0847 - val_loss: 36.6499 - val_mae: 4.4680\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 64.6356 - mae: 4.9803 - val_loss: 36.2099 - val_mae: 4.5330\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 63.6340 - mae: 5.0504 - val_loss: 35.4506 - val_mae: 4.4994\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 62.4730 - mae: 4.9571 - val_loss: 34.4971 - val_mae: 4.3907\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 61.7405 - mae: 4.9251 - val_loss: 33.9420 - val_mae: 4.3793\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 60.6636 - mae: 4.7977 - val_loss: 33.0706 - val_mae: 4.2364\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 59.8591 - mae: 4.8110 - val_loss: 32.9618 - val_mae: 4.3255\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 59.1548 - mae: 4.8247 - val_loss: 32.3794 - val_mae: 4.2643\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 58.3586 - mae: 4.7490 - val_loss: 31.6268 - val_mae: 4.1610\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 57.8247 - mae: 4.7797 - val_loss: 31.7029 - val_mae: 4.2342\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.9378 - mae: 4.7473 - val_loss: 30.7527 - val_mae: 4.0764\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.5723 - mae: 4.6108 - val_loss: 30.5255 - val_mae: 4.0759\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.0163 - mae: 4.6869 - val_loss: 30.3902 - val_mae: 4.0972\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 55.5087 - mae: 4.6558 - val_loss: 30.0016 - val_mae: 4.0540\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 55.2945 - mae: 4.7011 - val_loss: 29.6519 - val_mae: 4.0003\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 55.1448 - mae: 4.5058 - val_loss: 29.5215 - val_mae: 4.0044\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 54.5635 - mae: 4.6320 - val_loss: 29.5398 - val_mae: 4.0436\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.3120 - mae: 4.5866 - val_loss: 29.0246 - val_mae: 3.9201\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.7711 - mae: 4.5921 - val_loss: 29.0681 - val_mae: 3.9847\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.9748 - mae: 4.5274 - val_loss: 29.1354 - val_mae: 4.0209\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.3651 - mae: 4.5979 - val_loss: 28.8584 - val_mae: 3.9770\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.1174 - mae: 4.5612 - val_loss: 28.4239 - val_mae: 3.8708\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.8909 - mae: 4.5386 - val_loss: 28.6005 - val_mae: 3.9549\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.7182 - mae: 4.5231 - val_loss: 28.2055 - val_mae: 3.8661\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.5796 - mae: 4.5346 - val_loss: 28.2242 - val_mae: 3.9010\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 52.2563 - mae: 4.4912 - val_loss: 27.9471 - val_mae: 3.8377\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.2297 - mae: 4.4466 - val_loss: 27.9681 - val_mae: 3.8762\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.0723 - mae: 4.4577 - val_loss: 28.1125 - val_mae: 3.9255\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 52.0467 - mae: 4.4822 - val_loss: 27.8144 - val_mae: 3.8560\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 51.6267 - mae: 4.5275 - val_loss: 27.9940 - val_mae: 3.9012\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 51.5678 - mae: 4.4303 - val_loss: 27.6373 - val_mae: 3.8409\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 51.4737 - mae: 4.5084 - val_loss: 27.7565 - val_mae: 3.8752\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 51.3405 - mae: 4.3967 - val_loss: 27.4503 - val_mae: 3.8193\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 51.3570 - mae: 4.5210 - val_loss: 27.3642 - val_mae: 3.8042\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 51.0359 - mae: 4.4264 - val_loss: 27.3982 - val_mae: 3.8357\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 50.9597 - mae: 4.4132 - val_loss: 27.3285 - val_mae: 3.8362\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.8053 - mae: 4.4297 - val_loss: 27.5073 - val_mae: 3.8604\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.6110 - mae: 4.4401 - val_loss: 27.2335 - val_mae: 3.8211\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 50.5785 - mae: 4.4488 - val_loss: 27.2046 - val_mae: 3.8225\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.5207 - mae: 4.3766 - val_loss: 27.0669 - val_mae: 3.7980\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.3307 - mae: 4.3733 - val_loss: 27.1315 - val_mae: 3.8203\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.4190 - mae: 4.5061 - val_loss: 27.0621 - val_mae: 3.7917\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.5786 - mae: 4.3239 - val_loss: 26.9549 - val_mae: 3.8025\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.3074 - mae: 4.5075 - val_loss: 26.9823 - val_mae: 3.7915\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.1564 - mae: 4.3642 - val_loss: 26.9334 - val_mae: 3.7843\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 55.7943 - mae: 4.6339\n",
      "Mean Absolute Error on Test Data: 4.633918762207031\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.33669488723103314\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 95.1473 - mae: 8.0195 - val_loss: 80.0914 - val_mae: 7.1954\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 71.3864 - mae: 6.4511 - val_loss: 52.8388 - val_mae: 5.2881\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.6725 - mae: 4.4883 - val_loss: 30.3128 - val_mae: 3.9173\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.1952 - mae: 3.9962 - val_loss: 28.9934 - val_mae: 4.1388\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8821 - mae: 4.0638 - val_loss: 28.1526 - val_mae: 3.9717\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.5127 - mae: 3.9753 - val_loss: 27.8749 - val_mae: 3.9526\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.4531 - mae: 3.9294 - val_loss: 27.5732 - val_mae: 3.9295\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.3093 - mae: 4.0404 - val_loss: 27.3573 - val_mae: 3.9246\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.0866 - mae: 3.9012 - val_loss: 27.0869 - val_mae: 3.8702\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.9489 - mae: 3.9441 - val_loss: 26.8471 - val_mae: 3.8622\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.8887 - mae: 3.8940 - val_loss: 26.7816 - val_mae: 3.8956\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.7454 - mae: 3.9915 - val_loss: 26.4527 - val_mae: 3.8365\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.6787 - mae: 3.9059 - val_loss: 26.3224 - val_mae: 3.8400\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.5672 - mae: 3.8856 - val_loss: 26.1902 - val_mae: 3.8037\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.4090 - mae: 3.9174 - val_loss: 26.1996 - val_mae: 3.8620\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.4039 - mae: 3.9775 - val_loss: 25.8343 - val_mae: 3.7757\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2930 - mae: 3.8783 - val_loss: 25.7192 - val_mae: 3.7418\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2275 - mae: 3.8843 - val_loss: 25.6740 - val_mae: 3.7710\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.1289 - mae: 3.8999 - val_loss: 25.6171 - val_mae: 3.7952\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2692 - mae: 3.9450 - val_loss: 25.4394 - val_mae: 3.7685\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.2635 - mae: 3.8632 - val_loss: 25.4521 - val_mae: 3.7793\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.0412 - mae: 3.8992 - val_loss: 25.3797 - val_mae: 3.7572\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.9720 - mae: 3.9221 - val_loss: 25.2017 - val_mae: 3.7434\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.9047 - mae: 3.8434 - val_loss: 25.1595 - val_mae: 3.7312\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.8852 - mae: 3.9263 - val_loss: 25.0858 - val_mae: 3.7281\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.9123 - mae: 3.8132 - val_loss: 25.0836 - val_mae: 3.7203\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.8360 - mae: 3.9528 - val_loss: 25.0174 - val_mae: 3.7368\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.8338 - mae: 3.8367 - val_loss: 24.9919 - val_mae: 3.7362\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.7330 - mae: 3.8888 - val_loss: 24.9335 - val_mae: 3.7414\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.6229 - mae: 3.9028 - val_loss: 24.8711 - val_mae: 3.6944\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.5490 - mae: 3.8170 - val_loss: 24.8368 - val_mae: 3.7054\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.5311 - mae: 3.8625 - val_loss: 24.7829 - val_mae: 3.7113\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.4481 - mae: 3.8686 - val_loss: 24.6929 - val_mae: 3.7209\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.4477 - mae: 3.8613 - val_loss: 24.7264 - val_mae: 3.6639\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.4558 - mae: 3.8613 - val_loss: 24.6317 - val_mae: 3.7069\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2839 - mae: 3.8640 - val_loss: 24.5721 - val_mae: 3.6591\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.4859 - mae: 3.8285 - val_loss: 24.6041 - val_mae: 3.7142\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3400 - mae: 3.8263 - val_loss: 24.6176 - val_mae: 3.6756\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2518 - mae: 3.8442 - val_loss: 24.5753 - val_mae: 3.7141\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2593 - mae: 3.8704 - val_loss: 24.4914 - val_mae: 3.6434\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2165 - mae: 3.8618 - val_loss: 24.4149 - val_mae: 3.6794\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3216 - mae: 3.8111 - val_loss: 24.6950 - val_mae: 3.7492\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1950 - mae: 3.8571 - val_loss: 24.3986 - val_mae: 3.6671\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0583 - mae: 3.8734 - val_loss: 24.3221 - val_mae: 3.6511\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0879 - mae: 3.7507 - val_loss: 24.4852 - val_mae: 3.7007\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1488 - mae: 3.8907 - val_loss: 24.3343 - val_mae: 3.6747\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.9319 - mae: 3.8329 - val_loss: 24.3065 - val_mae: 3.6385\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1124 - mae: 3.8066 - val_loss: 24.4535 - val_mae: 3.7344\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8328 - mae: 3.8375 - val_loss: 24.2107 - val_mae: 3.6117\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8225 - mae: 3.7798 - val_loss: 24.2812 - val_mae: 3.6951\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 24.1072 - mae: 3.6501\n",
      "Mean Absolute Error on Test Data: 3.650059938430786\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.22968800466096628\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 13ms/step - loss: 72.8118 - mae: 7.1366 - val_loss: 63.4367 - val_mae: 6.4959\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 54.0525 - mae: 5.7444 - val_loss: 40.2513 - val_mae: 4.6352\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.2873 - mae: 3.8307 - val_loss: 20.9440 - val_mae: 3.2086\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 21.2264 - mae: 3.4354 - val_loss: 20.7723 - val_mae: 3.4813\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0007 - mae: 3.3553 - val_loss: 20.3643 - val_mae: 3.2652\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7943 - mae: 3.3764 - val_loss: 20.3208 - val_mae: 3.3341\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6849 - mae: 3.3204 - val_loss: 20.2674 - val_mae: 3.2733\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5772 - mae: 3.3513 - val_loss: 20.2639 - val_mae: 3.3204\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6333 - mae: 3.3208 - val_loss: 20.2120 - val_mae: 3.2803\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.6013 - mae: 3.3855 - val_loss: 20.1839 - val_mae: 3.2814\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.4758 - mae: 3.3121 - val_loss: 20.1630 - val_mae: 3.2782\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.4269 - mae: 3.3512 - val_loss: 20.1647 - val_mae: 3.2781\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3743 - mae: 3.3212 - val_loss: 20.1513 - val_mae: 3.2878\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3048 - mae: 3.3299 - val_loss: 20.1636 - val_mae: 3.3025\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2683 - mae: 3.3318 - val_loss: 20.1103 - val_mae: 3.2831\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2482 - mae: 3.3294 - val_loss: 20.1198 - val_mae: 3.2866\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2234 - mae: 3.3212 - val_loss: 20.1125 - val_mae: 3.2980\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1897 - mae: 3.3499 - val_loss: 20.0649 - val_mae: 3.2746\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.2019 - mae: 3.3337 - val_loss: 20.0670 - val_mae: 3.2551\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1608 - mae: 3.2960 - val_loss: 20.0927 - val_mae: 3.2993\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1681 - mae: 3.3565 - val_loss: 20.0607 - val_mae: 3.2481\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.1245 - mae: 3.3271 - val_loss: 20.0742 - val_mae: 3.2892\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0940 - mae: 3.3080 - val_loss: 20.1017 - val_mae: 3.3022\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.0844 - mae: 3.3363 - val_loss: 20.0820 - val_mae: 3.2686\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.0498 - mae: 3.3367 - val_loss: 20.0648 - val_mae: 3.2703\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9757 - mae: 3.3104 - val_loss: 20.0380 - val_mae: 3.2855\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9389 - mae: 3.3283 - val_loss: 20.0445 - val_mae: 3.2880\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9542 - mae: 3.3084 - val_loss: 20.0344 - val_mae: 3.2641\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9353 - mae: 3.3211 - val_loss: 20.0655 - val_mae: 3.2819\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8859 - mae: 3.3145 - val_loss: 20.0312 - val_mae: 3.2845\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9245 - mae: 3.3166 - val_loss: 20.0253 - val_mae: 3.2783\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.8471 - mae: 3.3364 - val_loss: 20.0039 - val_mae: 3.2746\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8053 - mae: 3.3100 - val_loss: 20.0070 - val_mae: 3.2638\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7775 - mae: 3.3039 - val_loss: 19.9875 - val_mae: 3.2616\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8257 - mae: 3.3258 - val_loss: 19.9676 - val_mae: 3.2792\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8129 - mae: 3.2830 - val_loss: 19.9580 - val_mae: 3.2868\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7826 - mae: 3.3475 - val_loss: 19.9321 - val_mae: 3.2543\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7669 - mae: 3.2806 - val_loss: 19.9458 - val_mae: 3.2973\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.7167 - mae: 3.3314 - val_loss: 19.9222 - val_mae: 3.2926\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.6363 - mae: 3.2888 - val_loss: 19.9247 - val_mae: 3.2630\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6588 - mae: 3.3082 - val_loss: 19.9486 - val_mae: 3.3091\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5956 - mae: 3.3259 - val_loss: 19.8749 - val_mae: 3.2430\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6705 - mae: 3.2813 - val_loss: 19.8384 - val_mae: 3.2994\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5491 - mae: 3.3068 - val_loss: 19.8159 - val_mae: 3.2430\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6613 - mae: 3.3036 - val_loss: 19.7967 - val_mae: 3.2354\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5012 - mae: 3.3036 - val_loss: 19.8820 - val_mae: 3.3191\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5413 - mae: 3.3102 - val_loss: 19.6904 - val_mae: 3.2398\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4563 - mae: 3.2960 - val_loss: 19.6997 - val_mae: 3.2739\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.4562 - mae: 3.2732 - val_loss: 19.6449 - val_mae: 3.2489\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.5663 - mae: 3.3266 - val_loss: 19.6639 - val_mae: 3.2256\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 24.7388 - mae: 3.3958\n",
      "Mean Absolute Error on Test Data: 3.3957631587982178\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09709699152036733\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 16ms/step - loss: 6.9809 - mae: 1.8238 - val_loss: 8.6184 - val_mae: 1.5757\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.8177 - mae: 1.2498 - val_loss: 6.9343 - val_mae: 1.3517\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.8118 - mae: 1.1621 - val_loss: 6.5162 - val_mae: 1.4720\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.7211 - mae: 1.2054 - val_loss: 6.4965 - val_mae: 1.4356\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.6794 - mae: 1.1612 - val_loss: 6.4821 - val_mae: 1.4201\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.6512 - mae: 1.1786 - val_loss: 6.4547 - val_mae: 1.4381\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.6115 - mae: 1.1736 - val_loss: 6.4420 - val_mae: 1.4180\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5820 - mae: 1.1526 - val_loss: 6.4230 - val_mae: 1.4181\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5351 - mae: 1.1529 - val_loss: 6.4048 - val_mae: 1.4186\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.5125 - mae: 1.1517 - val_loss: 6.3776 - val_mae: 1.4373\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.4693 - mae: 1.1665 - val_loss: 6.3772 - val_mae: 1.4392\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.4419 - mae: 1.1510 - val_loss: 6.3816 - val_mae: 1.4236\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.4216 - mae: 1.1639 - val_loss: 6.3686 - val_mae: 1.4503\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3974 - mae: 1.1876 - val_loss: 6.3604 - val_mae: 1.4501\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3863 - mae: 1.1525 - val_loss: 6.3732 - val_mae: 1.4050\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3716 - mae: 1.1345 - val_loss: 6.3466 - val_mae: 1.4185\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3522 - mae: 1.1848 - val_loss: 6.3358 - val_mae: 1.4501\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3294 - mae: 1.1569 - val_loss: 6.3434 - val_mae: 1.4115\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3189 - mae: 1.1518 - val_loss: 6.3287 - val_mae: 1.4466\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.3095 - mae: 1.1591 - val_loss: 6.3129 - val_mae: 1.4251\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.2980 - mae: 1.1989 - val_loss: 6.3286 - val_mae: 1.4819\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.3039 - mae: 1.1540 - val_loss: 6.3445 - val_mae: 1.3954\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2797 - mae: 1.1776 - val_loss: 6.3148 - val_mae: 1.4428\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2584 - mae: 1.1491 - val_loss: 6.3082 - val_mae: 1.4267\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.2389 - mae: 1.1659 - val_loss: 6.2985 - val_mae: 1.4383\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.2281 - mae: 1.1629 - val_loss: 6.2970 - val_mae: 1.4350\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2587 - mae: 1.1920 - val_loss: 6.2824 - val_mae: 1.4283\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.2247 - mae: 1.1397 - val_loss: 6.2874 - val_mae: 1.4102\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.1978 - mae: 1.1568 - val_loss: 6.2669 - val_mae: 1.4528\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2171 - mae: 1.1989 - val_loss: 6.2625 - val_mae: 1.4453\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2098 - mae: 1.1355 - val_loss: 6.2950 - val_mae: 1.4055\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.1679 - mae: 1.1465 - val_loss: 6.2644 - val_mae: 1.4597\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.2122 - mae: 1.2124 - val_loss: 6.2688 - val_mae: 1.4520\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 3.1622 - mae: 1.1627 - val_loss: 6.3044 - val_mae: 1.3999\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.1634 - mae: 1.1386 - val_loss: 6.2572 - val_mae: 1.4343\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.1377 - mae: 1.1646 - val_loss: 6.2478 - val_mae: 1.4377\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.1304 - mae: 1.1438 - val_loss: 6.2409 - val_mae: 1.4218\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.1268 - mae: 1.1655 - val_loss: 6.2373 - val_mae: 1.4341\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.1111 - mae: 1.1434 - val_loss: 6.2519 - val_mae: 1.4234\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.1043 - mae: 1.1729 - val_loss: 6.2293 - val_mae: 1.4424\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0922 - mae: 1.1289 - val_loss: 6.2276 - val_mae: 1.4115\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.0619 - mae: 1.1450 - val_loss: 6.2220 - val_mae: 1.4668\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0540 - mae: 1.1771 - val_loss: 6.2195 - val_mae: 1.4268\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 3.0451 - mae: 1.1296 - val_loss: 6.2159 - val_mae: 1.4094\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 3.0517 - mae: 1.1478 - val_loss: 6.2015 - val_mae: 1.4221\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0208 - mae: 1.1272 - val_loss: 6.1848 - val_mae: 1.4292\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0182 - mae: 1.1699 - val_loss: 6.1889 - val_mae: 1.4445\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0208 - mae: 1.1208 - val_loss: 6.1840 - val_mae: 1.4317\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0089 - mae: 1.1746 - val_loss: 6.1655 - val_mae: 1.4506\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.9500 - mae: 1.1394 - val_loss: 6.1812 - val_mae: 1.4034\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 1.9803 - mae: 1.0046\n",
      "Mean Absolute Error on Test Data: 1.004591941833496\n",
      "5/5 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.14650597171658541\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 154.8941 - mae: 10.0128 - val_loss: 140.7144 - val_mae: 9.2273\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 126.8795 - mae: 8.5566 - val_loss: 106.7220 - val_mae: 7.3750\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.1198 - mae: 6.3956 - val_loss: 67.1759 - val_mae: 5.4552\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 55.6686 - mae: 5.0240 - val_loss: 52.4381 - val_mae: 5.5511\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.8199 - mae: 5.1361 - val_loss: 51.8990 - val_mae: 5.5704\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.0317 - mae: 4.9540 - val_loss: 51.1112 - val_mae: 5.3760\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 49.5339 - mae: 4.9024 - val_loss: 50.4296 - val_mae: 5.4258\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 49.0271 - mae: 4.9471 - val_loss: 49.7208 - val_mae: 5.3419\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 48.4585 - mae: 4.8630 - val_loss: 49.1306 - val_mae: 5.2939\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 48.0290 - mae: 4.8086 - val_loss: 48.4978 - val_mae: 5.2802\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 47.5657 - mae: 4.8144 - val_loss: 47.8869 - val_mae: 5.2298\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 47.1539 - mae: 4.8084 - val_loss: 47.4013 - val_mae: 5.1718\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 46.7596 - mae: 4.7474 - val_loss: 46.9059 - val_mae: 5.1253\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 46.4351 - mae: 4.6948 - val_loss: 46.4761 - val_mae: 5.0577\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 46.0930 - mae: 4.7329 - val_loss: 46.0734 - val_mae: 5.1356\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.8603 - mae: 4.6516 - val_loss: 45.5665 - val_mae: 5.0202\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.5369 - mae: 4.6896 - val_loss: 45.2595 - val_mae: 5.0657\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.4119 - mae: 4.6448 - val_loss: 44.8472 - val_mae: 5.0138\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.1098 - mae: 4.6206 - val_loss: 44.5547 - val_mae: 4.9632\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.0292 - mae: 4.6361 - val_loss: 44.3810 - val_mae: 4.9083\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.9319 - mae: 4.6058 - val_loss: 44.1606 - val_mae: 4.9683\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.6951 - mae: 4.5947 - val_loss: 43.8912 - val_mae: 4.8995\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.6268 - mae: 4.6172 - val_loss: 43.7094 - val_mae: 4.9176\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.6261 - mae: 4.5935 - val_loss: 43.5516 - val_mae: 4.8555\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.4799 - mae: 4.5722 - val_loss: 43.4118 - val_mae: 4.8213\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.5587 - mae: 4.6093 - val_loss: 43.2621 - val_mae: 4.8181\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.5986 - mae: 4.5169 - val_loss: 43.3038 - val_mae: 4.9821\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.3200 - mae: 4.6379 - val_loss: 43.0570 - val_mae: 4.8255\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.4788 - mae: 4.5441 - val_loss: 42.9430 - val_mae: 4.9021\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.2857 - mae: 4.5480 - val_loss: 42.8443 - val_mae: 4.7816\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 44.0982 - mae: 4.5455 - val_loss: 42.8024 - val_mae: 4.8933\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.4796 - mae: 4.5946 - val_loss: 42.6329 - val_mae: 4.8326\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.0978 - mae: 4.5535 - val_loss: 42.6176 - val_mae: 4.8225\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.3070 - mae: 4.5703 - val_loss: 42.6375 - val_mae: 4.7483\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.0759 - mae: 4.5429 - val_loss: 42.5267 - val_mae: 4.8702\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.9675 - mae: 4.5581 - val_loss: 42.3983 - val_mae: 4.8139\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.0399 - mae: 4.5398 - val_loss: 42.4052 - val_mae: 4.7448\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.1161 - mae: 4.5717 - val_loss: 42.3328 - val_mae: 4.7566\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.2124 - mae: 4.5139 - val_loss: 42.3548 - val_mae: 4.9017\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.1058 - mae: 4.5751 - val_loss: 42.2213 - val_mae: 4.7627\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 43.8337 - mae: 4.5174 - val_loss: 42.1890 - val_mae: 4.8469\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.8770 - mae: 4.5814 - val_loss: 42.1204 - val_mae: 4.7587\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.6094 - mae: 4.4893 - val_loss: 42.5090 - val_mae: 4.9791\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.0397 - mae: 4.5938 - val_loss: 42.0758 - val_mae: 4.7643\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.7891 - mae: 4.5235 - val_loss: 42.0265 - val_mae: 4.7684\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.7972 - mae: 4.4977 - val_loss: 42.0458 - val_mae: 4.8259\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.8253 - mae: 4.5787 - val_loss: 42.0128 - val_mae: 4.7169\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.8648 - mae: 4.5545 - val_loss: 41.9168 - val_mae: 4.7902\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 43.9513 - mae: 4.4807 - val_loss: 41.8758 - val_mae: 4.7677\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 44.0506 - mae: 4.6530 - val_loss: 41.9362 - val_mae: 4.7089\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 31.9978 - mae: 4.2439\n",
      "Mean Absolute Error on Test Data: 4.243878364562988\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.20902950781708196\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 38.4633 - mae: 4.9457 - val_loss: 30.2258 - val_mae: 4.0073\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.5508 - mae: 3.7009 - val_loss: 19.2353 - val_mae: 2.8916\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1851 - mae: 2.8396 - val_loss: 15.0995 - val_mae: 2.8945\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0080 - mae: 2.8548 - val_loss: 15.2853 - val_mae: 2.9818\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9460 - mae: 2.7731 - val_loss: 14.8494 - val_mae: 2.8687\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7529 - mae: 2.7796 - val_loss: 14.8575 - val_mae: 2.9114\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6870 - mae: 2.7841 - val_loss: 14.6074 - val_mae: 2.8509\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6189 - mae: 2.7671 - val_loss: 14.5882 - val_mae: 2.8750\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5776 - mae: 2.7307 - val_loss: 14.4434 - val_mae: 2.8515\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.4663 - mae: 2.7604 - val_loss: 14.3869 - val_mae: 2.8640\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.3972 - mae: 2.7641 - val_loss: 14.2669 - val_mae: 2.8510\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2802 - mae: 2.7175 - val_loss: 14.1053 - val_mae: 2.8151\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2565 - mae: 2.7263 - val_loss: 14.0371 - val_mae: 2.8378\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2105 - mae: 2.7208 - val_loss: 13.8832 - val_mae: 2.8044\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1277 - mae: 2.7004 - val_loss: 13.8447 - val_mae: 2.8139\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1397 - mae: 2.7439 - val_loss: 13.7416 - val_mae: 2.8003\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.0734 - mae: 2.6794 - val_loss: 13.6764 - val_mae: 2.8062\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9949 - mae: 2.7146 - val_loss: 13.5839 - val_mae: 2.7876\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9659 - mae: 2.6972 - val_loss: 13.5707 - val_mae: 2.8058\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9712 - mae: 2.6746 - val_loss: 13.4787 - val_mae: 2.7945\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9214 - mae: 2.7162 - val_loss: 13.4353 - val_mae: 2.7901\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8937 - mae: 2.6628 - val_loss: 13.3804 - val_mae: 2.7820\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8312 - mae: 2.6934 - val_loss: 13.4534 - val_mae: 2.8209\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8387 - mae: 2.6998 - val_loss: 13.3407 - val_mae: 2.7945\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8225 - mae: 2.6655 - val_loss: 13.2395 - val_mae: 2.7735\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7854 - mae: 2.6742 - val_loss: 13.2692 - val_mae: 2.7830\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7555 - mae: 2.6661 - val_loss: 13.1664 - val_mae: 2.7667\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7418 - mae: 2.6843 - val_loss: 13.1423 - val_mae: 2.7594\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7278 - mae: 2.6664 - val_loss: 13.1313 - val_mae: 2.7659\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.7272 - mae: 2.6641 - val_loss: 13.1902 - val_mae: 2.7992\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6967 - mae: 2.6680 - val_loss: 13.0521 - val_mae: 2.7547\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 12.6674 - mae: 2.6691 - val_loss: 13.0346 - val_mae: 2.7526\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6644 - mae: 2.6365 - val_loss: 13.0766 - val_mae: 2.7745\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6649 - mae: 2.6531 - val_loss: 13.0922 - val_mae: 2.7949\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6217 - mae: 2.6826 - val_loss: 12.9336 - val_mae: 2.7209\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6335 - mae: 2.6158 - val_loss: 13.0127 - val_mae: 2.7656\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6252 - mae: 2.6833 - val_loss: 12.9216 - val_mae: 2.7273\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6047 - mae: 2.6147 - val_loss: 13.0285 - val_mae: 2.7860\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.6708 - mae: 2.6932 - val_loss: 12.9187 - val_mae: 2.6805\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.5504 - mae: 2.6203 - val_loss: 13.0289 - val_mae: 2.7874\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.5138 - mae: 2.6670 - val_loss: 12.8872 - val_mae: 2.7175\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4945 - mae: 2.6079 - val_loss: 12.9320 - val_mae: 2.7702\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4835 - mae: 2.6443 - val_loss: 12.8598 - val_mae: 2.7464\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4528 - mae: 2.6431 - val_loss: 12.8409 - val_mae: 2.7265\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4319 - mae: 2.6086 - val_loss: 12.8671 - val_mae: 2.7577\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4480 - mae: 2.6592 - val_loss: 12.8285 - val_mae: 2.7392\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4211 - mae: 2.6028 - val_loss: 12.8554 - val_mae: 2.7466\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.4160 - mae: 2.6291 - val_loss: 12.8890 - val_mae: 2.7662\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.4886 - mae: 2.6588 - val_loss: 12.8083 - val_mae: 2.6705\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.3629 - mae: 2.5888 - val_loss: 12.9434 - val_mae: 2.7832\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 14.6360 - mae: 2.9703\n",
      "Mean Absolute Error on Test Data: 2.970266103744507\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1079556191894333\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 15ms/step - loss: 9.4686 - mae: 1.9900 - val_loss: 4.0900 - val_mae: 1.3998\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 6.8145 - mae: 1.4955 - val_loss: 2.8656 - val_mae: 1.2762\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 5.7738 - mae: 1.5447 - val_loss: 3.1877 - val_mae: 1.4816\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6387 - mae: 1.5599 - val_loss: 2.8542 - val_mae: 1.3474\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6678 - mae: 1.4509 - val_loss: 2.7552 - val_mae: 1.2982\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6019 - mae: 1.5052 - val_loss: 2.8521 - val_mae: 1.3781\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5589 - mae: 1.5722 - val_loss: 2.8514 - val_mae: 1.3920\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4525 - mae: 1.4984 - val_loss: 2.6016 - val_mae: 1.2777\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5055 - mae: 1.4248 - val_loss: 2.5608 - val_mae: 1.2694\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.4068 - mae: 1.4890 - val_loss: 2.6089 - val_mae: 1.3107\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3480 - mae: 1.4901 - val_loss: 2.5359 - val_mae: 1.2818\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3389 - mae: 1.4465 - val_loss: 2.4787 - val_mae: 1.2628\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3112 - mae: 1.4697 - val_loss: 2.4929 - val_mae: 1.2760\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2872 - mae: 1.4717 - val_loss: 2.4584 - val_mae: 1.2659\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2983 - mae: 1.4514 - val_loss: 2.4398 - val_mae: 1.2612\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2521 - mae: 1.4559 - val_loss: 2.4018 - val_mae: 1.2445\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2729 - mae: 1.4446 - val_loss: 2.4445 - val_mae: 1.2688\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2351 - mae: 1.4718 - val_loss: 2.4868 - val_mae: 1.2903\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3197 - mae: 1.5984 - val_loss: 2.5691 - val_mae: 1.3260\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1918 - mae: 1.4660 - val_loss: 2.3520 - val_mae: 1.2196\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1984 - mae: 1.4443 - val_loss: 2.3983 - val_mae: 1.2495\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1828 - mae: 1.4838 - val_loss: 2.3969 - val_mae: 1.2501\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1790 - mae: 1.4653 - val_loss: 2.3326 - val_mae: 1.2139\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2239 - mae: 1.4168 - val_loss: 2.3282 - val_mae: 1.2106\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.1809 - mae: 1.4827 - val_loss: 2.4353 - val_mae: 1.2703\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1668 - mae: 1.4700 - val_loss: 2.4613 - val_mae: 1.2823\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4119 - mae: 1.6590 - val_loss: 2.7260 - val_mae: 1.3805\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2566 - mae: 1.4836 - val_loss: 2.3547 - val_mae: 1.2110\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1824 - mae: 1.4424 - val_loss: 2.4251 - val_mae: 1.2544\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1676 - mae: 1.4920 - val_loss: 2.4436 - val_mae: 1.2654\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1764 - mae: 1.4650 - val_loss: 2.3999 - val_mae: 1.2453\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1528 - mae: 1.4836 - val_loss: 2.4139 - val_mae: 1.2544\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1786 - mae: 1.4451 - val_loss: 2.3383 - val_mae: 1.2147\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1436 - mae: 1.4649 - val_loss: 2.3738 - val_mae: 1.2347\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1677 - mae: 1.4366 - val_loss: 2.3771 - val_mae: 1.2352\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4688 - mae: 1.6840 - val_loss: 3.0253 - val_mae: 1.4819\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.1888 - mae: 1.5219 - val_loss: 2.3397 - val_mae: 1.2027\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1739 - mae: 1.4496 - val_loss: 2.4000 - val_mae: 1.2455\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1312 - mae: 1.4768 - val_loss: 2.3742 - val_mae: 1.2347\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1341 - mae: 1.4599 - val_loss: 2.3655 - val_mae: 1.2310\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1285 - mae: 1.4701 - val_loss: 2.3949 - val_mae: 1.2463\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1683 - mae: 1.4448 - val_loss: 2.3165 - val_mae: 1.2008\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1259 - mae: 1.4792 - val_loss: 2.4076 - val_mae: 1.2525\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1168 - mae: 1.5038 - val_loss: 2.3628 - val_mae: 1.2283\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1624 - mae: 1.4353 - val_loss: 2.3162 - val_mae: 1.2060\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 5.1124 - mae: 1.4499 - val_loss: 2.3562 - val_mae: 1.2279\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.0971 - mae: 1.4853 - val_loss: 2.3729 - val_mae: 1.2356\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1069 - mae: 1.4506 - val_loss: 2.3487 - val_mae: 1.2178\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1018 - mae: 1.4571 - val_loss: 2.3726 - val_mae: 1.2334\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0926 - mae: 1.4741 - val_loss: 2.4138 - val_mae: 1.2519\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 6.0042 - mae: 1.5280\n",
      "Mean Absolute Error on Test Data: 1.5280158519744873\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1235002863421496\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 76.8136 - mae: 6.1736 - val_loss: 62.0661 - val_mae: 5.2973\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 59.4551 - mae: 4.8455 - val_loss: 43.8576 - val_mae: 4.0077\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.6586 - mae: 3.7766 - val_loss: 33.6176 - val_mae: 3.8782\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.9275 - mae: 4.0202 - val_loss: 33.6969 - val_mae: 4.0085\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.3745 - mae: 3.9457 - val_loss: 33.2729 - val_mae: 3.8852\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.9792 - mae: 3.8514 - val_loss: 33.1337 - val_mae: 3.9211\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.6922 - mae: 3.7930 - val_loss: 32.9757 - val_mae: 3.9150\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.1436 - mae: 3.9028 - val_loss: 32.8242 - val_mae: 3.9432\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.8197 - mae: 3.7896 - val_loss: 32.4267 - val_mae: 3.8884\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.2815 - mae: 3.8001 - val_loss: 32.1992 - val_mae: 3.8875\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8250 - mae: 3.8099 - val_loss: 31.9236 - val_mae: 3.8590\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.4291 - mae: 3.7604 - val_loss: 31.8010 - val_mae: 3.8615\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.0559 - mae: 3.7736 - val_loss: 31.6399 - val_mae: 3.8416\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.7335 - mae: 3.7333 - val_loss: 31.5562 - val_mae: 3.8301\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4443 - mae: 3.7164 - val_loss: 31.4278 - val_mae: 3.8183\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3980 - mae: 3.7550 - val_loss: 31.2813 - val_mae: 3.7852\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1624 - mae: 3.6664 - val_loss: 31.2366 - val_mae: 3.7931\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0047 - mae: 3.6714 - val_loss: 31.5453 - val_mae: 3.9162\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1268 - mae: 3.8198 - val_loss: 31.5577 - val_mae: 3.7139\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1685 - mae: 3.6248 - val_loss: 31.3954 - val_mae: 3.8853\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.6666 - mae: 3.6436 - val_loss: 31.2974 - val_mae: 3.8514\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4257 - mae: 3.7260 - val_loss: 31.1655 - val_mae: 3.7959\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.3566 - mae: 3.6259 - val_loss: 31.1770 - val_mae: 3.8177\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.3026 - mae: 3.6562 - val_loss: 31.2018 - val_mae: 3.8307\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.2058 - mae: 3.6479 - val_loss: 31.1551 - val_mae: 3.8316\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1539 - mae: 3.6757 - val_loss: 31.2319 - val_mae: 3.8441\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2251 - mae: 3.6759 - val_loss: 31.2178 - val_mae: 3.8436\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1340 - mae: 3.6320 - val_loss: 31.0935 - val_mae: 3.7849\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0831 - mae: 3.6339 - val_loss: 31.1898 - val_mae: 3.8459\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0970 - mae: 3.6592 - val_loss: 31.2862 - val_mae: 3.8679\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1486 - mae: 3.6576 - val_loss: 31.0613 - val_mae: 3.7716\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2940 - mae: 3.7102 - val_loss: 31.2872 - val_mae: 3.8785\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2564 - mae: 3.5687 - val_loss: 31.0168 - val_mae: 3.7792\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9566 - mae: 3.6910 - val_loss: 31.3139 - val_mae: 3.8890\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9560 - mae: 3.6765 - val_loss: 31.0457 - val_mae: 3.7860\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 30.9088 - mae: 3.6200 - val_loss: 31.0851 - val_mae: 3.8346\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9572 - mae: 3.6407 - val_loss: 30.9553 - val_mae: 3.7860\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.8493 - mae: 3.6269 - val_loss: 31.1090 - val_mae: 3.8464\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9324 - mae: 3.7169 - val_loss: 31.0697 - val_mae: 3.7512\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0372 - mae: 3.5617 - val_loss: 31.0427 - val_mae: 3.8502\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0253 - mae: 3.6112 - val_loss: 30.8879 - val_mae: 3.8135\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.8908 - mae: 3.7022 - val_loss: 30.9188 - val_mae: 3.8099\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7846 - mae: 3.6034 - val_loss: 30.9013 - val_mae: 3.7956\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7754 - mae: 3.6739 - val_loss: 30.8733 - val_mae: 3.8008\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.8713 - mae: 3.5390 - val_loss: 30.9239 - val_mae: 3.8198\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.2435 - mae: 3.7905 - val_loss: 30.8395 - val_mae: 3.7466\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9550 - mae: 3.6139 - val_loss: 30.7473 - val_mae: 3.7726\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7433 - mae: 3.6067 - val_loss: 30.6952 - val_mae: 3.8046\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6760 - mae: 3.6600 - val_loss: 30.6983 - val_mae: 3.7921\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6672 - mae: 3.5769 - val_loss: 30.7499 - val_mae: 3.7786\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 20.7252 - mae: 3.2802\n",
      "Mean Absolute Error on Test Data: 3.280177116394043\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.12678021063155276\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 107.2239 - mae: 8.1374 - val_loss: 118.3458 - val_mae: 7.8841\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 84.6205 - mae: 6.6563 - val_loss: 87.5838 - val_mae: 5.8883\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.8257 - mae: 4.5731 - val_loss: 57.7887 - val_mae: 4.2357\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.9762 - mae: 4.1912 - val_loss: 54.2609 - val_mae: 4.4435\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.8089 - mae: 4.2034 - val_loss: 54.0774 - val_mae: 4.2896\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.1690 - mae: 4.2065 - val_loss: 53.5995 - val_mae: 4.2908\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.6227 - mae: 4.1088 - val_loss: 53.4344 - val_mae: 4.2493\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.3505 - mae: 4.1020 - val_loss: 53.0381 - val_mae: 4.2592\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.0696 - mae: 4.0808 - val_loss: 52.8061 - val_mae: 4.2392\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.9169 - mae: 4.1618 - val_loss: 52.4125 - val_mae: 4.2459\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.5191 - mae: 4.0418 - val_loss: 52.3638 - val_mae: 4.2142\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.6587 - mae: 4.1732 - val_loss: 52.1642 - val_mae: 4.2055\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.3024 - mae: 4.0108 - val_loss: 51.8120 - val_mae: 4.2426\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.9698 - mae: 4.0666 - val_loss: 51.6565 - val_mae: 4.2419\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.8988 - mae: 4.0605 - val_loss: 51.3619 - val_mae: 4.2961\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.7483 - mae: 4.0638 - val_loss: 51.4226 - val_mae: 4.2213\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.5333 - mae: 4.0750 - val_loss: 51.1869 - val_mae: 4.2436\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.6220 - mae: 4.0603 - val_loss: 50.9412 - val_mae: 4.2928\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.4069 - mae: 4.0946 - val_loss: 51.2866 - val_mae: 4.1809\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.4647 - mae: 4.0196 - val_loss: 50.8110 - val_mae: 4.2429\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.2145 - mae: 4.0571 - val_loss: 50.7881 - val_mae: 4.2268\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.1552 - mae: 4.0748 - val_loss: 50.5880 - val_mae: 4.2489\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.0593 - mae: 4.0262 - val_loss: 50.5462 - val_mae: 4.2328\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.2560 - mae: 4.0219 - val_loss: 50.3571 - val_mae: 4.3184\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.1804 - mae: 4.0631 - val_loss: 50.4088 - val_mae: 4.2372\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.0580 - mae: 4.0916 - val_loss: 50.6682 - val_mae: 4.1717\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.9365 - mae: 4.0174 - val_loss: 50.2578 - val_mae: 4.2318\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.7574 - mae: 4.0567 - val_loss: 50.3060 - val_mae: 4.1969\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.8539 - mae: 3.9795 - val_loss: 50.0544 - val_mae: 4.2416\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.7441 - mae: 4.0877 - val_loss: 50.2378 - val_mae: 4.1711\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.7882 - mae: 4.0441 - val_loss: 50.1679 - val_mae: 4.1710\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5717 - mae: 3.9861 - val_loss: 49.8133 - val_mae: 4.2372\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.5382 - mae: 4.0641 - val_loss: 49.7902 - val_mae: 4.2058\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5821 - mae: 4.0233 - val_loss: 49.7879 - val_mae: 4.1876\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4602 - mae: 3.9790 - val_loss: 49.6171 - val_mae: 4.2260\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3872 - mae: 4.0315 - val_loss: 49.5699 - val_mae: 4.2095\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3189 - mae: 4.0309 - val_loss: 49.6529 - val_mae: 4.1737\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3766 - mae: 3.9823 - val_loss: 49.3544 - val_mae: 4.2560\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3212 - mae: 4.0315 - val_loss: 49.4557 - val_mae: 4.1837\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4011 - mae: 4.0104 - val_loss: 49.2381 - val_mae: 4.2443\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.0923 - mae: 4.0326 - val_loss: 49.5331 - val_mae: 4.1485\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4869 - mae: 4.0199 - val_loss: 49.2105 - val_mae: 4.1942\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4859 - mae: 4.0563 - val_loss: 49.3531 - val_mae: 4.1503\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.8719 - mae: 3.9390 - val_loss: 49.0836 - val_mae: 4.2108\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.2248 - mae: 3.9776 - val_loss: 49.2269 - val_mae: 4.1551\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0032 - mae: 4.0182 - val_loss: 49.0000 - val_mae: 4.1832\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0578 - mae: 4.0123 - val_loss: 48.9190 - val_mae: 4.1858\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 34.9361 - mae: 3.9398 - val_loss: 48.9377 - val_mae: 4.1656\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9864 - mae: 4.0434 - val_loss: 49.0500 - val_mae: 4.1311\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9595 - mae: 3.9313 - val_loss: 48.6832 - val_mae: 4.2005\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 28.0599 - mae: 3.8272\n",
      "Mean Absolute Error on Test Data: 3.8271896839141846\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12045851287991227\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 111.7320 - mae: 8.5361 - val_loss: 117.7486 - val_mae: 7.9388\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 86.7085 - mae: 6.9754 - val_loss: 87.2611 - val_mae: 5.9481\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.2527 - mae: 5.0295 - val_loss: 58.6419 - val_mae: 4.3835\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.6308 - mae: 4.3879 - val_loss: 53.7272 - val_mae: 4.8029\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.6172 - mae: 4.5079 - val_loss: 52.8778 - val_mae: 4.6589\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.0296 - mae: 4.3884 - val_loss: 52.2910 - val_mae: 4.5324\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.6141 - mae: 4.3551 - val_loss: 51.6904 - val_mae: 4.5179\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 37.2567 - mae: 4.3371 - val_loss: 51.1164 - val_mae: 4.5085\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.0373 - mae: 4.3738 - val_loss: 50.5354 - val_mae: 4.4831\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.8089 - mae: 4.2764 - val_loss: 50.1000 - val_mae: 4.4461\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.3615 - mae: 4.3053 - val_loss: 49.6521 - val_mae: 4.4036\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.1900 - mae: 4.3053 - val_loss: 49.0342 - val_mae: 4.3923\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.9265 - mae: 4.2408 - val_loss: 48.8906 - val_mae: 4.2791\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.6424 - mae: 4.2404 - val_loss: 48.1950 - val_mae: 4.3183\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5092 - mae: 4.2845 - val_loss: 47.6716 - val_mae: 4.3411\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.2074 - mae: 4.2497 - val_loss: 47.4565 - val_mae: 4.2133\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.1327 - mae: 4.1958 - val_loss: 46.8957 - val_mae: 4.2417\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9056 - mae: 4.2298 - val_loss: 46.6156 - val_mae: 4.1938\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.7578 - mae: 4.1675 - val_loss: 46.2748 - val_mae: 4.1808\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.6906 - mae: 4.2019 - val_loss: 45.8697 - val_mae: 4.1586\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.4779 - mae: 4.2031 - val_loss: 45.4970 - val_mae: 4.1569\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.3299 - mae: 4.1870 - val_loss: 45.3083 - val_mae: 4.1122\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 34.2140 - mae: 4.1525 - val_loss: 44.9686 - val_mae: 4.1048\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.1310 - mae: 4.1309 - val_loss: 44.6999 - val_mae: 4.1104\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.0988 - mae: 4.1635 - val_loss: 44.2076 - val_mae: 4.1457\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.9217 - mae: 4.2030 - val_loss: 44.1710 - val_mae: 4.0553\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8783 - mae: 4.0765 - val_loss: 43.9365 - val_mae: 4.0551\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8245 - mae: 4.1441 - val_loss: 43.6423 - val_mae: 4.0560\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.6204 - mae: 4.1423 - val_loss: 43.3152 - val_mae: 4.0562\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.5012 - mae: 4.1426 - val_loss: 43.2721 - val_mae: 4.0035\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.5369 - mae: 4.1504 - val_loss: 43.0063 - val_mae: 4.0040\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.3451 - mae: 4.0516 - val_loss: 42.7553 - val_mae: 3.9958\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.1799 - mae: 4.0959 - val_loss: 42.2972 - val_mae: 4.0460\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.1249 - mae: 4.0987 - val_loss: 42.3627 - val_mae: 3.9548\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.9508 - mae: 4.0591 - val_loss: 41.9006 - val_mae: 4.0221\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.8751 - mae: 4.1089 - val_loss: 41.7987 - val_mae: 3.9523\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.8072 - mae: 4.0577 - val_loss: 41.7693 - val_mae: 3.9307\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.6913 - mae: 4.0408 - val_loss: 41.2814 - val_mae: 3.9715\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.9108 - mae: 4.1167 - val_loss: 41.7570 - val_mae: 3.8704\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.6416 - mae: 4.0377 - val_loss: 41.2115 - val_mae: 3.8946\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4525 - mae: 4.0421 - val_loss: 40.8491 - val_mae: 3.9259\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.5559 - mae: 4.0897 - val_loss: 40.7375 - val_mae: 3.8943\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.5210 - mae: 4.0128 - val_loss: 40.4254 - val_mae: 3.9275\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2923 - mae: 4.0594 - val_loss: 40.6525 - val_mae: 3.8536\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.3305 - mae: 4.0385 - val_loss: 40.1061 - val_mae: 3.9063\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2371 - mae: 4.0186 - val_loss: 40.2143 - val_mae: 3.8959\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1873 - mae: 4.0605 - val_loss: 40.0700 - val_mae: 3.8612\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1140 - mae: 4.0086 - val_loss: 39.7419 - val_mae: 3.8770\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.9561 - mae: 4.0205 - val_loss: 39.7023 - val_mae: 3.8518\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.9659 - mae: 4.0168 - val_loss: 39.8942 - val_mae: 3.8065\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 33.7248 - mae: 4.2343\n",
      "Mean Absolute Error on Test Data: 4.234299182891846\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.29764314309911666\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 21ms/step - loss: 4.3707 - mae: 1.7465 - val_loss: 4.4320 - val_mae: 1.5218\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 2.8000 - mae: 1.2670 - val_loss: 3.1433 - val_mae: 1.1312\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.7970 - mae: 0.8466 - val_loss: 2.4291 - val_mae: 0.9645\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.3030 - mae: 0.7944 - val_loss: 2.2871 - val_mae: 1.0617\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.3006 - mae: 0.8527 - val_loss: 2.3083 - val_mae: 1.0998\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2628 - mae: 0.8400 - val_loss: 2.2669 - val_mae: 1.0578\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2458 - mae: 0.8163 - val_loss: 2.2530 - val_mae: 1.0370\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2451 - mae: 0.8091 - val_loss: 2.2439 - val_mae: 1.0329\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2356 - mae: 0.8085 - val_loss: 2.2363 - val_mae: 1.0328\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2277 - mae: 0.8139 - val_loss: 2.2336 - val_mae: 1.0471\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2225 - mae: 0.8160 - val_loss: 2.2224 - val_mae: 1.0393\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2163 - mae: 0.8053 - val_loss: 2.2115 - val_mae: 1.0264\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2098 - mae: 0.8084 - val_loss: 2.2079 - val_mae: 1.0394\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2116 - mae: 0.8156 - val_loss: 2.2014 - val_mae: 1.0409\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1985 - mae: 0.7945 - val_loss: 2.1947 - val_mae: 1.0047\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2006 - mae: 0.7978 - val_loss: 2.1870 - val_mae: 1.0314\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1893 - mae: 0.7990 - val_loss: 2.1812 - val_mae: 1.0313\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1882 - mae: 0.8033 - val_loss: 2.1751 - val_mae: 1.0232\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1791 - mae: 0.7922 - val_loss: 2.1679 - val_mae: 1.0186\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1744 - mae: 0.7917 - val_loss: 2.1583 - val_mae: 1.0164\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1739 - mae: 0.7882 - val_loss: 2.1523 - val_mae: 1.0098\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1667 - mae: 0.7892 - val_loss: 2.1502 - val_mae: 1.0227\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1696 - mae: 0.7980 - val_loss: 2.1437 - val_mae: 1.0218\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1634 - mae: 0.7801 - val_loss: 2.1357 - val_mae: 0.9872\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1644 - mae: 0.7899 - val_loss: 2.1412 - val_mae: 1.0299\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1549 - mae: 0.7856 - val_loss: 2.1279 - val_mae: 0.9936\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1506 - mae: 0.7804 - val_loss: 2.1268 - val_mae: 1.0039\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1478 - mae: 0.7824 - val_loss: 2.1233 - val_mae: 1.0080\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1467 - mae: 0.7862 - val_loss: 2.1249 - val_mae: 1.0085\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1452 - mae: 0.7878 - val_loss: 2.1177 - val_mae: 1.0039\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1392 - mae: 0.7832 - val_loss: 2.1121 - val_mae: 0.9978\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1432 - mae: 0.7752 - val_loss: 2.1056 - val_mae: 0.9870\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1447 - mae: 0.7891 - val_loss: 2.1209 - val_mae: 1.0196\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1309 - mae: 0.7885 - val_loss: 2.1070 - val_mae: 0.9896\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1399 - mae: 0.7680 - val_loss: 2.1032 - val_mae: 0.9854\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1279 - mae: 0.7798 - val_loss: 2.1039 - val_mae: 1.0018\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1282 - mae: 0.7790 - val_loss: 2.0965 - val_mae: 0.9891\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1235 - mae: 0.7788 - val_loss: 2.0925 - val_mae: 0.9889\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1224 - mae: 0.7803 - val_loss: 2.0971 - val_mae: 1.0024\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1251 - mae: 0.7792 - val_loss: 2.0879 - val_mae: 0.9852\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1171 - mae: 0.7799 - val_loss: 2.0907 - val_mae: 0.9977\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1181 - mae: 0.7866 - val_loss: 2.0912 - val_mae: 1.0009\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1233 - mae: 0.7761 - val_loss: 2.0813 - val_mae: 0.9802\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1113 - mae: 0.7796 - val_loss: 2.0877 - val_mae: 1.0007\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1087 - mae: 0.7814 - val_loss: 2.0743 - val_mae: 0.9821\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1101 - mae: 0.7695 - val_loss: 2.0761 - val_mae: 0.9807\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1138 - mae: 0.7909 - val_loss: 2.0839 - val_mae: 1.0011\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1049 - mae: 0.7777 - val_loss: 2.0737 - val_mae: 0.9856\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1087 - mae: 0.7731 - val_loss: 2.0744 - val_mae: 0.9852\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1044 - mae: 0.7849 - val_loss: 2.0635 - val_mae: 0.9845\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.9822 - mae: 0.9279\n",
      "Mean Absolute Error on Test Data: 0.9278785586357117\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08369984914530049\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 106.4105 - mae: 8.3098 - val_loss: 107.6035 - val_mae: 7.9517\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 87.9268 - mae: 7.1580 - val_loss: 83.8844 - val_mae: 6.3623\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 60.5085 - mae: 5.2707 - val_loss: 53.1675 - val_mae: 4.3805\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.5123 - mae: 3.9109 - val_loss: 43.2475 - val_mae: 4.4747\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.8810 - mae: 4.1863 - val_loss: 43.0945 - val_mae: 4.4781\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5447 - mae: 3.9794 - val_loss: 42.8305 - val_mae: 4.3019\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.2337 - mae: 4.0165 - val_loss: 42.6585 - val_mae: 4.3976\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.0974 - mae: 3.9878 - val_loss: 42.4468 - val_mae: 4.3629\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9804 - mae: 3.9371 - val_loss: 42.2584 - val_mae: 4.2770\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.6191 - mae: 3.9674 - val_loss: 42.0940 - val_mae: 4.3687\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.5616 - mae: 3.9453 - val_loss: 41.9039 - val_mae: 4.2968\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.3938 - mae: 3.9454 - val_loss: 41.8240 - val_mae: 4.3704\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.2684 - mae: 3.9298 - val_loss: 41.6427 - val_mae: 4.2486\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.0544 - mae: 3.9063 - val_loss: 41.5229 - val_mae: 4.3067\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.9730 - mae: 3.9279 - val_loss: 41.3798 - val_mae: 4.2835\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.0470 - mae: 3.8436 - val_loss: 41.3305 - val_mae: 4.2334\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8791 - mae: 3.9466 - val_loss: 41.1787 - val_mae: 4.2956\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.7206 - mae: 3.8680 - val_loss: 41.1047 - val_mae: 4.2530\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.5803 - mae: 3.9103 - val_loss: 41.0330 - val_mae: 4.3009\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 33.4837 - mae: 3.8588 - val_loss: 40.9450 - val_mae: 4.2450\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.4015 - mae: 3.8468 - val_loss: 40.8869 - val_mae: 4.2942\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.2794 - mae: 3.8943 - val_loss: 40.7843 - val_mae: 4.2666\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.2237 - mae: 3.8637 - val_loss: 40.7154 - val_mae: 4.2461\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.1668 - mae: 3.8696 - val_loss: 40.6822 - val_mae: 4.2375\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.1346 - mae: 3.8109 - val_loss: 40.5854 - val_mae: 4.2704\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.9998 - mae: 3.8524 - val_loss: 40.5087 - val_mae: 4.2393\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.9184 - mae: 3.8661 - val_loss: 40.4635 - val_mae: 4.1927\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.0819 - mae: 3.8865 - val_loss: 40.3678 - val_mae: 4.2186\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.9435 - mae: 3.7589 - val_loss: 40.2913 - val_mae: 4.2015\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.6968 - mae: 3.8648 - val_loss: 40.2329 - val_mae: 4.2836\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.6468 - mae: 3.8169 - val_loss: 40.1502 - val_mae: 4.2378\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.6073 - mae: 3.8558 - val_loss: 40.1034 - val_mae: 4.1998\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.5782 - mae: 3.7636 - val_loss: 40.0349 - val_mae: 4.1943\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4088 - mae: 3.8578 - val_loss: 39.9374 - val_mae: 4.2431\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3533 - mae: 3.7960 - val_loss: 39.9235 - val_mae: 4.1911\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.3571 - mae: 3.7904 - val_loss: 39.8827 - val_mae: 4.1698\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2059 - mae: 3.8474 - val_loss: 39.8101 - val_mae: 4.2198\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1251 - mae: 3.8134 - val_loss: 39.7702 - val_mae: 4.1646\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0669 - mae: 3.7581 - val_loss: 39.6422 - val_mae: 4.1969\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.9860 - mae: 3.8491 - val_loss: 39.6433 - val_mae: 4.1746\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.0633 - mae: 3.7403 - val_loss: 39.5623 - val_mae: 4.2033\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.9869 - mae: 3.8570 - val_loss: 39.5513 - val_mae: 4.1733\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.7914 - mae: 3.7627 - val_loss: 39.5885 - val_mae: 4.1443\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.6763 - mae: 3.7659 - val_loss: 39.4175 - val_mae: 4.2200\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8229 - mae: 3.8229 - val_loss: 39.4353 - val_mae: 4.1284\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.5708 - mae: 3.7878 - val_loss: 39.3260 - val_mae: 4.1760\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5414 - mae: 3.7899 - val_loss: 39.3209 - val_mae: 4.1477\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5178 - mae: 3.7776 - val_loss: 39.2624 - val_mae: 4.1615\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4110 - mae: 3.7653 - val_loss: 39.2129 - val_mae: 4.1400\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4494 - mae: 3.7447 - val_loss: 39.1601 - val_mae: 4.2195\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 23.8822 - mae: 3.5493\n",
      "Mean Absolute Error on Test Data: 3.5493454933166504\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.13838764431557748\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 85.5903 - mae: 7.3422 - val_loss: 73.7533 - val_mae: 6.7837\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 68.4844 - mae: 6.1277 - val_loss: 52.8967 - val_mae: 5.3060\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 45.0737 - mae: 4.3685 - val_loss: 29.8220 - val_mae: 3.8077\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.8167 - mae: 3.7694 - val_loss: 26.2181 - val_mae: 3.8797\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.1133 - mae: 3.9528 - val_loss: 25.5715 - val_mae: 3.7520\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.6209 - mae: 3.8184 - val_loss: 25.2017 - val_mae: 3.7152\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.3627 - mae: 3.8204 - val_loss: 24.9280 - val_mae: 3.6920\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 29.2613 - mae: 3.8164 - val_loss: 24.6425 - val_mae: 3.6874\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.0364 - mae: 3.7277 - val_loss: 24.4393 - val_mae: 3.6565\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.1859 - mae: 3.9017 - val_loss: 24.1279 - val_mae: 3.6324\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.7879 - mae: 3.7316 - val_loss: 23.8849 - val_mae: 3.6299\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.6837 - mae: 3.8186 - val_loss: 23.6994 - val_mae: 3.5952\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.4072 - mae: 3.6396 - val_loss: 23.4899 - val_mae: 3.5957\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.0679 - mae: 3.7721 - val_loss: 23.3246 - val_mae: 3.6316\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.9817 - mae: 3.7120 - val_loss: 23.0996 - val_mae: 3.5754\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.7802 - mae: 3.7128 - val_loss: 22.9598 - val_mae: 3.5668\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.6913 - mae: 3.6955 - val_loss: 22.8053 - val_mae: 3.5502\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.5750 - mae: 3.6947 - val_loss: 22.6702 - val_mae: 3.5418\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.5034 - mae: 3.6890 - val_loss: 22.6121 - val_mae: 3.5577\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.5071 - mae: 3.7002 - val_loss: 22.5182 - val_mae: 3.5151\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3842 - mae: 3.6921 - val_loss: 22.4123 - val_mae: 3.5426\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3231 - mae: 3.6776 - val_loss: 22.3940 - val_mae: 3.5008\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3913 - mae: 3.6595 - val_loss: 22.3099 - val_mae: 3.5375\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 27.2638 - mae: 3.6817 - val_loss: 22.2752 - val_mae: 3.5162\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2106 - mae: 3.6885 - val_loss: 22.2088 - val_mae: 3.5251\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2433 - mae: 3.6661 - val_loss: 22.2229 - val_mae: 3.4954\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1894 - mae: 3.6498 - val_loss: 22.1839 - val_mae: 3.5061\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1214 - mae: 3.6845 - val_loss: 22.1447 - val_mae: 3.5217\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.2009 - mae: 3.6515 - val_loss: 22.2239 - val_mae: 3.4871\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1469 - mae: 3.6699 - val_loss: 22.1137 - val_mae: 3.5130\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0660 - mae: 3.6583 - val_loss: 22.1160 - val_mae: 3.5216\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1796 - mae: 3.6851 - val_loss: 22.2381 - val_mae: 3.4798\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1442 - mae: 3.6242 - val_loss: 22.2087 - val_mae: 3.5692\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1635 - mae: 3.6811 - val_loss: 22.1228 - val_mae: 3.4963\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1693 - mae: 3.7219 - val_loss: 22.1512 - val_mae: 3.4898\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0781 - mae: 3.6186 - val_loss: 22.0854 - val_mae: 3.5064\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 27.0249 - mae: 3.6569 - val_loss: 22.1188 - val_mae: 3.5416\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0165 - mae: 3.6420 - val_loss: 22.0781 - val_mae: 3.5173\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0075 - mae: 3.6397 - val_loss: 22.1010 - val_mae: 3.5416\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0310 - mae: 3.6919 - val_loss: 22.2147 - val_mae: 3.4852\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0635 - mae: 3.6636 - val_loss: 22.0877 - val_mae: 3.5250\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.9640 - mae: 3.6194 - val_loss: 22.1590 - val_mae: 3.4922\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8533 - mae: 3.6296 - val_loss: 22.1008 - val_mae: 3.5437\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.0403 - mae: 3.7227 - val_loss: 22.1834 - val_mae: 3.4854\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.9776 - mae: 3.6106 - val_loss: 22.0936 - val_mae: 3.5495\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 26.9802 - mae: 3.6583 - val_loss: 22.0834 - val_mae: 3.5080\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8932 - mae: 3.6349 - val_loss: 22.0962 - val_mae: 3.5055\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8343 - mae: 3.6381 - val_loss: 22.0763 - val_mae: 3.5127\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.8508 - mae: 3.6380 - val_loss: 22.1375 - val_mae: 3.5563\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.1485 - mae: 3.6200 - val_loss: 22.0646 - val_mae: 3.5326\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 20.6362 - mae: 3.3383\n",
      "Mean Absolute Error on Test Data: 3.3383374214172363\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.14311644376089194\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 14ms/step - loss: 8.1230 - mae: 2.0942 - val_loss: 8.1123 - val_mae: 2.0310\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.0512 - mae: 1.6248 - val_loss: 5.5412 - val_mae: 1.5308\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.0233 - mae: 1.3468 - val_loss: 4.1815 - val_mae: 1.5213\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8178 - mae: 1.4609 - val_loss: 4.1595 - val_mae: 1.5046\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7255 - mae: 1.3669 - val_loss: 4.2025 - val_mae: 1.4562\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6984 - mae: 1.3652 - val_loss: 4.1445 - val_mae: 1.4885\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6724 - mae: 1.3758 - val_loss: 4.1292 - val_mae: 1.4925\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6579 - mae: 1.3627 - val_loss: 4.1267 - val_mae: 1.4851\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.6450 - mae: 1.3916 - val_loss: 4.1332 - val_mae: 1.4751\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6159 - mae: 1.3457 - val_loss: 4.1026 - val_mae: 1.4880\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.5999 - mae: 1.3183 - val_loss: 4.0859 - val_mae: 1.4933\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.5657 - mae: 1.3874 - val_loss: 4.0745 - val_mae: 1.4946\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.5541 - mae: 1.3228 - val_loss: 4.1006 - val_mae: 1.4742\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.5402 - mae: 1.3719 - val_loss: 4.0502 - val_mae: 1.5001\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.5133 - mae: 1.3203 - val_loss: 4.0590 - val_mae: 1.4903\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4974 - mae: 1.3540 - val_loss: 4.0628 - val_mae: 1.4925\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3.4939 - mae: 1.3306 - val_loss: 4.0704 - val_mae: 1.4840\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4740 - mae: 1.3182 - val_loss: 4.0413 - val_mae: 1.5081\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4602 - mae: 1.3304 - val_loss: 4.0595 - val_mae: 1.4856\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4511 - mae: 1.3290 - val_loss: 4.0259 - val_mae: 1.5066\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4439 - mae: 1.3330 - val_loss: 4.0061 - val_mae: 1.5213\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4675 - mae: 1.3465 - val_loss: 4.0651 - val_mae: 1.4812\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4081 - mae: 1.3165 - val_loss: 3.9949 - val_mae: 1.5225\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3.4259 - mae: 1.3466 - val_loss: 4.0319 - val_mae: 1.4839\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3.4127 - mae: 1.3144 - val_loss: 3.9913 - val_mae: 1.5033\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4138 - mae: 1.3198 - val_loss: 3.9824 - val_mae: 1.5040\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4078 - mae: 1.3081 - val_loss: 3.9664 - val_mae: 1.5199\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4067 - mae: 1.3526 - val_loss: 3.9974 - val_mae: 1.4824\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3883 - mae: 1.3101 - val_loss: 3.9637 - val_mae: 1.4880\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.3845 - mae: 1.2990 - val_loss: 3.9457 - val_mae: 1.5176\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3798 - mae: 1.3225 - val_loss: 3.9701 - val_mae: 1.4775\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3514 - mae: 1.3096 - val_loss: 3.9438 - val_mae: 1.4887\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3509 - mae: 1.3065 - val_loss: 3.9446 - val_mae: 1.4799\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3377 - mae: 1.3171 - val_loss: 3.9143 - val_mae: 1.4899\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3476 - mae: 1.3016 - val_loss: 3.8998 - val_mae: 1.4936\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3208 - mae: 1.3054 - val_loss: 3.9382 - val_mae: 1.4590\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.3409 - mae: 1.3046 - val_loss: 3.9015 - val_mae: 1.4796\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3134 - mae: 1.3152 - val_loss: 3.8910 - val_mae: 1.4624\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3229 - mae: 1.2940 - val_loss: 3.8656 - val_mae: 1.5046\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2963 - mae: 1.3178 - val_loss: 3.9459 - val_mae: 1.4432\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.3030 - mae: 1.2778 - val_loss: 3.8533 - val_mae: 1.4704\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2843 - mae: 1.3048 - val_loss: 3.8783 - val_mae: 1.4574\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2774 - mae: 1.2962 - val_loss: 3.8402 - val_mae: 1.4866\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2660 - mae: 1.3073 - val_loss: 3.8645 - val_mae: 1.4485\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.2664 - mae: 1.2743 - val_loss: 3.8145 - val_mae: 1.4863\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2894 - mae: 1.3076 - val_loss: 3.8414 - val_mae: 1.4489\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2559 - mae: 1.3129 - val_loss: 3.8928 - val_mae: 1.4378\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.2621 - mae: 1.2853 - val_loss: 3.8597 - val_mae: 1.4400\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2604 - mae: 1.2752 - val_loss: 3.8177 - val_mae: 1.5127\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3.2599 - mae: 1.3037 - val_loss: 3.7804 - val_mae: 1.4601\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.5207 - mae: 1.3717\n",
      "Mean Absolute Error on Test Data: 1.3716751337051392\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.18057251136604657\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 22ms/step - loss: 1.8444 - mae: 0.9491 - val_loss: 1.6027 - val_mae: 0.7579\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1448 - mae: 0.7905 - val_loss: 1.6167 - val_mae: 0.9512\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.1462 - mae: 0.8268 - val_loss: 1.5175 - val_mae: 0.8833\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0738 - mae: 0.7744 - val_loss: 1.4841 - val_mae: 0.8351\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0668 - mae: 0.7508 - val_loss: 1.4678 - val_mae: 0.8232\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0570 - mae: 0.7615 - val_loss: 1.4549 - val_mae: 0.8425\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0414 - mae: 0.7497 - val_loss: 1.4339 - val_mae: 0.8190\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0308 - mae: 0.7439 - val_loss: 1.4184 - val_mae: 0.8206\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0216 - mae: 0.7549 - val_loss: 1.4045 - val_mae: 0.8306\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0095 - mae: 0.7399 - val_loss: 1.3832 - val_mae: 0.7960\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9967 - mae: 0.7354 - val_loss: 1.3802 - val_mae: 0.8267\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.9884 - mae: 0.7355 - val_loss: 1.3718 - val_mae: 0.8219\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0187 - mae: 0.7822 - val_loss: 1.3801 - val_mae: 0.8486\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9780 - mae: 0.7181 - val_loss: 1.3623 - val_mae: 0.7768\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9722 - mae: 0.7170 - val_loss: 1.3653 - val_mae: 0.8360\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9790 - mae: 0.7440 - val_loss: 1.3566 - val_mae: 0.8210\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9675 - mae: 0.7312 - val_loss: 1.3501 - val_mae: 0.8087\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9662 - mae: 0.7304 - val_loss: 1.3561 - val_mae: 0.8105\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.9603 - mae: 0.7193 - val_loss: 1.3650 - val_mae: 0.7951\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9639 - mae: 0.7153 - val_loss: 1.3589 - val_mae: 0.8185\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9622 - mae: 0.7377 - val_loss: 1.3580 - val_mae: 0.8240\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9642 - mae: 0.7267 - val_loss: 1.3610 - val_mae: 0.8228\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9554 - mae: 0.7262 - val_loss: 1.3650 - val_mae: 0.8283\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9685 - mae: 0.7523 - val_loss: 1.3771 - val_mae: 0.8201\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9602 - mae: 0.7183 - val_loss: 1.3801 - val_mae: 0.8190\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9535 - mae: 0.7305 - val_loss: 1.3772 - val_mae: 0.8210\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9517 - mae: 0.7347 - val_loss: 1.3789 - val_mae: 0.8206\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9512 - mae: 0.7189 - val_loss: 1.3748 - val_mae: 0.8232\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9569 - mae: 0.7427 - val_loss: 1.3706 - val_mae: 0.8337\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.9468 - mae: 0.7260 - val_loss: 1.3607 - val_mae: 0.8032\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9832 - mae: 0.6947 - val_loss: 1.3549 - val_mae: 0.8154\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9651 - mae: 0.7516 - val_loss: 1.3562 - val_mae: 0.8203\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9533 - mae: 0.7009 - val_loss: 1.3793 - val_mae: 0.7772\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9613 - mae: 0.7165 - val_loss: 1.3620 - val_mae: 0.8304\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9403 - mae: 0.7281 - val_loss: 1.3546 - val_mae: 0.7941\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9551 - mae: 0.6964 - val_loss: 1.3648 - val_mae: 0.8067\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9354 - mae: 0.7258 - val_loss: 1.3741 - val_mae: 0.8312\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9517 - mae: 0.7164 - val_loss: 1.3756 - val_mae: 0.8159\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9324 - mae: 0.7171 - val_loss: 1.3816 - val_mae: 0.8416\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9567 - mae: 0.7446 - val_loss: 1.3705 - val_mae: 0.8294\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.9462 - mae: 0.7422 - val_loss: 1.3614 - val_mae: 0.8094\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9696 - mae: 0.6955 - val_loss: 1.3639 - val_mae: 0.7965\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9354 - mae: 0.7211 - val_loss: 1.3686 - val_mae: 0.8276\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9345 - mae: 0.7150 - val_loss: 1.3663 - val_mae: 0.8088\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9302 - mae: 0.7161 - val_loss: 1.3639 - val_mae: 0.8124\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9295 - mae: 0.7098 - val_loss: 1.3691 - val_mae: 0.7894\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9335 - mae: 0.6982 - val_loss: 1.3685 - val_mae: 0.8157\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9347 - mae: 0.7259 - val_loss: 1.3616 - val_mae: 0.8174\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.9364 - mae: 0.7267 - val_loss: 1.3557 - val_mae: 0.8022\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.9280 - mae: 0.7005 - val_loss: 1.3502 - val_mae: 0.8060\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.5068 - mae: 0.7491\n",
      "Mean Absolute Error on Test Data: 0.7491419911384583\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.05703173254285987\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 12ms/step - loss: 29.5832 - mae: 3.9472 - val_loss: 15.4829 - val_mae: 2.9931\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 21.6517 - mae: 2.9944 - val_loss: 8.8730 - val_mae: 2.0805\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 15.0561 - mae: 2.4195 - val_loss: 6.8601 - val_mae: 1.9649\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.3437 - mae: 2.5326 - val_loss: 7.1153 - val_mae: 2.0365\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 13.1462 - mae: 2.4549 - val_loss: 6.7377 - val_mae: 1.9392\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.9750 - mae: 2.4386 - val_loss: 6.8205 - val_mae: 1.9683\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.8524 - mae: 2.4841 - val_loss: 6.7829 - val_mae: 1.9582\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.7694 - mae: 2.3921 - val_loss: 6.6129 - val_mae: 1.9064\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.5872 - mae: 2.4231 - val_loss: 6.7322 - val_mae: 1.9420\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.5111 - mae: 2.4511 - val_loss: 6.6624 - val_mae: 1.9219\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.3804 - mae: 2.3902 - val_loss: 6.5534 - val_mae: 1.8928\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 12.3202 - mae: 2.4070 - val_loss: 6.6296 - val_mae: 1.9136\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.1990 - mae: 2.3790 - val_loss: 6.6251 - val_mae: 1.9111\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.1207 - mae: 2.3701 - val_loss: 6.7903 - val_mae: 1.9527\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.0525 - mae: 2.4068 - val_loss: 6.6674 - val_mae: 1.9148\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.9853 - mae: 2.3592 - val_loss: 6.7645 - val_mae: 1.9377\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.9449 - mae: 2.3672 - val_loss: 6.8563 - val_mae: 1.9547\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 11.9423 - mae: 2.3460 - val_loss: 6.8028 - val_mae: 1.9377\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.9437 - mae: 2.4254 - val_loss: 6.7817 - val_mae: 1.9258\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.8693 - mae: 2.3243 - val_loss: 7.0209 - val_mae: 1.9808\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.8237 - mae: 2.3935 - val_loss: 6.9135 - val_mae: 1.9481\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7748 - mae: 2.3477 - val_loss: 6.9643 - val_mae: 1.9580\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 11.7534 - mae: 2.3514 - val_loss: 6.9328 - val_mae: 1.9460\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7628 - mae: 2.3448 - val_loss: 7.0978 - val_mae: 1.9760\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7473 - mae: 2.3406 - val_loss: 7.1667 - val_mae: 1.9863\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7192 - mae: 2.3576 - val_loss: 6.9794 - val_mae: 1.9476\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7546 - mae: 2.3759 - val_loss: 7.0047 - val_mae: 1.9458\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7153 - mae: 2.2983 - val_loss: 7.2945 - val_mae: 2.0032\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.7365 - mae: 2.3910 - val_loss: 6.9672 - val_mae: 1.9389\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 11.7044 - mae: 2.2966 - val_loss: 7.3230 - val_mae: 2.0071\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6828 - mae: 2.4093 - val_loss: 7.0644 - val_mae: 1.9507\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6409 - mae: 2.3189 - val_loss: 7.1477 - val_mae: 1.9655\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6526 - mae: 2.3124 - val_loss: 7.1506 - val_mae: 1.9669\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 11.6282 - mae: 2.3779 - val_loss: 7.3057 - val_mae: 1.9870\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5667 - mae: 2.3156 - val_loss: 7.0029 - val_mae: 1.9408\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6498 - mae: 2.3611 - val_loss: 7.0388 - val_mae: 1.9457\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5897 - mae: 2.3149 - val_loss: 7.1892 - val_mae: 1.9714\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6077 - mae: 2.3192 - val_loss: 7.3939 - val_mae: 2.0063\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6150 - mae: 2.3608 - val_loss: 7.2154 - val_mae: 1.9703\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5434 - mae: 2.3369 - val_loss: 7.2379 - val_mae: 1.9732\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5368 - mae: 2.3025 - val_loss: 7.3265 - val_mae: 1.9893\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 11.5406 - mae: 2.3517 - val_loss: 7.1116 - val_mae: 1.9497\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6630 - mae: 2.3709 - val_loss: 7.0573 - val_mae: 1.9462\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6351 - mae: 2.3215 - val_loss: 7.3791 - val_mae: 1.9991\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5883 - mae: 2.3019 - val_loss: 7.3145 - val_mae: 1.9925\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.6451 - mae: 2.3641 - val_loss: 7.0857 - val_mae: 1.9491\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5028 - mae: 2.3634 - val_loss: 7.1821 - val_mae: 1.9614\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.4454 - mae: 2.2853 - val_loss: 7.2436 - val_mae: 1.9727\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.4653 - mae: 2.3638 - val_loss: 7.1587 - val_mae: 1.9569\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5166 - mae: 2.3161 - val_loss: 7.1700 - val_mae: 1.9594\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.8688 - mae: 2.2787\n",
      "Mean Absolute Error on Test Data: 2.2787411212921143\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0904195189856164\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 15ms/step - loss: 10.8411 - mae: 2.0777 - val_loss: 4.5455 - val_mae: 1.4668\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 7.4104 - mae: 1.6220 - val_loss: 3.6589 - val_mae: 1.4512\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.8569 - mae: 1.7363 - val_loss: 3.7088 - val_mae: 1.4878\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.7753 - mae: 1.6849 - val_loss: 3.5286 - val_mae: 1.4169\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.7174 - mae: 1.6616 - val_loss: 3.5284 - val_mae: 1.4328\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.6622 - mae: 1.6904 - val_loss: 3.4766 - val_mae: 1.4177\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.6122 - mae: 1.6471 - val_loss: 3.4243 - val_mae: 1.3974\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5722 - mae: 1.6438 - val_loss: 3.4338 - val_mae: 1.4118\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5538 - mae: 1.6781 - val_loss: 3.3893 - val_mae: 1.3949\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5516 - mae: 1.5983 - val_loss: 3.3996 - val_mae: 1.4029\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4650 - mae: 1.6748 - val_loss: 3.4058 - val_mae: 1.4111\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.3979 - mae: 1.6328 - val_loss: 3.3730 - val_mae: 1.3958\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3896 - mae: 1.6281 - val_loss: 3.3323 - val_mae: 1.3711\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3699 - mae: 1.6280 - val_loss: 3.4556 - val_mae: 1.4409\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3264 - mae: 1.6051 - val_loss: 3.3875 - val_mae: 1.4093\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2992 - mae: 1.6257 - val_loss: 3.3439 - val_mae: 1.3849\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.2621 - mae: 1.5993 - val_loss: 3.4183 - val_mae: 1.4240\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2469 - mae: 1.6085 - val_loss: 3.3935 - val_mae: 1.4096\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2153 - mae: 1.6146 - val_loss: 3.4054 - val_mae: 1.4179\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1948 - mae: 1.5971 - val_loss: 3.3548 - val_mae: 1.3938\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1889 - mae: 1.6066 - val_loss: 3.3804 - val_mae: 1.4098\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1655 - mae: 1.5748 - val_loss: 3.4058 - val_mae: 1.4254\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1915 - mae: 1.6302 - val_loss: 3.3348 - val_mae: 1.3833\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1328 - mae: 1.5596 - val_loss: 3.4396 - val_mae: 1.4436\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1301 - mae: 1.6379 - val_loss: 3.4107 - val_mae: 1.4301\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1916 - mae: 1.5630 - val_loss: 3.5128 - val_mae: 1.4727\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1287 - mae: 1.6537 - val_loss: 3.3207 - val_mae: 1.3943\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1018 - mae: 1.5664 - val_loss: 3.4825 - val_mae: 1.4590\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1083 - mae: 1.6233 - val_loss: 3.3066 - val_mae: 1.3960\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0667 - mae: 1.6257 - val_loss: 3.3347 - val_mae: 1.4064\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0233 - mae: 1.5812 - val_loss: 3.2890 - val_mae: 1.3862\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0342 - mae: 1.6066 - val_loss: 3.3349 - val_mae: 1.4113\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9959 - mae: 1.5408 - val_loss: 3.3602 - val_mae: 1.4274\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9717 - mae: 1.6232 - val_loss: 3.2928 - val_mae: 1.3918\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9632 - mae: 1.5749 - val_loss: 3.3156 - val_mae: 1.4108\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9242 - mae: 1.5659 - val_loss: 3.3342 - val_mae: 1.4193\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9476 - mae: 1.5786 - val_loss: 3.4043 - val_mae: 1.4443\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9194 - mae: 1.5665 - val_loss: 3.2878 - val_mae: 1.4026\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9587 - mae: 1.6121 - val_loss: 3.3079 - val_mae: 1.3928\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8625 - mae: 1.5416 - val_loss: 3.5442 - val_mae: 1.4833\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8773 - mae: 1.6077 - val_loss: 3.3404 - val_mae: 1.4182\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 5.8356 - mae: 1.5548 - val_loss: 3.3448 - val_mae: 1.4280\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8396 - mae: 1.5875 - val_loss: 3.3110 - val_mae: 1.4016\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8054 - mae: 1.5677 - val_loss: 3.3241 - val_mae: 1.4167\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8369 - mae: 1.5449 - val_loss: 3.4226 - val_mae: 1.4525\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8504 - mae: 1.6013 - val_loss: 3.2536 - val_mae: 1.3617\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.8688 - mae: 1.5463 - val_loss: 3.5793 - val_mae: 1.4903\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.7621 - mae: 1.5680 - val_loss: 3.2587 - val_mae: 1.3915\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.7600 - mae: 1.5702 - val_loss: 3.3508 - val_mae: 1.4139\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.7265 - mae: 1.5632 - val_loss: 3.3581 - val_mae: 1.4282\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.5964 - mae: 1.5344\n",
      "Mean Absolute Error on Test Data: 1.5344454050064087\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.15322764167345404\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 18ms/step - loss: 4.7231 - mae: 1.6195 - val_loss: 4.3002 - val_mae: 1.3716\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.0493 - mae: 1.1271 - val_loss: 2.9243 - val_mae: 1.0879\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.2768 - mae: 1.0704 - val_loss: 2.7470 - val_mae: 1.1068\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.1983 - mae: 1.0548 - val_loss: 2.7147 - val_mae: 1.0757\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.1521 - mae: 1.0358 - val_loss: 2.6665 - val_mae: 1.0747\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.1102 - mae: 1.0218 - val_loss: 2.6345 - val_mae: 1.0608\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0795 - mae: 1.0170 - val_loss: 2.6045 - val_mae: 1.0652\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0558 - mae: 1.0033 - val_loss: 2.5925 - val_mae: 1.0506\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0438 - mae: 1.0099 - val_loss: 2.5803 - val_mae: 1.0416\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 2.0139 - mae: 1.0033 - val_loss: 2.5615 - val_mae: 1.0462\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 2.0022 - mae: 1.0007 - val_loss: 2.5538 - val_mae: 1.0390\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9964 - mae: 0.9830 - val_loss: 2.5482 - val_mae: 1.0336\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.9924 - mae: 1.0144 - val_loss: 2.5403 - val_mae: 1.0290\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9747 - mae: 0.9707 - val_loss: 2.5422 - val_mae: 1.0189\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9675 - mae: 0.9956 - val_loss: 2.5321 - val_mae: 1.0347\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9601 - mae: 0.9939 - val_loss: 2.5346 - val_mae: 1.0174\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9675 - mae: 0.9727 - val_loss: 2.5282 - val_mae: 1.0266\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.9559 - mae: 0.9960 - val_loss: 2.5353 - val_mae: 1.0155\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9581 - mae: 0.9756 - val_loss: 2.5354 - val_mae: 1.0125\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9620 - mae: 0.9966 - val_loss: 2.5259 - val_mae: 1.0328\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9484 - mae: 0.9861 - val_loss: 2.5601 - val_mae: 1.0061\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9563 - mae: 0.9719 - val_loss: 2.5289 - val_mae: 1.0386\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 1.9413 - mae: 0.9877 - val_loss: 2.5350 - val_mae: 1.0109\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 7ms/step - loss: 1.9353 - mae: 0.9798 - val_loss: 2.5205 - val_mae: 1.0246\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9325 - mae: 0.9815 - val_loss: 2.5367 - val_mae: 1.0114\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9316 - mae: 0.9707 - val_loss: 2.5267 - val_mae: 1.0263\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9422 - mae: 0.9829 - val_loss: 2.5203 - val_mae: 1.0280\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9459 - mae: 1.0004 - val_loss: 2.5498 - val_mae: 1.0074\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9367 - mae: 0.9775 - val_loss: 2.5203 - val_mae: 1.0169\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9290 - mae: 0.9768 - val_loss: 2.5302 - val_mae: 1.0146\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.9398 - mae: 0.9974 - val_loss: 2.5497 - val_mae: 1.0074\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9287 - mae: 0.9700 - val_loss: 2.5186 - val_mae: 1.0438\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9208 - mae: 0.9945 - val_loss: 2.5270 - val_mae: 1.0156\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9166 - mae: 0.9622 - val_loss: 2.5265 - val_mae: 1.0168\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9134 - mae: 0.9894 - val_loss: 2.5219 - val_mae: 1.0138\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.9242 - mae: 0.9779 - val_loss: 2.5312 - val_mae: 1.0082\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9040 - mae: 0.9624 - val_loss: 2.5131 - val_mae: 1.0365\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9376 - mae: 0.9969 - val_loss: 2.5755 - val_mae: 1.0093\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8993 - mae: 0.9718 - val_loss: 2.5148 - val_mae: 1.0489\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 6ms/step - loss: 1.9110 - mae: 0.9759 - val_loss: 2.5271 - val_mae: 1.0124\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.9245 - mae: 1.0040 - val_loss: 2.5258 - val_mae: 1.0117\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8973 - mae: 0.9628 - val_loss: 2.5154 - val_mae: 1.0269\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8949 - mae: 0.9858 - val_loss: 2.5297 - val_mae: 1.0169\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8970 - mae: 0.9650 - val_loss: 2.5088 - val_mae: 1.0282\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8884 - mae: 0.9675 - val_loss: 2.5023 - val_mae: 1.0301\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8902 - mae: 0.9819 - val_loss: 2.5399 - val_mae: 1.0186\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 1.8892 - mae: 0.9720 - val_loss: 2.5241 - val_mae: 1.0272\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8893 - mae: 0.9755 - val_loss: 2.5116 - val_mae: 1.0265\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8778 - mae: 0.9656 - val_loss: 2.5070 - val_mae: 1.0340\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.8854 - mae: 0.9794 - val_loss: 2.5571 - val_mae: 1.0135\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.9387 - mae: 1.0593\n",
      "Mean Absolute Error on Test Data: 1.0593024492263794\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12117841686649022\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 13ms/step - loss: 19.1591 - mae: 2.9461 - val_loss: 11.6076 - val_mae: 2.2468\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 12.6949 - mae: 2.1430 - val_loss: 7.4659 - val_mae: 1.8912\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.1545 - mae: 1.9981 - val_loss: 7.4377 - val_mae: 2.0484\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.2232 - mae: 2.0872 - val_loss: 7.1758 - val_mae: 1.9602\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 10.0378 - mae: 1.9765 - val_loss: 7.0790 - val_mae: 1.9149\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.9923 - mae: 1.9628 - val_loss: 7.0333 - val_mae: 1.9551\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 9.9184 - mae: 1.9681 - val_loss: 6.9295 - val_mae: 1.9225\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.8203 - mae: 2.0273 - val_loss: 6.9202 - val_mae: 1.9654\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.6831 - mae: 1.9713 - val_loss: 6.7300 - val_mae: 1.8799\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.7109 - mae: 1.9232 - val_loss: 6.7191 - val_mae: 1.9175\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5176 - mae: 1.9516 - val_loss: 6.6259 - val_mae: 1.8797\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.4533 - mae: 1.9317 - val_loss: 6.5790 - val_mae: 1.8860\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.3700 - mae: 1.9226 - val_loss: 6.5251 - val_mae: 1.8750\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 9.3232 - mae: 1.9393 - val_loss: 6.4676 - val_mae: 1.8654\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2378 - mae: 1.9084 - val_loss: 6.4862 - val_mae: 1.8957\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2704 - mae: 1.8909 - val_loss: 6.5512 - val_mae: 1.9385\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.1876 - mae: 1.9517 - val_loss: 6.4255 - val_mae: 1.8787\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0992 - mae: 1.9220 - val_loss: 6.4157 - val_mae: 1.8796\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0773 - mae: 1.9062 - val_loss: 6.4538 - val_mae: 1.9062\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0683 - mae: 1.9191 - val_loss: 6.5426 - val_mae: 1.9419\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0350 - mae: 1.9091 - val_loss: 6.4168 - val_mae: 1.8889\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.9872 - mae: 1.8961 - val_loss: 6.4381 - val_mae: 1.8903\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9.0083 - mae: 1.9354 - val_loss: 6.4164 - val_mae: 1.8903\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0050 - mae: 1.8935 - val_loss: 6.5158 - val_mae: 1.9323\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.9266 - mae: 1.8937 - val_loss: 6.4665 - val_mae: 1.9163\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.9047 - mae: 1.9193 - val_loss: 6.4312 - val_mae: 1.8856\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8638 - mae: 1.9036 - val_loss: 6.4668 - val_mae: 1.9278\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8417 - mae: 1.8902 - val_loss: 6.3904 - val_mae: 1.8890\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.8618 - mae: 1.8880 - val_loss: 6.5020 - val_mae: 1.9405\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8251 - mae: 1.8897 - val_loss: 6.4238 - val_mae: 1.9096\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8170 - mae: 1.9140 - val_loss: 6.3932 - val_mae: 1.8281\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8363 - mae: 1.8699 - val_loss: 6.4654 - val_mae: 1.9319\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7637 - mae: 1.9223 - val_loss: 6.3799 - val_mae: 1.8804\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7747 - mae: 1.8642 - val_loss: 6.4609 - val_mae: 1.9428\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.7236 - mae: 1.8758 - val_loss: 6.3882 - val_mae: 1.9200\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7086 - mae: 1.8757 - val_loss: 6.5138 - val_mae: 1.9658\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7418 - mae: 1.9082 - val_loss: 6.4232 - val_mae: 1.9295\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6821 - mae: 1.8774 - val_loss: 6.4170 - val_mae: 1.8780\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8928 - mae: 1.9380 - val_loss: 6.3720 - val_mae: 1.8365\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7863 - mae: 1.8487 - val_loss: 6.7050 - val_mae: 2.0364\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7460 - mae: 1.9048 - val_loss: 6.3303 - val_mae: 1.9111\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6519 - mae: 1.8826 - val_loss: 6.5295 - val_mae: 1.9818\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5741 - mae: 1.8880 - val_loss: 6.4275 - val_mae: 1.8116\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7003 - mae: 1.8599 - val_loss: 6.3734 - val_mae: 1.9257\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5610 - mae: 1.8577 - val_loss: 6.4306 - val_mae: 1.9386\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5155 - mae: 1.8612 - val_loss: 6.3845 - val_mae: 1.9408\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4956 - mae: 1.8993 - val_loss: 6.3552 - val_mae: 1.8799\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5022 - mae: 1.8535 - val_loss: 6.3934 - val_mae: 1.9474\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4587 - mae: 1.8587 - val_loss: 6.3685 - val_mae: 1.9324\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4949 - mae: 1.8433 - val_loss: 6.5288 - val_mae: 1.9972\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.1576 - mae: 2.0258\n",
      "Mean Absolute Error on Test Data: 2.0257797241210938\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.11862204435717072\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 13ms/step - loss: 119.8164 - mae: 8.9121 - val_loss: 100.3785 - val_mae: 7.7772\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 97.1963 - mae: 7.6139 - val_loss: 75.0530 - val_mae: 6.1146\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 65.1828 - mae: 5.6305 - val_loss: 45.2073 - val_mae: 4.1634\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4796 - mae: 4.3525 - val_loss: 39.0793 - val_mae: 4.5745\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.2212 - mae: 4.5626 - val_loss: 38.7220 - val_mae: 4.5224\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.7683 - mae: 4.4683 - val_loss: 37.9898 - val_mae: 4.3773\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 37.5074 - mae: 4.3990 - val_loss: 38.0634 - val_mae: 4.4519\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.3322 - mae: 4.4602 - val_loss: 37.9032 - val_mae: 4.4464\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 37.2400 - mae: 4.3894 - val_loss: 37.4743 - val_mae: 4.3783\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 36.9989 - mae: 4.4004 - val_loss: 37.2119 - val_mae: 4.3534\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.8989 - mae: 4.4031 - val_loss: 36.9692 - val_mae: 4.3370\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.7096 - mae: 4.3515 - val_loss: 36.7584 - val_mae: 4.3307\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.6661 - mae: 4.4156 - val_loss: 36.5468 - val_mae: 4.3193\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 36.5202 - mae: 4.3084 - val_loss: 36.3809 - val_mae: 4.3060\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.4658 - mae: 4.3879 - val_loss: 36.1037 - val_mae: 4.2463\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.2132 - mae: 4.3402 - val_loss: 36.1929 - val_mae: 4.3107\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.1726 - mae: 4.3297 - val_loss: 35.9232 - val_mae: 4.2721\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.0935 - mae: 4.3968 - val_loss: 35.9523 - val_mae: 4.2989\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 36.1143 - mae: 4.2745 - val_loss: 35.7172 - val_mae: 4.2624\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 35.9116 - mae: 4.3740 - val_loss: 35.8107 - val_mae: 4.3036\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.8479 - mae: 4.3133 - val_loss: 35.5417 - val_mae: 4.2500\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.8109 - mae: 4.3253 - val_loss: 35.3518 - val_mae: 4.2010\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.5999 - mae: 4.3026 - val_loss: 35.5939 - val_mae: 4.2851\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.6557 - mae: 4.3426 - val_loss: 35.3538 - val_mae: 4.2358\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.7321 - mae: 4.3408 - val_loss: 35.2332 - val_mae: 4.2012\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5644 - mae: 4.2863 - val_loss: 35.2826 - val_mae: 4.2381\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 35.4769 - mae: 4.3378 - val_loss: 35.1142 - val_mae: 4.1970\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5936 - mae: 4.2623 - val_loss: 35.3141 - val_mae: 4.2651\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4495 - mae: 4.3164 - val_loss: 35.2009 - val_mae: 4.2483\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 35.4918 - mae: 4.3372 - val_loss: 34.8861 - val_mae: 4.1301\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.4500 - mae: 4.2989 - val_loss: 35.0036 - val_mae: 4.1876\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.5067 - mae: 4.3109 - val_loss: 35.4117 - val_mae: 4.3003\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3926 - mae: 4.2842 - val_loss: 34.9659 - val_mae: 4.1944\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.2300 - mae: 4.2973 - val_loss: 35.1389 - val_mae: 4.2465\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3021 - mae: 4.3052 - val_loss: 35.1290 - val_mae: 4.2544\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.2271 - mae: 4.3063 - val_loss: 34.8593 - val_mae: 4.1734\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3292 - mae: 4.2614 - val_loss: 34.7933 - val_mae: 4.1567\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3159 - mae: 4.3614 - val_loss: 34.7753 - val_mae: 4.1560\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3843 - mae: 4.2199 - val_loss: 34.8970 - val_mae: 4.2108\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.2599 - mae: 4.3299 - val_loss: 34.8509 - val_mae: 4.1949\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.1454 - mae: 4.3000 - val_loss: 34.8224 - val_mae: 4.1829\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0668 - mae: 4.2656 - val_loss: 34.7962 - val_mae: 4.1751\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0794 - mae: 4.2799 - val_loss: 34.8325 - val_mae: 4.1940\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.1736 - mae: 4.3276 - val_loss: 34.6012 - val_mae: 4.0931\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 35.1582 - mae: 4.2340 - val_loss: 34.6472 - val_mae: 4.1344\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0412 - mae: 4.3019 - val_loss: 34.7232 - val_mae: 4.1678\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0075 - mae: 4.2691 - val_loss: 34.8456 - val_mae: 4.2135\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9463 - mae: 4.3145 - val_loss: 34.6353 - val_mae: 4.1414\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.9789 - mae: 4.2084 - val_loss: 34.6800 - val_mae: 4.1520\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0680 - mae: 4.3484 - val_loss: 34.7481 - val_mae: 4.1836\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 42.6038 - mae: 4.0560\n",
      "Mean Absolute Error on Test Data: 4.056008815765381\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.11932201913670859\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 11ms/step - loss: 16.5050 - mae: 2.8480 - val_loss: 9.4289 - val_mae: 2.1904\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 11.2006 - mae: 2.0933 - val_loss: 5.3879 - val_mae: 1.7463\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 8.3732 - mae: 1.9242 - val_loss: 5.1035 - val_mae: 1.8281\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 8.1022 - mae: 1.9876 - val_loss: 4.9600 - val_mae: 1.7934\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 8.0115 - mae: 1.9130 - val_loss: 4.8257 - val_mae: 1.7544\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.9298 - mae: 1.9105 - val_loss: 4.7756 - val_mae: 1.7496\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.8674 - mae: 1.9125 - val_loss: 4.7140 - val_mae: 1.7388\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.8120 - mae: 1.9080 - val_loss: 4.6585 - val_mae: 1.7265\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.7727 - mae: 1.8871 - val_loss: 4.6489 - val_mae: 1.7263\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.7299 - mae: 1.9123 - val_loss: 4.5723 - val_mae: 1.7032\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.7123 - mae: 1.8809 - val_loss: 4.5864 - val_mae: 1.7097\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.6625 - mae: 1.8856 - val_loss: 4.5424 - val_mae: 1.6938\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.6381 - mae: 1.8778 - val_loss: 4.5028 - val_mae: 1.6758\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5845 - mae: 1.8599 - val_loss: 4.5482 - val_mae: 1.6935\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5811 - mae: 1.8976 - val_loss: 4.5749 - val_mae: 1.6987\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.6028 - mae: 1.8439 - val_loss: 4.5028 - val_mae: 1.6725\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5473 - mae: 1.8671 - val_loss: 4.5510 - val_mae: 1.6865\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5314 - mae: 1.8954 - val_loss: 4.5541 - val_mae: 1.6837\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5092 - mae: 1.8576 - val_loss: 4.5120 - val_mae: 1.6626\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.5107 - mae: 1.8562 - val_loss: 4.5581 - val_mae: 1.6784\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4875 - mae: 1.8560 - val_loss: 4.5616 - val_mae: 1.6771\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4952 - mae: 1.8813 - val_loss: 4.5645 - val_mae: 1.6748\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4728 - mae: 1.8396 - val_loss: 4.5366 - val_mae: 1.6601\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.4667 - mae: 1.8614 - val_loss: 4.6035 - val_mae: 1.6816\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4474 - mae: 1.8668 - val_loss: 4.5845 - val_mae: 1.6754\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4775 - mae: 1.8409 - val_loss: 4.6221 - val_mae: 1.6858\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4650 - mae: 1.8733 - val_loss: 4.5520 - val_mae: 1.6526\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4684 - mae: 1.8623 - val_loss: 4.5693 - val_mae: 1.6652\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4215 - mae: 1.8525 - val_loss: 4.6041 - val_mae: 1.6753\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4481 - mae: 1.8796 - val_loss: 4.5770 - val_mae: 1.6601\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4302 - mae: 1.8226 - val_loss: 4.6083 - val_mae: 1.6755\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3879 - mae: 1.8611 - val_loss: 4.6234 - val_mae: 1.6800\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3770 - mae: 1.8517 - val_loss: 4.5992 - val_mae: 1.6690\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3955 - mae: 1.8703 - val_loss: 4.5772 - val_mae: 1.6573\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3748 - mae: 1.8194 - val_loss: 4.6207 - val_mae: 1.6761\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.4315 - mae: 1.8816 - val_loss: 4.5868 - val_mae: 1.6539\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3626 - mae: 1.8431 - val_loss: 4.6304 - val_mae: 1.6790\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3535 - mae: 1.8339 - val_loss: 4.6190 - val_mae: 1.6759\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3419 - mae: 1.8469 - val_loss: 4.6476 - val_mae: 1.6822\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3376 - mae: 1.8724 - val_loss: 4.6005 - val_mae: 1.6653\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3588 - mae: 1.8528 - val_loss: 4.6194 - val_mae: 1.6725\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3554 - mae: 1.8490 - val_loss: 4.6270 - val_mae: 1.6752\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.4509 - mae: 1.8258 - val_loss: 4.8035 - val_mae: 1.7201\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.3837 - mae: 1.8612 - val_loss: 4.6114 - val_mae: 1.6667\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.2934 - mae: 1.8665 - val_loss: 4.6548 - val_mae: 1.6845\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.2821 - mae: 1.8335 - val_loss: 4.6581 - val_mae: 1.6843\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.2874 - mae: 1.8243 - val_loss: 4.6192 - val_mae: 1.6726\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.3220 - mae: 1.8891 - val_loss: 4.6134 - val_mae: 1.6629\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.2960 - mae: 1.8280 - val_loss: 4.6309 - val_mae: 1.6754\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 7.2340 - mae: 1.8531 - val_loss: 4.6193 - val_mae: 1.6675\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 4.5171 - mae: 1.6010\n",
      "Mean Absolute Error on Test Data: 1.6010111570358276\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.14658419797790279\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 13ms/step - loss: 31.4778 - mae: 4.6512 - val_loss: 32.4851 - val_mae: 4.5157\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9682 - mae: 3.4947 - val_loss: 19.9281 - val_mae: 3.1102\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 11.3410 - mae: 2.4349 - val_loss: 12.3891 - val_mae: 2.4119\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6721 - mae: 2.4298 - val_loss: 12.2538 - val_mae: 2.4195\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.4371 - mae: 2.3472 - val_loss: 12.4670 - val_mae: 2.3939\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.3529 - mae: 2.3406 - val_loss: 12.2170 - val_mae: 2.4010\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 9.2744 - mae: 2.3302 - val_loss: 12.2680 - val_mae: 2.3892\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.2145 - mae: 2.3180 - val_loss: 12.2796 - val_mae: 2.3851\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.1633 - mae: 2.3083 - val_loss: 12.1836 - val_mae: 2.3913\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.1312 - mae: 2.3061 - val_loss: 12.1788 - val_mae: 2.3884\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.0869 - mae: 2.3205 - val_loss: 12.2050 - val_mae: 2.3840\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.0259 - mae: 2.2920 - val_loss: 12.2081 - val_mae: 2.3838\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 9.0070 - mae: 2.2839 - val_loss: 12.1547 - val_mae: 2.3924\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.9578 - mae: 2.2798 - val_loss: 12.2032 - val_mae: 2.3872\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 8.9273 - mae: 2.2911 - val_loss: 12.1940 - val_mae: 2.3854\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8977 - mae: 2.2699 - val_loss: 12.2217 - val_mae: 2.3857\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8960 - mae: 2.2857 - val_loss: 12.3142 - val_mae: 2.3809\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8694 - mae: 2.2437 - val_loss: 12.2871 - val_mae: 2.3841\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 8.9194 - mae: 2.3053 - val_loss: 12.3386 - val_mae: 2.3827\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8505 - mae: 2.2412 - val_loss: 12.2174 - val_mae: 2.3930\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.8779 - mae: 2.2829 - val_loss: 12.4973 - val_mae: 2.3829\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7990 - mae: 2.2603 - val_loss: 12.2377 - val_mae: 2.3970\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7583 - mae: 2.2494 - val_loss: 12.3856 - val_mae: 2.3853\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.7659 - mae: 2.2643 - val_loss: 12.3218 - val_mae: 2.3915\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7229 - mae: 2.2354 - val_loss: 12.3008 - val_mae: 2.3928\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7304 - mae: 2.2504 - val_loss: 12.3259 - val_mae: 2.3926\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7417 - mae: 2.2609 - val_loss: 12.4407 - val_mae: 2.3888\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7163 - mae: 2.2423 - val_loss: 12.4149 - val_mae: 2.3935\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.6595 - mae: 2.2303 - val_loss: 12.3190 - val_mae: 2.4031\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6885 - mae: 2.2297 - val_loss: 12.3040 - val_mae: 2.4036\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.7251 - mae: 2.2699 - val_loss: 12.4956 - val_mae: 2.3934\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6621 - mae: 2.2359 - val_loss: 12.3479 - val_mae: 2.4046\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6559 - mae: 2.2345 - val_loss: 12.2993 - val_mae: 2.4100\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6618 - mae: 2.2185 - val_loss: 12.3305 - val_mae: 2.4093\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 8.6262 - mae: 2.2508 - val_loss: 12.3449 - val_mae: 2.4043\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6057 - mae: 2.2216 - val_loss: 12.3628 - val_mae: 2.4054\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5984 - mae: 2.2317 - val_loss: 12.3507 - val_mae: 2.4089\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6364 - mae: 2.2413 - val_loss: 12.5416 - val_mae: 2.3981\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 8.6000 - mae: 2.2223 - val_loss: 12.2375 - val_mae: 2.4312\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6185 - mae: 2.2341 - val_loss: 12.3322 - val_mae: 2.4073\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5817 - mae: 2.2517 - val_loss: 12.4586 - val_mae: 2.4010\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.6103 - mae: 2.2126 - val_loss: 12.4309 - val_mae: 2.4024\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5428 - mae: 2.2237 - val_loss: 12.3337 - val_mae: 2.4134\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5574 - mae: 2.2264 - val_loss: 12.3787 - val_mae: 2.4030\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5602 - mae: 2.2133 - val_loss: 12.3201 - val_mae: 2.4111\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5317 - mae: 2.2298 - val_loss: 12.3945 - val_mae: 2.4020\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5441 - mae: 2.2287 - val_loss: 12.3870 - val_mae: 2.4032\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 8.5194 - mae: 2.2089 - val_loss: 12.4137 - val_mae: 2.4029\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5525 - mae: 2.2488 - val_loss: 12.5193 - val_mae: 2.4006\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.5358 - mae: 2.1971 - val_loss: 12.3484 - val_mae: 2.4089\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.1166 - mae: 2.1327\n",
      "Mean Absolute Error on Test Data: 2.13273286819458\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08553345298460724\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 71.4046 - mae: 6.9320 - val_loss: 60.3654 - val_mae: 6.2242\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 50.5329 - mae: 5.3358 - val_loss: 37.8486 - val_mae: 4.4034\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 28.9949 - mae: 3.5056 - val_loss: 21.9038 - val_mae: 3.5478\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.0892 - mae: 3.3170 - val_loss: 21.5388 - val_mae: 3.7211\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.1287 - mae: 3.3082 - val_loss: 21.3439 - val_mae: 3.6353\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.9485 - mae: 3.2829 - val_loss: 21.3038 - val_mae: 3.6296\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.8121 - mae: 3.2335 - val_loss: 21.2626 - val_mae: 3.6359\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.7319 - mae: 3.2983 - val_loss: 21.2189 - val_mae: 3.6276\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.6653 - mae: 3.2275 - val_loss: 21.1771 - val_mae: 3.6429\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4998 - mae: 3.2496 - val_loss: 21.1488 - val_mae: 3.6083\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4180 - mae: 3.2263 - val_loss: 21.1106 - val_mae: 3.6241\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4039 - mae: 3.2734 - val_loss: 21.0447 - val_mae: 3.6199\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3471 - mae: 3.2197 - val_loss: 20.9945 - val_mae: 3.6240\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1993 - mae: 3.2428 - val_loss: 20.9662 - val_mae: 3.6179\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1529 - mae: 3.2379 - val_loss: 20.9636 - val_mae: 3.5873\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.1067 - mae: 3.2018 - val_loss: 20.9091 - val_mae: 3.6261\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1070 - mae: 3.2573 - val_loss: 20.8510 - val_mae: 3.5939\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0363 - mae: 3.2136 - val_loss: 20.8163 - val_mae: 3.6032\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0098 - mae: 3.2352 - val_loss: 20.8311 - val_mae: 3.5709\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9492 - mae: 3.2230 - val_loss: 20.7458 - val_mae: 3.5764\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.9528 - mae: 3.1972 - val_loss: 20.7350 - val_mae: 3.6132\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9833 - mae: 3.2527 - val_loss: 20.7270 - val_mae: 3.5540\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7663 - mae: 3.1744 - val_loss: 20.6219 - val_mae: 3.5957\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8092 - mae: 3.2654 - val_loss: 20.5931 - val_mae: 3.5616\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8447 - mae: 3.1824 - val_loss: 20.5561 - val_mae: 3.5857\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7562 - mae: 3.1933 - val_loss: 20.4936 - val_mae: 3.5659\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7565 - mae: 3.2367 - val_loss: 20.4746 - val_mae: 3.5487\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7916 - mae: 3.1788 - val_loss: 20.4544 - val_mae: 3.5276\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6271 - mae: 3.2222 - val_loss: 20.3738 - val_mae: 3.5656\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6803 - mae: 3.2029 - val_loss: 20.3703 - val_mae: 3.5793\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.7732 - mae: 3.1794 - val_loss: 20.2983 - val_mae: 3.5185\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7781 - mae: 3.2846 - val_loss: 20.2908 - val_mae: 3.5258\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6292 - mae: 3.1304 - val_loss: 20.1988 - val_mae: 3.5278\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6248 - mae: 3.2203 - val_loss: 20.1806 - val_mae: 3.5216\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6315 - mae: 3.2432 - val_loss: 20.1540 - val_mae: 3.5089\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.7322 - mae: 3.1362 - val_loss: 20.0626 - val_mae: 3.5364\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4899 - mae: 3.1866 - val_loss: 20.0508 - val_mae: 3.5140\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6237 - mae: 3.2390 - val_loss: 20.2027 - val_mae: 3.4671\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5908 - mae: 3.0967 - val_loss: 19.9303 - val_mae: 3.5390\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6879 - mae: 3.2727 - val_loss: 20.0144 - val_mae: 3.4933\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.4876 - mae: 3.1887 - val_loss: 19.9781 - val_mae: 3.4924\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4250 - mae: 3.1336 - val_loss: 19.8210 - val_mae: 3.5115\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4337 - mae: 3.1932 - val_loss: 19.8240 - val_mae: 3.4948\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3476 - mae: 3.1838 - val_loss: 19.8208 - val_mae: 3.4978\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4554 - mae: 3.1263 - val_loss: 19.7724 - val_mae: 3.5251\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4159 - mae: 3.2358 - val_loss: 19.7785 - val_mae: 3.4716\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5417 - mae: 3.1722 - val_loss: 19.7067 - val_mae: 3.5152\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3973 - mae: 3.1462 - val_loss: 19.6872 - val_mae: 3.5062\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3244 - mae: 3.1908 - val_loss: 19.6854 - val_mae: 3.4790\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.2867 - mae: 3.1415 - val_loss: 19.6256 - val_mae: 3.4868\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 35.1023 - mae: 3.8145\n",
      "Mean Absolute Error on Test Data: 3.814530611038208\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.11149123174861497\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 2s 14ms/step - loss: 14.3827 - mae: 2.8092 - val_loss: 11.4404 - val_mae: 2.2835\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 9.0472 - mae: 2.0123 - val_loss: 7.0247 - val_mae: 1.8194\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3394 - mae: 1.8170 - val_loss: 6.4365 - val_mae: 1.9207\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2212 - mae: 1.8747 - val_loss: 6.3214 - val_mae: 1.8573\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1095 - mae: 1.8279 - val_loss: 6.3056 - val_mae: 1.8475\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.0636 - mae: 1.8081 - val_loss: 6.2911 - val_mae: 1.8404\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.0326 - mae: 1.8006 - val_loss: 6.2834 - val_mae: 1.8424\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.9950 - mae: 1.8071 - val_loss: 6.2592 - val_mae: 1.8297\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.9576 - mae: 1.7860 - val_loss: 6.2286 - val_mae: 1.8143\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.9637 - mae: 1.8092 - val_loss: 6.2129 - val_mae: 1.8174\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 5.8944 - mae: 1.7632 - val_loss: 6.1960 - val_mae: 1.7994\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.8780 - mae: 1.7803 - val_loss: 6.1909 - val_mae: 1.8086\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.8612 - mae: 1.7768 - val_loss: 6.1941 - val_mae: 1.8020\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.8529 - mae: 1.7436 - val_loss: 6.1792 - val_mae: 1.7977\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 5.8268 - mae: 1.7865 - val_loss: 6.1620 - val_mae: 1.8069\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.8195 - mae: 1.7692 - val_loss: 6.1692 - val_mae: 1.8070\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7902 - mae: 1.7651 - val_loss: 6.1348 - val_mae: 1.7886\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7759 - mae: 1.7513 - val_loss: 6.1209 - val_mae: 1.7870\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7699 - mae: 1.7520 - val_loss: 6.1384 - val_mae: 1.8057\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7700 - mae: 1.7667 - val_loss: 6.0991 - val_mae: 1.7836\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7559 - mae: 1.7443 - val_loss: 6.1357 - val_mae: 1.8026\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7417 - mae: 1.7460 - val_loss: 6.1107 - val_mae: 1.7952\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7311 - mae: 1.7645 - val_loss: 6.1055 - val_mae: 1.8084\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7290 - mae: 1.7572 - val_loss: 6.0655 - val_mae: 1.7711\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7287 - mae: 1.7632 - val_loss: 6.0458 - val_mae: 1.7793\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7074 - mae: 1.7257 - val_loss: 6.0798 - val_mae: 1.8019\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 5.6973 - mae: 1.7692 - val_loss: 6.0454 - val_mae: 1.7720\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6852 - mae: 1.7248 - val_loss: 6.0627 - val_mae: 1.7851\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.7381 - mae: 1.7675 - val_loss: 6.0132 - val_mae: 1.7582\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6763 - mae: 1.7323 - val_loss: 6.0065 - val_mae: 1.7797\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6564 - mae: 1.7444 - val_loss: 6.0106 - val_mae: 1.7726\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6998 - mae: 1.7605 - val_loss: 6.0282 - val_mae: 1.7906\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6499 - mae: 1.7154 - val_loss: 6.0356 - val_mae: 1.7776\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6569 - mae: 1.7414 - val_loss: 5.9934 - val_mae: 1.7766\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6515 - mae: 1.7220 - val_loss: 6.0868 - val_mae: 1.8152\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6454 - mae: 1.7770 - val_loss: 6.0045 - val_mae: 1.7547\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6474 - mae: 1.7015 - val_loss: 6.0002 - val_mae: 1.7993\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5994 - mae: 1.7481 - val_loss: 5.9822 - val_mae: 1.7755\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5958 - mae: 1.7070 - val_loss: 6.0176 - val_mae: 1.7973\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 5.5968 - mae: 1.7484 - val_loss: 5.9284 - val_mae: 1.7774\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.6017 - mae: 1.7312 - val_loss: 6.0124 - val_mae: 1.8058\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5659 - mae: 1.7235 - val_loss: 5.9744 - val_mae: 1.7827\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5654 - mae: 1.7048 - val_loss: 5.9975 - val_mae: 1.8049\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5680 - mae: 1.7488 - val_loss: 5.9338 - val_mae: 1.7659\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5519 - mae: 1.7142 - val_loss: 5.9728 - val_mae: 1.7913\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5648 - mae: 1.7272 - val_loss: 5.9306 - val_mae: 1.7887\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5681 - mae: 1.7004 - val_loss: 6.0150 - val_mae: 1.8220\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 5.5599 - mae: 1.7379 - val_loss: 5.9455 - val_mae: 1.7493\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5352 - mae: 1.7150 - val_loss: 5.9374 - val_mae: 1.8034\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 5.5545 - mae: 1.7017 - val_loss: 5.9699 - val_mae: 1.8158\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.4381 - mae: 1.8761\n",
      "Mean Absolute Error on Test Data: 1.8761011362075806\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.13282473213760115\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 32.4098 - mae: 4.2017 - val_loss: 26.3699 - val_mae: 3.4504\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4875 - mae: 3.1877 - val_loss: 18.0055 - val_mae: 2.5471\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8946 - mae: 2.5813 - val_loss: 14.8694 - val_mae: 2.6055\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.6329 - mae: 2.7000 - val_loss: 14.7891 - val_mae: 2.5956\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 14.4898 - mae: 2.6077 - val_loss: 14.6572 - val_mae: 2.5454\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3309 - mae: 2.6175 - val_loss: 14.5977 - val_mae: 2.5729\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2631 - mae: 2.6176 - val_loss: 14.5030 - val_mae: 2.5589\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1429 - mae: 2.5879 - val_loss: 14.4199 - val_mae: 2.5319\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0820 - mae: 2.6103 - val_loss: 14.3738 - val_mae: 2.5443\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.9734 - mae: 2.6028 - val_loss: 14.2995 - val_mae: 2.5231\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0017 - mae: 2.5516 - val_loss: 14.3220 - val_mae: 2.5699\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8342 - mae: 2.6278 - val_loss: 14.2605 - val_mae: 2.5643\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7948 - mae: 2.5513 - val_loss: 14.1432 - val_mae: 2.5032\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7259 - mae: 2.6094 - val_loss: 14.1051 - val_mae: 2.5188\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6266 - mae: 2.5652 - val_loss: 14.0755 - val_mae: 2.5187\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.7494 - mae: 2.6428 - val_loss: 14.0077 - val_mae: 2.4810\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5370 - mae: 2.5378 - val_loss: 14.0405 - val_mae: 2.5402\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5444 - mae: 2.6640 - val_loss: 14.0371 - val_mae: 2.5473\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4630 - mae: 2.5454 - val_loss: 13.9300 - val_mae: 2.5002\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4132 - mae: 2.6216 - val_loss: 13.9395 - val_mae: 2.5227\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.4223 - mae: 2.5550 - val_loss: 13.9515 - val_mae: 2.5421\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4093 - mae: 2.6188 - val_loss: 13.8204 - val_mae: 2.4623\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4082 - mae: 2.5097 - val_loss: 13.7968 - val_mae: 2.4634\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2700 - mae: 2.5418 - val_loss: 13.8417 - val_mae: 2.5105\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.2317 - mae: 2.5614 - val_loss: 13.7851 - val_mae: 2.4884\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.1949 - mae: 2.5419 - val_loss: 13.7715 - val_mae: 2.4846\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1830 - mae: 2.5413 - val_loss: 13.7468 - val_mae: 2.4787\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1521 - mae: 2.5698 - val_loss: 13.7904 - val_mae: 2.5045\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1220 - mae: 2.5274 - val_loss: 13.6857 - val_mae: 2.4493\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1358 - mae: 2.5430 - val_loss: 13.7564 - val_mae: 2.4979\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.1048 - mae: 2.5486 - val_loss: 13.6901 - val_mae: 2.4721\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.1364 - mae: 2.5960 - val_loss: 13.6953 - val_mae: 2.4738\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0874 - mae: 2.5354 - val_loss: 13.6799 - val_mae: 2.4791\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0411 - mae: 2.5415 - val_loss: 13.6387 - val_mae: 2.4673\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0167 - mae: 2.5230 - val_loss: 13.6354 - val_mae: 2.4752\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0314 - mae: 2.5613 - val_loss: 13.6149 - val_mae: 2.4686\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.0094 - mae: 2.5088 - val_loss: 13.5897 - val_mae: 2.4507\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.0300 - mae: 2.5242 - val_loss: 13.5712 - val_mae: 2.4493\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9567 - mae: 2.5017 - val_loss: 13.5499 - val_mae: 2.4485\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9308 - mae: 2.5145 - val_loss: 13.6672 - val_mae: 2.5044\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.2242 - mae: 2.6375 - val_loss: 13.5837 - val_mae: 2.4727\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9256 - mae: 2.5138 - val_loss: 13.5417 - val_mae: 2.4540\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9333 - mae: 2.4926 - val_loss: 13.5526 - val_mae: 2.4693\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9004 - mae: 2.5552 - val_loss: 13.5130 - val_mae: 2.4601\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.9315 - mae: 2.4810 - val_loss: 13.5056 - val_mae: 2.4611\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8572 - mae: 2.5267 - val_loss: 13.4476 - val_mae: 2.4463\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8448 - mae: 2.5086 - val_loss: 13.4494 - val_mae: 2.4565\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8341 - mae: 2.5151 - val_loss: 13.3849 - val_mae: 2.4101\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8086 - mae: 2.4780 - val_loss: 13.4995 - val_mae: 2.4770\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 12.8488 - mae: 2.5449 - val_loss: 13.4051 - val_mae: 2.4458\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 13.2109 - mae: 2.5942\n",
      "Mean Absolute Error on Test Data: 2.5941905975341797\n",
      "7/7 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.10177599068714627\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 71.5788 - mae: 6.9066 - val_loss: 69.4050 - val_mae: 6.6995\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 56.3888 - mae: 5.7854 - val_loss: 49.5263 - val_mae: 5.1820\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.7880 - mae: 3.9954 - val_loss: 26.5828 - val_mae: 3.5179\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.2433 - mae: 3.4028 - val_loss: 23.3931 - val_mae: 3.5885\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5311 - mae: 3.4268 - val_loss: 22.8747 - val_mae: 3.4397\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0688 - mae: 3.3357 - val_loss: 22.5210 - val_mae: 3.4362\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.7115 - mae: 3.3038 - val_loss: 22.2944 - val_mae: 3.4113\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 20.3892 - mae: 3.2867 - val_loss: 22.0695 - val_mae: 3.3971\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2328 - mae: 3.2572 - val_loss: 21.8830 - val_mae: 3.4022\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0263 - mae: 3.3057 - val_loss: 21.7145 - val_mae: 3.4048\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.8231 - mae: 3.2301 - val_loss: 21.6573 - val_mae: 3.3650\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.7426 - mae: 3.2540 - val_loss: 21.5528 - val_mae: 3.3603\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 19.5456 - mae: 3.2266 - val_loss: 21.4352 - val_mae: 3.3608\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.4317 - mae: 3.2365 - val_loss: 21.3991 - val_mae: 3.3464\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.3581 - mae: 3.1862 - val_loss: 21.3020 - val_mae: 3.3543\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.2861 - mae: 3.2528 - val_loss: 21.2721 - val_mae: 3.3433\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 19.2348 - mae: 3.1730 - val_loss: 21.2412 - val_mae: 3.3447\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1448 - mae: 3.2266 - val_loss: 21.2080 - val_mae: 3.3399\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1181 - mae: 3.2007 - val_loss: 21.1917 - val_mae: 3.3322\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.1740 - mae: 3.2041 - val_loss: 21.1440 - val_mae: 3.3697\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9865 - mae: 3.2164 - val_loss: 21.3152 - val_mae: 3.3243\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9267 - mae: 3.1514 - val_loss: 21.0638 - val_mae: 3.3378\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.9697 - mae: 3.2343 - val_loss: 21.1141 - val_mae: 3.3274\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0157 - mae: 3.1642 - val_loss: 21.0720 - val_mae: 3.3756\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.8319 - mae: 3.1923 - val_loss: 21.0344 - val_mae: 3.3369\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7388 - mae: 3.1518 - val_loss: 21.0223 - val_mae: 3.3325\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7142 - mae: 3.1809 - val_loss: 20.9688 - val_mae: 3.3392\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6636 - mae: 3.1788 - val_loss: 21.0102 - val_mae: 3.3374\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6169 - mae: 3.1521 - val_loss: 20.9843 - val_mae: 3.3383\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6280 - mae: 3.1814 - val_loss: 21.0338 - val_mae: 3.3299\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6533 - mae: 3.1254 - val_loss: 20.9552 - val_mae: 3.3423\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.5547 - mae: 3.2113 - val_loss: 21.0893 - val_mae: 3.3305\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.6471 - mae: 3.1296 - val_loss: 20.9193 - val_mae: 3.3470\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4565 - mae: 3.1464 - val_loss: 21.0176 - val_mae: 3.3327\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4162 - mae: 3.1210 - val_loss: 20.9502 - val_mae: 3.3648\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4042 - mae: 3.1883 - val_loss: 21.0453 - val_mae: 3.3364\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3995 - mae: 3.1497 - val_loss: 20.8798 - val_mae: 3.3371\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4170 - mae: 3.0827 - val_loss: 20.8596 - val_mae: 3.3417\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.3496 - mae: 3.1969 - val_loss: 20.8355 - val_mae: 3.3531\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3458 - mae: 3.1215 - val_loss: 20.8331 - val_mae: 3.3516\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3101 - mae: 3.1741 - val_loss: 20.9278 - val_mae: 3.3371\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2896 - mae: 3.1188 - val_loss: 20.8374 - val_mae: 3.3322\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.1914 - mae: 3.1417 - val_loss: 20.9591 - val_mae: 3.3191\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.2098 - mae: 3.0998 - val_loss: 20.7336 - val_mae: 3.3497\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.1155 - mae: 3.1369 - val_loss: 20.7183 - val_mae: 3.3177\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.0530 - mae: 3.0911 - val_loss: 20.6714 - val_mae: 3.3356\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.1138 - mae: 3.1844 - val_loss: 20.9301 - val_mae: 3.3202\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 18.2185 - mae: 3.0747 - val_loss: 20.6051 - val_mae: 3.3522\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 18.2558 - mae: 3.1057 - val_loss: 20.6095 - val_mae: 3.3427\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.0006 - mae: 3.1660 - val_loss: 20.7100 - val_mae: 3.3047\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 15.8040 - mae: 3.1009\n",
      "Mean Absolute Error on Test Data: 3.1008784770965576\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04740287728873083\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 14ms/step - loss: 133.7062 - mae: 9.4892 - val_loss: 115.5979 - val_mae: 8.8846\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 100.8772 - mae: 7.6535 - val_loss: 76.4639 - val_mae: 6.5296\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 62.1081 - mae: 5.2350 - val_loss: 40.4332 - val_mae: 4.3615\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 42.4487 - mae: 4.3357 - val_loss: 36.3811 - val_mae: 4.4512\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 42.1663 - mae: 4.5269 - val_loss: 36.2279 - val_mae: 4.3483\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.8226 - mae: 4.3377 - val_loss: 36.2166 - val_mae: 4.3222\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.7608 - mae: 4.3616 - val_loss: 36.0905 - val_mae: 4.3415\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.7553 - mae: 4.3578 - val_loss: 36.0086 - val_mae: 4.3396\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.7148 - mae: 4.4152 - val_loss: 35.9331 - val_mae: 4.3327\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.6311 - mae: 4.3338 - val_loss: 35.8896 - val_mae: 4.3195\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.6209 - mae: 4.3839 - val_loss: 35.8371 - val_mae: 4.3200\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.5375 - mae: 4.3561 - val_loss: 35.7942 - val_mae: 4.3120\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.5012 - mae: 4.3336 - val_loss: 35.6614 - val_mae: 4.3256\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.4630 - mae: 4.3683 - val_loss: 35.6567 - val_mae: 4.3033\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 41.4693 - mae: 4.3673 - val_loss: 35.5487 - val_mae: 4.3081\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.4726 - mae: 4.3038 - val_loss: 35.4544 - val_mae: 4.3261\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.4253 - mae: 4.4380 - val_loss: 35.4370 - val_mae: 4.3016\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.6716 - mae: 4.2669 - val_loss: 35.3771 - val_mae: 4.3032\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.3944 - mae: 4.4562 - val_loss: 35.2899 - val_mae: 4.3257\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.2669 - mae: 4.3024 - val_loss: 35.3355 - val_mae: 4.2873\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.2186 - mae: 4.3627 - val_loss: 35.2159 - val_mae: 4.2887\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.1550 - mae: 4.3292 - val_loss: 35.1831 - val_mae: 4.2848\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.0905 - mae: 4.3684 - val_loss: 35.0323 - val_mae: 4.3024\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 41.0422 - mae: 4.3232 - val_loss: 35.0871 - val_mae: 4.2780\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.0019 - mae: 4.3469 - val_loss: 35.0100 - val_mae: 4.2789\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.2954 - mae: 4.2787 - val_loss: 34.8940 - val_mae: 4.2865\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.1291 - mae: 4.4313 - val_loss: 34.8556 - val_mae: 4.2815\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 40.8712 - mae: 4.3378 - val_loss: 34.9214 - val_mae: 4.2653\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.9105 - mae: 4.3136 - val_loss: 34.7141 - val_mae: 4.2783\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.8477 - mae: 4.3091 - val_loss: 34.6798 - val_mae: 4.2694\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.8065 - mae: 4.3534 - val_loss: 34.6166 - val_mae: 4.2710\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.7868 - mae: 4.2845 - val_loss: 34.5783 - val_mae: 4.2705\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.7497 - mae: 4.3613 - val_loss: 34.6514 - val_mae: 4.2547\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 40.7631 - mae: 4.2647 - val_loss: 34.4704 - val_mae: 4.2669\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.7292 - mae: 4.3422 - val_loss: 34.5206 - val_mae: 4.2546\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.6209 - mae: 4.2878 - val_loss: 34.3087 - val_mae: 4.2728\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.5629 - mae: 4.3730 - val_loss: 34.2574 - val_mae: 4.2684\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4784 - mae: 4.2928 - val_loss: 34.3995 - val_mae: 4.2417\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 40.5461 - mae: 4.3167 - val_loss: 34.2469 - val_mae: 4.2482\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4926 - mae: 4.2970 - val_loss: 34.3410 - val_mae: 4.2308\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.3864 - mae: 4.2669 - val_loss: 33.9797 - val_mae: 4.2847\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4139 - mae: 4.3216 - val_loss: 34.1170 - val_mae: 4.2359\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.4668 - mae: 4.3416 - val_loss: 33.9486 - val_mae: 4.2461\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.2109 - mae: 4.2725 - val_loss: 33.9912 - val_mae: 4.2286\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.1893 - mae: 4.2680 - val_loss: 33.7593 - val_mae: 4.2548\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.0664 - mae: 4.2960 - val_loss: 33.7952 - val_mae: 4.2315\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.1869 - mae: 4.2854 - val_loss: 33.9995 - val_mae: 4.2138\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.0088 - mae: 4.2287 - val_loss: 33.6499 - val_mae: 4.2775\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 40.1042 - mae: 4.3706 - val_loss: 33.6193 - val_mae: 4.2216\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 39.9526 - mae: 4.2494 - val_loss: 33.6290 - val_mae: 4.2171\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 31.6323 - mae: 4.0021\n",
      "Mean Absolute Error on Test Data: 4.002114772796631\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.14062791142216735\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all\n",
    "df_test = dict_test_merged\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_Nem','Ruzgar_Hizi','Yagis']\n",
    "output_var = df\n",
    "target = 'Bildirimsiz_sum'\n",
    "# ilceler = []\n",
    "\n",
    "# NN 3\n",
    "# ilceler = ['izmir-konak','izmir-kinik']\n",
    "all_submissions = []\n",
    "for ilce in ilceler:\n",
    "    df = merged_all[ilce]\n",
    "    df_test = dict_test_merged[ilce]\n",
    "    output_var = df['Bildirimsiz_sum']\n",
    "\n",
    "    # ilcelerin numerizasyonu\n",
    "    columns_tonumerate = ['Bayram_Flag']\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "    # test csv dosyasi numerizasyon\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_transform = scaler.fit_transform(df[features])\n",
    "    feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "    feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "    feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    feature_test = scaler2.fit_transform(df_test[features])\n",
    "    feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)\n",
    "\n",
    "\n",
    "\n",
    "    X = feature_transform\n",
    "    y = output_var\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Mean Absolute Error on Test Data:\", mae)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"R-squared:\", r2)\n",
    "\n",
    "    predictions_new = model.predict(feature_test)\n",
    "    predictions_new = np.round(predictions_new).astype(int)\n",
    "    df_test['bildirimsiz_sum'] = predictions_new\n",
    "    df_test.to_csv('test_with_predictions.csv', index=False)\n",
    "\n",
    "    df_test.rename(columns={'Ilce': 'ilce'}, inplace=True)\n",
    "    df_test.rename(columns={'Tarih': 'tarih'}, inplace=True)\n",
    "    df_test.rename(columns={'Bildirimli_sum': 'bildirimli_sum'}, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    all_submissions.append(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tarih              ilce  bildirimli_sum  bildirimsiz_sum\n",
      "0  2024-01-04      izmir-aliaga               0                7\n",
      "1  2024-01-05      izmir-aliaga               0                5\n",
      "2  2024-01-06      izmir-aliaga               0                5\n",
      "3  2024-01-07      izmir-aliaga               0                8\n",
      "4  2024-01-08      izmir-aliaga               0                6\n",
      "..        ...               ...             ...              ...\n",
      "23 2024-01-27  manisa-yunusemre               1                5\n",
      "24 2024-01-28  manisa-yunusemre               0                7\n",
      "25 2024-01-29  manisa-yunusemre               0                8\n",
      "26 2024-01-30  manisa-yunusemre               1               10\n",
      "27 2024-01-31  manisa-yunusemre               2                8\n",
      "\n",
      "[1316 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "birlesmisunited = all_submissions[0]\n",
    "for i in range(1,len(ilceler)): # 1 2 3 4   47\n",
    "    birlesmisunited = pd.concat([birlesmisunited,all_submissions[i]])\n",
    "\n",
    "# birlesmisunited.drop('Gun', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Bayram_Flag', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Sicaklik', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Bagil_Nem', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Ruzgar_Hizi', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Yagis', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Gün', axis=1, inplace=True)\n",
    "\n",
    "print(birlesmisunited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited['tarih'] = pd.to_datetime(birlesmisunited['tarih'])\n",
    "birlesmisunited = birlesmisunited.sort_values(by='tarih')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique id olusturulmali\n",
    "# her tarih kendi icinde ilce adina gore siralanmali\n",
    "birlesmisunited.drop('bildirimli_sum', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited = birlesmisunited.sort_values(by=['tarih', 'ilce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited['unique_id'] = birlesmisunited['tarih'].astype(str) + '-' + birlesmisunited['ilce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.drop(['tarih', 'ilce'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.insert(0, 'unique_id', birlesmisunited.pop('unique_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>bildirimsiz_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-04-izmir-aliaga</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-04-izmir-balcova</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-04-izmir-bayindir</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-04-izmir-bayrakli</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-04-izmir-bergama</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-31-manisa-sehzadeler</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-31-manisa-selendi</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-31-manisa-soma</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-31-manisa-turgutlu</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2024-01-31-manisa-yunusemre</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1316 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       unique_id  bildirimsiz_sum\n",
       "0        2024-01-04-izmir-aliaga                7\n",
       "0       2024-01-04-izmir-balcova                2\n",
       "0      2024-01-04-izmir-bayindir                6\n",
       "0      2024-01-04-izmir-bayrakli                3\n",
       "0       2024-01-04-izmir-bergama                9\n",
       "..                           ...              ...\n",
       "27  2024-01-31-manisa-sehzadeler                7\n",
       "27     2024-01-31-manisa-selendi                4\n",
       "27        2024-01-31-manisa-soma                7\n",
       "27    2024-01-31-manisa-turgutlu                5\n",
       "27   2024-01-31-manisa-yunusemre                8\n",
       "\n",
       "[1316 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birlesmisunited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.to_csv('subbb.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
