{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\\n\\n    Yapilmasi gerekenler:\\n        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\\n        - Farkli grafikler cikartmaya calis.\\n        - ML.\\n        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\\n        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\\n\\n    Sorunlar:\\n        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\\n        ortalamasi az olur. Burada farkli bir yontem bul.\\n        Gunluk maks min alinabilir\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1-) Notlar\n",
    "\"\"\"\n",
    "    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\n",
    "\n",
    "    Yapilmasi gerekenler:\n",
    "        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\n",
    "        - Farkli grafikler cikartmaya calis.\n",
    "        - ML.\n",
    "        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\n",
    "        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\n",
    "\n",
    "    Sorunlar:\n",
    "        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\n",
    "        ortalamasi az olur. Burada farkli bir yontem bul.\n",
    "        Gunluk maks min alinabilir\n",
    "\"\"\"\n",
    "# 1-) read and preproccess train.csv\n",
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "# 3-) read and preprocess weather.csv\n",
    "# 4-) read and preprocess holidays.csv\n",
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi\n",
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği\n",
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli ve gerekli degil)\n",
    "# 10-) test icin birlestirme islemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-) Import required moduls and libraries\n",
    "\n",
    "# bildirimisiz_sum tahmin edilecek\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import os\n",
    "from unidecode import unidecode # to convert Turkish characters to English\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sm\n",
    "import statsmodels.api as sa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tarih         ilce  bildirimsiz_sum  bildirimli_sum\n",
      "19236 2021-01-01  izmir-konak                9               0\n",
      "19237 2021-01-02  izmir-konak               20               0\n",
      "19238 2021-01-03  izmir-konak                7               1\n",
      "19239 2021-01-04  izmir-konak               16               1\n",
      "19240 2021-01-05  izmir-konak                3               0\n",
      "...          ...          ...              ...             ...\n",
      "20355 2024-01-27  izmir-konak               12               3\n",
      "20356 2024-01-28  izmir-konak               13               1\n",
      "20357 2024-01-29  izmir-konak               22               0\n",
      "20358 2024-01-30  izmir-konak               28               1\n",
      "20359 2024-01-31  izmir-konak               16               0\n",
      "\n",
      "[1124 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-) read and preproccess train.csv\n",
    "train = pd.read_csv(\"./train.csv\", low_memory=False) # 46.944 satir, 4 kolon\n",
    "\n",
    "#print(train[\"tarih\"]) # 1.098 farkli tarih var, 47 farkli ilce var\n",
    "\n",
    "tarihler = []\n",
    "for i in train[\"tarih\"]:\n",
    "    tarihler.append(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "train[\"tarih\"] = tarihler\n",
    "\n",
    "# print(train.dtypes)\n",
    "\n",
    "dict :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak o ilcenin verisi (1096 gun) df olarak tutar\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "print(dict[\"izmir-konak\"])\n",
    "ilceler = (list(dict.keys()))\n",
    "#print(dict.keys()) # keys olarak her ilceyi, values olarak o ilcelerin bulundugu satirlari icerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'izmir-aliaga': 1106, 'izmir-balcova': 698, 'izmir-bayindir': 1105, 'izmir-bayrakli': 1086, 'izmir-bergama': 1120, 'izmir-beydag': 673, 'izmir-bornova': 1124, 'izmir-buca': 1115, 'izmir-cesme': 1125, 'izmir-cigli': 1071, 'izmir-dikili': 1119, 'izmir-foca': 1086, 'izmir-gaziemir': 920, 'izmir-guzelbahce': 856, 'izmir-karabaglar': 1100, 'izmir-karaburun': 1089, 'izmir-karsiyaka': 1085, 'izmir-kemalpasa': 1118, 'izmir-kinik': 914, 'izmir-kiraz': 1097, 'izmir-konak': 1124, 'izmir-menderes': 1125, 'izmir-menemen': 1119, 'izmir-narlidere': 783, 'izmir-odemis': 1124, 'izmir-seferihisar': 1111, 'izmir-selcuk': 872, 'izmir-tire': 1107, 'izmir-torbali': 1124, 'izmir-urla': 1122, 'manisa-ahmetli': 622, 'manisa-akhisar': 1126, 'manisa-alasehir': 1119, 'manisa-demirci': 938, 'manisa-golmarmara': 566, 'manisa-gordes': 1059, 'manisa-kirkagac': 950, 'manisa-koprubasi': 805, 'manisa-kula': 1039, 'manisa-salihli': 1126, 'manisa-sarigol': 1027, 'manisa-saruhanli': 1105, 'manisa-sehzadeler': 1123, 'manisa-selendi': 993, 'manisa-soma': 1086, 'manisa-turgutlu': 1121, 'manisa-yunusemre': 1125}\n"
     ]
    }
   ],
   "source": [
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "\"\"\"\n",
    "for label in dict.keys(): # her ilce icin bildirimsiz ve bildirimli olarak grafiklerini cikart\n",
    "    print(dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.bar(dict[label][\"tarih\"],dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.title(label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(dict[\"izmir-konak\"][\"tarih\"],dict[\"izmir-konak\"][\"bildirimsiz_sum\"])\n",
    "plt.title(label)\n",
    "plt.margins(0.01)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# ilce tarih sayilarini al hepsinde esit veri yok\n",
    "ilce_tarih_sayilari = {}\n",
    "for name in dict.keys():\n",
    "    ilce_tarih_sayilari[name] = len(list(dict[name][\"tarih\"].to_dict().values()))\n",
    "\n",
    "print(ilce_tarih_sayilari)\n",
    "for name in dict.keys():\n",
    "    dict[name].set_index(\"tarih\", inplace=True)\n",
    "\n",
    "# train.set_index(\"tarih\", inplace=True) # train'in tarih kolonunu indexe cevir\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'lat', 'lon', 't_2m:C', 'effective_cloud_cover:p',\n",
      "       'global_rad:W', 'relative_humidity_2m:p', 'wind_dir_10m:d',\n",
      "       'wind_speed_10m:ms', 'prob_precip_1h:p', 't_apparent:C', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3-) read and preprocess weather.csv\n",
    "\n",
    "weather = pd.read_csv(\"./weather.csv\", low_memory=False)\n",
    "print(weather.columns) # onemli kolonlar: date, t_apparent:C (hissedilen sicaklik), wind_dir_10m:d (ruzgar yonu),\n",
    "# wind_speed_10m:ms (ruzgar hizi), prob_precip_1h:p (yagis), ilce\n",
    "\n",
    "# ilceleri ayir\n",
    "ilce_weather = {} # keys olarak ilceleri, values olarak o ilcelerin saatlik (1165 gun) hava durumlarini tutar\n",
    "for label, group in weather.groupby(\"name\"):\n",
    "    ilce_weather[label.lower()] = group\n",
    "\n",
    "# tarihleri tarih formatina cevir\n",
    "#print(ilce_weather[\"izmir-konak\"].dtypes)\n",
    "for name in ilce_weather.keys():\n",
    "\n",
    "    tarihler = [] # duzenli tarihleri burada tut\n",
    "    for date in ilce_weather[name][\"date\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ilce_weather[name][\"date\"] = tarihler # duzenli tarihleri date kolonuna ata\n",
    "    ilce_weather[name].set_index(\"date\", inplace=True) # tarihleri indexe cevir\n",
    "    ilce_weather[name][\"tarih\"] = ilce_weather[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "ilce_weather_day = {} # ilce hava durumu verilerini gunluk olarak tut (ortalama ile)\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_day[name] = ilce_weather[name].resample(\"D\").mean(numeric_only=True)# index'teki tarihleri gune cevir\n",
    "    ilce_weather_day[name][\"tarih\"] = ilce_weather_day[name].index\n",
    "\n",
    "#print(ilce_weather[\"izmir-konak\"][\"tarih\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lon', 't_2m:C', 'effective_cloud_cover:p', 'global_rad:W',\n",
      "       'relative_humidity_2m:p', 'wind_dir_10m:d', 'wind_speed_10m:ms',\n",
      "       'prob_precip_1h:p', 't_apparent:C', 'tarih'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ilce_weather_day[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "lat                               float64\n",
      "lon                               float64\n",
      "t_2m:C                            float64\n",
      "effective_cloud_cover:p           float64\n",
      "global_rad:W                      float64\n",
      "relative_humidity_2m:p            float64\n",
      "wind_dir_10m:d                    float64\n",
      "wind_speed_10m:ms                 float64\n",
      "prob_precip_1h:p                  float64\n",
      "t_apparent:C                      float64\n",
      "name                               object\n",
      "tarih                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3.1-) her ilcenin hava durumunda her gununu ayri ayri df lere koyup dict te tut (runtime: 6m 35s)\n",
    "\n",
    "ilce_weather_detailed = {} \n",
    "# {izmir-konak: {2021-01-01 : df , 2021-01-02 : df ,...} , manisa-akhisar: {2021-01-01 : df , 2021-01-02 : df ,...} }\n",
    "\n",
    "\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_detailed[name] = {}\n",
    "    for label,group in ilce_weather[name].groupby(\"date\"):\n",
    "\n",
    "        gun = label.strftime('%Y-%m-%d')\n",
    "        if gun in ilce_weather_detailed[name]:\n",
    "            ilce_weather_detailed[name][gun] = pd.concat([ilce_weather_detailed[name][gun], group], ignore_index=True)\n",
    "        else:\n",
    "            ilce_weather_detailed[name][gun] = group.copy()\n",
    "\n",
    "\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].keys())[0]))\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].values())[0]).dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat      lon  Sicaklik_max  Sicaklik_min  Bulutluluk_max  \\\n",
      "2021-01-01  38.4177  27.1283          15.3          11.9            90.0   \n",
      "2021-01-02  38.4177  27.1283          17.4          11.0            57.5   \n",
      "2021-01-03  38.4177  27.1283          15.3          11.2            99.8   \n",
      "2021-01-04  38.4177  27.1283          17.7          10.5            97.4   \n",
      "2021-01-05  38.4177  27.1283          16.7          11.2            99.7   \n",
      "\n",
      "            Bulutluluk_min  Guneslilik_max  Guneslilik_min  Bagil_nem_max  \\\n",
      "2021-01-01            28.2           275.4             0.0           93.5   \n",
      "2021-01-02            10.4           374.0             0.0           90.9   \n",
      "2021-01-03            12.4           151.9             0.0           84.6   \n",
      "2021-01-04             9.2           357.0             0.0           85.6   \n",
      "2021-01-05             5.4           362.3             0.0          100.0   \n",
      "\n",
      "            Bagil_nem_min  ...         Ilce      Tarih   Sicaklik  Bulutluluk  \\\n",
      "2021-01-01           82.3  ...  izmir-konak 2021-01-01  13.095833   59.033333   \n",
      "2021-01-02           64.9  ...  izmir-konak 2021-01-02  13.379167   29.912500   \n",
      "2021-01-03           72.9  ...  izmir-konak 2021-01-03  12.587500   69.916667   \n",
      "2021-01-04           55.8  ...  izmir-konak 2021-01-04  13.783333   45.604167   \n",
      "2021-01-05           59.6  ...  izmir-konak 2021-01-05  13.895833   35.670833   \n",
      "\n",
      "            Guneslilik  Bagil_nem  Ruzgar_yonu  Ruzgar_hizi      Yagis  \\\n",
      "2021-01-01   65.212500  87.962500   137.558333     3.129167   1.137500   \n",
      "2021-01-02   91.225000  80.720833   134.820833     2.158333   1.000000   \n",
      "2021-01-03   34.962500  79.725000   142.316667     2.300000   2.520833   \n",
      "2021-01-04   79.400000  71.362500   138.641667     3.979167   1.000000   \n",
      "2021-01-05   92.666667  82.308333   161.516667     2.591667  12.279167   \n",
      "\n",
      "           Hissedilen_sicaklik  \n",
      "2021-01-01           13.891667  \n",
      "2021-01-02           14.250000  \n",
      "2021-01-03           12.937500  \n",
      "2021-01-04           13.787500  \n",
      "2021-01-05           14.850000  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3.2-) 3.1'de ayrilan ilce gunlerini simdi her gun icin degerlerin min max'ini bulup ilce df'lerini tekrar olustur\n",
    "\n",
    "# runtime: 1m 4s\n",
    "weather_last = {} # key olarak ilceleri, value olarak da o ilcelerin hava durumu degerlerini min-max ile tutar\n",
    "for name in ilce_weather_detailed.keys():\n",
    "    weather_last[name] = pd.DataFrame()\n",
    "\n",
    "    for date, day_df in ilce_weather_detailed[name].items():\n",
    "        \n",
    "        # max min leri al\n",
    "        max_values = day_df.max()\n",
    "        min_values = day_df.min()\n",
    "\n",
    "        # satır oluştur\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Tarih\": [datetime.strptime(date, \"%Y-%m-%d\")],\n",
    "            \"lat\": [day_df[\"lat\"].iloc[0]],\n",
    "            \"lon\": [day_df[\"lon\"].iloc[0]],\n",
    "            \"Sicaklik_max\": [max_values[\"t_2m:C\"]],\n",
    "            \"Sicaklik_min\": [min_values[\"t_2m:C\"]],\n",
    "            \"Bulutluluk_max\": [max_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Bulutluluk_min\": [min_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Guneslilik_max\": [max_values.get(\"global_rad:W\", None)],  \n",
    "            \"Guneslilik_min\": [min_values.get(\"global_rad:W\", None)],  \n",
    "            \"Bagil_nem_max\": [max_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Bagil_nem_min\": [min_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Ruzgar_yonu_max\": [max_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_yonu_min\": [min_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_hizi_max\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Ruzgar_hizi_min\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Yagis_max\": [max_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Yagis_min\": [min_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Hissedilen_sicaklik_max\": [max_values.get(\"t_apparent:C\", None)],\n",
    "            \"Hissedilen_sicaklik_min\": [min_values.get(\"t_apparent:C\", None)],\n",
    "            \"Ilce\": [day_df[\"name\"].iloc[0].lower()]  # Ilce ekle\n",
    "        })\n",
    "\n",
    "        # her gunu o ilcenin df ine ekle\n",
    "        weather_last[name] = pd.concat([weather_last[name], new_row], ignore_index=True)\n",
    "\n",
    "  \n",
    "new_column_names = {\n",
    "    \"lat\" : \"lat\", \"lot\" : \"lot\", \"Sicaklik_max\" : \"Sicaklik_max\", \"Sicaklik_min\" : \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\" : \"Bulutluluk_max\", \"Bulutluluk_min\" : \"Bulutluluk_min\", \"Guneslilik_max\" : \"Guneslilik_max\",\n",
    "    \"Guneslilik_min\" : \"Guneslilik_min\", \"Bagil_nem_max\" : \"Bagil_nem_max\", \"Bagil_nem_min\" : \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\" : \"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\" : \"Ruzgar_yonu_min\", \"Ruzgar_hizi_max\" : \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi_min\" : \"Ruzgar_hizi_min\", \"Yagis_max\" : \"Yagis_max\", \"Yagis_min\" : \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\" : \"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\" : \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\" : \"Ilce\", \"t_2m:C\" : \"Sicaklik\", \"effective_cloud_cover:p\" : \"Bulutluluk\", \"global_rad:W\" : \"Guneslilik\",\n",
    "    \"relative_humidity_2m:p\" : \"Bagil_nem\", \"wind_dir_10m:d\" : \"Ruzgar_yonu\", \"wind_speed_10m:ms\" : \"Ruzgar_hizi\",\n",
    "    \"prob_precip_1h:p\" : \"Yagis\", \"t_apparent:C\" : \"Hissedilen_sicaklik\", \"Tarih\" : \"Tarih\"\n",
    "}\n",
    "\n",
    "for name in weather_last.keys():\n",
    "    weather_last[name].set_index(\"Tarih\", inplace=True) # tarih kolonunu indexe ata\n",
    "    weather_last[name][\"Tarih\"] = weather_last[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "    weather_last[name] = pd.concat([weather_last[name], ilce_weather_day[name][[\"t_2m:C\",\"effective_cloud_cover:p\",\n",
    "    \"global_rad:W\", \"relative_humidity_2m:p\",\"wind_dir_10m:d\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\",\n",
    "    \"t_apparent:C\"]]], axis=1) # mean leri ekle\n",
    "    \n",
    "    weather_last[name] = weather_last[name].rename(columns=new_column_names) # kolonlari tekrar isimlendir\n",
    "    \n",
    "\n",
    "print(weather_last[\"izmir-konak\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lon', 'Sicaklik_max', 'Sicaklik_min', 'Bulutluluk_max',\n",
      "       'Bulutluluk_min', 'Guneslilik_max', 'Guneslilik_min', 'Bagil_nem_max',\n",
      "       'Bagil_nem_min', 'Ruzgar_yonu_max', 'Ruzgar_yonu_min',\n",
      "       'Ruzgar_hizi_max', 'Ruzgar_hizi_min', 'Yagis_max', 'Yagis_min',\n",
      "       'Hissedilen_sicaklik_max', 'Hissedilen_sicaklik_min', 'Ilce', 'Tarih',\n",
      "       'Sicaklik', 'Bulutluluk', 'Guneslilik', 'Bagil_nem', 'Ruzgar_yonu',\n",
      "       'Ruzgar_hizi', 'Yagis', 'Hissedilen_sicaklik'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(weather_last[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tatil Adı'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4-) read and preprocess holidays.csv\n",
    "\n",
    "holiday = pd.read_csv(\"./holidays.csv\", low_memory=False)\n",
    "\n",
    "# print(holiday.head())\n",
    "\n",
    "holiday[\"tarih\"] = holiday['Yıl'].astype(str) + '-' + holiday['Ay'].astype(str) + '-' + holiday['Gün'].astype(str)\n",
    "holiday['tarih'] = pd.to_datetime(holiday['tarih'], format='%Y-%m-%d')\n",
    "holiday.set_index(\"tarih\", inplace=True)\n",
    "holiday = holiday.drop(columns=[\"Yıl\", \"Ay\", \"Gün\"])\n",
    "\n",
    "print(holiday.columns) # index olarak tarihi (YY-AA-GG), Bayram_Flag olarak da bayram ismini tutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tarih         ilce  bildirimsiz_sum  bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "                 Tatil Adı  \n",
      "tarih                       \n",
      "2021-01-01  New Year's Day  \n",
      "2021-01-02             NaN  \n",
      "2021-01-03             NaN  \n",
      "2021-01-04             NaN  \n",
      "2021-01-05             NaN  \n",
      "...                    ...  \n",
      "2024-01-27             NaN  \n",
      "2024-01-28             NaN  \n",
      "2024-01-29             NaN  \n",
      "2024-01-30             NaN  \n",
      "2024-01-31             NaN  \n",
      "\n",
      "[1124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "\n",
    "def merge_holiday(df1, df2=holiday):\n",
    "    merged_df = pd.merge(df1, df2[\"Tatil Adı\"], left_index=True, right_index=True, how=\"left\")\n",
    "    #df1[\"Bayramlar\"] = df2[\"Bayram_Flag\"]\n",
    "    return merged_df\n",
    "\n",
    "dict_holiday = {}\n",
    "for name in dict.keys():\n",
    "    dict_holiday[name] = merge_holiday(dict[name],holiday)\n",
    "    dict_holiday[name]['tarih'] = dict_holiday[name].index\n",
    "    dict_holiday[name] = dict_holiday[name].reindex(columns=[\"tarih\", \"ilce\", \"bildirimsiz_sum\", \"bildirimli_sum\", \"Tatil Adı\"])\n",
    "    \n",
    "print(dict_holiday[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                                                     \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500    1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000    2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833    3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000    4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167    5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xesth\\AppData\\Local\\Temp\\ipykernel_16792\\3102189797.py:35: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n"
     ]
    }
   ],
   "source": [
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "\n",
    "def merge_weather(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\n",
    "    \"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "\"\"\" ekstra eklenebilecek kolonlar : (bunlari ustteki diger kolonarin arkasina ekleyebilirsin, ayni sekilde alttaki isimlendirmeye de eklemeyi unutma)\n",
    "\"Bulutluluk_max\", \"Bulutluluk_min\", \"Guneslilik_max\", \"Guneslilik_min\",\"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\",\"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\"\n",
    ",\"Bulutluluk\",\"Guneslilik\",\"Ruzgar_yonu\",\"Hissedilen_sicaklik\"\n",
    "\"\"\"\n",
    "merged_all = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all[name] = merge_weather(dict_holiday[name], weather_last[name])\n",
    "\n",
    "    merged_all[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "    \n",
    "    merged_all[name]['Gün'] = range(1, len(merged_all[name]) + 1)\n",
    "\n",
    "print(merged_all[\"izmir-konak\"].head())\n",
    "\n",
    "all_in_one = pd.concat(merged_all.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "#print(\"\\nall_in_one: \\n\\n\",all_in_one.dtypes)\n",
    "\n",
    "merged_all_week = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_week[name] = merged_all[name].resample(\"W\").sum(numeric_only=True)\n",
    "#print(merged_all_week[\"izmir-konak\"])\n",
    "\n",
    "merged_all_month = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n",
    "#print(merged_all_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi (runtime: 19s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimli_siz\"):\n",
    "    os.makedirs(\"./graphs/bildirimli_siz\")\n",
    "    print(\"bildirimli_siz klasoru olustu\")\n",
    "\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "#     plt.figure(figsize=(17,8))\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "#     plt.xlabel(\"Tarih\", fontsize=13)\n",
    "#     plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "#     plt.margins(0.01)\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.subplots_adjust(bottom=0.15)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\\n\\nanaliz = pd.concat([\\n    ayristirma2.observed,\\n    ayristirma2.trend,\\n    ayristirma2.seasonal,\\n    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\\n], axis=1)\\nanaliz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\\n\\nindeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\\n\\n\\nX = sa.add_constant(indeks)\\nmodel = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\\nsonuc = model.fit()\\nrsquared_value = sonuc.rsquared\\ny = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\\n\\nyeni_satirlar = pd.DataFrame(index=y)\\nanaliz = pd.concat([analiz, yeni_satirlar])\\n\\n# not: bu degerleri ayarla\\nmev = [\\n    1.038656,\\n    0.973940,\\n    0.987404,\\n    1.038656\\n]\\n\\nnan_indices = analiz.index[analiz[\\'Mevsimsellik\\'].isna()]\\nfor i, index in enumerate(nan_indices):\\n    if i < len(mev):\\n        analiz.at[index, \\'Mevsimsellik\\'] = mev[i]\\n#print(analiz[\"Mevsimsellik\"])\\n\\ngirdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\\nregmodel = sonuc.predict(sa.add_constant(girdi))\\n\\nanaliz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\\n\\n\\nprint(analiz.head())\\n\\nplt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\')\\n#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\', horizontalalignment=\\'left\\')\\nplt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\\n#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\\nplt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği (runtime: 18s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimsiz_detailed\"):\n",
    "    os.makedirs(\"./graphs/bildirimsiz_detailed\")\n",
    "    print(\"bildirimsiz_detailed klasoru olustu\")\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "\n",
    "    # plt.figure(figsize=(17,8))\n",
    "    # plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    # plt.title(\"{} - Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    # plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    # window_size = 3  # Hareketli ortalama penceresi\n",
    "    # merged_all_week[name]['Moving_Average'] = merged_all_week[name][\"Bildirimsiz_sum\"].rolling(window=window_size, center=True).mean()\n",
    "    # #plt.plot(merged_all_week[name]['Moving_Average'], label=\"MHO\", color=\"black\")\n",
    "\n",
    "    # ortalama = merged_all_week[name][\"Bildirimsiz_sum\"].mean()\n",
    "    # plt.axhline(y=ortalama, color='orange', linestyle='--', label='Ortalama %{:.1f}'.format(ortalama),linewidth=2.2)\n",
    "    # alpha = 0.2  # Yumuşatma parametresi \n",
    "    # # formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "    # merged_all_week[name]['EWMA'] = merged_all_week[name][\"Bildirimsiz_sum\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    # plt.plot(merged_all_week[name]['EWMA'], label=\"EWMA\", color=\"red\", lw=2.9)\n",
    "\n",
    "\n",
    "    # plt.margins(0.01)\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.subplots_adjust(bottom=0.15)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"./graphs/bildirimsiz_detailed/{}.png\".format(name))\n",
    "    #plt.show()\n",
    "\n",
    "\"\"\"\n",
    "ayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\n",
    "\n",
    "analiz = pd.concat([\n",
    "    ayristirma2.observed,\n",
    "    ayristirma2.trend,\n",
    "    ayristirma2.seasonal,\n",
    "    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\n",
    "], axis=1)\n",
    "analiz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\n",
    "\n",
    "indeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\n",
    "\n",
    "\n",
    "X = sa.add_constant(indeks)\n",
    "model = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\n",
    "sonuc = model.fit()\n",
    "rsquared_value = sonuc.rsquared\n",
    "y = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\n",
    "\n",
    "yeni_satirlar = pd.DataFrame(index=y)\n",
    "analiz = pd.concat([analiz, yeni_satirlar])\n",
    "\n",
    "# not: bu degerleri ayarla\n",
    "mev = [\n",
    "    1.038656,\n",
    "    0.973940,\n",
    "    0.987404,\n",
    "    1.038656\n",
    "]\n",
    "\n",
    "nan_indices = analiz.index[analiz['Mevsimsellik'].isna()]\n",
    "for i, index in enumerate(nan_indices):\n",
    "    if i < len(mev):\n",
    "        analiz.at[index, 'Mevsimsellik'] = mev[i]\n",
    "#print(analiz[\"Mevsimsellik\"])\n",
    "\n",
    "girdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\n",
    "regmodel = sonuc.predict(sa.add_constant(girdi))\n",
    "\n",
    "analiz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\n",
    "\n",
    "\n",
    "print(analiz.head())\n",
    "\n",
    "plt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top')\n",
    "#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "plt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\n",
    "#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\n",
    "plt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.0: 30.0, 6.0: 26.285714285714285, 7.0: 34.170212765957444, 8.2: 31.0, 8.5: 36.0, 8.6: 34.0, 8.9: 37.0, 10.3: 26.0, 11.6: 26.0, 12.7: 35.0, 16.6: 37.0, 18.2: 44.0, 18.9: 28.0, 20.7: 35.0, 22.1: 26.0, 23.7: 33.0, 27.7: 25.0, 28.8: 52.5, 29.0: 32.0, 41.3: 35.0, 44.0: 69.0, 44.3: 27.0, 52.6: 22.0, 54.3: 32.0, 54.5: 13.0, 55.0: 20.0, 56.5: 18.0, 57.2: 43.0, 58.5: 37.0, 60.0: 44.0, 60.1: 28.0, 61.6: 29.0, 67.1: 32.0, 69.3: 45.0, 73.1: 22.0, 73.7: 32.0, 77.0: 61.0, 80.7: 41.0, 85.5: 23.0, 88.3: 38.0, 88.4: 37.0, 91.3: 49.0, 98.6: 72.0, 99.9: 41.0, 100.8: 54.0, 101.0: 31.0, 101.3: 27.0, 102.4: 28.0, 102.9: 18.0, 104.2: 40.0, 105.8: 53.0, 106.6: 30.0, 107.8: 20.0, 108.7: 35.0, 120.2: 40.0, 120.4: 28.0, 122.3: 31.0, 125.6: 54.0, 127.6: 39.0, 139.2: 53.0, 141.70000000000002: 35.0, 143.8: 68.0, 154.6: 27.0, 157.2: 41.0, 157.60000000000002: 31.0, 159.3: 47.0, 162.0: 58.0, 164.8: 32.0, 174.4: 29.0, 175.5: 37.0, 175.7: 33.0, 178.0: 27.0, 178.5: 53.0, 178.6: 60.0, 179.1: 53.0, 180.0: 30.0, 180.6: 28.0, 184.0: 43.0, 186.3: 68.0, 190.4: 35.0, 191.2: 84.0, 191.5: 33.0, 191.6: 32.0, 197.8: 66.0, 202.89999999999998: 35.0, 209.8: 45.0, 212.5: 40.0, 215.6: 59.0, 216.9: 37.0, 219.0: 30.0, 223.6: 25.0, 233.6: 35.0, 237.3: 64.0, 241.2: 35.0, 244.1: 39.0, 246.5: 62.0, 252.1: 52.0, 263.8: 64.0, 272.5: 27.0, 280.1: 60.0, 300.6: 35.0, 308.1: 40.0, 323.4: 76.0, 327.6: 65.0, 335.0: 35.0, 352.9: 85.0, 393.6: 48.0, 413.7: 40.0}\n",
      "     Ort. Yagis  Ort. Kesinti\n",
      "0           5.0     30.000000\n",
      "1           6.0     26.285714\n",
      "2           7.0     34.170213\n",
      "3           8.2     31.000000\n",
      "4           8.5     36.000000\n",
      "..          ...           ...\n",
      "103       327.6     65.000000\n",
      "104       335.0     35.000000\n",
      "105       352.9     85.000000\n",
      "106       393.6     48.000000\n",
      "107       413.7     40.000000\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli, gerekli degil)\n",
    "\n",
    "yagis_dict = {}\n",
    "for label,group in merged_all_week[\"izmir-aliaga\"].groupby(\"Yagis_max\"):\n",
    "    yagis_dict[label] = group\n",
    "\n",
    "yagis_dict_toplamlari = {}\n",
    "for deger in yagis_dict.keys():\n",
    "    yagis_dict_toplamlari[deger] = yagis_dict[deger][\"Bildirimsiz_sum\"].mean()\n",
    "print(yagis_dict_toplamlari)\n",
    "\n",
    "hesaplamalar = pd.DataFrame(list(yagis_dict_toplamlari.items()), columns=['Ort. Yagis', 'Ort. Kesinti'])\n",
    "print(hesaplamalar)\n",
    "window_size = 3  # Hareketli ortalama penceresi\n",
    "hesaplamalar['Moving_Average'] = hesaplamalar[\"Ort. Kesinti\"].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "alpha = 0.3  # Yumuşatma parametresi \n",
    "# formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "hesaplamalar['EWMA'] = hesaplamalar[\"Ort. Kesinti\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# #plt.bar(merged_all_week[\"izmir-aliaga\"][\"Yagis\"],merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], width=0.05, label=\"Bildirimsiz\")\n",
    "# plt.scatter(yagis_dict_toplamlari.keys(), yagis_dict_toplamlari.values(), label=\"Ort. Kesinti\")\n",
    "# #plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['Moving_Average'], label=\"MHO\", color=\"red\")\n",
    "# plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['EWMA'], label=\"EWMA\", color=\"red\")\n",
    "# plt.margins(0.01)\n",
    "# plt.title(\"Izmir_Aliaga Yagis - Bildirimsiz (Haftalik)\")\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.xlabel(\"Ortalama Yagis Miktari\")\n",
    "# plt.ylabel(\"Ortalama Elektrik Kesintisi\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tarih         ilce  bildirimli_sum\n",
      "18   2024-02-01  izmir-konak               4\n",
      "65   2024-02-02  izmir-konak               1\n",
      "112  2024-02-03  izmir-konak               1\n",
      "159  2024-02-04  izmir-konak               0\n",
      "206  2024-02-05  izmir-konak               2\n",
      "Index(['Tarih', 'Ilce', 'Bildirimli_sum'], dtype='object')\n",
      "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
      "tarih                                                                          \n",
      "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
      "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
      "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
      "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
      "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
      "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
      "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
      "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
      "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
      "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
      "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
      "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
      "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
      "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
      "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
      "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
      "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
      "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
      "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
      "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
      "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
      "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
      "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
      "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
      "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
      "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
      "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
      "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
      "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
      "\n",
      "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
      "tarih                                                                     \n",
      "2024-02-01           4.6           89.5           50.8              2.9   \n",
      "2024-02-02           3.7           92.1           48.5              3.3   \n",
      "2024-02-03           5.6           88.7           49.0              4.7   \n",
      "2024-02-04           5.9           86.1           56.0              1.6   \n",
      "2024-02-05           7.1           95.7           59.0              2.7   \n",
      "2024-02-06           9.3           95.3           60.8              3.0   \n",
      "2024-02-07          12.0           95.7           67.3              6.9   \n",
      "2024-02-08          12.7           89.8           62.3              6.5   \n",
      "2024-02-09          10.8           94.2           60.1              3.5   \n",
      "2024-02-10          11.0           93.2           66.3              6.2   \n",
      "2024-02-11          13.8           82.0           54.5              8.0   \n",
      "2024-02-12          12.1           90.0           65.2              7.9   \n",
      "2024-02-13           8.9           96.1           52.5              3.9   \n",
      "2024-02-14           8.0           99.9           47.6              5.6   \n",
      "2024-02-15           8.2           80.6           59.9              5.3   \n",
      "2024-02-16           7.0           84.0           53.4              4.2   \n",
      "2024-02-17           7.4           87.0           53.2              4.0   \n",
      "2024-02-18           6.8           87.2           53.2              4.0   \n",
      "2024-02-19           6.3           84.7           49.8              3.7   \n",
      "2024-02-20           5.1           89.9           52.6              2.9   \n",
      "2024-02-21           6.1           88.8           44.0              3.4   \n",
      "2024-02-22           8.0           75.4           40.9              2.5   \n",
      "2024-02-23           8.6           95.3           62.4              4.3   \n",
      "2024-02-24           9.7           97.4           55.6              2.8   \n",
      "2024-02-25          10.6           90.8           48.1              3.1   \n",
      "2024-02-26          10.9           84.1           47.7              5.4   \n",
      "2024-02-27          11.8           87.6           50.0              2.6   \n",
      "2024-02-28          10.7           91.7           44.1              2.4   \n",
      "2024-02-29           9.9           92.8           41.2              2.6   \n",
      "\n",
      "            Ruzgar_hizi_min  Yagis_max  Yagis_min   Sicaklik  Bagil_nem  \\\n",
      "tarih                                                                     \n",
      "2024-02-01              2.9        1.0        1.0   7.416667  76.075000   \n",
      "2024-02-02              3.3        1.0        1.0   7.537500  76.600000   \n",
      "2024-02-03              4.7        1.0        1.0   8.354167  70.429167   \n",
      "2024-02-04              1.6        8.7        1.0   9.300000  72.108333   \n",
      "2024-02-05              2.7        1.0        1.0  11.412500  81.879167   \n",
      "2024-02-06              3.0        1.0        1.0  13.341667  82.562500   \n",
      "2024-02-07              6.9        4.6        1.0  14.958333  84.025000   \n",
      "2024-02-08              6.5        1.0        1.0  14.925000  80.550000   \n",
      "2024-02-09              3.5       57.1        1.0  13.379167  83.950000   \n",
      "2024-02-10              6.2       14.1        1.0  14.108333  79.545833   \n",
      "2024-02-11              8.0       94.2        1.0  15.645833  70.191667   \n",
      "2024-02-12              7.9       95.0        1.0  14.679167  78.145833   \n",
      "2024-02-13              3.9       31.7        1.0  11.904167  82.216667   \n",
      "2024-02-14              5.6        1.0        1.0  11.454167  76.887500   \n",
      "2024-02-15              5.3       68.7        1.0   9.841667  72.545833   \n",
      "2024-02-16              4.2        1.0        1.0   9.875000  72.495833   \n",
      "2024-02-17              4.0        1.0        1.0  10.062500  73.662500   \n",
      "2024-02-18              4.0        1.0        1.0  10.137500  73.129167   \n",
      "2024-02-19              3.7        1.0        1.0   9.700000  70.825000   \n",
      "2024-02-20              2.9        1.0        1.0   9.154167  75.720833   \n",
      "2024-02-21              3.4        8.3        1.0   9.941667  69.854167   \n",
      "2024-02-22              2.5        1.0        1.0  11.979167  60.583333   \n",
      "2024-02-23              4.3        1.0        1.0  12.425000  79.912500   \n",
      "2024-02-24              2.8        1.0        1.0  13.795833  81.883333   \n",
      "2024-02-25              3.1       25.7        1.0  14.058333  71.991667   \n",
      "2024-02-26              5.4       31.1        1.0  14.300000  67.937500   \n",
      "2024-02-27              2.6        1.0        1.0  14.954167  75.770833   \n",
      "2024-02-28              2.4        1.0        1.0  15.362500  73.537500   \n",
      "2024-02-29              2.6       85.1        1.0  15.991667  68.441667   \n",
      "\n",
      "            Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                    \n",
      "2024-02-01     2.054167   1.000000    1  \n",
      "2024-02-02     1.691667   1.000000    2  \n",
      "2024-02-03     2.483333   1.000000    3  \n",
      "2024-02-04     1.083333   1.616667    4  \n",
      "2024-02-05     2.004167   1.000000    5  \n",
      "2024-02-06     2.337500   1.000000    6  \n",
      "2024-02-07     4.529167   1.245833    7  \n",
      "2024-02-08     4.483333   1.000000    8  \n",
      "2024-02-09     2.491667   5.058333    9  \n",
      "2024-02-10     4.133333   2.512500   10  \n",
      "2024-02-11     6.191667  15.691667   11  \n",
      "2024-02-12     6.387500  37.950000   12  \n",
      "2024-02-13     2.108333   3.087500   13  \n",
      "2024-02-14     2.820833   1.000000   14  \n",
      "2024-02-15     4.150000   7.166667   15  \n",
      "2024-02-16     2.641667   1.000000   16  \n",
      "2024-02-17     2.487500   1.000000   17  \n",
      "2024-02-18     2.637500   1.000000   18  \n",
      "2024-02-19     2.166667   1.000000   19  \n",
      "2024-02-20     1.504167   1.000000   20  \n",
      "2024-02-21     1.695833   1.562500   21  \n",
      "2024-02-22     1.137500   1.000000   22  \n",
      "2024-02-23     2.583333   1.000000   23  \n",
      "2024-02-24     2.112500   1.000000   24  \n",
      "2024-02-25     2.170833   2.216667   25  \n",
      "2024-02-26     2.712500   5.658333   26  \n",
      "2024-02-27     1.112500   1.000000   27  \n",
      "2024-02-28     1.566667   1.000000   28  \n",
      "2024-02-29     1.633333  14.195833   29  \n"
     ]
    }
   ],
   "source": [
    "# 10-) test icin birlestirme islemleri\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\", low_memory=False) # 47 ilce icin 28 gunluk veriler var. (tarih, ilce, bildirimli_sum)\n",
    "#print(test)\n",
    "\n",
    "dict_test :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak ilcelerin 4 ocak - 31 ocak arasi verilerini df olarak tutar\n",
    "for label, group in test.groupby(\"ilce\"):\n",
    "    dict_test[label] = group\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].head())\n",
    "\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    tarihler = [] # duzgun tarihleri tutacak\n",
    "    for date in dict_test[name][\"tarih\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d\")) \n",
    "\n",
    "    dict_test[name][\"tarih\"] = tarihler # duzeltilmis tarihleri ata\n",
    "\n",
    "    dict_test[name].set_index(\"tarih\", inplace=True) # tarih kolonunu index'e ata\n",
    "    dict_test[name][\"Tarih\"] = dict_test[name].index # tarih kolonunu yeniden olustur\n",
    "    dict_test[name] = dict_test[name].iloc[:, [2, 0, 1]] # kolon siralarini duzenle\n",
    "    dict_test[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\"] # kolon isimlerini duzenle\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].columns)\n",
    "\n",
    "\n",
    "dict_test_merged = {} # birlestirilenleri tutacak dict\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    gecici = merge_holiday(dict_test[name], holiday) # test'e holiday ekle\n",
    "    dict_test_merged[name] = merge_weather(gecici, weather_last[name]) # sonra weather'i ekle\n",
    "\n",
    "    dict_test_merged[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "\n",
    "    dict_test_merged[name]['Gün'] = range(1, len(dict_test_merged[name]) + 1) # gun kolonu ekle (1-28 arasi oluyor)\n",
    "\n",
    "print(dict_test_merged[\"izmir-konak\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKINE OGRENMESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "            Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                \n",
      "2021-01-01            5          15.3          11.9           93.5   \n",
      "2021-01-02           12          17.4          11.0           90.9   \n",
      "2021-01-03           12          15.3          11.2           84.6   \n",
      "2021-01-04           12          17.7          10.5           85.6   \n",
      "2021-01-05           12          16.7          11.2          100.0   \n",
      "...                 ...           ...           ...            ...   \n",
      "2024-01-27           12          12.7           4.6           89.1   \n",
      "2024-01-28           12          10.8           4.9           91.6   \n",
      "2024-01-29           12           8.9           3.9           83.9   \n",
      "2024-01-30           12           9.0           4.4           76.3   \n",
      "2024-01-31           12          11.4           4.5           77.6   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "...                   ...              ...              ...        ...   \n",
      "2024-01-27           45.5              2.1              2.1        1.0   \n",
      "2024-01-28           43.5              4.8              4.8        1.0   \n",
      "2024-01-29           52.2              6.9              6.9        1.0   \n",
      "2024-01-30           50.2              6.5              6.5       53.5   \n",
      "2024-01-31           47.7              5.6              5.6        1.0   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis   Gün  \n",
      "tarih                                                                      \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500     1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000     2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833     3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000     4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167     5  \n",
      "...               ...        ...        ...          ...        ...   ...  \n",
      "2024-01-27        1.0   8.379167  72.575000     1.100000   1.000000  1120  \n",
      "2024-01-28        1.0   7.587500  70.383333     2.925000   1.000000  1121  \n",
      "2024-01-29        1.0   5.970833  69.762500     4.650000   1.000000  1122  \n",
      "2024-01-30        1.0   6.475000  64.350000     4.837500   6.612500  1123  \n",
      "2024-01-31        1.0   7.191667  65.329167     3.695833   1.000000  1124  \n",
      "\n",
      "[1124 rows x 18 columns]\n",
      "['izmir-aliaga', 'izmir-balcova', 'izmir-bayindir', 'izmir-bayrakli', 'izmir-bergama', 'izmir-beydag', 'izmir-bornova', 'izmir-buca', 'izmir-cesme', 'izmir-cigli', 'izmir-dikili', 'izmir-foca', 'izmir-gaziemir', 'izmir-guzelbahce', 'izmir-karabaglar', 'izmir-karaburun', 'izmir-karsiyaka', 'izmir-kemalpasa', 'izmir-kinik', 'izmir-kiraz', 'izmir-konak', 'izmir-menderes', 'izmir-menemen', 'izmir-narlidere', 'izmir-odemis', 'izmir-seferihisar', 'izmir-selcuk', 'izmir-tire', 'izmir-torbali', 'izmir-urla', 'manisa-ahmetli', 'manisa-akhisar', 'manisa-alasehir', 'manisa-demirci', 'manisa-golmarmara', 'manisa-gordes', 'manisa-kirkagac', 'manisa-koprubasi', 'manisa-kula', 'manisa-salihli', 'manisa-sarigol', 'manisa-saruhanli', 'manisa-sehzadeler', 'manisa-selendi', 'manisa-soma', 'manisa-turgutlu', 'manisa-yunusemre']\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all['izmir-konak']\n",
    "df_test = dict_test_merged[\"izmir-konak\"]\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "output_var = df['Bildirimsiz_sum']\n",
    "target = 'Bildirimsiz_sum'\n",
    "ilceler = []\n",
    "\n",
    "dict = {}\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "ilceler = list(dict.keys())\n",
    "\n",
    "print(df)\n",
    "print(ilceler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ilcelerin numerizasyonu\n",
    "columns_tonumerate = ['Bayram_Flag']\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "# test csv dosyasi numerizasyon\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "# indexi gun yapmak gerek!!!!!\n",
    "# gunu scale etmemek gerek!!!\n",
    "# bayrami da scale etmesek olur!\n",
    "# #Scaling\n",
    "scaler = MinMaxScaler()\n",
    "feature_transform = scaler.fit_transform(df[features])\n",
    "feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "feature_test = scaler2.fit_transform(df_test[features])\n",
    "feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bildirimli_sum</th>\n",
       "      <th>Sicaklik</th>\n",
       "      <th>Bayram_Flag</th>\n",
       "      <th>Bagil_nem</th>\n",
       "      <th>Ruzgar_hizi</th>\n",
       "      <th>Yagis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarih</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-01</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660860</td>\n",
       "      <td>0.183032</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.683256</td>\n",
       "      <td>0.114690</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-03</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.109329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420014</td>\n",
       "      <td>0.263943</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-04</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.219631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.465986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908461</td>\n",
       "      <td>0.173606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.690962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937611</td>\n",
       "      <td>0.236449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.879495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649647</td>\n",
       "      <td>0.006653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-08</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.851760</td>\n",
       "      <td>0.641005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-09</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.695335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996801</td>\n",
       "      <td>0.265515</td>\n",
       "      <td>0.109833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808923</td>\n",
       "      <td>0.575020</td>\n",
       "      <td>0.040934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.959670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.397609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.523324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922858</td>\n",
       "      <td>0.193244</td>\n",
       "      <td>0.056495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.470845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695521</td>\n",
       "      <td>0.327573</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-15</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.282799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>0.578162</td>\n",
       "      <td>0.166892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-16</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.286686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508176</td>\n",
       "      <td>0.293794</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.308552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557945</td>\n",
       "      <td>0.264729</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-18</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.317298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535194</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-19</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.266278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.204242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.202624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645752</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-21</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395485</td>\n",
       "      <td>0.115475</td>\n",
       "      <td>0.015223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-22</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.532070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.584062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824565</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.743926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908638</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.774538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486669</td>\n",
       "      <td>0.205027</td>\n",
       "      <td>0.032927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-26</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313722</td>\n",
       "      <td>0.307148</td>\n",
       "      <td>0.126071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647885</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-28</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.926628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552613</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335229</td>\n",
       "      <td>0.103692</td>\n",
       "      <td>0.357127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bildirimli_sum  Sicaklik  Bayram_Flag  Bagil_nem  Ruzgar_hizi  \\\n",
       "tarih                                                                       \n",
       "2024-02-01            1.00  0.000000          0.0   0.660860     0.183032   \n",
       "2024-02-02            0.25  0.014091          0.0   0.683256     0.114690   \n",
       "2024-02-03            0.25  0.109329          0.0   0.420014     0.263943   \n",
       "2024-02-04            0.00  0.219631          0.0   0.491646     0.000000   \n",
       "2024-02-05            0.50  0.465986          0.0   0.908461     0.173606   \n",
       "2024-02-06            0.25  0.690962          0.0   0.937611     0.236449   \n",
       "2024-02-07            0.00  0.879495          0.0   1.000000     0.649647   \n",
       "2024-02-08            0.75  0.875607          0.0   0.851760     0.641005   \n",
       "2024-02-09            0.25  0.695335          0.0   0.996801     0.265515   \n",
       "2024-02-10            1.00  0.780369          0.0   0.808923     0.575020   \n",
       "2024-02-11            0.00  0.959670          0.0   0.409883     0.963079   \n",
       "2024-02-12            0.00  0.846939          0.0   0.749200     1.000000   \n",
       "2024-02-13            0.00  0.523324          0.0   0.922858     0.193244   \n",
       "2024-02-14            0.00  0.470845          0.0   0.695521     0.327573   \n",
       "2024-02-15            0.00  0.282799          0.0   0.510309     0.578162   \n",
       "2024-02-16            0.00  0.286686          0.0   0.508176     0.293794   \n",
       "2024-02-17            1.00  0.308552          0.0   0.557945     0.264729   \n",
       "2024-02-18            0.00  0.317298          0.0   0.535194     0.293009   \n",
       "2024-02-19            0.25  0.266278          0.0   0.436900     0.204242   \n",
       "2024-02-20            0.00  0.202624          0.0   0.645752     0.079340   \n",
       "2024-02-21            0.00  0.294461          0.0   0.395485     0.115475   \n",
       "2024-02-22            0.50  0.532070          0.0   0.000000     0.010212   \n",
       "2024-02-23            0.75  0.584062          0.0   0.824565     0.282797   \n",
       "2024-02-24            0.25  0.743926          0.0   0.908638     0.194030   \n",
       "2024-02-25            0.25  0.774538          0.0   0.486669     0.205027   \n",
       "2024-02-26            0.00  0.802721          0.0   0.313722     0.307148   \n",
       "2024-02-27            0.75  0.879009          0.0   0.647885     0.005499   \n",
       "2024-02-28            0.00  0.926628          0.0   0.552613     0.091123   \n",
       "2024-02-29            0.00  1.000000          0.0   0.335229     0.103692   \n",
       "\n",
       "               Yagis  \n",
       "tarih                 \n",
       "2024-02-01  0.000000  \n",
       "2024-02-02  0.000000  \n",
       "2024-02-03  0.000000  \n",
       "2024-02-04  0.016689  \n",
       "2024-02-05  0.000000  \n",
       "2024-02-06  0.000000  \n",
       "2024-02-07  0.006653  \n",
       "2024-02-08  0.000000  \n",
       "2024-02-09  0.109833  \n",
       "2024-02-10  0.040934  \n",
       "2024-02-11  0.397609  \n",
       "2024-02-12  1.000000  \n",
       "2024-02-13  0.056495  \n",
       "2024-02-14  0.000000  \n",
       "2024-02-15  0.166892  \n",
       "2024-02-16  0.000000  \n",
       "2024-02-17  0.000000  \n",
       "2024-02-18  0.000000  \n",
       "2024-02-19  0.000000  \n",
       "2024-02-20  0.000000  \n",
       "2024-02-21  0.015223  \n",
       "2024-02-22  0.000000  \n",
       "2024-02-23  0.000000  \n",
       "2024-02-24  0.000000  \n",
       "2024-02-25  0.032927  \n",
       "2024-02-26  0.126071  \n",
       "2024-02-27  0.000000  \n",
       "2024-02-28  0.000000  \n",
       "2024-02-29  0.357127  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test\n",
    "# output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y test-train elde edimi\n",
    "x = df[features]\n",
    "y = output_var # = df[\"target_var\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "#Splitting to Training set and Test set --- burasi timeseries icin split\n",
    "timesplit = TimeSeriesSplit(n_splits=15)\n",
    "for train_index, test_index in timesplit.split(feature_transform):\n",
    "        X_tr, X_te = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_tr, y_te = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 1s 1ms/step - loss: 124.9621\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 90.3540\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 56.3940\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 50.3566\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 49.5519\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 49.0773\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.6780\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.3244\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.0067\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.7194\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.4588\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.2218\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.0061\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.8094\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.6301\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.4665\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.3171\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.1806\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.0558\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.9417\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.8373\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.7415\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.6537\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.5730\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.4988\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.4304\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.3672\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 45.3088\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.2547\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.2044\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.1576\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.1140\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.0731\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.0349\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9989\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9651\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9331\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9028\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.8741\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.8468\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.8208\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7959\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7721\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7493\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7274\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7063\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6860\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6663\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6473\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6289\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6110\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5936\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5767\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5602\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5442\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5285\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5132\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4982\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4835\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4692\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4550\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4412\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4277\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4143\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4012\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3883\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3755\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3630\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3506\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3385\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3264\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3146\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3028\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2912\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2798\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2685\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2572\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2462\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2352\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2243\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2135\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2028\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1922\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1816\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1712\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1608\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1505\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1402\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1301\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1199\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1098\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0998\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0898\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0799\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0700\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0602\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0503\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0405\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0307\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0210\n",
      "3/3 [==============================] - 0s 932us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "#Process the data for LSTM\n",
    "trainX = np.array(X_tr)\n",
    "testX = np.array(X_te)\n",
    "X_tr = trainX.reshape(X_tr.shape[0], 1, X_tr.shape[1])\n",
    "X_te = testX.reshape(X_te.shape[0], 1, X_te.shape[1])\n",
    "\n",
    "#Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Model Training\n",
    "history=lstm.fit(X_tr, y_tr, epochs=100, batch_size=8, verbose=1, shuffle=False)\n",
    "\n",
    "#LSTM Prediction\n",
    "y_pr= lstm.predict(X_te)\n",
    "\n",
    "# Predicted vs True Adj Close Value – LSTM  --burasi copy paste\n",
    "plt.plot(y_te, label='True Value')\n",
    "plt.plot(y_pr, label='LSTM Value')\n",
    "plt.title(\"Prediction by LSTM\")\n",
    "plt.xlabel('Time Scale')\n",
    "plt.ylabel('Scaled USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# test_pred = lstm.predict(gercek test)\n",
    "# csv ye yazdir vs vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with class weights: 0.06222222222222222\n",
      "egitim verisi dogrulugu  1.0\n",
      "test verisi dogrulugu  0.06222222222222222\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "k=8\n",
    "neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_hat = neigh.predict(X_test)\n",
    "\n",
    "test_accuracy = neigh.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy with class weights:\", test_accuracy)\n",
    "print(\"egitim verisi dogrulugu \", metrics.accuracy_score(y_train,neigh.predict(X_train)))\n",
    "print(\"test verisi dogrulugu \", metrics.accuracy_score(y_test,y_hat))\n",
    "\n",
    "# test tahmin --tahmini yazmada sikinti cikiyor tek bir ilceden tahmin yapinca iste\n",
    "y_hat = neigh.predict(df_test[features])\n",
    "submission = pd.read_csv(\"sample_submission.csv\", low_memory=False)\n",
    "y_hat = np.round(y_hat).astype(int)\n",
    "df_test['bildirimsiz_sum'] = y_hat\n",
    "df_test.to_csv('knnprediction.csv', index=False)\n",
    "\n",
    "# optimal k degeri\n",
    "\n",
    "# # Define the range of k values to try\n",
    "# k_values = range(1, 21)\n",
    "\n",
    "# # Perform cross-validation for each value of k\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "\n",
    "# # Find the optimal value of k with the highest cross-validation score\n",
    "# optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "# print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "# # Train the model with the optimal k value\n",
    "# neigh = KNeighborsClassifier(n_neighbors=optimal_k).fit(X_train, y_train)\n",
    "# test_accuracy = neigh.score(X_test, y_test)\n",
    "# print(\"Test accuracy with optimal k:\", test_accuracy)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() specifies that the network is a linear stack of layers\n",
    "\n",
    "model.add() adds the hidden layer.\n",
    "\n",
    "Dense means that neurons between layers are fully connected\n",
    "\n",
    "input_dim defines the number of features in the training dataset\n",
    "\n",
    "activation defines the activation function\n",
    "\n",
    "loss selects the cost function\n",
    "\n",
    "optimizer selects the learning algorithm\n",
    "\n",
    "metrics selects the performance metrics to be saved for further analysis\n",
    "\n",
    "model.fit() initialize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 13ms/step - loss: 40.6942 - mae: 4.8690 - val_loss: 32.7457 - val_mae: 4.1111\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5754 - mae: 3.5881 - val_loss: 18.8914 - val_mae: 2.7386\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 17.6552 - mae: 2.8173 - val_loss: 15.8487 - val_mae: 2.8089\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.6324 - mae: 2.8691 - val_loss: 15.5235 - val_mae: 2.7196\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.4463 - mae: 2.7746 - val_loss: 15.3618 - val_mae: 2.7305\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.2818 - mae: 2.8018 - val_loss: 15.2415 - val_mae: 2.7426\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.1658 - mae: 2.8132 - val_loss: 15.1345 - val_mae: 2.7033\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.0243 - mae: 2.7744 - val_loss: 15.0513 - val_mae: 2.7486\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.1724 - mae: 2.8922 - val_loss: 14.9713 - val_mae: 2.7272\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.1579 - mae: 2.7274 - val_loss: 14.9425 - val_mae: 2.6902\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.8368 - mae: 2.7615 - val_loss: 14.8644 - val_mae: 2.7376\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.8100 - mae: 2.7923 - val_loss: 14.7883 - val_mae: 2.7240\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.9079 - mae: 2.8621 - val_loss: 14.7103 - val_mae: 2.7129\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.8233 - mae: 2.7179 - val_loss: 14.6860 - val_mae: 2.6968\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.7180 - mae: 2.7891 - val_loss: 14.6093 - val_mae: 2.7099\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.6495 - mae: 2.7218 - val_loss: 14.5687 - val_mae: 2.7038\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.7548 - mae: 2.8181 - val_loss: 14.5206 - val_mae: 2.7003\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5795 - mae: 2.7497 - val_loss: 14.4713 - val_mae: 2.7026\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5521 - mae: 2.7315 - val_loss: 14.4316 - val_mae: 2.7001\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5511 - mae: 2.7518 - val_loss: 14.3745 - val_mae: 2.7526\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.4578 - mae: 2.7707 - val_loss: 14.3482 - val_mae: 2.6956\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.4472 - mae: 2.7656 - val_loss: 14.2804 - val_mae: 2.7062\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.4656 - mae: 2.7345 - val_loss: 14.2554 - val_mae: 2.6957\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3623 - mae: 2.7470 - val_loss: 14.1981 - val_mae: 2.7173\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.4958 - mae: 2.8454 - val_loss: 14.1002 - val_mae: 2.7123\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.4776 - mae: 2.7079 - val_loss: 14.0631 - val_mae: 2.7162\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.3314 - mae: 2.7700 - val_loss: 14.0675 - val_mae: 2.6881\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3043 - mae: 2.7389 - val_loss: 14.0314 - val_mae: 2.6863\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.2139 - mae: 2.7254 - val_loss: 13.9772 - val_mae: 2.7014\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.1945 - mae: 2.7456 - val_loss: 13.9888 - val_mae: 2.6659\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.1747 - mae: 2.6918 - val_loss: 13.9411 - val_mae: 2.7076\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.3155 - mae: 2.7989 - val_loss: 13.9859 - val_mae: 2.6487\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.2198 - mae: 2.7172 - val_loss: 13.8862 - val_mae: 2.6906\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.1856 - mae: 2.6833 - val_loss: 13.8825 - val_mae: 2.6726\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.4571 - mae: 2.8643 - val_loss: 13.8997 - val_mae: 2.6555\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.3039 - mae: 2.6832 - val_loss: 13.8223 - val_mae: 2.6837\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0574 - mae: 2.7237 - val_loss: 13.8150 - val_mae: 2.6531\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0403 - mae: 2.7330 - val_loss: 13.7399 - val_mae: 2.6707\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0169 - mae: 2.7366 - val_loss: 13.7122 - val_mae: 2.6688\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.9593 - mae: 2.7110 - val_loss: 13.7458 - val_mae: 2.6376\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0124 - mae: 2.6727 - val_loss: 13.6781 - val_mae: 2.6678\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0038 - mae: 2.7324 - val_loss: 13.6485 - val_mae: 2.6386\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9124 - mae: 2.7036 - val_loss: 13.6354 - val_mae: 2.6360\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9189 - mae: 2.6874 - val_loss: 13.6013 - val_mae: 2.6334\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.8876 - mae: 2.6877 - val_loss: 13.5434 - val_mae: 2.6389\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0756 - mae: 2.7674 - val_loss: 13.5677 - val_mae: 2.6196\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.8082 - mae: 2.6824 - val_loss: 13.4888 - val_mae: 2.6348\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9489 - mae: 2.7475 - val_loss: 13.4747 - val_mae: 2.6194\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.8068 - mae: 2.6602 - val_loss: 13.4003 - val_mae: 2.6768\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0224 - mae: 2.7876 - val_loss: 13.8717 - val_mae: 2.5781\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 11.7241 - mae: 2.5703\n",
      "Mean Absolute Error on Test Data: 2.570312738418579\n",
      "7/7 [==============================] - 0s 919us/step\n",
      "R-squared: 0.02647202685693084\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 17ms/step - loss: 3.8432 - mae: 1.4548 - val_loss: 1.8990 - val_mae: 0.9417\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.4117 - mae: 1.0295 - val_loss: 1.2676 - val_mae: 0.8709\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8800 - mae: 1.0199 - val_loss: 1.3841 - val_mae: 0.9733\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8847 - mae: 1.0471 - val_loss: 1.3184 - val_mae: 0.9351\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8679 - mae: 1.0202 - val_loss: 1.2697 - val_mae: 0.9033\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8466 - mae: 1.0203 - val_loss: 1.2987 - val_mae: 0.9290\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8392 - mae: 1.0235 - val_loss: 1.2828 - val_mae: 0.9209\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8311 - mae: 1.0147 - val_loss: 1.2682 - val_mae: 0.9114\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8267 - mae: 1.0161 - val_loss: 1.2657 - val_mae: 0.9123\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8220 - mae: 1.0147 - val_loss: 1.2722 - val_mae: 0.9206\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8151 - mae: 1.0163 - val_loss: 1.2600 - val_mae: 0.9131\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8167 - mae: 1.0241 - val_loss: 1.2612 - val_mae: 0.9160\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8029 - mae: 1.0092 - val_loss: 1.2474 - val_mae: 0.9058\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 1.8056 - mae: 1.0007 - val_loss: 1.2341 - val_mae: 0.8957\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7899 - mae: 1.0059 - val_loss: 1.2663 - val_mae: 0.9267\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8083 - mae: 1.0097 - val_loss: 1.2398 - val_mae: 0.9044\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7992 - mae: 1.0294 - val_loss: 1.2985 - val_mae: 0.9540\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7978 - mae: 1.0025 - val_loss: 1.2092 - val_mae: 0.8713\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7738 - mae: 0.9962 - val_loss: 1.2635 - val_mae: 0.9291\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7853 - mae: 1.0149 - val_loss: 1.2339 - val_mae: 0.9029\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7785 - mae: 1.0140 - val_loss: 1.2522 - val_mae: 0.9202\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7720 - mae: 0.9997 - val_loss: 1.2338 - val_mae: 0.9042\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7659 - mae: 1.0055 - val_loss: 1.2430 - val_mae: 0.9135\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7662 - mae: 1.0003 - val_loss: 1.2203 - val_mae: 0.8934\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7607 - mae: 0.9972 - val_loss: 1.2342 - val_mae: 0.9062\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7590 - mae: 1.0014 - val_loss: 1.2487 - val_mae: 0.9204\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7650 - mae: 1.0202 - val_loss: 1.2456 - val_mae: 0.9171\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7476 - mae: 1.0013 - val_loss: 1.2191 - val_mae: 0.8917\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7614 - mae: 0.9837 - val_loss: 1.2266 - val_mae: 0.8997\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7474 - mae: 1.0035 - val_loss: 1.2598 - val_mae: 0.9289\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7493 - mae: 0.9987 - val_loss: 1.2239 - val_mae: 0.8970\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7491 - mae: 1.0056 - val_loss: 1.2313 - val_mae: 0.9040\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7500 - mae: 0.9883 - val_loss: 1.2154 - val_mae: 0.8878\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7439 - mae: 1.0028 - val_loss: 1.2488 - val_mae: 0.9195\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7375 - mae: 1.0014 - val_loss: 1.2244 - val_mae: 0.8968\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7389 - mae: 0.9871 - val_loss: 1.2341 - val_mae: 0.9052\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7414 - mae: 0.9965 - val_loss: 1.2268 - val_mae: 0.9002\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7336 - mae: 0.9993 - val_loss: 1.2381 - val_mae: 0.9088\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7320 - mae: 0.9971 - val_loss: 1.2274 - val_mae: 0.8987\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7370 - mae: 0.9989 - val_loss: 1.2568 - val_mae: 0.9231\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7351 - mae: 0.9894 - val_loss: 1.2285 - val_mae: 0.8979\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7279 - mae: 1.0000 - val_loss: 1.2541 - val_mae: 0.9212\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7246 - mae: 0.9864 - val_loss: 1.2182 - val_mae: 0.8875\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7237 - mae: 0.9982 - val_loss: 1.2453 - val_mae: 0.9122\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7172 - mae: 0.9899 - val_loss: 1.2352 - val_mae: 0.9020\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7162 - mae: 0.9936 - val_loss: 1.2355 - val_mae: 0.9025\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7152 - mae: 0.9866 - val_loss: 1.2491 - val_mae: 0.9131\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7126 - mae: 0.9910 - val_loss: 1.2273 - val_mae: 0.8930\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7085 - mae: 0.9808 - val_loss: 1.2554 - val_mae: 0.9169\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7076 - mae: 0.9989 - val_loss: 1.2596 - val_mae: 0.9192\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.4028 - mae: 0.9350\n",
      "Mean Absolute Error on Test Data: 0.9349752068519592\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09901640252070876\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 37.7171 - mae: 4.7438 - val_loss: 33.6774 - val_mae: 4.3868\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.2170 - mae: 3.4494 - val_loss: 20.2467 - val_mae: 3.1664\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.8021 - mae: 2.7096 - val_loss: 14.8908 - val_mae: 2.9410\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6780 - mae: 2.8482 - val_loss: 14.8284 - val_mae: 2.9145\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4969 - mae: 2.7760 - val_loss: 14.8287 - val_mae: 2.8800\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4809 - mae: 2.7510 - val_loss: 14.7196 - val_mae: 2.8909\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3985 - mae: 2.7321 - val_loss: 14.7570 - val_mae: 2.8699\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3640 - mae: 2.7146 - val_loss: 14.6692 - val_mae: 2.8786\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3047 - mae: 2.7721 - val_loss: 14.6042 - val_mae: 2.8847\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2793 - mae: 2.7549 - val_loss: 14.6045 - val_mae: 2.8732\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2275 - mae: 2.7342 - val_loss: 14.5947 - val_mae: 2.8689\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2129 - mae: 2.7595 - val_loss: 14.5278 - val_mae: 2.8849\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2123 - mae: 2.7177 - val_loss: 14.5775 - val_mae: 2.8711\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1462 - mae: 2.7565 - val_loss: 14.5552 - val_mae: 2.8724\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2016 - mae: 2.6967 - val_loss: 14.5802 - val_mae: 2.8709\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2175 - mae: 2.7984 - val_loss: 14.5959 - val_mae: 2.8642\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0904 - mae: 2.7139 - val_loss: 14.5717 - val_mae: 2.8707\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1599 - mae: 2.7876 - val_loss: 14.5280 - val_mae: 2.8783\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0421 - mae: 2.7357 - val_loss: 14.5648 - val_mae: 2.8713\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 14.0516 - mae: 2.7416 - val_loss: 14.6146 - val_mae: 2.8587\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0676 - mae: 2.7136 - val_loss: 14.5407 - val_mae: 2.8799\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0408 - mae: 2.7229 - val_loss: 14.5283 - val_mae: 2.8823\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0175 - mae: 2.7310 - val_loss: 14.5415 - val_mae: 2.8794\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9881 - mae: 2.7390 - val_loss: 14.5739 - val_mae: 2.8710\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9770 - mae: 2.7366 - val_loss: 14.5689 - val_mae: 2.8782\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1424 - mae: 2.6900 - val_loss: 14.5994 - val_mae: 2.8710\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9678 - mae: 2.7196 - val_loss: 14.6381 - val_mae: 2.8642\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9518 - mae: 2.7264 - val_loss: 14.5823 - val_mae: 2.8868\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9443 - mae: 2.7345 - val_loss: 14.6186 - val_mae: 2.8742\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9249 - mae: 2.7274 - val_loss: 14.6215 - val_mae: 2.8761\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9178 - mae: 2.7120 - val_loss: 14.6546 - val_mae: 2.8670\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9334 - mae: 2.7489 - val_loss: 14.6613 - val_mae: 2.8673\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9249 - mae: 2.6928 - val_loss: 14.6627 - val_mae: 2.8661\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8868 - mae: 2.7366 - val_loss: 14.6458 - val_mae: 2.8749\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9986 - mae: 2.6880 - val_loss: 14.6386 - val_mae: 2.8797\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8680 - mae: 2.7479 - val_loss: 14.6846 - val_mae: 2.8721\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8681 - mae: 2.7071 - val_loss: 14.6948 - val_mae: 2.8751\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8572 - mae: 2.7256 - val_loss: 14.7354 - val_mae: 2.8686\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8344 - mae: 2.7168 - val_loss: 14.7327 - val_mae: 2.8690\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9198 - mae: 2.6877 - val_loss: 14.6794 - val_mae: 2.8926\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8494 - mae: 2.7599 - val_loss: 14.8152 - val_mae: 2.8598\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8413 - mae: 2.6966 - val_loss: 14.7094 - val_mae: 2.8799\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8472 - mae: 2.7293 - val_loss: 14.7160 - val_mae: 2.8822\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.8616 - mae: 2.7102 - val_loss: 14.7217 - val_mae: 2.8848\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8043 - mae: 2.6957 - val_loss: 14.7472 - val_mae: 2.8735\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.7404 - mae: 2.7118 - val_loss: 14.6702 - val_mae: 2.9085\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7581 - mae: 2.7313 - val_loss: 14.7581 - val_mae: 2.8764\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8286 - mae: 2.6835 - val_loss: 14.7411 - val_mae: 2.8833\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8300 - mae: 2.7035 - val_loss: 14.6720 - val_mae: 2.8993\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8160 - mae: 2.7672 - val_loss: 14.7973 - val_mae: 2.8757\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 13.6290 - mae: 2.5181\n",
      "Mean Absolute Error on Test Data: 2.5181162357330322\n",
      "7/7 [==============================] - 0s 919us/step\n",
      "R-squared: 0.017111290900008735\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 32.7516 - mae: 3.9397 - val_loss: 43.3199 - val_mae: 3.6370\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.7505 - mae: 2.9236 - val_loss: 33.7247 - val_mae: 2.7826\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.3346 - mae: 2.6098 - val_loss: 30.5566 - val_mae: 2.8202\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.4105 - mae: 2.6547 - val_loss: 30.5375 - val_mae: 2.7806\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3338 - mae: 2.6152 - val_loss: 30.5093 - val_mae: 2.7728\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.2838 - mae: 2.6225 - val_loss: 30.4299 - val_mae: 2.7778\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.2466 - mae: 2.6326 - val_loss: 30.3902 - val_mae: 2.7782\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1928 - mae: 2.5909 - val_loss: 30.3566 - val_mae: 2.7696\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.1860 - mae: 2.6260 - val_loss: 30.2939 - val_mae: 2.7750\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1225 - mae: 2.5913 - val_loss: 30.1738 - val_mae: 2.7777\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1477 - mae: 2.5700 - val_loss: 30.0969 - val_mae: 2.7725\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1229 - mae: 2.6417 - val_loss: 30.0723 - val_mae: 2.7654\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0279 - mae: 2.5754 - val_loss: 30.0454 - val_mae: 2.7549\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0420 - mae: 2.5789 - val_loss: 29.9119 - val_mae: 2.7775\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9736 - mae: 2.6124 - val_loss: 29.8550 - val_mae: 2.7842\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9600 - mae: 2.5838 - val_loss: 29.8392 - val_mae: 2.7424\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9006 - mae: 2.5795 - val_loss: 29.6983 - val_mae: 2.7702\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8811 - mae: 2.6044 - val_loss: 29.6734 - val_mae: 2.7528\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9122 - mae: 2.5333 - val_loss: 29.4573 - val_mae: 2.7738\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8665 - mae: 2.6251 - val_loss: 29.5826 - val_mae: 2.7444\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7715 - mae: 2.5408 - val_loss: 29.3817 - val_mae: 2.7577\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7057 - mae: 2.5651 - val_loss: 29.2156 - val_mae: 2.7543\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6904 - mae: 2.5672 - val_loss: 29.1948 - val_mae: 2.7395\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6392 - mae: 2.5339 - val_loss: 29.0222 - val_mae: 2.7829\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6097 - mae: 2.5547 - val_loss: 28.9831 - val_mae: 2.7572\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.5543 - mae: 2.5445 - val_loss: 28.8851 - val_mae: 2.7627\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5458 - mae: 2.5578 - val_loss: 28.7794 - val_mae: 2.7442\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4952 - mae: 2.5272 - val_loss: 28.6549 - val_mae: 2.7443\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5235 - mae: 2.5492 - val_loss: 28.6950 - val_mae: 2.7241\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4734 - mae: 2.5566 - val_loss: 28.5290 - val_mae: 2.7711\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4037 - mae: 2.5242 - val_loss: 28.5423 - val_mae: 2.7251\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3598 - mae: 2.5241 - val_loss: 28.2527 - val_mae: 2.7708\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3190 - mae: 2.5510 - val_loss: 28.3212 - val_mae: 2.7271\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3217 - mae: 2.4967 - val_loss: 28.1175 - val_mae: 2.7647\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2713 - mae: 2.5268 - val_loss: 28.0855 - val_mae: 2.7412\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2261 - mae: 2.5153 - val_loss: 28.0533 - val_mae: 2.7235\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2387 - mae: 2.4641 - val_loss: 27.8750 - val_mae: 2.7610\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2069 - mae: 2.5824 - val_loss: 27.7974 - val_mae: 2.7442\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2124 - mae: 2.4671 - val_loss: 27.6845 - val_mae: 2.7551\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0984 - mae: 2.5215 - val_loss: 27.6450 - val_mae: 2.7581\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1410 - mae: 2.5490 - val_loss: 27.8777 - val_mae: 2.7073\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1778 - mae: 2.4735 - val_loss: 27.4279 - val_mae: 2.7717\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0231 - mae: 2.5146 - val_loss: 27.4721 - val_mae: 2.7339\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0268 - mae: 2.4989 - val_loss: 27.3554 - val_mae: 2.7417\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0707 - mae: 2.4716 - val_loss: 27.3768 - val_mae: 2.7266\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0126 - mae: 2.5019 - val_loss: 27.1731 - val_mae: 2.7293\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9844 - mae: 2.5623 - val_loss: 27.3229 - val_mae: 2.7317\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9428 - mae: 2.4495 - val_loss: 26.9661 - val_mae: 2.7450\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8740 - mae: 2.4943 - val_loss: 26.9650 - val_mae: 2.7475\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 15.8947 - mae: 2.4995 - val_loss: 27.0406 - val_mae: 2.7161\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 15.8886 - mae: 2.6499\n",
      "Mean Absolute Error on Test Data: 2.6499388217926025\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09549529505981125\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 63.4459 - mae: 6.3156 - val_loss: 57.8562 - val_mae: 5.8671\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.8026 - mae: 4.8910 - val_loss: 36.9335 - val_mae: 4.0985\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.3494 - mae: 3.4131 - val_loss: 23.4205 - val_mae: 3.3218\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.9697 - mae: 3.3306 - val_loss: 23.1027 - val_mae: 3.4241\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8302 - mae: 3.2934 - val_loss: 23.1021 - val_mae: 3.3288\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.7642 - mae: 3.2455 - val_loss: 22.9537 - val_mae: 3.3404\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6661 - mae: 3.2644 - val_loss: 22.8234 - val_mae: 3.3452\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6382 - mae: 3.2507 - val_loss: 22.7019 - val_mae: 3.3438\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.5292 - mae: 3.2776 - val_loss: 22.5914 - val_mae: 3.3608\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.4580 - mae: 3.2600 - val_loss: 22.5824 - val_mae: 3.3150\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.5552 - mae: 3.2363 - val_loss: 22.4129 - val_mae: 3.3624\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.4448 - mae: 3.2645 - val_loss: 22.4121 - val_mae: 3.3228\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.2901 - mae: 3.2474 - val_loss: 22.3481 - val_mae: 3.3200\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2648 - mae: 3.2289 - val_loss: 22.2772 - val_mae: 3.3208\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2697 - mae: 3.2888 - val_loss: 22.2066 - val_mae: 3.3152\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.2479 - mae: 3.2210 - val_loss: 22.1720 - val_mae: 3.3113\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1503 - mae: 3.2516 - val_loss: 22.1562 - val_mae: 3.3000\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1046 - mae: 3.2318 - val_loss: 21.9979 - val_mae: 3.3381\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0666 - mae: 3.2782 - val_loss: 22.1589 - val_mae: 3.2734\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1402 - mae: 3.1944 - val_loss: 21.9432 - val_mae: 3.3131\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0804 - mae: 3.2738 - val_loss: 21.9020 - val_mae: 3.3065\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9984 - mae: 3.2267 - val_loss: 21.8792 - val_mae: 3.2995\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9454 - mae: 3.2256 - val_loss: 21.7664 - val_mae: 3.3156\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9042 - mae: 3.2358 - val_loss: 21.8119 - val_mae: 3.2860\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8841 - mae: 3.2232 - val_loss: 21.7376 - val_mae: 3.2979\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8723 - mae: 3.2400 - val_loss: 21.6805 - val_mae: 3.2999\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9266 - mae: 3.2171 - val_loss: 21.6029 - val_mae: 3.3223\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9491 - mae: 3.2620 - val_loss: 21.6374 - val_mae: 3.2956\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8039 - mae: 3.2271 - val_loss: 21.6535 - val_mae: 3.2675\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7709 - mae: 3.2398 - val_loss: 21.5419 - val_mae: 3.2962\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6972 - mae: 3.2069 - val_loss: 21.6197 - val_mae: 3.2657\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7482 - mae: 3.2417 - val_loss: 21.6100 - val_mae: 3.2671\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7624 - mae: 3.1800 - val_loss: 21.4065 - val_mae: 3.2966\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6710 - mae: 3.2366 - val_loss: 21.4161 - val_mae: 3.2878\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6347 - mae: 3.2269 - val_loss: 21.4717 - val_mae: 3.2609\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5944 - mae: 3.2182 - val_loss: 21.3737 - val_mae: 3.2802\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6454 - mae: 3.2447 - val_loss: 21.4181 - val_mae: 3.2616\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5784 - mae: 3.2529 - val_loss: 21.3487 - val_mae: 3.2463\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6897 - mae: 3.1835 - val_loss: 21.1581 - val_mae: 3.3169\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4632 - mae: 3.2290 - val_loss: 21.2783 - val_mae: 3.2489\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4700 - mae: 3.2055 - val_loss: 21.2616 - val_mae: 3.2485\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4122 - mae: 3.2016 - val_loss: 21.0986 - val_mae: 3.2729\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3598 - mae: 3.2085 - val_loss: 21.0650 - val_mae: 3.2684\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3543 - mae: 3.2079 - val_loss: 21.0553 - val_mae: 3.2581\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3179 - mae: 3.2153 - val_loss: 21.0914 - val_mae: 3.2428\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2459 - mae: 3.1833 - val_loss: 21.1080 - val_mae: 3.2306\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3149 - mae: 3.2404 - val_loss: 21.0452 - val_mae: 3.2197\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1889 - mae: 3.1796 - val_loss: 20.9844 - val_mae: 3.2286\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1662 - mae: 3.2011 - val_loss: 20.8566 - val_mae: 3.2614\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2759 - mae: 3.1866 - val_loss: 20.8297 - val_mae: 3.2559\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 25.9145 - mae: 3.5444\n",
      "Mean Absolute Error on Test Data: 3.544403553009033\n",
      "7/7 [==============================] - 0s 833us/step\n",
      "R-squared: 0.09359430637445743\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 17ms/step - loss: 8.2551 - mae: 1.9175 - val_loss: 3.5411 - val_mae: 1.3584\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 6.0504 - mae: 1.3848 - val_loss: 2.1147 - val_mae: 0.9671\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.8190 - mae: 1.3004 - val_loss: 1.7922 - val_mae: 1.0560\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5964 - mae: 1.4266 - val_loss: 1.8815 - val_mae: 1.1468\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5581 - mae: 1.4334 - val_loss: 1.8337 - val_mae: 1.1141\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5165 - mae: 1.3790 - val_loss: 1.7678 - val_mae: 1.0623\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4895 - mae: 1.3588 - val_loss: 1.7768 - val_mae: 1.0809\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4506 - mae: 1.3616 - val_loss: 1.7432 - val_mae: 1.0575\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4246 - mae: 1.3602 - val_loss: 1.7690 - val_mae: 1.0859\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3963 - mae: 1.3703 - val_loss: 1.7368 - val_mae: 1.0665\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.3701 - mae: 1.3407 - val_loss: 1.7018 - val_mae: 1.0418\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.3467 - mae: 1.3400 - val_loss: 1.7266 - val_mae: 1.0649\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3297 - mae: 1.3492 - val_loss: 1.7482 - val_mae: 1.0809\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3069 - mae: 1.3581 - val_loss: 1.7190 - val_mae: 1.0607\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2928 - mae: 1.3201 - val_loss: 1.6977 - val_mae: 1.0441\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2647 - mae: 1.3374 - val_loss: 1.7319 - val_mae: 1.0729\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2719 - mae: 1.3737 - val_loss: 1.7648 - val_mae: 1.0941\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4.3112 - mae: 1.3028 - val_loss: 1.6663 - val_mae: 1.0092\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.2374 - mae: 1.3629 - val_loss: 1.8311 - val_mae: 1.1343\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2202 - mae: 1.3634 - val_loss: 1.7430 - val_mae: 1.0759\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2215 - mae: 1.3084 - val_loss: 1.6985 - val_mae: 1.0384\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2361 - mae: 1.3655 - val_loss: 1.7659 - val_mae: 1.0894\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1914 - mae: 1.3514 - val_loss: 1.7445 - val_mae: 1.0715\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1938 - mae: 1.3173 - val_loss: 1.7926 - val_mae: 1.1013\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1902 - mae: 1.3781 - val_loss: 1.8126 - val_mae: 1.1117\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1835 - mae: 1.3210 - val_loss: 1.7337 - val_mae: 1.0479\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1647 - mae: 1.3359 - val_loss: 1.8401 - val_mae: 1.1252\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1586 - mae: 1.3379 - val_loss: 1.7942 - val_mae: 1.0928\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1642 - mae: 1.3121 - val_loss: 1.8357 - val_mae: 1.1167\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1493 - mae: 1.3611 - val_loss: 1.9037 - val_mae: 1.1523\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1365 - mae: 1.3615 - val_loss: 1.8045 - val_mae: 1.0913\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1463 - mae: 1.3168 - val_loss: 1.7881 - val_mae: 1.0783\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1463 - mae: 1.3017 - val_loss: 1.7936 - val_mae: 1.0847\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1286 - mae: 1.3202 - val_loss: 1.8296 - val_mae: 1.1090\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1337 - mae: 1.3527 - val_loss: 1.8366 - val_mae: 1.1103\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1877 - mae: 1.3924 - val_loss: 1.8219 - val_mae: 1.0950\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1569 - mae: 1.2933 - val_loss: 1.7977 - val_mae: 1.0785\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1273 - mae: 1.3192 - val_loss: 1.8388 - val_mae: 1.1070\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1306 - mae: 1.3357 - val_loss: 1.9433 - val_mae: 1.1650\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1234 - mae: 1.3430 - val_loss: 1.8271 - val_mae: 1.0961\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 4.1139 - mae: 1.3242 - val_loss: 1.8848 - val_mae: 1.1301\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1069 - mae: 1.3278 - val_loss: 1.8401 - val_mae: 1.1015\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1151 - mae: 1.3086 - val_loss: 1.8501 - val_mae: 1.1073\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1072 - mae: 1.3368 - val_loss: 1.8748 - val_mae: 1.1200\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.0977 - mae: 1.3258 - val_loss: 1.8477 - val_mae: 1.1021\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1105 - mae: 1.3346 - val_loss: 1.8660 - val_mae: 1.1139\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1107 - mae: 1.3155 - val_loss: 1.8591 - val_mae: 1.1048\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1003 - mae: 1.3260 - val_loss: 1.8373 - val_mae: 1.0922\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.0963 - mae: 1.3326 - val_loss: 1.9004 - val_mae: 1.1294\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.0958 - mae: 1.3275 - val_loss: 1.9046 - val_mae: 1.1318\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.5812 - mae: 1.4505\n",
      "Mean Absolute Error on Test Data: 1.4504557847976685\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.16873890191335095\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 130.7416 - mae: 9.2896 - val_loss: 117.5515 - val_mae: 8.6758\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 108.4712 - mae: 8.0371 - val_loss: 90.6349 - val_mae: 7.0347\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 75.4312 - mae: 5.9342 - val_loss: 54.8365 - val_mae: 4.7998\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9317 - mae: 4.3727 - val_loss: 42.6228 - val_mae: 4.5797\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1633 - mae: 4.4844 - val_loss: 42.3426 - val_mae: 4.5237\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7819 - mae: 4.3861 - val_loss: 42.1846 - val_mae: 4.4917\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6610 - mae: 4.3733 - val_loss: 42.0499 - val_mae: 4.4816\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5952 - mae: 4.3498 - val_loss: 41.9202 - val_mae: 4.4865\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4379 - mae: 4.3670 - val_loss: 41.8130 - val_mae: 4.5014\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4265 - mae: 4.4233 - val_loss: 41.6792 - val_mae: 4.4837\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2250 - mae: 4.3181 - val_loss: 41.6450 - val_mae: 4.4469\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1203 - mae: 4.3420 - val_loss: 41.5124 - val_mae: 4.5040\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9828 - mae: 4.3640 - val_loss: 41.4292 - val_mae: 4.4571\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9554 - mae: 4.3169 - val_loss: 41.3684 - val_mae: 4.4839\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8309 - mae: 4.3215 - val_loss: 41.3117 - val_mae: 4.4804\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8056 - mae: 4.3508 - val_loss: 41.2500 - val_mae: 4.4586\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.7094 - mae: 4.3187 - val_loss: 41.2349 - val_mae: 4.4523\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6590 - mae: 4.3032 - val_loss: 41.1834 - val_mae: 4.5110\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5947 - mae: 4.3554 - val_loss: 41.1238 - val_mae: 4.4999\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4851 - mae: 4.3283 - val_loss: 41.1012 - val_mae: 4.4473\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5742 - mae: 4.2663 - val_loss: 41.0372 - val_mae: 4.4728\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4042 - mae: 4.3291 - val_loss: 41.0066 - val_mae: 4.4645\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3355 - mae: 4.3186 - val_loss: 40.9906 - val_mae: 4.4729\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2758 - mae: 4.2996 - val_loss: 40.9600 - val_mae: 4.4832\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2376 - mae: 4.3304 - val_loss: 40.9119 - val_mae: 4.4631\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1800 - mae: 4.2954 - val_loss: 40.9274 - val_mae: 4.4436\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3133 - mae: 4.3750 - val_loss: 40.8766 - val_mae: 4.4673\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1332 - mae: 4.2666 - val_loss: 40.8749 - val_mae: 4.4435\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1255 - mae: 4.2571 - val_loss: 40.8646 - val_mae: 4.4450\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9771 - mae: 4.3337 - val_loss: 40.8418 - val_mae: 4.4820\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9679 - mae: 4.2738 - val_loss: 40.8542 - val_mae: 4.4247\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7735 - mae: 4.2777 - val_loss: 40.8331 - val_mae: 4.4955\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8520 - mae: 4.3182 - val_loss: 40.7704 - val_mae: 4.4662\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8268 - mae: 4.2694 - val_loss: 40.8344 - val_mae: 4.4164\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8252 - mae: 4.2723 - val_loss: 40.7267 - val_mae: 4.4479\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9011 - mae: 4.3725 - val_loss: 40.7147 - val_mae: 4.4334\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6585 - mae: 4.2432 - val_loss: 40.7341 - val_mae: 4.4189\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.5926 - mae: 4.2326 - val_loss: 40.7211 - val_mae: 4.4533\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.5408 - mae: 4.3396 - val_loss: 40.6820 - val_mae: 4.4583\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4043 - mae: 4.2558 - val_loss: 40.6593 - val_mae: 4.4299\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4680 - mae: 4.2295 - val_loss: 40.6450 - val_mae: 4.4259\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4600 - mae: 4.2987 - val_loss: 40.6189 - val_mae: 4.4421\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3004 - mae: 4.2836 - val_loss: 40.6183 - val_mae: 4.4259\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.2156 - mae: 4.2412 - val_loss: 40.6553 - val_mae: 4.4049\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1200 - mae: 4.2118 - val_loss: 40.6357 - val_mae: 4.4646\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5836 - mae: 4.3549 - val_loss: 40.6608 - val_mae: 4.3901\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0980 - mae: 4.1995 - val_loss: 40.5971 - val_mae: 4.4466\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.1663 - mae: 4.2684 - val_loss: 40.5984 - val_mae: 4.4002\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.9159 - mae: 4.2692 - val_loss: 40.5585 - val_mae: 4.4322\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.0586 - mae: 4.2073 - val_loss: 40.5571 - val_mae: 4.4310\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 33.9078 - mae: 4.3108\n",
      "Mean Absolute Error on Test Data: 4.310844421386719\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.037642297447085804\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 3s 10ms/step - loss: 63.8434 - mae: 6.2063 - val_loss: 56.1802 - val_mae: 5.6759\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.2877 - mae: 4.6363 - val_loss: 34.0846 - val_mae: 3.8102\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4032 - mae: 3.3018 - val_loss: 24.2688 - val_mae: 3.4814\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.7268 - mae: 3.4377 - val_loss: 24.2284 - val_mae: 3.4760\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.5684 - mae: 3.2948 - val_loss: 24.3010 - val_mae: 3.4367\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.4656 - mae: 3.2908 - val_loss: 24.1125 - val_mae: 3.4619\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.6270 - mae: 3.4203 - val_loss: 24.2444 - val_mae: 3.4290\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.4840 - mae: 3.2412 - val_loss: 24.1462 - val_mae: 3.4255\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.3479 - mae: 3.3303 - val_loss: 23.9694 - val_mae: 3.4588\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.2905 - mae: 3.3092 - val_loss: 23.9896 - val_mae: 3.4411\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.3051 - mae: 3.2732 - val_loss: 23.9613 - val_mae: 3.4496\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.1588 - mae: 3.3122 - val_loss: 23.9584 - val_mae: 3.4275\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1149 - mae: 3.2762 - val_loss: 23.8880 - val_mae: 3.4362\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0597 - mae: 3.2838 - val_loss: 23.8883 - val_mae: 3.4248\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.0089 - mae: 3.2902 - val_loss: 23.8563 - val_mae: 3.4271\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.1589 - mae: 3.2677 - val_loss: 23.7749 - val_mae: 3.4740\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9399 - mae: 3.2990 - val_loss: 23.8210 - val_mae: 3.4215\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9196 - mae: 3.2468 - val_loss: 23.6870 - val_mae: 3.4231\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8658 - mae: 3.2898 - val_loss: 23.8052 - val_mae: 3.4042\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7641 - mae: 3.2567 - val_loss: 23.6214 - val_mae: 3.4241\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7181 - mae: 3.2827 - val_loss: 23.5902 - val_mae: 3.4441\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7895 - mae: 3.2654 - val_loss: 23.6225 - val_mae: 3.4307\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6714 - mae: 3.3237 - val_loss: 23.6186 - val_mae: 3.4121\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5958 - mae: 3.2625 - val_loss: 23.6837 - val_mae: 3.3992\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5486 - mae: 3.2556 - val_loss: 23.5357 - val_mae: 3.4268\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4912 - mae: 3.2490 - val_loss: 23.5951 - val_mae: 3.4264\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.5135 - mae: 3.2223 - val_loss: 23.5202 - val_mae: 3.4229\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5146 - mae: 3.3169 - val_loss: 23.6123 - val_mae: 3.3945\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4247 - mae: 3.2528 - val_loss: 23.5806 - val_mae: 3.4299\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3698 - mae: 3.2492 - val_loss: 23.6882 - val_mae: 3.3907\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3514 - mae: 3.2094 - val_loss: 23.3855 - val_mae: 3.4380\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2676 - mae: 3.2725 - val_loss: 23.6038 - val_mae: 3.4136\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2882 - mae: 3.2326 - val_loss: 23.4592 - val_mae: 3.4384\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2273 - mae: 3.2632 - val_loss: 23.5394 - val_mae: 3.3747\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.1513 - mae: 3.2159 - val_loss: 23.5684 - val_mae: 3.4339\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2165 - mae: 3.2867 - val_loss: 23.4547 - val_mae: 3.4154\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0489 - mae: 3.2364 - val_loss: 23.5712 - val_mae: 3.4106\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0869 - mae: 3.2901 - val_loss: 23.7355 - val_mae: 3.3953\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0123 - mae: 3.2615 - val_loss: 23.5750 - val_mae: 3.4018\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0749 - mae: 3.2074 - val_loss: 23.4318 - val_mae: 3.4250\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9542 - mae: 3.2464 - val_loss: 23.5717 - val_mae: 3.4353\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8771 - mae: 3.2243 - val_loss: 23.4923 - val_mae: 3.4020\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8735 - mae: 3.2436 - val_loss: 23.5535 - val_mae: 3.4228\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9230 - mae: 3.2708 - val_loss: 23.5592 - val_mae: 3.3742\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8801 - mae: 3.1919 - val_loss: 23.5359 - val_mae: 3.4475\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9022 - mae: 3.2587 - val_loss: 23.8317 - val_mae: 3.4283\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7518 - mae: 3.2575 - val_loss: 23.5896 - val_mae: 3.4125\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7607 - mae: 3.1792 - val_loss: 23.6185 - val_mae: 3.4589\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7585 - mae: 3.2872 - val_loss: 23.4262 - val_mae: 3.4062\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7369 - mae: 3.2261 - val_loss: 23.6698 - val_mae: 3.4167\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.6389 - mae: 3.2718\n",
      "Mean Absolute Error on Test Data: 3.2717690467834473\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.006569455786487488\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 208.2697 - mae: 12.5732 - val_loss: 169.3926 - val_mae: 10.9974\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 168.1344 - mae: 10.8675 - val_loss: 121.6970 - val_mae: 8.7143\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 105.1317 - mae: 7.7922 - val_loss: 62.8562 - val_mae: 5.7039\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 55.9472 - mae: 5.4119 - val_loss: 54.4413 - val_mae: 5.7399\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.7401 - mae: 5.4332 - val_loss: 52.9059 - val_mae: 5.5790\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.2301 - mae: 5.3134 - val_loss: 52.3771 - val_mae: 5.5148\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.9383 - mae: 5.3190 - val_loss: 52.2587 - val_mae: 5.5094\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.6912 - mae: 5.2929 - val_loss: 52.1886 - val_mae: 5.5092\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.6296 - mae: 5.2744 - val_loss: 52.4242 - val_mae: 5.5487\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.4023 - mae: 5.2834 - val_loss: 51.8822 - val_mae: 5.4855\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.1658 - mae: 5.2977 - val_loss: 52.1098 - val_mae: 5.5192\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.1089 - mae: 5.2643 - val_loss: 52.0959 - val_mae: 5.5282\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.8851 - mae: 5.2789 - val_loss: 51.7271 - val_mae: 5.4965\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.6770 - mae: 5.2489 - val_loss: 51.5141 - val_mae: 5.4770\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.8690 - mae: 5.3165 - val_loss: 51.3715 - val_mae: 5.4729\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.5131 - mae: 5.2471 - val_loss: 50.9343 - val_mae: 5.4323\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3084 - mae: 5.2120 - val_loss: 51.0944 - val_mae: 5.4639\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.2182 - mae: 5.2193 - val_loss: 51.2125 - val_mae: 5.4847\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.4720 - mae: 5.3480 - val_loss: 51.0188 - val_mae: 5.4780\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.6118 - mae: 5.1810 - val_loss: 50.6990 - val_mae: 5.4486\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.9988 - mae: 5.2671 - val_loss: 50.7988 - val_mae: 5.4752\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.9576 - mae: 5.2281 - val_loss: 50.6218 - val_mae: 5.4633\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.8338 - mae: 5.2026 - val_loss: 50.2791 - val_mae: 5.4299\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.7389 - mae: 5.2614 - val_loss: 50.5134 - val_mae: 5.4668\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.7843 - mae: 5.2553 - val_loss: 49.4358 - val_mae: 5.3406\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.5434 - mae: 5.1881 - val_loss: 49.9836 - val_mae: 5.4210\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.5008 - mae: 5.2737 - val_loss: 49.8297 - val_mae: 5.4222\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.3769 - mae: 5.2155 - val_loss: 49.5333 - val_mae: 5.3924\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.5634 - mae: 5.1728 - val_loss: 50.3261 - val_mae: 5.4881\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.3837 - mae: 5.2568 - val_loss: 49.5510 - val_mae: 5.4140\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1556 - mae: 5.1973 - val_loss: 49.1568 - val_mae: 5.3800\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1134 - mae: 5.2280 - val_loss: 49.1857 - val_mae: 5.3903\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1451 - mae: 5.1738 - val_loss: 49.3317 - val_mae: 5.4193\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9918 - mae: 5.2419 - val_loss: 48.8376 - val_mae: 5.3659\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1067 - mae: 5.1631 - val_loss: 48.9407 - val_mae: 5.3839\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.0452 - mae: 5.2041 - val_loss: 48.0255 - val_mae: 5.2864\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9867 - mae: 5.1341 - val_loss: 49.3130 - val_mae: 5.4439\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.1461 - mae: 5.2943 - val_loss: 48.0007 - val_mae: 5.2993\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.2660 - mae: 5.1705 - val_loss: 49.1297 - val_mae: 5.4363\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.9104 - mae: 5.1657 - val_loss: 48.3089 - val_mae: 5.3604\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9386 - mae: 5.2454 - val_loss: 47.7259 - val_mae: 5.2975\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.7080 - mae: 5.1489 - val_loss: 48.5662 - val_mae: 5.4024\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.6571 - mae: 5.2304 - val_loss: 47.6673 - val_mae: 5.3101\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.6850 - mae: 5.1370 - val_loss: 48.3483 - val_mae: 5.3937\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.4870 - mae: 5.2417 - val_loss: 47.6251 - val_mae: 5.3251\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.4666 - mae: 5.1315 - val_loss: 47.6349 - val_mae: 5.3349\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.4125 - mae: 5.1837 - val_loss: 48.0107 - val_mae: 5.3802\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.3696 - mae: 5.2207 - val_loss: 46.7484 - val_mae: 5.2363\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.4355 - mae: 5.1764 - val_loss: 46.8375 - val_mae: 5.2705\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.3055 - mae: 5.1190 - val_loss: 47.2849 - val_mae: 5.3305\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 41.3149 - mae: 5.0796\n",
      "Mean Absolute Error on Test Data: 5.079621315002441\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.030763220146859616\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 26.5602 - mae: 4.0357 - val_loss: 18.3180 - val_mae: 3.2043\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.7378 - mae: 3.0581 - val_loss: 10.9083 - val_mae: 2.4203\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.6707 - mae: 2.4012 - val_loss: 8.7742 - val_mae: 2.3967\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.4025 - mae: 2.4326 - val_loss: 8.5554 - val_mae: 2.3432\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.2578 - mae: 2.3746 - val_loss: 8.4699 - val_mae: 2.3283\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.2106 - mae: 2.3822 - val_loss: 8.4479 - val_mae: 2.3310\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1583 - mae: 2.3767 - val_loss: 8.4411 - val_mae: 2.3364\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1005 - mae: 2.3479 - val_loss: 8.3956 - val_mae: 2.3281\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0443 - mae: 2.3550 - val_loss: 8.3719 - val_mae: 2.3251\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0068 - mae: 2.3917 - val_loss: 8.3383 - val_mae: 2.3123\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9414 - mae: 2.3272 - val_loss: 8.3509 - val_mae: 2.3120\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9342 - mae: 2.3717 - val_loss: 8.3331 - val_mae: 2.2990\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9164 - mae: 2.3303 - val_loss: 8.4790 - val_mae: 2.3455\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8706 - mae: 2.3576 - val_loss: 8.3824 - val_mae: 2.3114\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9012 - mae: 2.3285 - val_loss: 8.5039 - val_mae: 2.3443\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8789 - mae: 2.3301 - val_loss: 8.4663 - val_mae: 2.3315\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8835 - mae: 2.3826 - val_loss: 8.3862 - val_mae: 2.3009\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8675 - mae: 2.3027 - val_loss: 8.4722 - val_mae: 2.3290\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8539 - mae: 2.3661 - val_loss: 8.4579 - val_mae: 2.3227\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8489 - mae: 2.3686 - val_loss: 8.3845 - val_mae: 2.2892\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8339 - mae: 2.3353 - val_loss: 8.3828 - val_mae: 2.2819\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8325 - mae: 2.3418 - val_loss: 8.4884 - val_mae: 2.3219\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8155 - mae: 2.3028 - val_loss: 8.4716 - val_mae: 2.3199\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7846 - mae: 2.3192 - val_loss: 8.5636 - val_mae: 2.3426\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7372 - mae: 2.3331 - val_loss: 8.4568 - val_mae: 2.3064\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7467 - mae: 2.3053 - val_loss: 8.5026 - val_mae: 2.3237\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7349 - mae: 2.3387 - val_loss: 8.5484 - val_mae: 2.3316\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7365 - mae: 2.3020 - val_loss: 8.4967 - val_mae: 2.3092\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7367 - mae: 2.3420 - val_loss: 8.5881 - val_mae: 2.3413\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7595 - mae: 2.2966 - val_loss: 8.5849 - val_mae: 2.3351\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 9.7052 - mae: 2.3300 - val_loss: 8.6605 - val_mae: 2.3526\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6987 - mae: 2.3495 - val_loss: 8.5055 - val_mae: 2.3012\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7456 - mae: 2.3388 - val_loss: 8.4783 - val_mae: 2.2813\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7006 - mae: 2.3057 - val_loss: 8.5646 - val_mae: 2.3174\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6668 - mae: 2.3257 - val_loss: 8.5778 - val_mae: 2.3164\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7195 - mae: 2.2981 - val_loss: 8.8515 - val_mae: 2.3928\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7260 - mae: 2.3131 - val_loss: 8.6122 - val_mae: 2.3264\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6696 - mae: 2.3417 - val_loss: 8.4914 - val_mae: 2.2772\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6800 - mae: 2.3079 - val_loss: 8.5900 - val_mae: 2.3127\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6304 - mae: 2.2775 - val_loss: 8.6160 - val_mae: 2.3201\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6094 - mae: 2.3122 - val_loss: 8.7164 - val_mae: 2.3495\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6098 - mae: 2.3316 - val_loss: 8.7037 - val_mae: 2.3437\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6059 - mae: 2.3202 - val_loss: 8.4970 - val_mae: 2.2682\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6344 - mae: 2.2839 - val_loss: 8.7267 - val_mae: 2.3582\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.5999 - mae: 2.3471 - val_loss: 8.5628 - val_mae: 2.2976\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6035 - mae: 2.2582 - val_loss: 8.7208 - val_mae: 2.3424\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7292 - mae: 2.3624 - val_loss: 8.5377 - val_mae: 2.2751\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6661 - mae: 2.3157 - val_loss: 8.5808 - val_mae: 2.2995\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5532 - mae: 2.2867 - val_loss: 8.6653 - val_mae: 2.3266\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5402 - mae: 2.3121 - val_loss: 8.6281 - val_mae: 2.3049\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.3394 - mae: 2.2779\n",
      "Mean Absolute Error on Test Data: 2.277857542037964\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.003746353733041996\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 42.0116 - mae: 5.4232 - val_loss: 39.8323 - val_mae: 5.0221\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.4648 - mae: 4.1793 - val_loss: 23.6241 - val_mae: 3.3769\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.4370 - mae: 2.7812 - val_loss: 14.0395 - val_mae: 2.7094\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.3898 - mae: 2.6909 - val_loss: 13.9556 - val_mae: 2.6990\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.3632 - mae: 2.6018 - val_loss: 13.9149 - val_mae: 2.6826\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2389 - mae: 2.6419 - val_loss: 13.8388 - val_mae: 2.6735\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.1505 - mae: 2.6058 - val_loss: 13.7401 - val_mae: 2.6766\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.1195 - mae: 2.5947 - val_loss: 13.7048 - val_mae: 2.6748\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0753 - mae: 2.6020 - val_loss: 13.6016 - val_mae: 2.6883\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.1034 - mae: 2.6511 - val_loss: 13.7595 - val_mae: 2.6379\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0004 - mae: 2.5679 - val_loss: 13.5129 - val_mae: 2.6673\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9464 - mae: 2.6121 - val_loss: 13.4490 - val_mae: 2.6712\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9356 - mae: 2.5946 - val_loss: 13.4998 - val_mae: 2.6495\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9678 - mae: 2.6157 - val_loss: 13.5122 - val_mae: 2.6398\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8989 - mae: 2.5659 - val_loss: 13.3734 - val_mae: 2.6696\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9295 - mae: 2.6148 - val_loss: 13.3900 - val_mae: 2.6545\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8568 - mae: 2.5835 - val_loss: 13.3228 - val_mae: 2.6706\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8541 - mae: 2.5907 - val_loss: 13.3226 - val_mae: 2.6546\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8370 - mae: 2.5975 - val_loss: 13.3089 - val_mae: 2.6621\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9248 - mae: 2.6275 - val_loss: 13.3174 - val_mae: 2.6486\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8906 - mae: 2.5570 - val_loss: 13.2837 - val_mae: 2.6716\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8209 - mae: 2.6070 - val_loss: 13.2382 - val_mae: 2.6726\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8344 - mae: 2.5948 - val_loss: 13.2218 - val_mae: 2.6809\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8677 - mae: 2.5889 - val_loss: 13.2290 - val_mae: 2.6659\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8050 - mae: 2.6280 - val_loss: 13.2186 - val_mae: 2.6533\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7724 - mae: 2.5724 - val_loss: 13.2238 - val_mae: 2.6497\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8435 - mae: 2.6159 - val_loss: 13.2666 - val_mae: 2.6305\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7498 - mae: 2.5719 - val_loss: 13.1430 - val_mae: 2.6837\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8142 - mae: 2.6463 - val_loss: 13.3112 - val_mae: 2.6226\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7865 - mae: 2.5607 - val_loss: 13.1506 - val_mae: 2.6848\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8223 - mae: 2.6130 - val_loss: 13.2062 - val_mae: 2.6383\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7303 - mae: 2.6067 - val_loss: 13.1173 - val_mae: 2.6620\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7199 - mae: 2.5916 - val_loss: 13.1180 - val_mae: 2.6590\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7131 - mae: 2.6062 - val_loss: 13.1295 - val_mae: 2.6474\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7216 - mae: 2.5761 - val_loss: 13.1110 - val_mae: 2.6537\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7069 - mae: 2.5649 - val_loss: 13.1156 - val_mae: 2.6525\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7059 - mae: 2.6019 - val_loss: 13.0653 - val_mae: 2.6743\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6871 - mae: 2.5769 - val_loss: 13.0720 - val_mae: 2.6590\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6674 - mae: 2.6015 - val_loss: 13.0604 - val_mae: 2.6531\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6891 - mae: 2.5895 - val_loss: 13.2172 - val_mae: 2.6221\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7911 - mae: 2.6005 - val_loss: 13.1770 - val_mae: 2.6281\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6532 - mae: 2.6016 - val_loss: 13.0323 - val_mae: 2.6515\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7158 - mae: 2.5512 - val_loss: 13.0055 - val_mae: 2.6791\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6588 - mae: 2.6264 - val_loss: 13.1014 - val_mae: 2.6332\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7313 - mae: 2.5880 - val_loss: 13.1809 - val_mae: 2.6172\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6921 - mae: 2.5559 - val_loss: 13.0250 - val_mae: 2.6755\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6479 - mae: 2.5797 - val_loss: 13.0475 - val_mae: 2.6580\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6165 - mae: 2.5814 - val_loss: 12.9998 - val_mae: 2.6654\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.5864 - mae: 2.6046 - val_loss: 13.1798 - val_mae: 2.6158\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6110 - mae: 2.5422 - val_loss: 13.0279 - val_mae: 2.6736\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.6317 - mae: 3.0448\n",
      "Mean Absolute Error on Test Data: 3.0448317527770996\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08279431833682516\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 10ms/step - loss: 25.2286 - mae: 4.1413 - val_loss: 24.2593 - val_mae: 3.8036\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0297 - mae: 3.1078 - val_loss: 14.8434 - val_mae: 2.7598\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5607 - mae: 2.2004 - val_loss: 10.0568 - val_mae: 2.4072\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1948 - mae: 2.1872 - val_loss: 10.0383 - val_mae: 2.4179\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1018 - mae: 2.1148 - val_loss: 10.0831 - val_mae: 2.3807\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0924 - mae: 2.1283 - val_loss: 10.0229 - val_mae: 2.3944\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0704 - mae: 2.1203 - val_loss: 10.0085 - val_mae: 2.3994\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0695 - mae: 2.1429 - val_loss: 9.9954 - val_mae: 2.4012\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0628 - mae: 2.1180 - val_loss: 10.0103 - val_mae: 2.3876\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0521 - mae: 2.1155 - val_loss: 9.9993 - val_mae: 2.3914\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0432 - mae: 2.1284 - val_loss: 9.9931 - val_mae: 2.3898\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0385 - mae: 2.1218 - val_loss: 9.9720 - val_mae: 2.3941\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0213 - mae: 2.1220 - val_loss: 9.9821 - val_mae: 2.3868\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0294 - mae: 2.1235 - val_loss: 9.9615 - val_mae: 2.4000\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0262 - mae: 2.1209 - val_loss: 9.9879 - val_mae: 2.3828\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0139 - mae: 2.1230 - val_loss: 9.9757 - val_mae: 2.3845\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0082 - mae: 2.1207 - val_loss: 9.9724 - val_mae: 2.3857\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0098 - mae: 2.1340 - val_loss: 9.9686 - val_mae: 2.3874\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0237 - mae: 2.1142 - val_loss: 9.9556 - val_mae: 2.3903\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9958 - mae: 2.1166 - val_loss: 9.9841 - val_mae: 2.3810\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0265 - mae: 2.1354 - val_loss: 9.9963 - val_mae: 2.3761\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0052 - mae: 2.1004 - val_loss: 9.9435 - val_mae: 2.3980\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9941 - mae: 2.1320 - val_loss: 9.9747 - val_mae: 2.3808\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0003 - mae: 2.1094 - val_loss: 9.9458 - val_mae: 2.3892\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0071 - mae: 2.1177 - val_loss: 10.0041 - val_mae: 2.3740\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9986 - mae: 2.1264 - val_loss: 9.9682 - val_mae: 2.3821\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9932 - mae: 2.1159 - val_loss: 9.9509 - val_mae: 2.3939\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9897 - mae: 2.1110 - val_loss: 9.9833 - val_mae: 2.3776\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9669 - mae: 2.1199 - val_loss: 9.9596 - val_mae: 2.3833\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9817 - mae: 2.1139 - val_loss: 9.9592 - val_mae: 2.3841\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 7.9698 - mae: 2.1301 - val_loss: 9.9709 - val_mae: 2.3794\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9672 - mae: 2.1049 - val_loss: 9.9926 - val_mae: 2.3758\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9652 - mae: 2.1217 - val_loss: 9.9423 - val_mae: 2.3962\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0042 - mae: 2.1052 - val_loss: 9.9819 - val_mae: 2.3758\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9546 - mae: 2.1217 - val_loss: 9.9479 - val_mae: 2.3892\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9765 - mae: 2.1278 - val_loss: 9.9915 - val_mae: 2.3746\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9917 - mae: 2.1181 - val_loss: 10.0032 - val_mae: 2.3716\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9721 - mae: 2.1181 - val_loss: 9.9547 - val_mae: 2.3816\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9671 - mae: 2.1101 - val_loss: 10.0214 - val_mae: 2.3685\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9651 - mae: 2.1044 - val_loss: 9.9515 - val_mae: 2.3842\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9450 - mae: 2.1290 - val_loss: 9.9595 - val_mae: 2.3801\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9550 - mae: 2.1264 - val_loss: 10.0276 - val_mae: 2.3665\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9638 - mae: 2.0966 - val_loss: 9.9920 - val_mae: 2.3710\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9479 - mae: 2.1180 - val_loss: 9.9632 - val_mae: 2.3753\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9442 - mae: 2.1164 - val_loss: 9.9572 - val_mae: 2.3800\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9844 - mae: 2.1131 - val_loss: 9.9408 - val_mae: 2.3881\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9577 - mae: 2.1137 - val_loss: 9.9499 - val_mae: 2.3842\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9411 - mae: 2.0984 - val_loss: 9.9689 - val_mae: 2.3766\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9528 - mae: 2.1188 - val_loss: 9.9642 - val_mae: 2.3799\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9537 - mae: 2.1255 - val_loss: 9.9643 - val_mae: 2.3767\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 10.1143 - mae: 2.2761\n",
      "Mean Absolute Error on Test Data: 2.2760732173919678\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.021191022386670366\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 12ms/step - loss: 8.6473 - mae: 2.0742 - val_loss: 7.2146 - val_mae: 1.8681\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.5732 - mae: 1.5170 - val_loss: 4.5637 - val_mae: 1.4946\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.5507 - mae: 1.5268 - val_loss: 4.2792 - val_mae: 1.5563\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4950 - mae: 1.5170 - val_loss: 4.3681 - val_mae: 1.5166\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4636 - mae: 1.4918 - val_loss: 4.3141 - val_mae: 1.5292\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4521 - mae: 1.5208 - val_loss: 4.3011 - val_mae: 1.5323\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4497 - mae: 1.5231 - val_loss: 4.3308 - val_mae: 1.5245\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4444 - mae: 1.5005 - val_loss: 4.3258 - val_mae: 1.5257\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4332 - mae: 1.4901 - val_loss: 4.3262 - val_mae: 1.5260\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.4272 - mae: 1.5021 - val_loss: 4.3107 - val_mae: 1.5313\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4140 - mae: 1.5045 - val_loss: 4.3324 - val_mae: 1.5262\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4139 - mae: 1.4967 - val_loss: 4.3140 - val_mae: 1.5309\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4132 - mae: 1.5000 - val_loss: 4.3263 - val_mae: 1.5287\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4082 - mae: 1.5157 - val_loss: 4.3082 - val_mae: 1.5342\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4067 - mae: 1.4902 - val_loss: 4.3453 - val_mae: 1.5273\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3971 - mae: 1.4984 - val_loss: 4.2955 - val_mae: 1.5430\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4032 - mae: 1.5173 - val_loss: 4.3720 - val_mae: 1.5244\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4437 - mae: 1.5431 - val_loss: 4.3218 - val_mae: 1.5368\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4035 - mae: 1.4627 - val_loss: 4.3936 - val_mae: 1.5206\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3877 - mae: 1.4891 - val_loss: 4.3384 - val_mae: 1.5313\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3949 - mae: 1.5258 - val_loss: 4.3331 - val_mae: 1.5323\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3716 - mae: 1.4973 - val_loss: 4.3272 - val_mae: 1.5342\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3829 - mae: 1.4977 - val_loss: 4.2994 - val_mae: 1.5446\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3754 - mae: 1.5152 - val_loss: 4.3652 - val_mae: 1.5271\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3726 - mae: 1.4972 - val_loss: 4.3341 - val_mae: 1.5333\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.3637 - mae: 1.4903 - val_loss: 4.3640 - val_mae: 1.5278\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3778 - mae: 1.5239 - val_loss: 4.3043 - val_mae: 1.5445\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4235 - mae: 1.4657 - val_loss: 4.3967 - val_mae: 1.5222\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3609 - mae: 1.5106 - val_loss: 4.3185 - val_mae: 1.5402\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3600 - mae: 1.5083 - val_loss: 4.3395 - val_mae: 1.5342\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3502 - mae: 1.5036 - val_loss: 4.3228 - val_mae: 1.5384\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3715 - mae: 1.4847 - val_loss: 4.3462 - val_mae: 1.5326\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3523 - mae: 1.5143 - val_loss: 4.3514 - val_mae: 1.5310\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3591 - mae: 1.4718 - val_loss: 4.3407 - val_mae: 1.5345\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3478 - mae: 1.5167 - val_loss: 4.3206 - val_mae: 1.5399\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3527 - mae: 1.4816 - val_loss: 4.3471 - val_mae: 1.5322\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3785 - mae: 1.5414 - val_loss: 4.3667 - val_mae: 1.5283\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4317 - mae: 1.4449 - val_loss: 4.3615 - val_mae: 1.5300\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3746 - mae: 1.5408 - val_loss: 4.3421 - val_mae: 1.5345\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3916 - mae: 1.4516 - val_loss: 4.3680 - val_mae: 1.5273\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3358 - mae: 1.5160 - val_loss: 4.3197 - val_mae: 1.5390\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 4.3304 - mae: 1.5137 - val_loss: 4.3488 - val_mae: 1.5307\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3268 - mae: 1.4769 - val_loss: 4.3803 - val_mae: 1.5255\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3189 - mae: 1.4788 - val_loss: 4.3259 - val_mae: 1.5355\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3251 - mae: 1.5126 - val_loss: 4.3279 - val_mae: 1.5350\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3244 - mae: 1.4778 - val_loss: 4.3812 - val_mae: 1.5233\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3251 - mae: 1.5024 - val_loss: 4.3496 - val_mae: 1.5283\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3095 - mae: 1.4765 - val_loss: 4.3906 - val_mae: 1.5227\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3461 - mae: 1.5081 - val_loss: 4.3938 - val_mae: 1.5203\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.3169 - mae: 1.4643 - val_loss: 4.3297 - val_mae: 1.5307\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.2746 - mae: 1.4208\n",
      "Mean Absolute Error on Test Data: 1.4207794666290283\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.027200503538195364\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 13ms/step - loss: 8.1714 - mae: 2.1182 - val_loss: 6.0377 - val_mae: 1.6424\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4.8808 - mae: 1.4554 - val_loss: 3.6677 - val_mae: 1.3008\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.7607 - mae: 1.4093 - val_loss: 3.5105 - val_mae: 1.3980\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.7691 - mae: 1.4095 - val_loss: 3.5127 - val_mae: 1.3401\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.7017 - mae: 1.3767 - val_loss: 3.4898 - val_mae: 1.3449\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6746 - mae: 1.4095 - val_loss: 3.4698 - val_mae: 1.3554\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6542 - mae: 1.3706 - val_loss: 3.4823 - val_mae: 1.3280\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6237 - mae: 1.3813 - val_loss: 3.4559 - val_mae: 1.3538\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6151 - mae: 1.3735 - val_loss: 3.4590 - val_mae: 1.3322\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6003 - mae: 1.3842 - val_loss: 3.4442 - val_mae: 1.3814\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5865 - mae: 1.3839 - val_loss: 3.4565 - val_mae: 1.3267\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5903 - mae: 1.3475 - val_loss: 3.4482 - val_mae: 1.3330\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5861 - mae: 1.3971 - val_loss: 3.4395 - val_mae: 1.3512\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5998 - mae: 1.3471 - val_loss: 3.4384 - val_mae: 1.3486\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5499 - mae: 1.3823 - val_loss: 3.4401 - val_mae: 1.3593\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5356 - mae: 1.3789 - val_loss: 3.4442 - val_mae: 1.3455\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5298 - mae: 1.3529 - val_loss: 3.4478 - val_mae: 1.3341\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.5231 - mae: 1.3526 - val_loss: 3.4450 - val_mae: 1.3414\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5224 - mae: 1.3546 - val_loss: 3.4410 - val_mae: 1.3589\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5194 - mae: 1.3854 - val_loss: 3.4456 - val_mae: 1.3608\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5168 - mae: 1.3593 - val_loss: 3.4481 - val_mae: 1.3621\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5082 - mae: 1.3854 - val_loss: 3.4476 - val_mae: 1.3685\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5197 - mae: 1.3591 - val_loss: 3.4573 - val_mae: 1.3531\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4969 - mae: 1.3538 - val_loss: 3.4580 - val_mae: 1.3475\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4938 - mae: 1.3660 - val_loss: 3.4570 - val_mae: 1.3482\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4946 - mae: 1.3558 - val_loss: 3.4555 - val_mae: 1.3443\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4894 - mae: 1.3592 - val_loss: 3.4571 - val_mae: 1.3600\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4808 - mae: 1.3710 - val_loss: 3.4758 - val_mae: 1.3394\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4875 - mae: 1.3516 - val_loss: 3.4717 - val_mae: 1.3740\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4870 - mae: 1.3720 - val_loss: 3.4456 - val_mae: 1.3726\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4733 - mae: 1.3838 - val_loss: 3.4442 - val_mae: 1.3601\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4895 - mae: 1.3424 - val_loss: 3.4417 - val_mae: 1.3681\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5190 - mae: 1.4195 - val_loss: 3.4391 - val_mae: 1.3600\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5186 - mae: 1.3326 - val_loss: 3.4466 - val_mae: 1.3488\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4620 - mae: 1.3684 - val_loss: 3.4451 - val_mae: 1.3655\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4721 - mae: 1.3506 - val_loss: 3.4475 - val_mae: 1.3412\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4616 - mae: 1.3796 - val_loss: 3.4475 - val_mae: 1.3636\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4642 - mae: 1.3353 - val_loss: 3.4457 - val_mae: 1.3557\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4722 - mae: 1.3901 - val_loss: 3.4328 - val_mae: 1.3703\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4597 - mae: 1.3595 - val_loss: 3.4273 - val_mae: 1.3601\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4497 - mae: 1.3650 - val_loss: 3.4396 - val_mae: 1.3341\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4527 - mae: 1.3435 - val_loss: 3.4376 - val_mae: 1.3566\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4655 - mae: 1.3909 - val_loss: 3.4420 - val_mae: 1.3372\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4475 - mae: 1.3376 - val_loss: 3.4335 - val_mae: 1.3373\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4452 - mae: 1.3570 - val_loss: 3.4296 - val_mae: 1.3463\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4561 - mae: 1.3606 - val_loss: 3.4361 - val_mae: 1.3508\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3.4416 - mae: 1.3359 - val_loss: 3.4429 - val_mae: 1.3388\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4260 - mae: 1.3510 - val_loss: 3.4385 - val_mae: 1.3562\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4379 - mae: 1.3324 - val_loss: 3.4442 - val_mae: 1.3416\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4364 - mae: 1.3745 - val_loss: 3.4288 - val_mae: 1.3529\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.3617 - mae: 1.5565\n",
      "Mean Absolute Error on Test Data: 1.5564793348312378\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.007287544301399551\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 44.7946 - mae: 5.2525 - val_loss: 51.4999 - val_mae: 4.6255\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4858 - mae: 4.0949 - val_loss: 38.5340 - val_mae: 3.5860\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0521 - mae: 3.1985 - val_loss: 30.6268 - val_mae: 3.4444\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.6825 - mae: 3.1790 - val_loss: 30.3492 - val_mae: 3.5148\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.5062 - mae: 3.1119 - val_loss: 30.0969 - val_mae: 3.4348\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.4523 - mae: 3.1470 - val_loss: 29.8971 - val_mae: 3.4457\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.2476 - mae: 3.0948 - val_loss: 29.7584 - val_mae: 3.4233\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1774 - mae: 3.0760 - val_loss: 29.6046 - val_mae: 3.4154\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1107 - mae: 3.0892 - val_loss: 29.4466 - val_mae: 3.4174\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0464 - mae: 3.0841 - val_loss: 29.3238 - val_mae: 3.4317\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9742 - mae: 3.0901 - val_loss: 29.1998 - val_mae: 3.4093\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9479 - mae: 3.0619 - val_loss: 29.2042 - val_mae: 3.3712\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9254 - mae: 3.0854 - val_loss: 29.0712 - val_mae: 3.3729\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8285 - mae: 3.0537 - val_loss: 28.9953 - val_mae: 3.3818\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8091 - mae: 3.0551 - val_loss: 28.9329 - val_mae: 3.3854\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7720 - mae: 3.0867 - val_loss: 28.8735 - val_mae: 3.3464\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7071 - mae: 3.0355 - val_loss: 28.8063 - val_mae: 3.3320\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6696 - mae: 3.0560 - val_loss: 28.7872 - val_mae: 3.3633\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6470 - mae: 3.0584 - val_loss: 28.7409 - val_mae: 3.3492\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5922 - mae: 3.0231 - val_loss: 28.7172 - val_mae: 3.3196\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5507 - mae: 3.0291 - val_loss: 28.6086 - val_mae: 3.3606\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5041 - mae: 3.0300 - val_loss: 28.5385 - val_mae: 3.3257\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.4845 - mae: 3.0219 - val_loss: 28.5477 - val_mae: 3.3704\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4444 - mae: 3.0341 - val_loss: 28.5271 - val_mae: 3.3121\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4493 - mae: 3.0220 - val_loss: 28.5513 - val_mae: 3.3711\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3842 - mae: 3.0110 - val_loss: 28.6066 - val_mae: 3.3029\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3869 - mae: 3.0386 - val_loss: 28.5759 - val_mae: 3.2948\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2907 - mae: 2.9900 - val_loss: 28.4168 - val_mae: 3.3365\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2155 - mae: 3.0086 - val_loss: 28.4911 - val_mae: 3.3047\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2036 - mae: 2.9915 - val_loss: 28.5851 - val_mae: 3.2931\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1908 - mae: 3.0136 - val_loss: 28.3872 - val_mae: 3.3168\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2491 - mae: 2.9548 - val_loss: 28.4244 - val_mae: 3.3299\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1066 - mae: 3.0227 - val_loss: 28.4319 - val_mae: 3.3149\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0478 - mae: 2.9763 - val_loss: 28.4365 - val_mae: 3.2989\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0471 - mae: 2.9747 - val_loss: 28.4340 - val_mae: 3.3081\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0131 - mae: 2.9834 - val_loss: 28.4265 - val_mae: 3.3240\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9632 - mae: 2.9899 - val_loss: 28.4772 - val_mae: 3.2808\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9487 - mae: 2.9728 - val_loss: 28.4704 - val_mae: 3.2994\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9133 - mae: 2.9599 - val_loss: 28.5022 - val_mae: 3.2710\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9438 - mae: 2.9829 - val_loss: 28.4188 - val_mae: 3.2733\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8909 - mae: 2.9709 - val_loss: 28.5197 - val_mae: 3.3138\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8576 - mae: 2.9952 - val_loss: 28.5635 - val_mae: 3.2584\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9529 - mae: 2.9316 - val_loss: 28.4606 - val_mae: 3.3283\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8945 - mae: 2.9880 - val_loss: 28.4371 - val_mae: 3.2838\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8554 - mae: 2.9805 - val_loss: 28.4918 - val_mae: 3.2394\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8224 - mae: 2.9677 - val_loss: 28.5756 - val_mae: 3.2854\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7772 - mae: 2.9375 - val_loss: 28.4782 - val_mae: 3.3231\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.7679 - mae: 2.9681 - val_loss: 28.5378 - val_mae: 3.2963\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7042 - mae: 2.9697 - val_loss: 28.5023 - val_mae: 3.2866\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6968 - mae: 2.9493 - val_loss: 28.5210 - val_mae: 3.2841\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.8569 - mae: 3.2850\n",
      "Mean Absolute Error on Test Data: 3.2849619388580322\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.08485811675460342\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 47.3917 - mae: 4.8050 - val_loss: 37.7872 - val_mae: 4.2967\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 38.3772 - mae: 3.9172 - val_loss: 27.3394 - val_mae: 3.2166\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 27.3964 - mae: 3.0607 - val_loss: 19.7753 - val_mae: 2.9361\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.7878 - mae: 3.1941 - val_loss: 19.4859 - val_mae: 3.1003\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.4662 - mae: 3.1434 - val_loss: 18.8067 - val_mae: 2.9523\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.1384 - mae: 3.0910 - val_loss: 18.3716 - val_mae: 2.9152\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.8689 - mae: 3.0052 - val_loss: 18.0034 - val_mae: 2.9565\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.6560 - mae: 3.0996 - val_loss: 17.6200 - val_mae: 2.8720\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.3368 - mae: 3.0164 - val_loss: 17.3147 - val_mae: 2.8979\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.2585 - mae: 3.0007 - val_loss: 17.1628 - val_mae: 2.9417\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.0767 - mae: 3.1015 - val_loss: 16.7806 - val_mae: 2.7752\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.0360 - mae: 2.9178 - val_loss: 16.6325 - val_mae: 2.8792\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.8060 - mae: 3.0725 - val_loss: 16.3692 - val_mae: 2.7652\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.7398 - mae: 2.9301 - val_loss: 16.2837 - val_mae: 2.8291\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5813 - mae: 2.9585 - val_loss: 16.1947 - val_mae: 2.8397\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5574 - mae: 3.0126 - val_loss: 16.0376 - val_mae: 2.7652\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.5465 - mae: 2.9076 - val_loss: 16.0446 - val_mae: 2.8446\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5501 - mae: 2.9903 - val_loss: 15.9258 - val_mae: 2.7810\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4241 - mae: 2.9965 - val_loss: 15.9131 - val_mae: 2.8185\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4583 - mae: 2.9103 - val_loss: 15.8664 - val_mae: 2.8304\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.4839 - mae: 2.9451 - val_loss: 15.8085 - val_mae: 2.8069\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5422 - mae: 3.0050 - val_loss: 15.7548 - val_mae: 2.7614\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2889 - mae: 2.9609 - val_loss: 15.7487 - val_mae: 2.8057\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3450 - mae: 2.9557 - val_loss: 15.7662 - val_mae: 2.8332\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2878 - mae: 2.9470 - val_loss: 15.7269 - val_mae: 2.8248\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2586 - mae: 2.9654 - val_loss: 15.6583 - val_mae: 2.7934\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2855 - mae: 2.9794 - val_loss: 15.6366 - val_mae: 2.7345\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2278 - mae: 2.8935 - val_loss: 15.6180 - val_mae: 2.8007\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1982 - mae: 2.9768 - val_loss: 15.6135 - val_mae: 2.8095\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2097 - mae: 2.9069 - val_loss: 15.5925 - val_mae: 2.7911\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1361 - mae: 2.9596 - val_loss: 15.5926 - val_mae: 2.8218\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1145 - mae: 2.9400 - val_loss: 15.5264 - val_mae: 2.7567\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1681 - mae: 2.9254 - val_loss: 15.5113 - val_mae: 2.7814\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1989 - mae: 2.8991 - val_loss: 15.5926 - val_mae: 2.8430\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2584 - mae: 2.9982 - val_loss: 15.5236 - val_mae: 2.8088\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1341 - mae: 2.9406 - val_loss: 15.5609 - val_mae: 2.8332\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0873 - mae: 2.9743 - val_loss: 15.4324 - val_mae: 2.7460\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0793 - mae: 2.8873 - val_loss: 15.4834 - val_mae: 2.8098\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0216 - mae: 2.9414 - val_loss: 15.4198 - val_mae: 2.7690\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0094 - mae: 2.9202 - val_loss: 15.4809 - val_mae: 2.8292\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0295 - mae: 2.9178 - val_loss: 15.4389 - val_mae: 2.7820\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9935 - mae: 2.9444 - val_loss: 15.3985 - val_mae: 2.7714\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0184 - mae: 2.9404 - val_loss: 15.4380 - val_mae: 2.8026\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0020 - mae: 2.9231 - val_loss: 15.4186 - val_mae: 2.8137\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0880 - mae: 2.8737 - val_loss: 15.3658 - val_mae: 2.7751\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0015 - mae: 2.9714 - val_loss: 15.3551 - val_mae: 2.7701\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9264 - mae: 2.8830 - val_loss: 15.3894 - val_mae: 2.7920\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0917 - mae: 2.9705 - val_loss: 15.3484 - val_mae: 2.7291\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9589 - mae: 2.9317 - val_loss: 15.3566 - val_mae: 2.7815\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9839 - mae: 2.9424 - val_loss: 15.3256 - val_mae: 2.7441\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 21.0451 - mae: 3.1981\n",
      "Mean Absolute Error on Test Data: 3.1981382369995117\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04729813152215678\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 43.2572 - mae: 5.3460 - val_loss: 63.3250 - val_mae: 5.7165\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.9507 - mae: 4.2293 - val_loss: 48.2894 - val_mae: 4.4556\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.9656 - mae: 3.0922 - val_loss: 32.9743 - val_mae: 3.3725\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1768 - mae: 2.9140 - val_loss: 30.3285 - val_mae: 3.4384\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9917 - mae: 2.9134 - val_loss: 30.9237 - val_mae: 3.3756\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8887 - mae: 2.8832 - val_loss: 30.6653 - val_mae: 3.3923\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9246 - mae: 2.8857 - val_loss: 30.5845 - val_mae: 3.3928\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8189 - mae: 2.8910 - val_loss: 30.6818 - val_mae: 3.3817\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8030 - mae: 2.8832 - val_loss: 30.6340 - val_mae: 3.3823\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7783 - mae: 2.8595 - val_loss: 30.8083 - val_mae: 3.3682\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7533 - mae: 2.8544 - val_loss: 30.3965 - val_mae: 3.3982\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7289 - mae: 2.8922 - val_loss: 30.5967 - val_mae: 3.3757\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7100 - mae: 2.8476 - val_loss: 30.5542 - val_mae: 3.3766\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7062 - mae: 2.8573 - val_loss: 30.4804 - val_mae: 3.3808\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6673 - mae: 2.8659 - val_loss: 30.4329 - val_mae: 3.3809\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6429 - mae: 2.8544 - val_loss: 30.5480 - val_mae: 3.3707\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6231 - mae: 2.8683 - val_loss: 30.5672 - val_mae: 3.3661\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6088 - mae: 2.8512 - val_loss: 30.6910 - val_mae: 3.3525\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6010 - mae: 2.8102 - val_loss: 30.2889 - val_mae: 3.3876\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5629 - mae: 2.8859 - val_loss: 30.2190 - val_mae: 3.3866\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5235 - mae: 2.8176 - val_loss: 30.6864 - val_mae: 3.3437\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4832 - mae: 2.8363 - val_loss: 30.0461 - val_mae: 3.3971\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5155 - mae: 2.8455 - val_loss: 30.4525 - val_mae: 3.3521\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4540 - mae: 2.8109 - val_loss: 30.0161 - val_mae: 3.3904\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4771 - mae: 2.8899 - val_loss: 30.1396 - val_mae: 3.3737\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4458 - mae: 2.7871 - val_loss: 30.3888 - val_mae: 3.3513\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.4110 - mae: 2.8471 - val_loss: 30.0630 - val_mae: 3.3735\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3624 - mae: 2.8174 - val_loss: 30.3539 - val_mae: 3.3452\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3741 - mae: 2.8325 - val_loss: 30.2796 - val_mae: 3.3472\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3346 - mae: 2.7979 - val_loss: 30.5179 - val_mae: 3.3338\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2949 - mae: 2.8146 - val_loss: 29.9232 - val_mae: 3.3726\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2682 - mae: 2.8132 - val_loss: 30.1405 - val_mae: 3.3477\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2395 - mae: 2.8006 - val_loss: 30.0674 - val_mae: 3.3526\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2149 - mae: 2.7986 - val_loss: 29.9696 - val_mae: 3.3562\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1886 - mae: 2.8101 - val_loss: 30.0067 - val_mae: 3.3465\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1618 - mae: 2.7939 - val_loss: 30.1540 - val_mae: 3.3315\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1693 - mae: 2.8051 - val_loss: 30.1076 - val_mae: 3.3315\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1853 - mae: 2.7843 - val_loss: 30.3132 - val_mae: 3.3220\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1828 - mae: 2.8323 - val_loss: 30.1290 - val_mae: 3.3230\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1677 - mae: 2.7795 - val_loss: 29.5985 - val_mae: 3.3739\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1259 - mae: 2.7795 - val_loss: 29.7458 - val_mae: 3.3455\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0624 - mae: 2.7784 - val_loss: 29.8468 - val_mae: 3.3383\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.0271 - mae: 2.7922 - val_loss: 30.0266 - val_mae: 3.3216\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0311 - mae: 2.7544 - val_loss: 29.5638 - val_mae: 3.3514\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9899 - mae: 2.7598 - val_loss: 29.7236 - val_mae: 3.3372\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9673 - mae: 2.7964 - val_loss: 29.9580 - val_mae: 3.3208\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9181 - mae: 2.7551 - val_loss: 29.6780 - val_mae: 3.3338\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 13.9368 - mae: 2.7697 - val_loss: 30.0724 - val_mae: 3.3027\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9875 - mae: 2.7410 - val_loss: 29.4920 - val_mae: 3.3437\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9100 - mae: 2.7854 - val_loss: 29.7503 - val_mae: 3.3194\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 11.5286 - mae: 2.6946\n",
      "Mean Absolute Error on Test Data: 2.694563150405884\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03408593327030629\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 69.3573 - mae: 6.5873 - val_loss: 48.5434 - val_mae: 5.4202\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8792 - mae: 5.0467 - val_loss: 28.6046 - val_mae: 3.5759\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.9131 - mae: 3.5273 - val_loss: 19.5572 - val_mae: 3.2724\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.2459 - mae: 3.6707 - val_loss: 19.7681 - val_mae: 3.3406\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9750 - mae: 3.5043 - val_loss: 19.4184 - val_mae: 3.2256\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8992 - mae: 3.5354 - val_loss: 19.5635 - val_mae: 3.2732\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8029 - mae: 3.5023 - val_loss: 19.4789 - val_mae: 3.2423\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8124 - mae: 3.5294 - val_loss: 19.4496 - val_mae: 3.2313\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.7290 - mae: 3.4911 - val_loss: 19.4968 - val_mae: 3.2501\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7002 - mae: 3.4973 - val_loss: 19.4876 - val_mae: 3.2480\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6472 - mae: 3.5220 - val_loss: 19.5944 - val_mae: 3.2781\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8088 - mae: 3.4696 - val_loss: 19.5714 - val_mae: 3.2768\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6946 - mae: 3.5621 - val_loss: 19.4024 - val_mae: 3.2145\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6086 - mae: 3.5088 - val_loss: 19.4678 - val_mae: 3.2314\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6313 - mae: 3.4580 - val_loss: 19.4868 - val_mae: 3.2427\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6071 - mae: 3.4903 - val_loss: 19.3738 - val_mae: 3.1964\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4952 - mae: 3.5271 - val_loss: 19.6767 - val_mae: 3.3056\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.5101 - mae: 3.5525 - val_loss: 19.4702 - val_mae: 3.2416\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4822 - mae: 3.4976 - val_loss: 19.2908 - val_mae: 3.1558\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.5348 - mae: 3.4838 - val_loss: 19.4152 - val_mae: 3.2121\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4625 - mae: 3.4609 - val_loss: 19.4588 - val_mae: 3.2317\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4340 - mae: 3.5571 - val_loss: 19.3504 - val_mae: 3.1965\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4818 - mae: 3.4135 - val_loss: 19.3722 - val_mae: 3.2013\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3917 - mae: 3.5369 - val_loss: 19.4059 - val_mae: 3.2275\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3286 - mae: 3.5083 - val_loss: 19.4058 - val_mae: 3.2149\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3039 - mae: 3.4450 - val_loss: 19.3926 - val_mae: 3.2064\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3171 - mae: 3.5088 - val_loss: 19.4526 - val_mae: 3.2169\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2344 - mae: 3.5155 - val_loss: 19.6272 - val_mae: 3.2642\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2514 - mae: 3.4864 - val_loss: 19.4497 - val_mae: 3.2137\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2169 - mae: 3.5239 - val_loss: 19.3781 - val_mae: 3.1792\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2911 - mae: 3.4438 - val_loss: 19.6001 - val_mae: 3.2612\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1310 - mae: 3.4779 - val_loss: 19.4259 - val_mae: 3.1874\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0726 - mae: 3.4794 - val_loss: 19.4835 - val_mae: 3.2123\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0885 - mae: 3.4949 - val_loss: 19.5186 - val_mae: 3.2245\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0730 - mae: 3.4564 - val_loss: 19.4916 - val_mae: 3.2025\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0275 - mae: 3.5013 - val_loss: 19.6558 - val_mae: 3.2601\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.1326 - mae: 3.5394 - val_loss: 19.3837 - val_mae: 3.1556\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1055 - mae: 3.3895 - val_loss: 19.4171 - val_mae: 3.1874\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0507 - mae: 3.5585 - val_loss: 19.4022 - val_mae: 3.1703\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9133 - mae: 3.4452 - val_loss: 19.5425 - val_mae: 3.2004\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9661 - mae: 3.5281 - val_loss: 19.5785 - val_mae: 3.2158\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8396 - mae: 3.4619 - val_loss: 19.5107 - val_mae: 3.1972\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8673 - mae: 3.5328 - val_loss: 19.4386 - val_mae: 3.1702\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8057 - mae: 3.4279 - val_loss: 19.4738 - val_mae: 3.1749\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7540 - mae: 3.4578 - val_loss: 19.6823 - val_mae: 3.2470\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.7240 - mae: 3.4800 - val_loss: 19.5638 - val_mae: 3.1892\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7395 - mae: 3.4724 - val_loss: 19.6481 - val_mae: 3.2207\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7077 - mae: 3.4508 - val_loss: 19.4911 - val_mae: 3.1755\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8002 - mae: 3.4299 - val_loss: 19.8078 - val_mae: 3.2457\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6498 - mae: 3.5327 - val_loss: 19.5182 - val_mae: 3.1678\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.3002 - mae: 3.2944\n",
      "Mean Absolute Error on Test Data: 3.2944259643554688\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.08417085906661326\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 12ms/step - loss: 7.0132 - mae: 2.0031 - val_loss: 4.7498 - val_mae: 1.4982\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.0796 - mae: 1.3718 - val_loss: 3.0073 - val_mae: 1.2677\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.1953 - mae: 1.3692 - val_loss: 3.0916 - val_mae: 1.3659\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.1674 - mae: 1.3609 - val_loss: 2.9788 - val_mae: 1.3019\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.1263 - mae: 1.3217 - val_loss: 2.9785 - val_mae: 1.3099\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.1153 - mae: 1.3458 - val_loss: 2.9701 - val_mae: 1.3095\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0899 - mae: 1.3070 - val_loss: 2.9421 - val_mae: 1.2845\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0818 - mae: 1.2912 - val_loss: 2.9357 - val_mae: 1.2874\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0533 - mae: 1.3071 - val_loss: 2.9329 - val_mae: 1.2931\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0481 - mae: 1.3349 - val_loss: 2.9384 - val_mae: 1.3014\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0226 - mae: 1.2887 - val_loss: 2.9139 - val_mae: 1.2795\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0005 - mae: 1.2899 - val_loss: 2.9146 - val_mae: 1.2872\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9956 - mae: 1.3148 - val_loss: 2.9051 - val_mae: 1.2808\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9791 - mae: 1.2879 - val_loss: 2.8977 - val_mae: 1.2761\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9651 - mae: 1.2717 - val_loss: 2.8979 - val_mae: 1.2766\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9721 - mae: 1.3140 - val_loss: 2.8960 - val_mae: 1.2784\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.9709 - mae: 1.2519 - val_loss: 2.8906 - val_mae: 1.2661\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9311 - mae: 1.2746 - val_loss: 2.9036 - val_mae: 1.2809\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9282 - mae: 1.2688 - val_loss: 2.9045 - val_mae: 1.2801\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9310 - mae: 1.3032 - val_loss: 2.8910 - val_mae: 1.2648\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9334 - mae: 1.2592 - val_loss: 2.9148 - val_mae: 1.2851\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9029 - mae: 1.2837 - val_loss: 2.8909 - val_mae: 1.2602\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9150 - mae: 1.2502 - val_loss: 2.8968 - val_mae: 1.2641\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8924 - mae: 1.2620 - val_loss: 2.9069 - val_mae: 1.2750\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8849 - mae: 1.2730 - val_loss: 2.9054 - val_mae: 1.2682\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8765 - mae: 1.2708 - val_loss: 2.9136 - val_mae: 1.2806\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8824 - mae: 1.2614 - val_loss: 2.9036 - val_mae: 1.2636\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8674 - mae: 1.2709 - val_loss: 2.9216 - val_mae: 1.2804\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8701 - mae: 1.2744 - val_loss: 2.9091 - val_mae: 1.2648\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.8652 - mae: 1.2633 - val_loss: 2.9083 - val_mae: 1.2660\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8660 - mae: 1.2482 - val_loss: 2.9165 - val_mae: 1.2749\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8612 - mae: 1.2856 - val_loss: 2.9219 - val_mae: 1.2770\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8595 - mae: 1.2545 - val_loss: 2.9165 - val_mae: 1.2781\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8590 - mae: 1.2802 - val_loss: 2.9141 - val_mae: 1.2645\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8465 - mae: 1.2587 - val_loss: 2.9213 - val_mae: 1.2700\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8396 - mae: 1.2673 - val_loss: 2.9338 - val_mae: 1.2770\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8423 - mae: 1.2637 - val_loss: 2.9344 - val_mae: 1.2762\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8355 - mae: 1.2657 - val_loss: 2.9198 - val_mae: 1.2566\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8523 - mae: 1.2598 - val_loss: 2.9515 - val_mae: 1.2896\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8359 - mae: 1.2517 - val_loss: 2.9337 - val_mae: 1.2728\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8408 - mae: 1.2743 - val_loss: 2.9280 - val_mae: 1.2582\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8255 - mae: 1.2677 - val_loss: 2.9506 - val_mae: 1.2770\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8188 - mae: 1.2458 - val_loss: 2.9454 - val_mae: 1.2684\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8227 - mae: 1.2728 - val_loss: 2.9405 - val_mae: 1.2744\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8173 - mae: 1.2487 - val_loss: 2.9390 - val_mae: 1.2601\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8253 - mae: 1.2474 - val_loss: 2.9723 - val_mae: 1.2946\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8345 - mae: 1.2964 - val_loss: 2.9332 - val_mae: 1.2441\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8391 - mae: 1.2448 - val_loss: 2.9407 - val_mae: 1.2778\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8095 - mae: 1.2692 - val_loss: 2.9479 - val_mae: 1.2666\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7975 - mae: 1.2494 - val_loss: 2.9533 - val_mae: 1.2669\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.5220 - mae: 1.1838\n",
      "Mean Absolute Error on Test Data: 1.1837512254714966\n",
      "6/6 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.05996662852359025\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 63.3157 - mae: 5.2244 - val_loss: 61.3590 - val_mae: 5.1754\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 53.8312 - mae: 4.3655 - val_loss: 48.8695 - val_mae: 4.2818\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 41.4446 - mae: 3.6083 - val_loss: 36.4387 - val_mae: 3.8691\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.9148 - mae: 3.9280 - val_loss: 34.7052 - val_mae: 4.1381\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.3967 - mae: 3.7967 - val_loss: 34.6940 - val_mae: 3.9531\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.0847 - mae: 3.8130 - val_loss: 34.1860 - val_mae: 3.9878\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.8585 - mae: 3.8180 - val_loss: 33.7656 - val_mae: 4.0103\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.7017 - mae: 3.7586 - val_loss: 33.7472 - val_mae: 3.9090\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.3827 - mae: 3.7695 - val_loss: 33.1675 - val_mae: 3.9731\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.2520 - mae: 3.8529 - val_loss: 32.9095 - val_mae: 3.9230\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.0138 - mae: 3.7313 - val_loss: 32.6065 - val_mae: 3.9177\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8232 - mae: 3.7820 - val_loss: 32.3861 - val_mae: 3.8810\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.6165 - mae: 3.7655 - val_loss: 32.0985 - val_mae: 3.8847\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.4003 - mae: 3.7557 - val_loss: 31.8027 - val_mae: 3.8493\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.3221 - mae: 3.6894 - val_loss: 31.4211 - val_mae: 3.8612\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.0484 - mae: 3.7312 - val_loss: 31.1651 - val_mae: 3.8476\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.8344 - mae: 3.7099 - val_loss: 31.0752 - val_mae: 3.8011\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.7497 - mae: 3.7129 - val_loss: 30.6776 - val_mae: 3.8207\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.5741 - mae: 3.6515 - val_loss: 30.5226 - val_mae: 3.7924\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4484 - mae: 3.6839 - val_loss: 30.2766 - val_mae: 3.8060\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2765 - mae: 3.7039 - val_loss: 30.0776 - val_mae: 3.8058\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2486 - mae: 3.7682 - val_loss: 29.9543 - val_mae: 3.7964\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.1762 - mae: 3.6809 - val_loss: 29.9315 - val_mae: 3.7474\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.2058 - mae: 3.6006 - val_loss: 29.7209 - val_mae: 3.7879\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8886 - mae: 3.7148 - val_loss: 29.7331 - val_mae: 3.7394\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8662 - mae: 3.6690 - val_loss: 29.5998 - val_mae: 3.7408\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8008 - mae: 3.5933 - val_loss: 29.5033 - val_mae: 3.7413\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8269 - mae: 3.7234 - val_loss: 29.4163 - val_mae: 3.7196\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.6829 - mae: 3.7011 - val_loss: 29.2076 - val_mae: 3.7463\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.6086 - mae: 3.5914 - val_loss: 29.3580 - val_mae: 3.7123\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4639 - mae: 3.6559 - val_loss: 29.3122 - val_mae: 3.7235\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.4692 - mae: 3.7126 - val_loss: 29.1144 - val_mae: 3.7306\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.5111 - mae: 3.5690 - val_loss: 29.0680 - val_mae: 3.7257\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.3811 - mae: 3.7264 - val_loss: 29.0383 - val_mae: 3.7262\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.3121 - mae: 3.6282 - val_loss: 29.1595 - val_mae: 3.6988\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2546 - mae: 3.5871 - val_loss: 29.0605 - val_mae: 3.6984\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2617 - mae: 3.6676 - val_loss: 28.9231 - val_mae: 3.7139\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2311 - mae: 3.6279 - val_loss: 29.3837 - val_mae: 3.6549\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1106 - mae: 3.6302 - val_loss: 28.8224 - val_mae: 3.7467\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.0910 - mae: 3.6011 - val_loss: 28.9587 - val_mae: 3.7066\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9949 - mae: 3.6533 - val_loss: 28.9429 - val_mae: 3.6920\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9118 - mae: 3.5929 - val_loss: 28.7986 - val_mae: 3.7169\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9369 - mae: 3.6780 - val_loss: 28.9236 - val_mae: 3.6806\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.9520 - mae: 3.5936 - val_loss: 29.0334 - val_mae: 3.6603\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7933 - mae: 3.6211 - val_loss: 28.6918 - val_mae: 3.7310\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.8614 - mae: 3.6202 - val_loss: 29.1214 - val_mae: 3.6536\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7067 - mae: 3.6063 - val_loss: 28.6691 - val_mae: 3.7227\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6584 - mae: 3.6376 - val_loss: 28.9069 - val_mae: 3.6727\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.6198 - mae: 3.5416 - val_loss: 28.8451 - val_mae: 3.6891\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7070 - mae: 3.5894 - val_loss: 28.6761 - val_mae: 3.7311\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 19.3412 - mae: 3.1354\n",
      "Mean Absolute Error on Test Data: 3.1354451179504395\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.15544489823283425\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 155.6584 - mae: 8.9917 - val_loss: 88.0907 - val_mae: 7.2762\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 122.8691 - mae: 7.1695 - val_loss: 53.7363 - val_mae: 5.0391\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 85.3340 - mae: 5.3037 - val_loss: 35.3239 - val_mae: 4.5661\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 74.1100 - mae: 5.4535 - val_loss: 36.3461 - val_mae: 4.8858\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 72.8720 - mae: 5.4397 - val_loss: 34.8294 - val_mae: 4.6884\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 72.0308 - mae: 5.2095 - val_loss: 34.4406 - val_mae: 4.6603\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 71.0519 - mae: 5.2765 - val_loss: 34.5449 - val_mae: 4.7180\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 70.7236 - mae: 5.2404 - val_loss: 33.7945 - val_mae: 4.6080\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.7172 - mae: 5.2518 - val_loss: 34.1858 - val_mae: 4.6965\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.2513 - mae: 5.1659 - val_loss: 33.6659 - val_mae: 4.6163\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.6935 - mae: 5.2094 - val_loss: 33.5567 - val_mae: 4.6073\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 68.2778 - mae: 5.1663 - val_loss: 33.0023 - val_mae: 4.5169\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.0034 - mae: 5.0970 - val_loss: 33.6571 - val_mae: 4.6299\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.5186 - mae: 5.0735 - val_loss: 33.1665 - val_mae: 4.5594\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 67.0968 - mae: 5.1164 - val_loss: 33.3706 - val_mae: 4.5921\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.8251 - mae: 5.0804 - val_loss: 33.0822 - val_mae: 4.5481\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.5992 - mae: 5.1123 - val_loss: 33.2256 - val_mae: 4.5664\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.4267 - mae: 5.0017 - val_loss: 32.8667 - val_mae: 4.5234\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.1213 - mae: 5.0679 - val_loss: 33.1312 - val_mae: 4.5501\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.1126 - mae: 4.9852 - val_loss: 32.9124 - val_mae: 4.5214\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.7783 - mae: 5.0825 - val_loss: 32.9276 - val_mae: 4.5206\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.0755 - mae: 5.1241 - val_loss: 32.6155 - val_mae: 4.4572\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.4410 - mae: 4.9069 - val_loss: 32.5389 - val_mae: 4.4705\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.2414 - mae: 5.0437 - val_loss: 32.8331 - val_mae: 4.4982\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.1856 - mae: 5.0013 - val_loss: 32.3480 - val_mae: 4.4045\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.9951 - mae: 4.9842 - val_loss: 32.6969 - val_mae: 4.4773\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.6550 - mae: 4.9836 - val_loss: 32.6248 - val_mae: 4.4596\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.5884 - mae: 4.9602 - val_loss: 32.3463 - val_mae: 4.4216\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.4749 - mae: 4.9029 - val_loss: 32.3427 - val_mae: 4.4287\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.4615 - mae: 4.8904 - val_loss: 32.8810 - val_mae: 4.4960\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.4096 - mae: 5.0154 - val_loss: 32.7020 - val_mae: 4.4638\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.1223 - mae: 4.9793 - val_loss: 32.4975 - val_mae: 4.4031\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.0036 - mae: 4.8855 - val_loss: 32.1267 - val_mae: 4.3681\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.9632 - mae: 4.9721 - val_loss: 32.4279 - val_mae: 4.4194\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.0870 - mae: 4.8602 - val_loss: 32.6132 - val_mae: 4.4464\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.5697 - mae: 4.9324 - val_loss: 32.0888 - val_mae: 4.3575\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.6482 - mae: 4.9048 - val_loss: 31.8891 - val_mae: 4.3210\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.5877 - mae: 4.9572 - val_loss: 32.8880 - val_mae: 4.4522\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.3790 - mae: 4.8502 - val_loss: 32.1599 - val_mae: 4.3730\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.3890 - mae: 4.8942 - val_loss: 32.4954 - val_mae: 4.4193\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.1725 - mae: 4.8536 - val_loss: 32.0150 - val_mae: 4.3509\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.6356 - mae: 4.9338 - val_loss: 31.8544 - val_mae: 4.2834\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.3633 - mae: 4.8186 - val_loss: 32.6310 - val_mae: 4.4584\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.3485 - mae: 4.9184 - val_loss: 31.9800 - val_mae: 4.3267\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 62.8913 - mae: 4.8315 - val_loss: 32.2304 - val_mae: 4.4030\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.8874 - mae: 4.8839 - val_loss: 32.4279 - val_mae: 4.4098\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.0269 - mae: 4.8318 - val_loss: 32.4090 - val_mae: 4.4165\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 62.7334 - mae: 4.9032 - val_loss: 32.3319 - val_mae: 4.3970\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.7764 - mae: 4.8218 - val_loss: 31.9391 - val_mae: 4.3628\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.6901 - mae: 4.8412 - val_loss: 31.9275 - val_mae: 4.3302\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 56.7115 - mae: 5.1273\n",
      "Mean Absolute Error on Test Data: 5.127337455749512\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.2149470758710904\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 88.0428 - mae: 7.6054 - val_loss: 78.7054 - val_mae: 7.0605\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.7474 - mae: 5.9726 - val_loss: 49.9716 - val_mae: 5.0978\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.6060 - mae: 4.1356 - val_loss: 29.3478 - val_mae: 3.9416\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.5798 - mae: 4.0215 - val_loss: 28.6073 - val_mae: 4.0291\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.2060 - mae: 3.9795 - val_loss: 28.4783 - val_mae: 3.9628\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.1148 - mae: 3.8821 - val_loss: 28.4472 - val_mae: 3.9452\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.0986 - mae: 3.9320 - val_loss: 28.2425 - val_mae: 3.9546\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.9447 - mae: 3.9160 - val_loss: 28.1915 - val_mae: 3.9331\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.9132 - mae: 3.9139 - val_loss: 27.9959 - val_mae: 3.9542\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.8613 - mae: 3.8883 - val_loss: 27.9312 - val_mae: 3.9229\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.7493 - mae: 3.9274 - val_loss: 27.8043 - val_mae: 3.9246\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.7902 - mae: 3.8826 - val_loss: 27.6495 - val_mae: 3.9424\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.7821 - mae: 3.8917 - val_loss: 27.5300 - val_mae: 3.9215\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.6833 - mae: 3.9558 - val_loss: 27.5264 - val_mae: 3.8948\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.7699 - mae: 3.8512 - val_loss: 27.3712 - val_mae: 3.9017\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5189 - mae: 3.9279 - val_loss: 27.3404 - val_mae: 3.8870\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5019 - mae: 3.8668 - val_loss: 27.3351 - val_mae: 3.8687\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3628 - mae: 3.8925 - val_loss: 27.1372 - val_mae: 3.8939\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3674 - mae: 3.8647 - val_loss: 27.1560 - val_mae: 3.8671\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3479 - mae: 3.8531 - val_loss: 26.9874 - val_mae: 3.8956\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3125 - mae: 3.9411 - val_loss: 27.0041 - val_mae: 3.8609\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3819 - mae: 3.8254 - val_loss: 26.8907 - val_mae: 3.8696\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2420 - mae: 3.8651 - val_loss: 26.8481 - val_mae: 3.8663\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4554 - mae: 3.9371 - val_loss: 26.9393 - val_mae: 3.8361\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1496 - mae: 3.8558 - val_loss: 26.7550 - val_mae: 3.8524\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1224 - mae: 3.8613 - val_loss: 26.7398 - val_mae: 3.8519\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0910 - mae: 3.8576 - val_loss: 26.7297 - val_mae: 3.8376\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0973 - mae: 3.8256 - val_loss: 26.6728 - val_mae: 3.8493\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0692 - mae: 3.8905 - val_loss: 26.6834 - val_mae: 3.8346\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0266 - mae: 3.8384 - val_loss: 26.6298 - val_mae: 3.8396\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.0535 - mae: 3.8796 - val_loss: 26.7756 - val_mae: 3.8062\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0142 - mae: 3.8190 - val_loss: 26.5516 - val_mae: 3.8474\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0287 - mae: 3.8402 - val_loss: 26.5005 - val_mae: 3.8397\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1380 - mae: 3.8951 - val_loss: 26.7385 - val_mae: 3.7980\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9728 - mae: 3.8500 - val_loss: 26.5150 - val_mae: 3.8249\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9315 - mae: 3.8520 - val_loss: 26.4991 - val_mae: 3.8345\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9620 - mae: 3.8442 - val_loss: 26.4852 - val_mae: 3.8278\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9455 - mae: 3.8606 - val_loss: 26.6109 - val_mae: 3.7938\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9501 - mae: 3.8087 - val_loss: 26.3940 - val_mae: 3.8547\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9334 - mae: 3.8744 - val_loss: 26.4309 - val_mae: 3.8133\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9024 - mae: 3.8207 - val_loss: 26.3979 - val_mae: 3.8333\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8413 - mae: 3.8525 - val_loss: 26.4207 - val_mae: 3.8175\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8349 - mae: 3.8062 - val_loss: 26.3850 - val_mae: 3.8151\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8075 - mae: 3.8639 - val_loss: 26.3574 - val_mae: 3.8137\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9034 - mae: 3.8127 - val_loss: 26.3027 - val_mae: 3.8405\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9019 - mae: 3.8560 - val_loss: 26.4023 - val_mae: 3.8164\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.8607 - mae: 3.8722 - val_loss: 26.4984 - val_mae: 3.7755\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8633 - mae: 3.8320 - val_loss: 26.3657 - val_mae: 3.7938\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9414 - mae: 3.8006 - val_loss: 26.2749 - val_mae: 3.8436\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8825 - mae: 3.8239 - val_loss: 26.2894 - val_mae: 3.8159\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 29.9481 - mae: 3.9608\n",
      "Mean Absolute Error on Test Data: 3.9608051776885986\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.09505979512432805\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 75.3907 - mae: 7.2579 - val_loss: 62.8555 - val_mae: 6.3404\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 55.9688 - mae: 5.8546 - val_loss: 40.4710 - val_mae: 4.4953\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.1509 - mae: 3.9843 - val_loss: 22.5276 - val_mae: 3.2783\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1462 - mae: 3.5009 - val_loss: 22.7070 - val_mae: 3.6622\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0676 - mae: 3.5100 - val_loss: 21.9429 - val_mae: 3.4081\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8994 - mae: 3.4588 - val_loss: 22.0545 - val_mae: 3.4931\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9227 - mae: 3.4858 - val_loss: 21.9853 - val_mae: 3.4711\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8470 - mae: 3.4583 - val_loss: 21.8870 - val_mae: 3.4198\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7840 - mae: 3.4647 - val_loss: 22.0040 - val_mae: 3.4935\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8149 - mae: 3.5017 - val_loss: 21.8896 - val_mae: 3.4531\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8341 - mae: 3.4428 - val_loss: 21.8716 - val_mae: 3.4536\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7896 - mae: 3.4837 - val_loss: 21.8672 - val_mae: 3.4577\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6777 - mae: 3.4508 - val_loss: 21.7683 - val_mae: 3.4093\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6787 - mae: 3.4368 - val_loss: 21.8884 - val_mae: 3.4777\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7009 - mae: 3.5047 - val_loss: 21.7647 - val_mae: 3.4218\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6954 - mae: 3.4163 - val_loss: 21.7164 - val_mae: 3.4015\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6126 - mae: 3.4643 - val_loss: 21.7638 - val_mae: 3.4378\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5570 - mae: 3.4643 - val_loss: 21.7441 - val_mae: 3.4324\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5362 - mae: 3.4630 - val_loss: 21.7724 - val_mae: 3.4555\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5052 - mae: 3.4538 - val_loss: 21.6493 - val_mae: 3.3859\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5267 - mae: 3.4407 - val_loss: 21.7624 - val_mae: 3.4609\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4744 - mae: 3.4539 - val_loss: 21.6184 - val_mae: 3.3822\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4492 - mae: 3.4469 - val_loss: 21.7165 - val_mae: 3.4494\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4697 - mae: 3.4669 - val_loss: 21.8132 - val_mae: 3.4895\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5591 - mae: 3.4451 - val_loss: 21.7143 - val_mae: 3.4538\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5154 - mae: 3.5033 - val_loss: 21.5646 - val_mae: 3.3715\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6442 - mae: 3.4241 - val_loss: 21.5957 - val_mae: 3.4010\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3566 - mae: 3.4702 - val_loss: 21.6152 - val_mae: 3.4191\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3517 - mae: 3.4139 - val_loss: 21.6207 - val_mae: 3.4226\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3099 - mae: 3.4568 - val_loss: 21.5894 - val_mae: 3.4166\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3071 - mae: 3.4801 - val_loss: 21.5241 - val_mae: 3.3847\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2928 - mae: 3.4150 - val_loss: 21.5335 - val_mae: 3.3977\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2371 - mae: 3.4279 - val_loss: 21.5916 - val_mae: 3.4316\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2680 - mae: 3.4741 - val_loss: 21.4914 - val_mae: 3.3801\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2470 - mae: 3.4419 - val_loss: 21.4625 - val_mae: 3.3576\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2559 - mae: 3.4247 - val_loss: 21.4493 - val_mae: 3.3451\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1668 - mae: 3.4459 - val_loss: 21.5136 - val_mae: 3.4065\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1327 - mae: 3.4346 - val_loss: 21.4977 - val_mae: 3.4031\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1147 - mae: 3.4329 - val_loss: 21.4338 - val_mae: 3.3699\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0778 - mae: 3.4199 - val_loss: 21.4187 - val_mae: 3.3672\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0648 - mae: 3.4159 - val_loss: 21.6362 - val_mae: 3.4669\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2289 - mae: 3.4686 - val_loss: 21.5952 - val_mae: 3.4583\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0658 - mae: 3.4087 - val_loss: 21.4705 - val_mae: 3.4077\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.9685 - mae: 3.4444 - val_loss: 21.3920 - val_mae: 3.3751\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0136 - mae: 3.4190 - val_loss: 21.3567 - val_mae: 3.3499\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9768 - mae: 3.4235 - val_loss: 21.3749 - val_mae: 3.3727\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8909 - mae: 3.4509 - val_loss: 21.4276 - val_mae: 3.4061\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 20.8607 - mae: 3.3987 - val_loss: 21.3495 - val_mae: 3.3719\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8390 - mae: 3.4112 - val_loss: 21.4569 - val_mae: 3.4321\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9420 - mae: 3.4391 - val_loss: 21.3305 - val_mae: 3.3747\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 27.4579 - mae: 3.4717\n",
      "Mean Absolute Error on Test Data: 3.4717042446136475\n",
      "7/7 [==============================] - 0s 833us/step\n",
      "R-squared: 0.07070200624393208\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 7.6702 - mae: 1.8259 - val_loss: 6.6758 - val_mae: 1.6752\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 5.6051 - mae: 1.3015 - val_loss: 4.8187 - val_mae: 1.3317\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6419 - mae: 1.2334 - val_loss: 4.1350 - val_mae: 1.3155\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4993 - mae: 1.2737 - val_loss: 4.1301 - val_mae: 1.3048\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4630 - mae: 1.2408 - val_loss: 4.1285 - val_mae: 1.2916\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4525 - mae: 1.2191 - val_loss: 4.1270 - val_mae: 1.2859\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4378 - mae: 1.2175 - val_loss: 4.1088 - val_mae: 1.2887\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4657 - mae: 1.2697 - val_loss: 4.0673 - val_mae: 1.3212\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4252 - mae: 1.2135 - val_loss: 4.1361 - val_mae: 1.2804\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4092 - mae: 1.2175 - val_loss: 4.0746 - val_mae: 1.3039\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3911 - mae: 1.2281 - val_loss: 4.0801 - val_mae: 1.2984\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3946 - mae: 1.2124 - val_loss: 4.0852 - val_mae: 1.2980\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3835 - mae: 1.2234 - val_loss: 4.0860 - val_mae: 1.3031\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3808 - mae: 1.2129 - val_loss: 4.0872 - val_mae: 1.3026\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3695 - mae: 1.2243 - val_loss: 4.0814 - val_mae: 1.3147\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3707 - mae: 1.2284 - val_loss: 4.0823 - val_mae: 1.3079\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3646 - mae: 1.2155 - val_loss: 4.0732 - val_mae: 1.3128\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3577 - mae: 1.2343 - val_loss: 4.0431 - val_mae: 1.3268\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3598 - mae: 1.2260 - val_loss: 4.0785 - val_mae: 1.3127\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3505 - mae: 1.2202 - val_loss: 4.0529 - val_mae: 1.3271\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3619 - mae: 1.2505 - val_loss: 4.0547 - val_mae: 1.3263\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3488 - mae: 1.2331 - val_loss: 4.0784 - val_mae: 1.3185\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3521 - mae: 1.2221 - val_loss: 4.0768 - val_mae: 1.3224\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3418 - mae: 1.2318 - val_loss: 4.0646 - val_mae: 1.3337\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3514 - mae: 1.2263 - val_loss: 4.1019 - val_mae: 1.3194\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3311 - mae: 1.2329 - val_loss: 4.0564 - val_mae: 1.3448\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3412 - mae: 1.2468 - val_loss: 4.0846 - val_mae: 1.3302\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3380 - mae: 1.2219 - val_loss: 4.1031 - val_mae: 1.3254\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3317 - mae: 1.2338 - val_loss: 4.0849 - val_mae: 1.3336\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3376 - mae: 1.2473 - val_loss: 4.0936 - val_mae: 1.3365\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3370 - mae: 1.2224 - val_loss: 4.1199 - val_mae: 1.3217\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3263 - mae: 1.2219 - val_loss: 4.0925 - val_mae: 1.3315\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3372 - mae: 1.2372 - val_loss: 4.1069 - val_mae: 1.3310\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3380 - mae: 1.2476 - val_loss: 4.1044 - val_mae: 1.3356\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3264 - mae: 1.2359 - val_loss: 4.1124 - val_mae: 1.3385\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3248 - mae: 1.2284 - val_loss: 4.1169 - val_mae: 1.3340\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3207 - mae: 1.2393 - val_loss: 4.1040 - val_mae: 1.3411\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3234 - mae: 1.2341 - val_loss: 4.1432 - val_mae: 1.3250\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3192 - mae: 1.2167 - val_loss: 4.1202 - val_mae: 1.3343\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3818 - mae: 1.2875 - val_loss: 4.1009 - val_mae: 1.3501\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3066 - mae: 1.2258 - val_loss: 4.1954 - val_mae: 1.3224\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3118 - mae: 1.2212 - val_loss: 4.1234 - val_mae: 1.3391\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3112 - mae: 1.2402 - val_loss: 4.1236 - val_mae: 1.3389\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3256 - mae: 1.2512 - val_loss: 4.1289 - val_mae: 1.3485\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3501 - mae: 1.2164 - val_loss: 4.1429 - val_mae: 1.3317\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3305 - mae: 1.2541 - val_loss: 4.1302 - val_mae: 1.3459\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3084 - mae: 1.2229 - val_loss: 4.1486 - val_mae: 1.3383\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.2986 - mae: 1.2335 - val_loss: 4.1090 - val_mae: 1.3488\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3098 - mae: 1.2296 - val_loss: 4.1440 - val_mae: 1.3356\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3183 - mae: 1.2499 - val_loss: 4.1370 - val_mae: 1.3417\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.3519 - mae: 1.0795\n",
      "Mean Absolute Error on Test Data: 1.0795265436172485\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03082362054568144\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 158.1018 - mae: 10.1704 - val_loss: 148.3696 - val_mae: 9.5648\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 134.9751 - mae: 9.0052 - val_loss: 120.4268 - val_mae: 8.0862\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 99.7426 - mae: 7.0492 - val_loss: 79.5026 - val_mae: 5.9086\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.5253 - mae: 5.1533 - val_loss: 55.7819 - val_mae: 5.3862\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.7383 - mae: 5.2177 - val_loss: 55.6407 - val_mae: 5.6875\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.0332 - mae: 5.0763 - val_loss: 55.1336 - val_mae: 5.4810\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.8225 - mae: 5.0450 - val_loss: 54.9639 - val_mae: 5.5103\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.6410 - mae: 5.0448 - val_loss: 54.8014 - val_mae: 5.5119\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.5895 - mae: 5.0798 - val_loss: 54.6459 - val_mae: 5.4723\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3679 - mae: 5.0115 - val_loss: 54.4668 - val_mae: 5.4576\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.2608 - mae: 5.0170 - val_loss: 54.3136 - val_mae: 5.4280\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 51.0464 - mae: 4.9844 - val_loss: 54.1309 - val_mae: 5.4705\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.9249 - mae: 5.0166 - val_loss: 53.9827 - val_mae: 5.4617\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8925 - mae: 5.0067 - val_loss: 53.8681 - val_mae: 5.4973\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.7493 - mae: 4.9906 - val_loss: 53.6803 - val_mae: 5.4273\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.5789 - mae: 5.0056 - val_loss: 53.5869 - val_mae: 5.4622\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.5243 - mae: 4.9667 - val_loss: 53.4440 - val_mae: 5.4291\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.3380 - mae: 5.0013 - val_loss: 53.3227 - val_mae: 5.3965\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.3113 - mae: 4.9887 - val_loss: 53.2403 - val_mae: 5.4206\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.1037 - mae: 4.9447 - val_loss: 53.1872 - val_mae: 5.3276\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.4629 - mae: 4.8871 - val_loss: 53.1163 - val_mae: 5.4532\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.4026 - mae: 5.0557 - val_loss: 53.0758 - val_mae: 5.2988\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.1143 - mae: 4.9210 - val_loss: 52.9086 - val_mae: 5.3564\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.9075 - mae: 4.9130 - val_loss: 52.8034 - val_mae: 5.3501\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9810 - mae: 4.9592 - val_loss: 52.7227 - val_mae: 5.3597\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 49.7670 - mae: 4.9316 - val_loss: 52.6844 - val_mae: 5.3607\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8394 - mae: 4.8931 - val_loss: 52.6399 - val_mae: 5.3656\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8887 - mae: 5.0145 - val_loss: 52.6020 - val_mae: 5.3577\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.6360 - mae: 4.9162 - val_loss: 52.5304 - val_mae: 5.3171\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.6306 - mae: 4.9527 - val_loss: 52.4593 - val_mae: 5.3321\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5771 - mae: 4.9477 - val_loss: 52.4149 - val_mae: 5.3191\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5630 - mae: 4.9139 - val_loss: 52.3991 - val_mae: 5.3380\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.6673 - mae: 4.9412 - val_loss: 52.3835 - val_mae: 5.2575\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5660 - mae: 4.9275 - val_loss: 52.2998 - val_mae: 5.2928\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.4872 - mae: 4.9134 - val_loss: 52.2934 - val_mae: 5.2904\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.4750 - mae: 4.8771 - val_loss: 52.2474 - val_mae: 5.3188\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.5755 - mae: 4.9706 - val_loss: 52.2143 - val_mae: 5.2775\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.5752 - mae: 4.8699 - val_loss: 52.2012 - val_mae: 5.2535\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3874 - mae: 4.9580 - val_loss: 52.2041 - val_mae: 5.3413\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3155 - mae: 4.8984 - val_loss: 52.2651 - val_mae: 5.2026\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.3612 - mae: 4.8775 - val_loss: 52.1664 - val_mae: 5.3407\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.3967 - mae: 4.9573 - val_loss: 52.1361 - val_mae: 5.2411\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5098 - mae: 4.8317 - val_loss: 52.1102 - val_mae: 5.3057\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.4209 - mae: 4.9353 - val_loss: 52.1127 - val_mae: 5.3301\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3258 - mae: 4.9409 - val_loss: 52.0905 - val_mae: 5.2315\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3301 - mae: 4.8761 - val_loss: 52.0491 - val_mae: 5.2627\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.3740 - mae: 4.9512 - val_loss: 52.0220 - val_mae: 5.2527\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.8220 - mae: 4.8424 - val_loss: 52.0652 - val_mae: 5.3166\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 49.3251 - mae: 4.9585 - val_loss: 51.9886 - val_mae: 5.2716\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.2143 - mae: 4.9208 - val_loss: 51.9892 - val_mae: 5.2676\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 41.8943 - mae: 4.7252\n",
      "Mean Absolute Error on Test Data: 4.725160121917725\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.09225723135770025\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 43.6211 - mae: 5.3822 - val_loss: 31.8674 - val_mae: 4.5046\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9989 - mae: 4.2426 - val_loss: 18.9588 - val_mae: 3.1021\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 17.8318 - mae: 2.9826 - val_loss: 11.7627 - val_mae: 2.6927\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.8339 - mae: 2.9461 - val_loss: 12.0650 - val_mae: 2.7637\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.7094 - mae: 2.8352 - val_loss: 11.8080 - val_mae: 2.7062\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6067 - mae: 2.8449 - val_loss: 11.8320 - val_mae: 2.7140\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5668 - mae: 2.8315 - val_loss: 11.7891 - val_mae: 2.7030\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5178 - mae: 2.8296 - val_loss: 11.7888 - val_mae: 2.7045\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5113 - mae: 2.8177 - val_loss: 11.8155 - val_mae: 2.7120\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4731 - mae: 2.8218 - val_loss: 11.7908 - val_mae: 2.7062\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4406 - mae: 2.8090 - val_loss: 11.7515 - val_mae: 2.6966\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4271 - mae: 2.8096 - val_loss: 11.8278 - val_mae: 2.7156\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4459 - mae: 2.8630 - val_loss: 11.7633 - val_mae: 2.7015\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3736 - mae: 2.8056 - val_loss: 11.7645 - val_mae: 2.7003\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3746 - mae: 2.8335 - val_loss: 11.7505 - val_mae: 2.6992\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3748 - mae: 2.7863 - val_loss: 11.7614 - val_mae: 2.7024\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3405 - mae: 2.8160 - val_loss: 11.7177 - val_mae: 2.6923\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3152 - mae: 2.7841 - val_loss: 11.7305 - val_mae: 2.6947\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3251 - mae: 2.8009 - val_loss: 11.8891 - val_mae: 2.7261\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3149 - mae: 2.7982 - val_loss: 11.7259 - val_mae: 2.6929\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2773 - mae: 2.8198 - val_loss: 11.7824 - val_mae: 2.7054\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2704 - mae: 2.8275 - val_loss: 11.7389 - val_mae: 2.6939\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2893 - mae: 2.7789 - val_loss: 11.8152 - val_mae: 2.7136\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2563 - mae: 2.7872 - val_loss: 11.8126 - val_mae: 2.7133\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2112 - mae: 2.8003 - val_loss: 11.8002 - val_mae: 2.7077\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2032 - mae: 2.8255 - val_loss: 11.8434 - val_mae: 2.7177\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1795 - mae: 2.7662 - val_loss: 11.8820 - val_mae: 2.7259\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2271 - mae: 2.8472 - val_loss: 11.8466 - val_mae: 2.7167\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2226 - mae: 2.7333 - val_loss: 11.8899 - val_mae: 2.7266\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2576 - mae: 2.8549 - val_loss: 11.8079 - val_mae: 2.6996\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2060 - mae: 2.7583 - val_loss: 11.9326 - val_mae: 2.7336\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1696 - mae: 2.7580 - val_loss: 11.9098 - val_mae: 2.7285\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 14.1817 - mae: 2.8163 - val_loss: 11.9517 - val_mae: 2.7370\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0990 - mae: 2.7727 - val_loss: 11.9008 - val_mae: 2.7253\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0749 - mae: 2.7775 - val_loss: 12.0478 - val_mae: 2.7563\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0930 - mae: 2.7553 - val_loss: 12.0440 - val_mae: 2.7551\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0548 - mae: 2.7941 - val_loss: 11.9812 - val_mae: 2.7414\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0377 - mae: 2.7519 - val_loss: 11.9874 - val_mae: 2.7437\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0548 - mae: 2.7824 - val_loss: 12.0046 - val_mae: 2.7429\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0734 - mae: 2.7708 - val_loss: 12.1207 - val_mae: 2.7674\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.9839 - mae: 2.7541 - val_loss: 11.9936 - val_mae: 2.7383\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0522 - mae: 2.8019 - val_loss: 11.9220 - val_mae: 2.7103\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0720 - mae: 2.7252 - val_loss: 12.1996 - val_mae: 2.7795\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0080 - mae: 2.7986 - val_loss: 12.0705 - val_mae: 2.7529\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9898 - mae: 2.7859 - val_loss: 12.0305 - val_mae: 2.7431\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0179 - mae: 2.7319 - val_loss: 12.0696 - val_mae: 2.7472\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9303 - mae: 2.7673 - val_loss: 12.0896 - val_mae: 2.7560\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9482 - mae: 2.7475 - val_loss: 12.0248 - val_mae: 2.7318\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8682 - mae: 2.7312 - val_loss: 12.2407 - val_mae: 2.7888\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9120 - mae: 2.7593 - val_loss: 12.1223 - val_mae: 2.7604\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 16.8128 - mae: 3.1009\n",
      "Mean Absolute Error on Test Data: 3.10091495513916\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.01786213915948054\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 14ms/step - loss: 9.7263 - mae: 2.0720 - val_loss: 6.8588 - val_mae: 1.7069\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 7.2473 - mae: 1.5393 - val_loss: 4.8407 - val_mae: 1.3559\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.8756 - mae: 1.4233 - val_loss: 4.3167 - val_mae: 1.4197\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.6149 - mae: 1.5217 - val_loss: 4.2835 - val_mae: 1.4229\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5344 - mae: 1.4476 - val_loss: 4.2442 - val_mae: 1.3734\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4661 - mae: 1.4565 - val_loss: 4.2201 - val_mae: 1.3976\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4229 - mae: 1.4554 - val_loss: 4.2068 - val_mae: 1.3936\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3654 - mae: 1.4323 - val_loss: 4.1915 - val_mae: 1.3672\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3238 - mae: 1.4378 - val_loss: 4.1916 - val_mae: 1.3951\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3179 - mae: 1.4182 - val_loss: 4.1888 - val_mae: 1.3718\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2653 - mae: 1.4682 - val_loss: 4.2214 - val_mae: 1.4275\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2200 - mae: 1.4444 - val_loss: 4.1949 - val_mae: 1.3667\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2322 - mae: 1.4064 - val_loss: 4.2193 - val_mae: 1.4083\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1806 - mae: 1.4535 - val_loss: 4.2161 - val_mae: 1.3827\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1764 - mae: 1.4371 - val_loss: 4.2244 - val_mae: 1.3751\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1625 - mae: 1.3899 - val_loss: 4.2427 - val_mae: 1.4001\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1273 - mae: 1.4578 - val_loss: 4.2586 - val_mae: 1.3994\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.1288 - mae: 1.4383 - val_loss: 4.2666 - val_mae: 1.3708\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1311 - mae: 1.3943 - val_loss: 4.2821 - val_mae: 1.4049\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0996 - mae: 1.4348 - val_loss: 4.2828 - val_mae: 1.3897\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1025 - mae: 1.4466 - val_loss: 4.3093 - val_mae: 1.4001\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0807 - mae: 1.4029 - val_loss: 4.3161 - val_mae: 1.4030\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0791 - mae: 1.4541 - val_loss: 4.3307 - val_mae: 1.4248\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0560 - mae: 1.4118 - val_loss: 4.3419 - val_mae: 1.3806\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.0646 - mae: 1.4019 - val_loss: 4.3514 - val_mae: 1.4413\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0834 - mae: 1.4565 - val_loss: 4.3543 - val_mae: 1.3832\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0348 - mae: 1.4202 - val_loss: 4.3816 - val_mae: 1.4341\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0256 - mae: 1.4389 - val_loss: 4.3612 - val_mae: 1.4109\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0180 - mae: 1.4215 - val_loss: 4.3829 - val_mae: 1.4159\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0193 - mae: 1.4443 - val_loss: 4.3888 - val_mae: 1.4092\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0330 - mae: 1.4002 - val_loss: 4.4062 - val_mae: 1.4308\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.0122 - mae: 1.4295 - val_loss: 4.4145 - val_mae: 1.4275\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0097 - mae: 1.4324 - val_loss: 4.4076 - val_mae: 1.3889\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0197 - mae: 1.4299 - val_loss: 4.4343 - val_mae: 1.4428\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9738 - mae: 1.4207 - val_loss: 4.4171 - val_mae: 1.3958\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0276 - mae: 1.4175 - val_loss: 4.4273 - val_mae: 1.3840\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9992 - mae: 1.4270 - val_loss: 4.4327 - val_mae: 1.4150\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9957 - mae: 1.3993 - val_loss: 4.4389 - val_mae: 1.4239\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9864 - mae: 1.4300 - val_loss: 4.4561 - val_mae: 1.4003\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4.9683 - mae: 1.3918 - val_loss: 4.4572 - val_mae: 1.4365\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0483 - mae: 1.5033 - val_loss: 4.4750 - val_mae: 1.4240\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9703 - mae: 1.4016 - val_loss: 4.4948 - val_mae: 1.4202\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9869 - mae: 1.4284 - val_loss: 4.5233 - val_mae: 1.4519\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9304 - mae: 1.4163 - val_loss: 4.5049 - val_mae: 1.3926\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9876 - mae: 1.4235 - val_loss: 4.5221 - val_mae: 1.4372\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9291 - mae: 1.4097 - val_loss: 4.5084 - val_mae: 1.4153\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 4.9218 - mae: 1.4294 - val_loss: 4.5136 - val_mae: 1.4301\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9136 - mae: 1.4174 - val_loss: 4.5186 - val_mae: 1.4267\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9119 - mae: 1.4019 - val_loss: 4.5183 - val_mae: 1.4247\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9177 - mae: 1.4332 - val_loss: 4.5379 - val_mae: 1.4068\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 5.1635 - mae: 1.4773\n",
      "Mean Absolute Error on Test Data: 1.4772703647613525\n",
      "6/6 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.10123786403355062\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 78.8588 - mae: 6.6178 - val_loss: 56.1875 - val_mae: 5.1951\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.2560 - mae: 5.6582 - val_loss: 43.3436 - val_mae: 4.0832\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.1406 - mae: 4.3115 - val_loss: 30.0514 - val_mae: 3.3989\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 35.0408 - mae: 3.8685 - val_loss: 30.5840 - val_mae: 3.9969\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.9856 - mae: 4.0434 - val_loss: 29.7106 - val_mae: 3.8500\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.7867 - mae: 3.9480 - val_loss: 29.5597 - val_mae: 3.8391\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.6162 - mae: 3.9782 - val_loss: 29.9248 - val_mae: 3.9163\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.7326 - mae: 4.0876 - val_loss: 29.4089 - val_mae: 3.8406\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.7960 - mae: 3.8605 - val_loss: 28.9083 - val_mae: 3.7618\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.2551 - mae: 3.9546 - val_loss: 29.5899 - val_mae: 3.8941\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.1877 - mae: 3.9349 - val_loss: 28.8393 - val_mae: 3.7819\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.1325 - mae: 3.9643 - val_loss: 28.9066 - val_mae: 3.8088\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.9671 - mae: 3.8778 - val_loss: 28.7060 - val_mae: 3.7778\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.9691 - mae: 3.9672 - val_loss: 28.7776 - val_mae: 3.8052\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.8369 - mae: 3.8562 - val_loss: 28.5589 - val_mae: 3.7701\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.6827 - mae: 3.9334 - val_loss: 28.7810 - val_mae: 3.8218\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.6293 - mae: 3.9885 - val_loss: 28.9269 - val_mae: 3.8542\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.5605 - mae: 3.9706 - val_loss: 28.5118 - val_mae: 3.7904\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.4848 - mae: 3.8611 - val_loss: 28.6108 - val_mae: 3.8094\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.3769 - mae: 3.9246 - val_loss: 28.4573 - val_mae: 3.7897\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.3940 - mae: 3.8343 - val_loss: 28.3124 - val_mae: 3.7715\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.3890 - mae: 4.0001 - val_loss: 28.5340 - val_mae: 3.8133\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.3207 - mae: 3.8444 - val_loss: 28.1712 - val_mae: 3.7503\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.1762 - mae: 3.8818 - val_loss: 28.0072 - val_mae: 3.7261\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.1758 - mae: 3.8275 - val_loss: 28.4363 - val_mae: 3.8096\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.1289 - mae: 3.9197 - val_loss: 28.5754 - val_mae: 3.8343\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0341 - mae: 3.9295 - val_loss: 28.0613 - val_mae: 3.7459\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0400 - mae: 3.8494 - val_loss: 27.7453 - val_mae: 3.6871\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0105 - mae: 3.8359 - val_loss: 28.1147 - val_mae: 3.7584\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.1062 - mae: 3.8514 - val_loss: 28.5216 - val_mae: 3.8300\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.9211 - mae: 3.8941 - val_loss: 28.0830 - val_mae: 3.7571\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.0883 - mae: 3.9310 - val_loss: 28.3478 - val_mae: 3.8130\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9041 - mae: 3.9023 - val_loss: 28.4047 - val_mae: 3.8205\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9652 - mae: 3.8719 - val_loss: 27.6010 - val_mae: 3.6705\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.9143 - mae: 3.8778 - val_loss: 28.1706 - val_mae: 3.7821\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7418 - mae: 3.9098 - val_loss: 28.3645 - val_mae: 3.8100\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7259 - mae: 3.9079 - val_loss: 27.8972 - val_mae: 3.7304\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.7369 - mae: 3.8297 - val_loss: 27.8605 - val_mae: 3.7226\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7711 - mae: 3.8060 - val_loss: 28.1969 - val_mae: 3.7883\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.8641 - mae: 3.9775 - val_loss: 28.2825 - val_mae: 3.7999\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7272 - mae: 3.8301 - val_loss: 28.1042 - val_mae: 3.7696\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.6199 - mae: 3.8807 - val_loss: 28.5951 - val_mae: 3.8573\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0820 - mae: 4.0219 - val_loss: 27.4716 - val_mae: 3.6506\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.0133 - mae: 3.7787 - val_loss: 28.0376 - val_mae: 3.7663\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.6828 - mae: 3.8083 - val_loss: 27.9665 - val_mae: 3.7493\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.5961 - mae: 3.8616 - val_loss: 28.0656 - val_mae: 3.7735\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.5738 - mae: 3.8878 - val_loss: 27.9872 - val_mae: 3.7580\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.5433 - mae: 3.8479 - val_loss: 28.2315 - val_mae: 3.8016\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.4571 - mae: 3.8769 - val_loss: 27.9237 - val_mae: 3.7516\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.4554 - mae: 3.8336 - val_loss: 28.2929 - val_mae: 3.8130\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 35.1182 - mae: 3.6789\n",
      "Mean Absolute Error on Test Data: 3.6789395809173584\n",
      "7/7 [==============================] - 0s 833us/step\n",
      "R-squared: 0.05408159291930559\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 112.5793 - mae: 8.2319 - val_loss: 90.0976 - val_mae: 7.5701\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 86.7448 - mae: 6.5322 - val_loss: 59.7285 - val_mae: 5.5510\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 57.1387 - mae: 4.5229 - val_loss: 33.8306 - val_mae: 4.0479\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9069 - mae: 4.0803 - val_loss: 31.7447 - val_mae: 4.3005\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.5509 - mae: 4.3561 - val_loss: 31.5863 - val_mae: 4.2410\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.0232 - mae: 4.1313 - val_loss: 31.6804 - val_mae: 4.1312\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.9341 - mae: 4.0943 - val_loss: 31.5290 - val_mae: 4.1599\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7899 - mae: 4.1983 - val_loss: 31.3966 - val_mae: 4.1991\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6557 - mae: 4.1798 - val_loss: 31.4438 - val_mae: 4.1362\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4688 - mae: 4.1395 - val_loss: 31.3549 - val_mae: 4.1720\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3961 - mae: 4.1443 - val_loss: 31.3834 - val_mae: 4.1260\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2504 - mae: 4.0949 - val_loss: 31.3223 - val_mae: 4.1492\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2498 - mae: 4.1939 - val_loss: 31.2852 - val_mae: 4.1448\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.1378 - mae: 4.1389 - val_loss: 31.2709 - val_mae: 4.1291\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.0130 - mae: 4.1095 - val_loss: 31.3258 - val_mae: 4.1096\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9735 - mae: 4.1113 - val_loss: 31.2221 - val_mae: 4.1392\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0034 - mae: 4.0945 - val_loss: 31.2424 - val_mae: 4.1405\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9725 - mae: 4.1566 - val_loss: 31.2007 - val_mae: 4.1515\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9486 - mae: 4.1668 - val_loss: 31.2902 - val_mae: 4.1171\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7654 - mae: 4.0637 - val_loss: 31.3284 - val_mae: 4.1041\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.7188 - mae: 4.1309 - val_loss: 31.2039 - val_mae: 4.1306\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7007 - mae: 4.1336 - val_loss: 31.2090 - val_mae: 4.1352\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5887 - mae: 4.0851 - val_loss: 31.2544 - val_mae: 4.1024\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6237 - mae: 4.1732 - val_loss: 31.1212 - val_mae: 4.1585\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6292 - mae: 4.0763 - val_loss: 31.1813 - val_mae: 4.1186\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5555 - mae: 4.1155 - val_loss: 31.0995 - val_mae: 4.1527\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5802 - mae: 4.1843 - val_loss: 31.3430 - val_mae: 4.0770\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5420 - mae: 4.0207 - val_loss: 31.1041 - val_mae: 4.1183\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5304 - mae: 4.1753 - val_loss: 31.0964 - val_mae: 4.1177\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3791 - mae: 4.1003 - val_loss: 31.1223 - val_mae: 4.1032\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3371 - mae: 4.1429 - val_loss: 31.0461 - val_mae: 4.1305\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3966 - mae: 4.1072 - val_loss: 31.0451 - val_mae: 4.1259\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2272 - mae: 4.0772 - val_loss: 31.1262 - val_mae: 4.0853\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3271 - mae: 4.1283 - val_loss: 31.0198 - val_mae: 4.1039\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2595 - mae: 4.0794 - val_loss: 31.0133 - val_mae: 4.1341\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1762 - mae: 4.1438 - val_loss: 31.0816 - val_mae: 4.0968\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1806 - mae: 4.1113 - val_loss: 30.9967 - val_mae: 4.1300\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1802 - mae: 4.1045 - val_loss: 30.9461 - val_mae: 4.1271\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1390 - mae: 4.0999 - val_loss: 31.0668 - val_mae: 4.1006\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0966 - mae: 4.0852 - val_loss: 30.9874 - val_mae: 4.1103\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9648 - mae: 4.1171 - val_loss: 30.9605 - val_mae: 4.1172\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0319 - mae: 4.0944 - val_loss: 31.0301 - val_mae: 4.0796\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9653 - mae: 4.0787 - val_loss: 30.8914 - val_mae: 4.1585\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.9546 - mae: 4.1078 - val_loss: 30.8731 - val_mae: 4.0989\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9774 - mae: 4.0889 - val_loss: 30.8906 - val_mae: 4.1536\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8734 - mae: 4.1140 - val_loss: 31.0331 - val_mae: 4.0604\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9801 - mae: 4.1031 - val_loss: 30.8487 - val_mae: 4.1250\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8532 - mae: 4.0706 - val_loss: 30.9560 - val_mae: 4.0925\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9032 - mae: 4.1347 - val_loss: 30.8747 - val_mae: 4.0822\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.9399 - mae: 4.0092 - val_loss: 30.8140 - val_mae: 4.1372\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 34.9401 - mae: 4.2758\n",
      "Mean Absolute Error on Test Data: 4.275845527648926\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.02965332615906391\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 126.6443 - mae: 8.9244 - val_loss: 80.5678 - val_mae: 7.0160\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 99.2191 - mae: 7.2595 - val_loss: 51.1031 - val_mae: 4.8408\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 61.0543 - mae: 5.0365 - val_loss: 31.9927 - val_mae: 4.0565\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.9162 - mae: 4.8784 - val_loss: 34.6759 - val_mae: 4.5312\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.1480 - mae: 4.8339 - val_loss: 33.5600 - val_mae: 4.3786\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.8641 - mae: 4.7295 - val_loss: 32.9674 - val_mae: 4.2858\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.6057 - mae: 4.7844 - val_loss: 33.8287 - val_mae: 4.4047\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.3564 - mae: 4.7482 - val_loss: 33.4355 - val_mae: 4.3381\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.3460 - mae: 4.7937 - val_loss: 32.9043 - val_mae: 4.2416\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.9995 - mae: 4.6582 - val_loss: 33.1488 - val_mae: 4.2720\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.0772 - mae: 4.8327 - val_loss: 33.5373 - val_mae: 4.3146\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.6859 - mae: 4.6811 - val_loss: 33.4423 - val_mae: 4.2906\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.5497 - mae: 4.6964 - val_loss: 33.7572 - val_mae: 4.3273\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.4666 - mae: 4.7260 - val_loss: 33.8584 - val_mae: 4.3364\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.4443 - mae: 4.6577 - val_loss: 33.3032 - val_mae: 4.2544\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.6565 - mae: 4.7959 - val_loss: 33.2728 - val_mae: 4.2443\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.3853 - mae: 4.6962 - val_loss: 33.1695 - val_mae: 4.2262\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2612 - mae: 4.6606 - val_loss: 33.8730 - val_mae: 4.3214\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2787 - mae: 4.7412 - val_loss: 33.2506 - val_mae: 4.2321\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.1088 - mae: 4.6860 - val_loss: 33.8881 - val_mae: 4.3230\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.0304 - mae: 4.6685 - val_loss: 33.7342 - val_mae: 4.3009\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1131 - mae: 4.6789 - val_loss: 33.7222 - val_mae: 4.3017\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9715 - mae: 4.6591 - val_loss: 34.0653 - val_mae: 4.3395\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1973 - mae: 4.7143 - val_loss: 33.8999 - val_mae: 4.3189\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.9508 - mae: 4.7681 - val_loss: 33.7030 - val_mae: 4.2971\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7777 - mae: 4.6589 - val_loss: 33.3168 - val_mae: 4.2238\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.8504 - mae: 4.6525 - val_loss: 32.9847 - val_mae: 4.2036\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7072 - mae: 4.6730 - val_loss: 34.1778 - val_mae: 4.3632\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7189 - mae: 4.6948 - val_loss: 33.4969 - val_mae: 4.2691\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6000 - mae: 4.6540 - val_loss: 33.1945 - val_mae: 4.2224\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7084 - mae: 4.6382 - val_loss: 33.4234 - val_mae: 4.2663\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7155 - mae: 4.7717 - val_loss: 33.4630 - val_mae: 4.2539\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7145 - mae: 4.6295 - val_loss: 33.8408 - val_mae: 4.3080\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.5806 - mae: 4.7063 - val_loss: 33.2055 - val_mae: 4.2184\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4048 - mae: 4.6321 - val_loss: 33.3109 - val_mae: 4.2540\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4549 - mae: 4.7158 - val_loss: 32.9001 - val_mae: 4.1871\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3567 - mae: 4.6052 - val_loss: 33.2335 - val_mae: 4.2544\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3026 - mae: 4.6778 - val_loss: 33.1162 - val_mae: 4.2256\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4131 - mae: 4.6051 - val_loss: 33.1752 - val_mae: 4.2465\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3358 - mae: 4.6252 - val_loss: 33.3703 - val_mae: 4.2700\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3145 - mae: 4.7192 - val_loss: 32.5982 - val_mae: 4.1458\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3167 - mae: 4.6143 - val_loss: 32.4938 - val_mae: 4.1207\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4646 - mae: 4.5444 - val_loss: 33.5894 - val_mae: 4.3173\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2272 - mae: 4.7297 - val_loss: 32.5022 - val_mae: 4.1344\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0666 - mae: 4.6036 - val_loss: 32.7598 - val_mae: 4.1713\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9998 - mae: 4.6041 - val_loss: 32.4574 - val_mae: 4.1490\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9398 - mae: 4.6084 - val_loss: 32.9365 - val_mae: 4.2077\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.8905 - mae: 4.6266 - val_loss: 33.0790 - val_mae: 4.2216\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.0107 - mae: 4.6347 - val_loss: 33.4260 - val_mae: 4.2745\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9275 - mae: 4.6118 - val_loss: 32.8521 - val_mae: 4.2011\n",
      "8/8 [==============================] - 0s 930us/step - loss: 36.5697 - mae: 4.2634\n",
      "Mean Absolute Error on Test Data: 4.263449192047119\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.09881406506198642\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 18ms/step - loss: 3.5430 - mae: 1.4224 - val_loss: 3.2664 - val_mae: 1.1446\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.1095 - mae: 0.9136 - val_loss: 2.3176 - val_mae: 0.9692\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5770 - mae: 0.8708 - val_loss: 2.2828 - val_mae: 1.0981\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6118 - mae: 0.9290 - val_loss: 2.2506 - val_mae: 1.0716\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5637 - mae: 0.8858 - val_loss: 2.2412 - val_mae: 1.0206\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5570 - mae: 0.8742 - val_loss: 2.2336 - val_mae: 1.0338\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5426 - mae: 0.8850 - val_loss: 2.2345 - val_mae: 1.0450\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5346 - mae: 0.8874 - val_loss: 2.2355 - val_mae: 1.0465\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5249 - mae: 0.8752 - val_loss: 2.2319 - val_mae: 1.0341\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5084 - mae: 0.8757 - val_loss: 2.2356 - val_mae: 1.0514\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5010 - mae: 0.8795 - val_loss: 2.2332 - val_mae: 1.0414\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4951 - mae: 0.8757 - val_loss: 2.2429 - val_mae: 1.0513\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4851 - mae: 0.8658 - val_loss: 2.2391 - val_mae: 1.0254\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.4804 - mae: 0.8594 - val_loss: 2.2340 - val_mae: 1.0380\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4724 - mae: 0.8678 - val_loss: 2.2339 - val_mae: 1.0468\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4677 - mae: 0.8729 - val_loss: 2.2290 - val_mae: 1.0430\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4606 - mae: 0.8640 - val_loss: 2.2249 - val_mae: 1.0323\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4580 - mae: 0.8569 - val_loss: 2.2214 - val_mae: 1.0320\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4548 - mae: 0.8654 - val_loss: 2.2250 - val_mae: 1.0447\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4501 - mae: 0.8546 - val_loss: 2.2232 - val_mae: 1.0330\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.4486 - mae: 0.8634 - val_loss: 2.2234 - val_mae: 1.0408\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4390 - mae: 0.8557 - val_loss: 2.2128 - val_mae: 1.0326\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4491 - mae: 0.8703 - val_loss: 2.2082 - val_mae: 1.0292\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4521 - mae: 0.8319 - val_loss: 2.2135 - val_mae: 1.0092\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4215 - mae: 0.8501 - val_loss: 2.2214 - val_mae: 1.0680\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4417 - mae: 0.8837 - val_loss: 2.2079 - val_mae: 1.0159\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.4488 - mae: 0.8325 - val_loss: 2.2020 - val_mae: 1.0163\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4203 - mae: 0.8536 - val_loss: 2.1997 - val_mae: 1.0395\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4144 - mae: 0.8479 - val_loss: 2.1951 - val_mae: 1.0256\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4091 - mae: 0.8480 - val_loss: 2.1917 - val_mae: 1.0338\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4123 - mae: 0.8538 - val_loss: 2.1896 - val_mae: 1.0387\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4007 - mae: 0.8440 - val_loss: 2.1921 - val_mae: 1.0125\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4028 - mae: 0.8412 - val_loss: 2.1904 - val_mae: 1.0381\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.3992 - mae: 0.8452 - val_loss: 2.1849 - val_mae: 1.0184\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4035 - mae: 0.8412 - val_loss: 2.1834 - val_mae: 1.0199\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3939 - mae: 0.8527 - val_loss: 2.1818 - val_mae: 1.0427\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3880 - mae: 0.8454 - val_loss: 2.1791 - val_mae: 1.0207\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4062 - mae: 0.8373 - val_loss: 2.1757 - val_mae: 1.0352\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3902 - mae: 0.8511 - val_loss: 2.1709 - val_mae: 1.0169\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3931 - mae: 0.8550 - val_loss: 2.1747 - val_mae: 1.0334\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.3862 - mae: 0.8368 - val_loss: 2.1735 - val_mae: 1.0263\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3786 - mae: 0.8350 - val_loss: 2.1694 - val_mae: 1.0229\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3833 - mae: 0.8536 - val_loss: 2.1658 - val_mae: 1.0380\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.3909 - mae: 0.8314 - val_loss: 2.1613 - val_mae: 1.0047\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3781 - mae: 0.8477 - val_loss: 2.1548 - val_mae: 1.0257\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3673 - mae: 0.8431 - val_loss: 2.1667 - val_mae: 1.0236\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.3611 - mae: 0.8316 - val_loss: 2.1635 - val_mae: 1.0137\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3651 - mae: 0.8279 - val_loss: 2.1548 - val_mae: 1.0202\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3696 - mae: 0.8471 - val_loss: 2.1632 - val_mae: 1.0251\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.3708 - mae: 0.8271 - val_loss: 2.1546 - val_mae: 1.0135\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.1435 - mae: 0.8485\n",
      "Mean Absolute Error on Test Data: 0.8485493063926697\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.002846814375709883\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 86.5424 - mae: 7.6059 - val_loss: 100.1041 - val_mae: 7.3708\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 60.5673 - mae: 5.8052 - val_loss: 67.8145 - val_mae: 5.1848\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.3837 - mae: 3.8622 - val_loss: 46.2223 - val_mae: 4.2321\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.1212 - mae: 3.8305 - val_loss: 45.5345 - val_mae: 4.3290\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8172 - mae: 3.7165 - val_loss: 45.9166 - val_mae: 4.2466\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.7446 - mae: 3.6893 - val_loss: 45.5895 - val_mae: 4.2815\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7290 - mae: 3.7142 - val_loss: 45.5292 - val_mae: 4.2725\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6737 - mae: 3.6821 - val_loss: 45.7570 - val_mae: 4.2356\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6563 - mae: 3.7462 - val_loss: 45.3632 - val_mae: 4.2594\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.5370 - mae: 3.6927 - val_loss: 45.6713 - val_mae: 4.2252\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6680 - mae: 3.7233 - val_loss: 45.4668 - val_mae: 4.2351\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.5081 - mae: 3.6471 - val_loss: 45.4201 - val_mae: 4.2392\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.4503 - mae: 3.7044 - val_loss: 45.4265 - val_mae: 4.2300\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.3223 - mae: 3.6490 - val_loss: 45.3448 - val_mae: 4.2449\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.3949 - mae: 3.7170 - val_loss: 45.4038 - val_mae: 4.2330\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.2756 - mae: 3.6486 - val_loss: 45.2217 - val_mae: 4.2355\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.3793 - mae: 3.7116 - val_loss: 45.5241 - val_mae: 4.2133\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1719 - mae: 3.6410 - val_loss: 45.1094 - val_mae: 4.2383\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.2336 - mae: 3.7136 - val_loss: 45.1635 - val_mae: 4.2176\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1504 - mae: 3.6556 - val_loss: 45.0945 - val_mae: 4.2478\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0628 - mae: 3.6593 - val_loss: 44.8995 - val_mae: 4.2403\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0772 - mae: 3.6422 - val_loss: 44.8958 - val_mae: 4.2519\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1092 - mae: 3.7292 - val_loss: 45.1779 - val_mae: 4.2048\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0324 - mae: 3.6013 - val_loss: 44.8315 - val_mae: 4.2529\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.0830 - mae: 3.7170 - val_loss: 45.2057 - val_mae: 4.1976\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9700 - mae: 3.6100 - val_loss: 44.7075 - val_mae: 4.2610\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8745 - mae: 3.6620 - val_loss: 44.7961 - val_mae: 4.2379\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8277 - mae: 3.6508 - val_loss: 45.0350 - val_mae: 4.2081\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8373 - mae: 3.6491 - val_loss: 45.0485 - val_mae: 4.2078\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8877 - mae: 3.6227 - val_loss: 44.6361 - val_mae: 4.2620\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7782 - mae: 3.6449 - val_loss: 45.2533 - val_mae: 4.1782\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9077 - mae: 3.6617 - val_loss: 45.0819 - val_mae: 4.1960\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7356 - mae: 3.6065 - val_loss: 44.5481 - val_mae: 4.2403\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7002 - mae: 3.6324 - val_loss: 44.5406 - val_mae: 4.2573\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.6395 - mae: 3.6445 - val_loss: 44.9609 - val_mae: 4.1934\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.5938 - mae: 3.6255 - val_loss: 44.6602 - val_mae: 4.2211\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.6469 - mae: 3.5964 - val_loss: 44.3827 - val_mae: 4.2686\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.6325 - mae: 3.6828 - val_loss: 45.2227 - val_mae: 4.1728\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.5826 - mae: 3.6265 - val_loss: 44.6490 - val_mae: 4.2082\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5729 - mae: 3.5708 - val_loss: 44.4782 - val_mae: 4.2496\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.4680 - mae: 3.6861 - val_loss: 44.8452 - val_mae: 4.1869\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5287 - mae: 3.5824 - val_loss: 44.5471 - val_mae: 4.2177\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3852 - mae: 3.6305 - val_loss: 44.3972 - val_mae: 4.2269\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.3731 - mae: 3.5919 - val_loss: 44.4673 - val_mae: 4.2007\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.3183 - mae: 3.6598 - val_loss: 44.5774 - val_mae: 4.2134\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3906 - mae: 3.6065 - val_loss: 44.3329 - val_mae: 4.2387\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.2458 - mae: 3.5938 - val_loss: 44.5619 - val_mae: 4.2248\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3532 - mae: 3.6301 - val_loss: 45.0748 - val_mae: 4.1579\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.1318 - mae: 3.5772 - val_loss: 44.2260 - val_mae: 4.2899\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.1214 - mae: 3.6322 - val_loss: 44.7279 - val_mae: 4.1919\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 49.0255 - mae: 4.3728\n",
      "Mean Absolute Error on Test Data: 4.372799396514893\n",
      "8/8 [==============================] - 0s 931us/step\n",
      "R-squared: 0.04085970132305283\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 84.6616 - mae: 7.3660 - val_loss: 72.2502 - val_mae: 6.6168\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.1129 - mae: 6.0451 - val_loss: 49.6763 - val_mae: 4.9022\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8380 - mae: 4.2056 - val_loss: 28.7931 - val_mae: 3.6165\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.4967 - mae: 3.7573 - val_loss: 27.4002 - val_mae: 3.9090\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.2718 - mae: 3.8280 - val_loss: 26.9544 - val_mae: 3.7634\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.1178 - mae: 3.7750 - val_loss: 26.8693 - val_mae: 3.7540\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 29.0869 - mae: 3.7428 - val_loss: 26.8101 - val_mae: 3.7445\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.9466 - mae: 3.7604 - val_loss: 26.7787 - val_mae: 3.7918\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.9709 - mae: 3.7506 - val_loss: 26.6967 - val_mae: 3.7815\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.9101 - mae: 3.7710 - val_loss: 26.6213 - val_mae: 3.7126\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.8435 - mae: 3.7851 - val_loss: 26.5676 - val_mae: 3.7578\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.6962 - mae: 3.7448 - val_loss: 26.5012 - val_mae: 3.7361\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.7560 - mae: 3.7422 - val_loss: 26.5578 - val_mae: 3.8156\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.6071 - mae: 3.8133 - val_loss: 26.3874 - val_mae: 3.7459\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.5250 - mae: 3.7133 - val_loss: 26.3382 - val_mae: 3.7323\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.5360 - mae: 3.7998 - val_loss: 26.3079 - val_mae: 3.7585\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4350 - mae: 3.7598 - val_loss: 26.2500 - val_mae: 3.7195\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4674 - mae: 3.6855 - val_loss: 26.2126 - val_mae: 3.7058\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4230 - mae: 3.7708 - val_loss: 26.2272 - val_mae: 3.7795\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4323 - mae: 3.7051 - val_loss: 26.1466 - val_mae: 3.7364\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1941 - mae: 3.7516 - val_loss: 26.1472 - val_mae: 3.7775\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1918 - mae: 3.7524 - val_loss: 26.0649 - val_mae: 3.7316\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.2617 - mae: 3.7214 - val_loss: 26.0613 - val_mae: 3.6964\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.1820 - mae: 3.7059 - val_loss: 26.0847 - val_mae: 3.7889\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.1355 - mae: 3.7354 - val_loss: 25.9968 - val_mae: 3.7504\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0688 - mae: 3.7335 - val_loss: 25.9605 - val_mae: 3.7324\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0471 - mae: 3.7145 - val_loss: 25.9652 - val_mae: 3.6946\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0724 - mae: 3.6817 - val_loss: 25.9511 - val_mae: 3.7570\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0951 - mae: 3.7951 - val_loss: 25.9203 - val_mae: 3.7237\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0025 - mae: 3.6990 - val_loss: 25.9142 - val_mae: 3.7013\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0374 - mae: 3.6919 - val_loss: 25.9753 - val_mae: 3.7981\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9422 - mae: 3.7438 - val_loss: 25.8914 - val_mae: 3.7096\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9627 - mae: 3.6945 - val_loss: 25.8812 - val_mae: 3.7168\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8813 - mae: 3.7323 - val_loss: 25.8935 - val_mae: 3.7683\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8653 - mae: 3.7288 - val_loss: 25.8570 - val_mae: 3.7377\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9166 - mae: 3.7136 - val_loss: 25.8618 - val_mae: 3.7622\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8374 - mae: 3.7142 - val_loss: 25.8711 - val_mae: 3.6981\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9149 - mae: 3.7413 - val_loss: 25.8565 - val_mae: 3.7216\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.8359 - mae: 3.6804 - val_loss: 25.8310 - val_mae: 3.7387\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8393 - mae: 3.7151 - val_loss: 25.8399 - val_mae: 3.7518\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.8063 - mae: 3.7267 - val_loss: 25.8494 - val_mae: 3.7705\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8110 - mae: 3.7193 - val_loss: 25.8226 - val_mae: 3.7258\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.7737 - mae: 3.7425 - val_loss: 25.8234 - val_mae: 3.7503\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7509 - mae: 3.6805 - val_loss: 25.8180 - val_mae: 3.7506\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7796 - mae: 3.7457 - val_loss: 25.8137 - val_mae: 3.7233\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7195 - mae: 3.6788 - val_loss: 25.8179 - val_mae: 3.7579\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7249 - mae: 3.7219 - val_loss: 25.8025 - val_mae: 3.7459\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.6915 - mae: 3.7275 - val_loss: 25.8423 - val_mae: 3.7769\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.6744 - mae: 3.7312 - val_loss: 25.8149 - val_mae: 3.7635\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.7399 - mae: 3.7544 - val_loss: 25.8118 - val_mae: 3.7385\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 28.6261 - mae: 3.9137\n",
      "Mean Absolute Error on Test Data: 3.9136946201324463\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.018157289251582553\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 13ms/step - loss: 8.1877 - mae: 2.0668 - val_loss: 5.7029 - val_mae: 1.5449\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.0425 - mae: 1.4452 - val_loss: 3.6326 - val_mae: 1.2575\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.0379 - mae: 1.4830 - val_loss: 3.6663 - val_mae: 1.4232\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.0346 - mae: 1.4538 - val_loss: 3.5860 - val_mae: 1.3070\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9952 - mae: 1.4268 - val_loss: 3.5984 - val_mae: 1.3587\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9944 - mae: 1.4762 - val_loss: 3.5866 - val_mae: 1.3300\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9760 - mae: 1.4166 - val_loss: 3.5873 - val_mae: 1.3131\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9610 - mae: 1.4215 - val_loss: 3.5919 - val_mae: 1.3402\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.9751 - mae: 1.4733 - val_loss: 3.5916 - val_mae: 1.3376\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9475 - mae: 1.4293 - val_loss: 3.5924 - val_mae: 1.3333\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9480 - mae: 1.4377 - val_loss: 3.5971 - val_mae: 1.3415\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9356 - mae: 1.4330 - val_loss: 3.6017 - val_mae: 1.3489\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9260 - mae: 1.4329 - val_loss: 3.5984 - val_mae: 1.3367\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9300 - mae: 1.4230 - val_loss: 3.5992 - val_mae: 1.3382\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9127 - mae: 1.4414 - val_loss: 3.6054 - val_mae: 1.3481\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.9073 - mae: 1.4469 - val_loss: 3.6026 - val_mae: 1.3378\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9093 - mae: 1.4101 - val_loss: 3.6050 - val_mae: 1.3396\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8990 - mae: 1.4430 - val_loss: 3.6096 - val_mae: 1.3427\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8914 - mae: 1.4269 - val_loss: 3.6086 - val_mae: 1.3412\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8911 - mae: 1.4182 - val_loss: 3.6270 - val_mae: 1.3632\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8889 - mae: 1.4386 - val_loss: 3.6196 - val_mae: 1.3497\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8880 - mae: 1.4471 - val_loss: 3.6187 - val_mae: 1.3228\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8811 - mae: 1.3990 - val_loss: 3.6206 - val_mae: 1.3467\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8736 - mae: 1.4557 - val_loss: 3.6280 - val_mae: 1.3492\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8633 - mae: 1.4131 - val_loss: 3.6265 - val_mae: 1.3359\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8602 - mae: 1.4199 - val_loss: 3.6348 - val_mae: 1.3554\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8550 - mae: 1.4174 - val_loss: 3.6424 - val_mae: 1.3697\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8644 - mae: 1.4601 - val_loss: 3.6356 - val_mae: 1.3449\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8827 - mae: 1.3876 - val_loss: 3.6534 - val_mae: 1.3779\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8903 - mae: 1.4775 - val_loss: 3.6441 - val_mae: 1.3250\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8559 - mae: 1.3990 - val_loss: 3.6459 - val_mae: 1.3597\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8326 - mae: 1.4325 - val_loss: 3.6570 - val_mae: 1.3763\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8269 - mae: 1.4160 - val_loss: 3.6390 - val_mae: 1.3522\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8149 - mae: 1.4176 - val_loss: 3.6561 - val_mae: 1.3694\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8289 - mae: 1.4295 - val_loss: 3.6500 - val_mae: 1.3523\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8163 - mae: 1.4005 - val_loss: 3.6756 - val_mae: 1.3831\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8218 - mae: 1.4502 - val_loss: 3.6544 - val_mae: 1.3593\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8020 - mae: 1.4312 - val_loss: 3.6533 - val_mae: 1.3499\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8185 - mae: 1.4167 - val_loss: 3.6700 - val_mae: 1.3244\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7945 - mae: 1.4161 - val_loss: 3.6917 - val_mae: 1.4010\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7974 - mae: 1.4101 - val_loss: 3.6644 - val_mae: 1.3445\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7806 - mae: 1.4250 - val_loss: 3.6717 - val_mae: 1.3660\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7756 - mae: 1.4211 - val_loss: 3.6755 - val_mae: 1.3565\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7771 - mae: 1.4280 - val_loss: 3.6765 - val_mae: 1.3796\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7666 - mae: 1.4069 - val_loss: 3.6678 - val_mae: 1.3502\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7963 - mae: 1.4390 - val_loss: 3.6789 - val_mae: 1.3381\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7708 - mae: 1.3983 - val_loss: 3.6782 - val_mae: 1.3756\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7785 - mae: 1.4171 - val_loss: 3.6806 - val_mae: 1.3498\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7665 - mae: 1.4170 - val_loss: 3.6855 - val_mae: 1.3786\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7509 - mae: 1.4416 - val_loss: 3.6847 - val_mae: 1.3745\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.7196 - mae: 1.3712\n",
      "Mean Absolute Error on Test Data: 1.3711992502212524\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0952082351738085\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 21ms/step - loss: 2.3805 - mae: 1.1282 - val_loss: 1.4537 - val_mae: 0.8359\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.4107 - mae: 0.7437 - val_loss: 0.9647 - val_mae: 0.7889\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2314 - mae: 0.8160 - val_loss: 0.9661 - val_mae: 0.8333\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2162 - mae: 0.8172 - val_loss: 0.9334 - val_mae: 0.7974\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1902 - mae: 0.7758 - val_loss: 0.9330 - val_mae: 0.7721\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1824 - mae: 0.7749 - val_loss: 0.9109 - val_mae: 0.7849\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1645 - mae: 0.7772 - val_loss: 0.9026 - val_mae: 0.7799\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1530 - mae: 0.7732 - val_loss: 0.8932 - val_mae: 0.7768\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1420 - mae: 0.7720 - val_loss: 0.8861 - val_mae: 0.7794\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1313 - mae: 0.7713 - val_loss: 0.8828 - val_mae: 0.7659\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1246 - mae: 0.7582 - val_loss: 0.8715 - val_mae: 0.7641\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1178 - mae: 0.7548 - val_loss: 0.8641 - val_mae: 0.7679\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1094 - mae: 0.7697 - val_loss: 0.8538 - val_mae: 0.7712\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.1144 - mae: 0.7550 - val_loss: 0.8477 - val_mae: 0.7601\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1058 - mae: 0.7694 - val_loss: 0.8454 - val_mae: 0.7661\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0987 - mae: 0.7519 - val_loss: 0.8403 - val_mae: 0.7545\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0929 - mae: 0.7615 - val_loss: 0.8416 - val_mae: 0.7698\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1035 - mae: 0.7795 - val_loss: 0.8318 - val_mae: 0.7587\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1145 - mae: 0.7401 - val_loss: 0.8298 - val_mae: 0.7535\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0956 - mae: 0.7721 - val_loss: 0.8303 - val_mae: 0.7636\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0941 - mae: 0.7390 - val_loss: 0.8429 - val_mae: 0.7505\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0886 - mae: 0.7580 - val_loss: 0.8351 - val_mae: 0.7683\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0874 - mae: 0.7664 - val_loss: 0.8365 - val_mae: 0.7671\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0797 - mae: 0.7567 - val_loss: 0.8301 - val_mae: 0.7556\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0843 - mae: 0.7365 - val_loss: 0.8343 - val_mae: 0.7594\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0759 - mae: 0.7481 - val_loss: 0.8317 - val_mae: 0.7658\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0755 - mae: 0.7512 - val_loss: 0.8273 - val_mae: 0.7608\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0763 - mae: 0.7514 - val_loss: 0.8270 - val_mae: 0.7662\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0716 - mae: 0.7563 - val_loss: 0.8229 - val_mae: 0.7584\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0765 - mae: 0.7411 - val_loss: 0.8211 - val_mae: 0.7549\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0704 - mae: 0.7458 - val_loss: 0.8224 - val_mae: 0.7603\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0725 - mae: 0.7550 - val_loss: 0.8240 - val_mae: 0.7565\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0681 - mae: 0.7420 - val_loss: 0.8237 - val_mae: 0.7557\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0733 - mae: 0.7555 - val_loss: 0.8203 - val_mae: 0.7582\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0664 - mae: 0.7440 - val_loss: 0.8249 - val_mae: 0.7576\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0606 - mae: 0.7474 - val_loss: 0.8231 - val_mae: 0.7644\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0692 - mae: 0.7721 - val_loss: 0.8180 - val_mae: 0.7648\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0599 - mae: 0.7550 - val_loss: 0.8197 - val_mae: 0.7571\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0646 - mae: 0.7469 - val_loss: 0.8216 - val_mae: 0.7589\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0605 - mae: 0.7494 - val_loss: 0.8196 - val_mae: 0.7587\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0531 - mae: 0.7414 - val_loss: 0.8221 - val_mae: 0.7511\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0561 - mae: 0.7498 - val_loss: 0.8224 - val_mae: 0.7635\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0525 - mae: 0.7396 - val_loss: 0.8238 - val_mae: 0.7554\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0634 - mae: 0.7643 - val_loss: 0.8258 - val_mae: 0.7694\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.0454 - mae: 0.7426 - val_loss: 0.8236 - val_mae: 0.7511\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0441 - mae: 0.7450 - val_loss: 0.8208 - val_mae: 0.7678\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0490 - mae: 0.7529 - val_loss: 0.8207 - val_mae: 0.7555\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0517 - mae: 0.7347 - val_loss: 0.8154 - val_mae: 0.7590\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0519 - mae: 0.7636 - val_loss: 0.8221 - val_mae: 0.7654\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0483 - mae: 0.7291 - val_loss: 0.8235 - val_mae: 0.7496\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.7286 - mae: 0.8091\n",
      "Mean Absolute Error on Test Data: 0.8090606927871704\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.11645871382397699\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 25.6943 - mae: 3.7711 - val_loss: 26.7154 - val_mae: 3.2929\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.7804 - mae: 2.7695 - val_loss: 18.5206 - val_mae: 2.4775\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.0268 - mae: 2.3189 - val_loss: 15.9049 - val_mae: 2.5424\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.5012 - mae: 2.4240 - val_loss: 15.8343 - val_mae: 2.4799\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.4091 - mae: 2.3445 - val_loss: 15.7965 - val_mae: 2.4626\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3635 - mae: 2.3154 - val_loss: 15.8110 - val_mae: 2.4244\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3093 - mae: 2.3317 - val_loss: 15.6930 - val_mae: 2.4632\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.2441 - mae: 2.3218 - val_loss: 15.6782 - val_mae: 2.4223\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.2433 - mae: 2.3253 - val_loss: 15.5829 - val_mae: 2.4480\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1864 - mae: 2.3659 - val_loss: 15.5487 - val_mae: 2.4381\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1350 - mae: 2.3398 - val_loss: 15.5126 - val_mae: 2.4214\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1117 - mae: 2.3023 - val_loss: 15.4683 - val_mae: 2.4261\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1042 - mae: 2.3627 - val_loss: 15.4074 - val_mae: 2.4226\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0659 - mae: 2.2852 - val_loss: 15.4399 - val_mae: 2.4099\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0740 - mae: 2.3447 - val_loss: 15.4232 - val_mae: 2.4080\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0084 - mae: 2.2678 - val_loss: 15.4403 - val_mae: 2.3989\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9893 - mae: 2.2884 - val_loss: 15.3622 - val_mae: 2.4221\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.9800 - mae: 2.2815 - val_loss: 15.3378 - val_mae: 2.4332\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9756 - mae: 2.3179 - val_loss: 15.2888 - val_mae: 2.4434\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9475 - mae: 2.3214 - val_loss: 15.2717 - val_mae: 2.4278\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9355 - mae: 2.2963 - val_loss: 15.3170 - val_mae: 2.4124\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9228 - mae: 2.3062 - val_loss: 15.2431 - val_mae: 2.4436\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9283 - mae: 2.2921 - val_loss: 15.3591 - val_mae: 2.3976\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9372 - mae: 2.3344 - val_loss: 15.2448 - val_mae: 2.4526\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.9498 - mae: 2.2698 - val_loss: 15.2478 - val_mae: 2.4322\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9116 - mae: 2.3360 - val_loss: 15.1626 - val_mae: 2.4284\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9074 - mae: 2.3063 - val_loss: 15.2309 - val_mae: 2.4119\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9420 - mae: 2.3422 - val_loss: 15.2008 - val_mae: 2.4181\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8655 - mae: 2.2817 - val_loss: 15.2470 - val_mae: 2.4026\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8462 - mae: 2.3071 - val_loss: 15.1719 - val_mae: 2.4416\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8389 - mae: 2.3011 - val_loss: 15.3104 - val_mae: 2.4006\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9097 - mae: 2.2734 - val_loss: 15.1827 - val_mae: 2.4469\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8428 - mae: 2.2808 - val_loss: 15.2181 - val_mae: 2.4204\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8282 - mae: 2.3087 - val_loss: 15.2219 - val_mae: 2.4254\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8717 - mae: 2.2592 - val_loss: 15.2597 - val_mae: 2.4266\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8306 - mae: 2.3332 - val_loss: 15.1499 - val_mae: 2.4539\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8276 - mae: 2.3054 - val_loss: 15.2583 - val_mae: 2.4124\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8381 - mae: 2.2575 - val_loss: 15.1815 - val_mae: 2.4234\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.8059 - mae: 2.3018 - val_loss: 15.1679 - val_mae: 2.4347\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8213 - mae: 2.2867 - val_loss: 15.2192 - val_mae: 2.4108\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8390 - mae: 2.3098 - val_loss: 15.1879 - val_mae: 2.4104\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8719 - mae: 2.3366 - val_loss: 15.2100 - val_mae: 2.4030\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8167 - mae: 2.2975 - val_loss: 15.1353 - val_mae: 2.4081\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8781 - mae: 2.2746 - val_loss: 15.1474 - val_mae: 2.4281\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7784 - mae: 2.2736 - val_loss: 15.2086 - val_mae: 2.4085\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8048 - mae: 2.2925 - val_loss: 15.1339 - val_mae: 2.4472\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7651 - mae: 2.2837 - val_loss: 15.1551 - val_mae: 2.4046\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7622 - mae: 2.2811 - val_loss: 15.1383 - val_mae: 2.4172\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8058 - mae: 2.2596 - val_loss: 15.1224 - val_mae: 2.4394\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8000 - mae: 2.3232 - val_loss: 15.1247 - val_mae: 2.4265\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.9803 - mae: 2.3457\n",
      "Mean Absolute Error on Test Data: 2.3457415103912354\n",
      "7/7 [==============================] - 0s 999us/step\n",
      "R-squared: 0.06283616415598636\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 15ms/step - loss: 12.1298 - mae: 2.3884 - val_loss: 6.4136 - val_mae: 1.9055\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 9.5272 - mae: 1.8635 - val_loss: 4.0667 - val_mae: 1.4604\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 7.1046 - mae: 1.5735 - val_loss: 3.0645 - val_mae: 1.4272\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5014 - mae: 1.6477 - val_loss: 3.1917 - val_mae: 1.4769\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4926 - mae: 1.6497 - val_loss: 3.0810 - val_mae: 1.4366\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4728 - mae: 1.5984 - val_loss: 3.0735 - val_mae: 1.4321\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4695 - mae: 1.6335 - val_loss: 3.1191 - val_mae: 1.4499\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4303 - mae: 1.6154 - val_loss: 3.0756 - val_mae: 1.4317\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4239 - mae: 1.6005 - val_loss: 3.0834 - val_mae: 1.4335\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4224 - mae: 1.5935 - val_loss: 3.0930 - val_mae: 1.4366\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4259 - mae: 1.6344 - val_loss: 3.1045 - val_mae: 1.4399\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3936 - mae: 1.5956 - val_loss: 3.0844 - val_mae: 1.4284\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3963 - mae: 1.5924 - val_loss: 3.0758 - val_mae: 1.4231\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.3651 - mae: 1.6088 - val_loss: 3.1706 - val_mae: 1.4632\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3704 - mae: 1.5976 - val_loss: 3.1125 - val_mae: 1.4353\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3454 - mae: 1.6121 - val_loss: 3.1138 - val_mae: 1.4344\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3335 - mae: 1.5828 - val_loss: 3.1009 - val_mae: 1.4267\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3320 - mae: 1.5825 - val_loss: 3.1318 - val_mae: 1.4391\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 6.3410 - mae: 1.6337 - val_loss: 3.1687 - val_mae: 1.4543\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3244 - mae: 1.5722 - val_loss: 3.1009 - val_mae: 1.4208\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3137 - mae: 1.6069 - val_loss: 3.1654 - val_mae: 1.4507\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3254 - mae: 1.5672 - val_loss: 3.1132 - val_mae: 1.4259\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2662 - mae: 1.5916 - val_loss: 3.2134 - val_mae: 1.4700\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2707 - mae: 1.6146 - val_loss: 3.1431 - val_mae: 1.4374\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.2721 - mae: 1.5643 - val_loss: 3.1132 - val_mae: 1.4199\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2783 - mae: 1.6045 - val_loss: 3.1583 - val_mae: 1.4443\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2693 - mae: 1.5518 - val_loss: 3.1244 - val_mae: 1.4227\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2481 - mae: 1.6082 - val_loss: 3.2106 - val_mae: 1.4658\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2567 - mae: 1.5580 - val_loss: 3.1206 - val_mae: 1.4195\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3380 - mae: 1.6414 - val_loss: 3.1865 - val_mae: 1.4533\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2115 - mae: 1.5458 - val_loss: 3.1236 - val_mae: 1.4166\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.2213 - mae: 1.5746 - val_loss: 3.2056 - val_mae: 1.4630\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1921 - mae: 1.5680 - val_loss: 3.1488 - val_mae: 1.4336\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2048 - mae: 1.5519 - val_loss: 3.1473 - val_mae: 1.4348\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1738 - mae: 1.5845 - val_loss: 3.1996 - val_mae: 1.4572\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1784 - mae: 1.5705 - val_loss: 3.1750 - val_mae: 1.4464\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1586 - mae: 1.5791 - val_loss: 3.2007 - val_mae: 1.4547\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1511 - mae: 1.5547 - val_loss: 3.1506 - val_mae: 1.4345\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1443 - mae: 1.5815 - val_loss: 3.2026 - val_mae: 1.4576\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1511 - mae: 1.5530 - val_loss: 3.1432 - val_mae: 1.4248\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1154 - mae: 1.5790 - val_loss: 3.2232 - val_mae: 1.4626\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1150 - mae: 1.5704 - val_loss: 3.1434 - val_mae: 1.4257\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1054 - mae: 1.5374 - val_loss: 3.1579 - val_mae: 1.4327\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0952 - mae: 1.5893 - val_loss: 3.2460 - val_mae: 1.4695\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1044 - mae: 1.5479 - val_loss: 3.1741 - val_mae: 1.4374\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0724 - mae: 1.5391 - val_loss: 3.2054 - val_mae: 1.4515\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0692 - mae: 1.5581 - val_loss: 3.1888 - val_mae: 1.4441\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0624 - mae: 1.5809 - val_loss: 3.1832 - val_mae: 1.4393\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0433 - mae: 1.5532 - val_loss: 3.1490 - val_mae: 1.4185\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0241 - mae: 1.5281 - val_loss: 3.2208 - val_mae: 1.4548\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 6.2334 - mae: 1.7059\n",
      "Mean Absolute Error on Test Data: 1.7058783769607544\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.108482924743185\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 14ms/step - loss: 4.6673 - mae: 1.5106 - val_loss: 1.8647 - val_mae: 0.9228\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 3.0333 - mae: 1.1022 - val_loss: 1.3075 - val_mae: 0.9147\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.6226 - mae: 1.1537 - val_loss: 1.4945 - val_mae: 1.0085\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.5712 - mae: 1.1313 - val_loss: 1.3534 - val_mae: 0.9367\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.5401 - mae: 1.0994 - val_loss: 1.3388 - val_mae: 0.9311\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.5213 - mae: 1.0864 - val_loss: 1.3350 - val_mae: 0.9307\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4901 - mae: 1.0900 - val_loss: 1.3307 - val_mae: 0.9295\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4649 - mae: 1.0809 - val_loss: 1.3429 - val_mae: 0.9399\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.4538 - mae: 1.1015 - val_loss: 1.3723 - val_mae: 0.9585\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4539 - mae: 1.0813 - val_loss: 1.3815 - val_mae: 0.9623\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4323 - mae: 1.0776 - val_loss: 1.3756 - val_mae: 0.9605\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4374 - mae: 1.1123 - val_loss: 1.3303 - val_mae: 0.9309\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.4094 - mae: 1.0714 - val_loss: 1.3545 - val_mae: 0.9450\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3976 - mae: 1.0750 - val_loss: 1.4028 - val_mae: 0.9703\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.4127 - mae: 1.1275 - val_loss: 1.4202 - val_mae: 0.9807\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3907 - mae: 1.0835 - val_loss: 1.3567 - val_mae: 0.9466\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3824 - mae: 1.0928 - val_loss: 1.4027 - val_mae: 0.9699\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3946 - mae: 1.0676 - val_loss: 1.3878 - val_mae: 0.9618\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3728 - mae: 1.0692 - val_loss: 1.3614 - val_mae: 0.9439\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3766 - mae: 1.0670 - val_loss: 1.3538 - val_mae: 0.9375\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3726 - mae: 1.0634 - val_loss: 1.4473 - val_mae: 0.9900\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3810 - mae: 1.0955 - val_loss: 1.3242 - val_mae: 0.9154\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3867 - mae: 1.0301 - val_loss: 1.4277 - val_mae: 0.9799\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3750 - mae: 1.0993 - val_loss: 1.3730 - val_mae: 0.9518\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3898 - mae: 1.0643 - val_loss: 1.4392 - val_mae: 0.9861\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3868 - mae: 1.0649 - val_loss: 1.3781 - val_mae: 0.9518\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3499 - mae: 1.0852 - val_loss: 1.3760 - val_mae: 0.9486\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3469 - mae: 1.0534 - val_loss: 1.3827 - val_mae: 0.9515\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3404 - mae: 1.0793 - val_loss: 1.4137 - val_mae: 0.9694\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3432 - mae: 1.0848 - val_loss: 1.3560 - val_mae: 0.9366\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3329 - mae: 1.0561 - val_loss: 1.3686 - val_mae: 0.9403\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3344 - mae: 1.0581 - val_loss: 1.4674 - val_mae: 0.9910\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3490 - mae: 1.0898 - val_loss: 1.3714 - val_mae: 0.9385\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3273 - mae: 1.0587 - val_loss: 1.4115 - val_mae: 0.9634\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3306 - mae: 1.0882 - val_loss: 1.3879 - val_mae: 0.9487\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3428 - mae: 1.0370 - val_loss: 1.4265 - val_mae: 0.9680\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3160 - mae: 1.0668 - val_loss: 1.3994 - val_mae: 0.9513\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3148 - mae: 1.0790 - val_loss: 1.4502 - val_mae: 0.9760\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3234 - mae: 1.0739 - val_loss: 1.4187 - val_mae: 0.9600\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3492 - mae: 1.0327 - val_loss: 1.4670 - val_mae: 0.9839\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3038 - mae: 1.0725 - val_loss: 1.4309 - val_mae: 0.9668\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3113 - mae: 1.0374 - val_loss: 1.4083 - val_mae: 0.9543\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3082 - mae: 1.0860 - val_loss: 1.5141 - val_mae: 1.0040\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3137 - mae: 1.0542 - val_loss: 1.3717 - val_mae: 0.9277\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3044 - mae: 1.0522 - val_loss: 1.4149 - val_mae: 0.9587\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2940 - mae: 1.0681 - val_loss: 1.4167 - val_mae: 0.9527\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3052 - mae: 1.0361 - val_loss: 1.4778 - val_mae: 0.9866\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3097 - mae: 1.0996 - val_loss: 1.4725 - val_mae: 0.9835\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2874 - mae: 1.0473 - val_loss: 1.4282 - val_mae: 0.9582\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2873 - mae: 1.0597 - val_loss: 1.3850 - val_mae: 0.9379\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.2277 - mae: 1.2344\n",
      "Mean Absolute Error on Test Data: 1.2343790531158447\n",
      "6/6 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.04209145133310266\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 11ms/step - loss: 17.4319 - mae: 2.9398 - val_loss: 22.3404 - val_mae: 2.9361\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.5158 - mae: 2.1260 - val_loss: 16.0197 - val_mae: 2.4428\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.9945 - mae: 2.0330 - val_loss: 14.9963 - val_mae: 2.5043\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8648 - mae: 2.0446 - val_loss: 15.1326 - val_mae: 2.4508\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8284 - mae: 2.0060 - val_loss: 14.9577 - val_mae: 2.4507\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7729 - mae: 2.0098 - val_loss: 14.8766 - val_mae: 2.4412\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.7436 - mae: 2.0334 - val_loss: 14.7749 - val_mae: 2.4362\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6835 - mae: 2.0108 - val_loss: 14.7965 - val_mae: 2.4187\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6869 - mae: 1.9840 - val_loss: 14.7101 - val_mae: 2.4136\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6509 - mae: 2.0297 - val_loss: 14.6071 - val_mae: 2.4135\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6155 - mae: 2.0060 - val_loss: 14.5754 - val_mae: 2.3970\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5906 - mae: 2.0013 - val_loss: 14.5649 - val_mae: 2.3854\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.5717 - mae: 2.0000 - val_loss: 14.4744 - val_mae: 2.3877\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5556 - mae: 2.0091 - val_loss: 14.4456 - val_mae: 2.3764\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5287 - mae: 2.0068 - val_loss: 14.4077 - val_mae: 2.3713\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5450 - mae: 2.0060 - val_loss: 14.2256 - val_mae: 2.3814\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5255 - mae: 1.9982 - val_loss: 14.2091 - val_mae: 2.3715\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5703 - mae: 1.9937 - val_loss: 14.1340 - val_mae: 2.3821\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5397 - mae: 2.0205 - val_loss: 14.3265 - val_mae: 2.3488\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4782 - mae: 2.0094 - val_loss: 14.2122 - val_mae: 2.3543\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4681 - mae: 2.0051 - val_loss: 14.2043 - val_mae: 2.3515\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4878 - mae: 1.9846 - val_loss: 14.0554 - val_mae: 2.3682\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4713 - mae: 2.0016 - val_loss: 14.0845 - val_mae: 2.3553\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4385 - mae: 2.0153 - val_loss: 14.1130 - val_mae: 2.3479\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4477 - mae: 2.0011 - val_loss: 14.0570 - val_mae: 2.3462\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4346 - mae: 1.9882 - val_loss: 14.0477 - val_mae: 2.3478\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4417 - mae: 1.9978 - val_loss: 14.0664 - val_mae: 2.3468\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4255 - mae: 1.9954 - val_loss: 13.9927 - val_mae: 2.3420\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4217 - mae: 2.0084 - val_loss: 14.1077 - val_mae: 2.3364\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4193 - mae: 1.9941 - val_loss: 13.9543 - val_mae: 2.3505\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4081 - mae: 2.0156 - val_loss: 14.1294 - val_mae: 2.3284\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4197 - mae: 1.9915 - val_loss: 13.9772 - val_mae: 2.3382\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5007 - mae: 1.9969 - val_loss: 14.0649 - val_mae: 2.3340\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4065 - mae: 2.0159 - val_loss: 14.0316 - val_mae: 2.3333\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3752 - mae: 1.9930 - val_loss: 14.0030 - val_mae: 2.3348\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3769 - mae: 1.9898 - val_loss: 13.8968 - val_mae: 2.3422\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4337 - mae: 1.9878 - val_loss: 13.9218 - val_mae: 2.3477\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4377 - mae: 2.0216 - val_loss: 13.8960 - val_mae: 2.3508\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3817 - mae: 1.9961 - val_loss: 14.0645 - val_mae: 2.3222\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3532 - mae: 1.9879 - val_loss: 13.9653 - val_mae: 2.3401\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3733 - mae: 2.0045 - val_loss: 14.0109 - val_mae: 2.3268\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3672 - mae: 2.0076 - val_loss: 13.9706 - val_mae: 2.3242\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3511 - mae: 1.9921 - val_loss: 13.9061 - val_mae: 2.3415\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3354 - mae: 1.9993 - val_loss: 14.0289 - val_mae: 2.3199\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.3465 - mae: 1.9841 - val_loss: 14.0125 - val_mae: 2.3270\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3743 - mae: 2.0225 - val_loss: 13.9691 - val_mae: 2.3278\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3524 - mae: 1.9926 - val_loss: 14.0573 - val_mae: 2.3136\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3341 - mae: 1.9880 - val_loss: 13.8986 - val_mae: 2.3356\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3081 - mae: 2.0094 - val_loss: 13.9830 - val_mae: 2.3193\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3817 - mae: 2.0038 - val_loss: 13.9618 - val_mae: 2.3260\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 6.7171 - mae: 1.7543\n",
      "Mean Absolute Error on Test Data: 1.754286527633667\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.08369869167203126\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 114.4121 - mae: 8.7075 - val_loss: 113.4255 - val_mae: 8.3199\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 87.3869 - mae: 7.0197 - val_loss: 80.0666 - val_mae: 6.3105\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 53.7013 - mae: 4.9208 - val_loss: 48.4855 - val_mae: 4.5142\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 38.3696 - mae: 4.4039 - val_loss: 44.0143 - val_mae: 4.7799\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.5332 - mae: 4.4768 - val_loss: 44.0462 - val_mae: 4.6180\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.4629 - mae: 4.3719 - val_loss: 43.9060 - val_mae: 4.6064\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.4343 - mae: 4.3444 - val_loss: 43.7082 - val_mae: 4.6020\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.2728 - mae: 4.3994 - val_loss: 43.5016 - val_mae: 4.6176\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.2594 - mae: 4.3855 - val_loss: 43.3832 - val_mae: 4.5939\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.2114 - mae: 4.3847 - val_loss: 43.3220 - val_mae: 4.5796\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.1559 - mae: 4.3669 - val_loss: 43.0976 - val_mae: 4.5867\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.1268 - mae: 4.4005 - val_loss: 42.9598 - val_mae: 4.5788\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.0774 - mae: 4.4007 - val_loss: 42.8988 - val_mae: 4.5682\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.0464 - mae: 4.4089 - val_loss: 42.7825 - val_mae: 4.5754\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.9995 - mae: 4.3392 - val_loss: 42.7428 - val_mae: 4.5537\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.9588 - mae: 4.4144 - val_loss: 42.5589 - val_mae: 4.5853\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.8574 - mae: 4.3518 - val_loss: 42.7539 - val_mae: 4.5155\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.8846 - mae: 4.3358 - val_loss: 42.3599 - val_mae: 4.5633\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8818 - mae: 4.4183 - val_loss: 42.5504 - val_mae: 4.5232\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.8778 - mae: 4.3322 - val_loss: 42.1851 - val_mae: 4.5591\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.8069 - mae: 4.4197 - val_loss: 42.2162 - val_mae: 4.5502\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8648 - mae: 4.3076 - val_loss: 42.2034 - val_mae: 4.5235\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8902 - mae: 4.3977 - val_loss: 42.2620 - val_mae: 4.4993\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.7347 - mae: 4.3357 - val_loss: 41.8561 - val_mae: 4.5591\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6717 - mae: 4.3871 - val_loss: 42.0258 - val_mae: 4.5097\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.6421 - mae: 4.3381 - val_loss: 41.9608 - val_mae: 4.5124\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.6791 - mae: 4.3318 - val_loss: 41.7224 - val_mae: 4.5335\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.7018 - mae: 4.3771 - val_loss: 41.8439 - val_mae: 4.5057\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6230 - mae: 4.3594 - val_loss: 41.5912 - val_mae: 4.5553\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6458 - mae: 4.3476 - val_loss: 41.6034 - val_mae: 4.5217\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.6852 - mae: 4.3813 - val_loss: 41.8995 - val_mae: 4.4701\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.5714 - mae: 4.3506 - val_loss: 41.4194 - val_mae: 4.5445\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5170 - mae: 4.3833 - val_loss: 41.5431 - val_mae: 4.4972\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6892 - mae: 4.3037 - val_loss: 41.3738 - val_mae: 4.5226\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5970 - mae: 4.3556 - val_loss: 41.3168 - val_mae: 4.5399\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5359 - mae: 4.3816 - val_loss: 41.5604 - val_mae: 4.4697\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5595 - mae: 4.3170 - val_loss: 41.3688 - val_mae: 4.4968\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4856 - mae: 4.3950 - val_loss: 41.3479 - val_mae: 4.4800\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5217 - mae: 4.2882 - val_loss: 41.2551 - val_mae: 4.5062\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4784 - mae: 4.3969 - val_loss: 41.3265 - val_mae: 4.4779\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4132 - mae: 4.3146 - val_loss: 41.1277 - val_mae: 4.4902\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5505 - mae: 4.3597 - val_loss: 41.4063 - val_mae: 4.4547\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5552 - mae: 4.3502 - val_loss: 41.2838 - val_mae: 4.4786\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4750 - mae: 4.3862 - val_loss: 41.1399 - val_mae: 4.4650\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3920 - mae: 4.3595 - val_loss: 41.2312 - val_mae: 4.4492\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3800 - mae: 4.3041 - val_loss: 41.1392 - val_mae: 4.4483\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3357 - mae: 4.3640 - val_loss: 41.0103 - val_mae: 4.4758\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3863 - mae: 4.3559 - val_loss: 41.0727 - val_mae: 4.4560\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3102 - mae: 4.3133 - val_loss: 40.9259 - val_mae: 4.4751\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.2998 - mae: 4.3450 - val_loss: 41.0370 - val_mae: 4.4544\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 47.6192 - mae: 4.4568\n",
      "Mean Absolute Error on Test Data: 4.456765174865723\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: -0.004828636673965825\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 11ms/step - loss: 16.6275 - mae: 2.9166 - val_loss: 14.1597 - val_mae: 2.4997\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 11.1467 - mae: 2.1677 - val_loss: 9.0563 - val_mae: 1.8580\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.2007 - mae: 1.9623 - val_loss: 8.0336 - val_mae: 1.9059\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1385 - mae: 2.0423 - val_loss: 8.0174 - val_mae: 1.8451\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0031 - mae: 1.9739 - val_loss: 8.0076 - val_mae: 1.8391\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9144 - mae: 1.9789 - val_loss: 7.9714 - val_mae: 1.8563\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.8559 - mae: 1.9729 - val_loss: 7.9829 - val_mae: 1.8431\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.8167 - mae: 1.9623 - val_loss: 7.9758 - val_mae: 1.8543\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.7655 - mae: 1.9456 - val_loss: 7.9909 - val_mae: 1.8345\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.7445 - mae: 1.9516 - val_loss: 7.9724 - val_mae: 1.8661\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.7040 - mae: 1.9653 - val_loss: 7.9904 - val_mae: 1.8422\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.7120 - mae: 1.9299 - val_loss: 7.9785 - val_mae: 1.8580\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.6775 - mae: 1.9706 - val_loss: 7.9741 - val_mae: 1.8494\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6523 - mae: 1.9456 - val_loss: 7.9877 - val_mae: 1.8450\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6384 - mae: 1.9534 - val_loss: 7.9893 - val_mae: 1.8414\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6123 - mae: 1.9337 - val_loss: 7.9719 - val_mae: 1.8523\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5988 - mae: 1.9494 - val_loss: 7.9982 - val_mae: 1.8620\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6203 - mae: 1.9546 - val_loss: 8.0186 - val_mae: 1.8304\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.5886 - mae: 1.9364 - val_loss: 8.0038 - val_mae: 1.8656\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5635 - mae: 1.9311 - val_loss: 8.0105 - val_mae: 1.8714\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5854 - mae: 1.9671 - val_loss: 8.0414 - val_mae: 1.8344\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5835 - mae: 1.9305 - val_loss: 8.0127 - val_mae: 1.8485\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5505 - mae: 1.9420 - val_loss: 8.0270 - val_mae: 1.8506\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5551 - mae: 1.9442 - val_loss: 8.0132 - val_mae: 1.8606\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.6137 - mae: 1.9102 - val_loss: 8.0187 - val_mae: 1.8900\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5436 - mae: 1.9519 - val_loss: 8.0122 - val_mae: 1.8774\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5089 - mae: 1.9373 - val_loss: 8.0351 - val_mae: 1.8695\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5283 - mae: 1.9265 - val_loss: 8.0395 - val_mae: 1.8756\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5174 - mae: 1.9546 - val_loss: 8.0493 - val_mae: 1.8528\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5009 - mae: 1.9209 - val_loss: 8.0535 - val_mae: 1.8645\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.5833 - mae: 1.9820 - val_loss: 8.0832 - val_mae: 1.8519\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5349 - mae: 1.9261 - val_loss: 8.0578 - val_mae: 1.8703\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4794 - mae: 1.9408 - val_loss: 8.0523 - val_mae: 1.8590\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4871 - mae: 1.9139 - val_loss: 8.0688 - val_mae: 1.8514\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5015 - mae: 1.9374 - val_loss: 8.0525 - val_mae: 1.8799\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4969 - mae: 1.9658 - val_loss: 8.0711 - val_mae: 1.8535\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.4722 - mae: 1.9123 - val_loss: 8.0877 - val_mae: 1.8972\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5111 - mae: 1.9250 - val_loss: 8.0277 - val_mae: 1.8539\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4901 - mae: 1.9547 - val_loss: 8.0603 - val_mae: 1.8603\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4850 - mae: 1.9330 - val_loss: 8.0696 - val_mae: 1.8663\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4548 - mae: 1.9329 - val_loss: 8.0895 - val_mae: 1.8389\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4525 - mae: 1.9062 - val_loss: 8.0863 - val_mae: 1.8897\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4271 - mae: 1.9330 - val_loss: 8.0599 - val_mae: 1.8627\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4273 - mae: 1.9420 - val_loss: 8.0764 - val_mae: 1.8735\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4255 - mae: 1.9305 - val_loss: 8.0950 - val_mae: 1.8410\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4669 - mae: 1.9444 - val_loss: 8.0711 - val_mae: 1.8542\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4575 - mae: 1.9006 - val_loss: 8.0568 - val_mae: 1.8787\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4560 - mae: 1.9518 - val_loss: 8.1339 - val_mae: 1.8291\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4084 - mae: 1.9139 - val_loss: 8.0842 - val_mae: 1.8877\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4202 - mae: 1.9145 - val_loss: 8.0504 - val_mae: 1.8764\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 5.7759 - mae: 1.7655\n",
      "Mean Absolute Error on Test Data: 1.7655274868011475\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.0583827748012864\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 31.7704 - mae: 4.7600 - val_loss: 30.4394 - val_mae: 4.2138\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 19.9417 - mae: 3.4333 - val_loss: 18.0045 - val_mae: 2.9173\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 10.1448 - mae: 2.2876 - val_loss: 12.9121 - val_mae: 2.6630\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 9.0504 - mae: 2.3198 - val_loss: 12.8958 - val_mae: 2.6465\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.8368 - mae: 2.2340 - val_loss: 12.8891 - val_mae: 2.6405\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.8431 - mae: 2.2889 - val_loss: 12.8909 - val_mae: 2.6622\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.7836 - mae: 2.2187 - val_loss: 12.9380 - val_mae: 2.6119\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.7007 - mae: 2.2184 - val_loss: 12.8918 - val_mae: 2.6357\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6990 - mae: 2.2337 - val_loss: 12.9115 - val_mae: 2.6201\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6505 - mae: 2.1995 - val_loss: 12.9003 - val_mae: 2.6219\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6213 - mae: 2.2109 - val_loss: 12.8844 - val_mae: 2.6352\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6157 - mae: 2.2015 - val_loss: 12.8835 - val_mae: 2.6311\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5645 - mae: 2.2022 - val_loss: 12.8926 - val_mae: 2.6170\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.5801 - mae: 2.1905 - val_loss: 12.8749 - val_mae: 2.6310\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5417 - mae: 2.1971 - val_loss: 12.9084 - val_mae: 2.6112\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5359 - mae: 2.1806 - val_loss: 12.8834 - val_mae: 2.6308\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4962 - mae: 2.2070 - val_loss: 12.8908 - val_mae: 2.6342\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4847 - mae: 2.2193 - val_loss: 12.8900 - val_mae: 2.6273\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4992 - mae: 2.2019 - val_loss: 12.9221 - val_mae: 2.6147\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4892 - mae: 2.2127 - val_loss: 12.9123 - val_mae: 2.6162\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4605 - mae: 2.1950 - val_loss: 12.8869 - val_mae: 2.6232\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4963 - mae: 2.2395 - val_loss: 12.9204 - val_mae: 2.6110\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5166 - mae: 2.1793 - val_loss: 12.8737 - val_mae: 2.6364\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4905 - mae: 2.2390 - val_loss: 12.9196 - val_mae: 2.6048\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4197 - mae: 2.1981 - val_loss: 12.9185 - val_mae: 2.6253\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4411 - mae: 2.2265 - val_loss: 12.9115 - val_mae: 2.6156\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4037 - mae: 2.1675 - val_loss: 12.9005 - val_mae: 2.6387\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4979 - mae: 2.2519 - val_loss: 12.8911 - val_mae: 2.6184\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4722 - mae: 2.1743 - val_loss: 12.8793 - val_mae: 2.6289\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3523 - mae: 2.2001 - val_loss: 12.8915 - val_mae: 2.6224\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3434 - mae: 2.2016 - val_loss: 12.9175 - val_mae: 2.6342\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3327 - mae: 2.1982 - val_loss: 12.9153 - val_mae: 2.6136\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3522 - mae: 2.1940 - val_loss: 12.9028 - val_mae: 2.6334\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3299 - mae: 2.1775 - val_loss: 12.9121 - val_mae: 2.6179\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3079 - mae: 2.1820 - val_loss: 12.8980 - val_mae: 2.6239\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3044 - mae: 2.1880 - val_loss: 12.8791 - val_mae: 2.6279\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2998 - mae: 2.1922 - val_loss: 12.8985 - val_mae: 2.6189\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2845 - mae: 2.1774 - val_loss: 12.8965 - val_mae: 2.6166\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2933 - mae: 2.1747 - val_loss: 12.8778 - val_mae: 2.6181\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.2784 - mae: 2.1811 - val_loss: 12.8909 - val_mae: 2.6270\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2940 - mae: 2.1756 - val_loss: 12.8779 - val_mae: 2.6183\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.2680 - mae: 2.1980 - val_loss: 12.9403 - val_mae: 2.6143\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.2881 - mae: 2.1714 - val_loss: 12.9567 - val_mae: 2.6203\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.2534 - mae: 2.1873 - val_loss: 12.9387 - val_mae: 2.6310\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2413 - mae: 2.1846 - val_loss: 12.8717 - val_mae: 2.6354\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2733 - mae: 2.1729 - val_loss: 12.8747 - val_mae: 2.6335\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.2518 - mae: 2.1775 - val_loss: 12.8955 - val_mae: 2.6102\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.2789 - mae: 2.1963 - val_loss: 12.8885 - val_mae: 2.5975\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2523 - mae: 2.1677 - val_loss: 12.8453 - val_mae: 2.6362\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.2012 - mae: 2.2028 - val_loss: 12.9571 - val_mae: 2.5924\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.6856 - mae: 2.3446\n",
      "Mean Absolute Error on Test Data: 2.3445677757263184\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.012435425449498605\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 70.8465 - mae: 6.7899 - val_loss: 68.8731 - val_mae: 6.2546\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.7937 - mae: 4.9083 - val_loss: 42.1818 - val_mae: 4.1207\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7070 - mae: 3.4240 - val_loss: 29.9306 - val_mae: 3.5805\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1617 - mae: 3.5867 - val_loss: 29.9146 - val_mae: 3.5818\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.8764 - mae: 3.4498 - val_loss: 30.1322 - val_mae: 3.5360\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9052 - mae: 3.3958 - val_loss: 30.0742 - val_mae: 3.5411\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.7323 - mae: 3.4737 - val_loss: 29.8899 - val_mae: 3.5740\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6944 - mae: 3.4592 - val_loss: 30.0923 - val_mae: 3.5351\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6919 - mae: 3.4032 - val_loss: 29.9128 - val_mae: 3.5597\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.5928 - mae: 3.4507 - val_loss: 29.9672 - val_mae: 3.5471\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5581 - mae: 3.4239 - val_loss: 29.8922 - val_mae: 3.5580\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7498 - mae: 3.4225 - val_loss: 30.0604 - val_mae: 3.5287\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6601 - mae: 3.5055 - val_loss: 29.9165 - val_mae: 3.5441\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4744 - mae: 3.3789 - val_loss: 29.8983 - val_mae: 3.5422\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.3652 - mae: 3.4357 - val_loss: 29.8541 - val_mae: 3.5490\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2895 - mae: 3.4329 - val_loss: 29.9446 - val_mae: 3.5328\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3580 - mae: 3.3790 - val_loss: 29.7764 - val_mae: 3.5626\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3084 - mae: 3.4711 - val_loss: 30.0494 - val_mae: 3.5219\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2977 - mae: 3.3552 - val_loss: 29.7424 - val_mae: 3.5719\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.1809 - mae: 3.4592 - val_loss: 29.8974 - val_mae: 3.5392\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2266 - mae: 3.4012 - val_loss: 29.7734 - val_mae: 3.5637\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1356 - mae: 3.3964 - val_loss: 29.7723 - val_mae: 3.5617\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.0856 - mae: 3.4115 - val_loss: 29.7698 - val_mae: 3.5733\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.0793 - mae: 3.4195 - val_loss: 29.7764 - val_mae: 3.5765\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9644 - mae: 3.4003 - val_loss: 30.1644 - val_mae: 3.5166\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9003 - mae: 3.4033 - val_loss: 29.8225 - val_mae: 3.5712\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.9154 - mae: 3.3846 - val_loss: 29.9626 - val_mae: 3.5405\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.9856 - mae: 3.4606 - val_loss: 30.1656 - val_mae: 3.5209\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9771 - mae: 3.3797 - val_loss: 30.1835 - val_mae: 3.5169\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8963 - mae: 3.3229 - val_loss: 29.9157 - val_mae: 3.5512\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8662 - mae: 3.4695 - val_loss: 29.9973 - val_mae: 3.5410\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8273 - mae: 3.3394 - val_loss: 29.8856 - val_mae: 3.5594\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8249 - mae: 3.4520 - val_loss: 30.0998 - val_mae: 3.5235\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.0359 - mae: 3.3545 - val_loss: 29.8782 - val_mae: 3.5681\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.7042 - mae: 3.4055 - val_loss: 30.0373 - val_mae: 3.5352\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8110 - mae: 3.3818 - val_loss: 30.0224 - val_mae: 3.5376\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7637 - mae: 3.3690 - val_loss: 29.9776 - val_mae: 3.5503\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6348 - mae: 3.3595 - val_loss: 30.0480 - val_mae: 3.5350\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6098 - mae: 3.3627 - val_loss: 30.1855 - val_mae: 3.5235\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6590 - mae: 3.3995 - val_loss: 30.0548 - val_mae: 3.5381\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6929 - mae: 3.3418 - val_loss: 29.9728 - val_mae: 3.5577\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5616 - mae: 3.3932 - val_loss: 30.2041 - val_mae: 3.5226\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6551 - mae: 3.4114 - val_loss: 30.3433 - val_mae: 3.5193\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.7154 - mae: 3.3018 - val_loss: 29.9908 - val_mae: 3.5586\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5653 - mae: 3.4026 - val_loss: 30.2950 - val_mae: 3.5195\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6986 - mae: 3.3239 - val_loss: 30.0161 - val_mae: 3.5949\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5010 - mae: 3.3836 - val_loss: 30.1982 - val_mae: 3.5301\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.4904 - mae: 3.3652 - val_loss: 30.1504 - val_mae: 3.5355\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.5568 - mae: 3.3312 - val_loss: 30.0241 - val_mae: 3.5751\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.5046 - mae: 3.3928 - val_loss: 30.1903 - val_mae: 3.5375\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 27.0852 - mae: 3.4152\n",
      "Mean Absolute Error on Test Data: 3.4151604175567627\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.07916118567049546\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 12ms/step - loss: 15.5364 - mae: 2.9220 - val_loss: 14.2657 - val_mae: 2.5736\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 10.3925 - mae: 2.1156 - val_loss: 9.0312 - val_mae: 1.8742\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 7.1016 - mae: 1.8526 - val_loss: 7.5739 - val_mae: 1.9500\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.8612 - mae: 1.9235 - val_loss: 7.5781 - val_mae: 1.8726\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.8016 - mae: 1.8326 - val_loss: 7.5880 - val_mae: 1.8570\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7860 - mae: 1.8859 - val_loss: 7.5335 - val_mae: 1.8826\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7265 - mae: 1.8446 - val_loss: 7.5459 - val_mae: 1.8561\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6921 - mae: 1.8501 - val_loss: 7.5267 - val_mae: 1.8551\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.6630 - mae: 1.8551 - val_loss: 7.5119 - val_mae: 1.8531\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6498 - mae: 1.8485 - val_loss: 7.5087 - val_mae: 1.8510\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6315 - mae: 1.8265 - val_loss: 7.4783 - val_mae: 1.8660\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6633 - mae: 1.8585 - val_loss: 7.5260 - val_mae: 1.8405\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.6211 - mae: 1.8566 - val_loss: 7.5080 - val_mae: 1.8441\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6074 - mae: 1.8304 - val_loss: 7.4775 - val_mae: 1.8518\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6005 - mae: 1.8505 - val_loss: 7.4874 - val_mae: 1.8487\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6059 - mae: 1.8186 - val_loss: 7.4851 - val_mae: 1.8491\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5782 - mae: 1.8452 - val_loss: 7.4623 - val_mae: 1.8649\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6007 - mae: 1.8365 - val_loss: 7.5357 - val_mae: 1.8330\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5643 - mae: 1.8330 - val_loss: 7.4354 - val_mae: 1.8830\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5621 - mae: 1.8517 - val_loss: 7.5567 - val_mae: 1.8351\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5814 - mae: 1.8192 - val_loss: 7.4460 - val_mae: 1.8765\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5490 - mae: 1.8413 - val_loss: 7.4966 - val_mae: 1.8515\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5604 - mae: 1.8223 - val_loss: 7.4629 - val_mae: 1.8725\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.5454 - mae: 1.8444 - val_loss: 7.5076 - val_mae: 1.8539\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5517 - mae: 1.8354 - val_loss: 7.5085 - val_mae: 1.8479\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5495 - mae: 1.8116 - val_loss: 7.4548 - val_mae: 1.8842\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5418 - mae: 1.8291 - val_loss: 7.5069 - val_mae: 1.8558\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5605 - mae: 1.8519 - val_loss: 7.5585 - val_mae: 1.8378\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5219 - mae: 1.8104 - val_loss: 7.4807 - val_mae: 1.8666\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5104 - mae: 1.8315 - val_loss: 7.4901 - val_mae: 1.8711\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5304 - mae: 1.8476 - val_loss: 7.5599 - val_mae: 1.8373\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5408 - mae: 1.8023 - val_loss: 7.4723 - val_mae: 1.8842\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5266 - mae: 1.8541 - val_loss: 7.5345 - val_mae: 1.8450\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5068 - mae: 1.8012 - val_loss: 7.4877 - val_mae: 1.8774\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5161 - mae: 1.8589 - val_loss: 7.5898 - val_mae: 1.8431\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5111 - mae: 1.7989 - val_loss: 7.4737 - val_mae: 1.8879\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5124 - mae: 1.8393 - val_loss: 7.5695 - val_mae: 1.8435\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5149 - mae: 1.8460 - val_loss: 7.5496 - val_mae: 1.8530\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.5046 - mae: 1.8017 - val_loss: 7.5840 - val_mae: 1.8470\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4673 - mae: 1.8411 - val_loss: 7.4999 - val_mae: 1.8888\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5030 - mae: 1.8179 - val_loss: 7.4950 - val_mae: 1.8999\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.4580 - mae: 1.8417 - val_loss: 7.5470 - val_mae: 1.8605\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4839 - mae: 1.8039 - val_loss: 7.4695 - val_mae: 1.8970\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4556 - mae: 1.8415 - val_loss: 7.5122 - val_mae: 1.8869\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4371 - mae: 1.8104 - val_loss: 7.5421 - val_mae: 1.8819\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4788 - mae: 1.8113 - val_loss: 7.5048 - val_mae: 1.8900\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4426 - mae: 1.8561 - val_loss: 7.6045 - val_mae: 1.8618\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4679 - mae: 1.8249 - val_loss: 7.5202 - val_mae: 1.8861\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4256 - mae: 1.8116 - val_loss: 7.5352 - val_mae: 1.8576\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4266 - mae: 1.7990 - val_loss: 7.4993 - val_mae: 1.9105\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.9568 - mae: 1.9402\n",
      "Mean Absolute Error on Test Data: 1.9402154684066772\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.020705243439852583\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 32.9232 - mae: 4.0815 - val_loss: 16.9619 - val_mae: 3.0015\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 24.0437 - mae: 3.1533 - val_loss: 10.0016 - val_mae: 2.2003\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1089 - mae: 2.6524 - val_loss: 8.6517 - val_mae: 2.3343\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1955 - mae: 2.8227 - val_loss: 8.8307 - val_mae: 2.3770\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1286 - mae: 2.7625 - val_loss: 8.4826 - val_mae: 2.2857\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0439 - mae: 2.7472 - val_loss: 8.6140 - val_mae: 2.3236\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9949 - mae: 2.7327 - val_loss: 8.5819 - val_mae: 2.3149\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.9396 - mae: 2.7338 - val_loss: 8.5687 - val_mae: 2.3099\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9357 - mae: 2.7228 - val_loss: 8.6584 - val_mae: 2.3299\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8821 - mae: 2.7545 - val_loss: 8.6480 - val_mae: 2.3261\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8244 - mae: 2.7557 - val_loss: 8.6779 - val_mae: 2.3299\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7900 - mae: 2.7116 - val_loss: 8.4893 - val_mae: 2.2769\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.7959 - mae: 2.6965 - val_loss: 8.6451 - val_mae: 2.3196\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7187 - mae: 2.7383 - val_loss: 8.6160 - val_mae: 2.3135\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7375 - mae: 2.7117 - val_loss: 8.7740 - val_mae: 2.3480\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6614 - mae: 2.7031 - val_loss: 8.5617 - val_mae: 2.2970\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6517 - mae: 2.6788 - val_loss: 8.6753 - val_mae: 2.3297\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.6655 - mae: 2.7171 - val_loss: 8.8537 - val_mae: 2.3665\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6622 - mae: 2.7013 - val_loss: 8.4789 - val_mae: 2.2739\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5252 - mae: 2.6956 - val_loss: 8.7567 - val_mae: 2.3455\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5597 - mae: 2.7343 - val_loss: 8.5761 - val_mae: 2.2982\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5649 - mae: 2.7000 - val_loss: 8.5977 - val_mae: 2.3053\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4936 - mae: 2.6688 - val_loss: 8.4355 - val_mae: 2.2640\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4472 - mae: 2.6745 - val_loss: 8.5117 - val_mae: 2.2847\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4609 - mae: 2.6820 - val_loss: 8.5855 - val_mae: 2.3032\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4297 - mae: 2.6787 - val_loss: 8.4140 - val_mae: 2.2584\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4199 - mae: 2.6870 - val_loss: 8.4985 - val_mae: 2.2751\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.3971 - mae: 2.6488 - val_loss: 8.5260 - val_mae: 2.2884\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4547 - mae: 2.6990 - val_loss: 8.3622 - val_mae: 2.2414\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3505 - mae: 2.6607 - val_loss: 8.6233 - val_mae: 2.3078\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3440 - mae: 2.6920 - val_loss: 8.5057 - val_mae: 2.2785\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.3442 - mae: 2.6400 - val_loss: 8.5062 - val_mae: 2.2809\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3405 - mae: 2.6760 - val_loss: 8.3953 - val_mae: 2.2486\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2772 - mae: 2.6497 - val_loss: 8.5461 - val_mae: 2.2914\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2969 - mae: 2.6687 - val_loss: 8.4279 - val_mae: 2.2579\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2590 - mae: 2.6511 - val_loss: 8.5155 - val_mae: 2.2815\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.2743 - mae: 2.6844 - val_loss: 8.4680 - val_mae: 2.2720\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2363 - mae: 2.6266 - val_loss: 8.4120 - val_mae: 2.2556\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2709 - mae: 2.6956 - val_loss: 8.4205 - val_mae: 2.2563\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2874 - mae: 2.6320 - val_loss: 8.4984 - val_mae: 2.2778\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3151 - mae: 2.6181 - val_loss: 8.6789 - val_mae: 2.3163\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2142 - mae: 2.6960 - val_loss: 8.4155 - val_mae: 2.2579\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2361 - mae: 2.6031 - val_loss: 8.6376 - val_mae: 2.3167\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2166 - mae: 2.7053 - val_loss: 8.4670 - val_mae: 2.2649\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2075 - mae: 2.6278 - val_loss: 8.6460 - val_mae: 2.3150\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.1448 - mae: 2.6476 - val_loss: 8.4157 - val_mae: 2.2565\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1436 - mae: 2.6320 - val_loss: 8.5379 - val_mae: 2.2949\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0915 - mae: 2.6440 - val_loss: 8.4891 - val_mae: 2.2783\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1125 - mae: 2.6333 - val_loss: 8.4221 - val_mae: 2.2552\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.0754 - mae: 2.6418 - val_loss: 8.4313 - val_mae: 2.2614\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 16.2250 - mae: 2.8783\n",
      "Mean Absolute Error on Test Data: 2.8783295154571533\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04113316902873332\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 68.5171 - mae: 6.7224 - val_loss: 61.5784 - val_mae: 6.2161\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.6499 - mae: 5.3803 - val_loss: 40.6952 - val_mae: 4.5892\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.1028 - mae: 3.7613 - val_loss: 23.3692 - val_mae: 3.3068\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1393 - mae: 3.4783 - val_loss: 22.1220 - val_mae: 3.4361\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2721 - mae: 3.5649 - val_loss: 21.9052 - val_mae: 3.3169\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0938 - mae: 3.4502 - val_loss: 21.7572 - val_mae: 3.3241\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9945 - mae: 3.5007 - val_loss: 21.6721 - val_mae: 3.3234\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8938 - mae: 3.4657 - val_loss: 21.6489 - val_mae: 3.2956\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8629 - mae: 3.4549 - val_loss: 21.5577 - val_mae: 3.2894\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7913 - mae: 3.4630 - val_loss: 21.4377 - val_mae: 3.3103\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7579 - mae: 3.4514 - val_loss: 21.4162 - val_mae: 3.2880\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7344 - mae: 3.4460 - val_loss: 21.3626 - val_mae: 3.2982\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6939 - mae: 3.4452 - val_loss: 21.3014 - val_mae: 3.2929\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6146 - mae: 3.4657 - val_loss: 21.2517 - val_mae: 3.2920\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.5860 - mae: 3.4541 - val_loss: 21.2654 - val_mae: 3.2763\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5318 - mae: 3.4287 - val_loss: 21.2018 - val_mae: 3.3025\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5349 - mae: 3.4725 - val_loss: 21.2226 - val_mae: 3.2714\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5456 - mae: 3.4045 - val_loss: 21.1696 - val_mae: 3.2972\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.4562 - mae: 3.4712 - val_loss: 21.1526 - val_mae: 3.2928\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4775 - mae: 3.4578 - val_loss: 21.1976 - val_mae: 3.2675\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3950 - mae: 3.4391 - val_loss: 21.0969 - val_mae: 3.2990\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4349 - mae: 3.4183 - val_loss: 21.1060 - val_mae: 3.2865\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3734 - mae: 3.4354 - val_loss: 21.1435 - val_mae: 3.2592\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3630 - mae: 3.4377 - val_loss: 21.0544 - val_mae: 3.2925\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3449 - mae: 3.4117 - val_loss: 21.0746 - val_mae: 3.2793\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4200 - mae: 3.4795 - val_loss: 21.1535 - val_mae: 3.2517\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4846 - mae: 3.4074 - val_loss: 21.0446 - val_mae: 3.3024\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3013 - mae: 3.4198 - val_loss: 21.1471 - val_mae: 3.2512\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3361 - mae: 3.4651 - val_loss: 21.0185 - val_mae: 3.2757\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3938 - mae: 3.4188 - val_loss: 21.2277 - val_mae: 3.2427\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3282 - mae: 3.4404 - val_loss: 21.0316 - val_mae: 3.2645\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2316 - mae: 3.4479 - val_loss: 20.9984 - val_mae: 3.2756\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3039 - mae: 3.3798 - val_loss: 20.9841 - val_mae: 3.2714\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2912 - mae: 3.4693 - val_loss: 21.0648 - val_mae: 3.2516\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2753 - mae: 3.4122 - val_loss: 21.0434 - val_mae: 3.2579\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2254 - mae: 3.3912 - val_loss: 20.9656 - val_mae: 3.2865\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2376 - mae: 3.4446 - val_loss: 20.9922 - val_mae: 3.2681\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2405 - mae: 3.4728 - val_loss: 21.0756 - val_mae: 3.2456\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1989 - mae: 3.3810 - val_loss: 20.9593 - val_mae: 3.2681\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1693 - mae: 3.4389 - val_loss: 20.9654 - val_mae: 3.2599\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3003 - mae: 3.3714 - val_loss: 20.9230 - val_mae: 3.2880\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2086 - mae: 3.4898 - val_loss: 20.9749 - val_mae: 3.2553\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3492 - mae: 3.4130 - val_loss: 20.9213 - val_mae: 3.2781\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0921 - mae: 3.4204 - val_loss: 21.0169 - val_mae: 3.2442\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1747 - mae: 3.3928 - val_loss: 20.9058 - val_mae: 3.2839\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0863 - mae: 3.4354 - val_loss: 21.0547 - val_mae: 3.2348\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1374 - mae: 3.3725 - val_loss: 20.9190 - val_mae: 3.2804\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0943 - mae: 3.4186 - val_loss: 20.9270 - val_mae: 3.2605\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0550 - mae: 3.4252 - val_loss: 20.9230 - val_mae: 3.2605\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0644 - mae: 3.4039 - val_loss: 20.8769 - val_mae: 3.2514\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 19.2161 - mae: 3.2288\n",
      "Mean Absolute Error on Test Data: 3.228844404220581\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.013640708771060828\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 135.7236 - mae: 9.6190 - val_loss: 121.3307 - val_mae: 9.2572\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 106.7170 - mae: 8.0051 - val_loss: 82.9184 - val_mae: 6.9887\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.3897 - mae: 5.5153 - val_loss: 42.3840 - val_mae: 4.5742\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7517 - mae: 4.3975 - val_loss: 35.7920 - val_mae: 4.4965\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9005 - mae: 4.5526 - val_loss: 35.7139 - val_mae: 4.4346\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8189 - mae: 4.3922 - val_loss: 35.9115 - val_mae: 4.4149\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7215 - mae: 4.4765 - val_loss: 35.5782 - val_mae: 4.4347\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.7018 - mae: 4.4482 - val_loss: 35.8479 - val_mae: 4.4092\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5560 - mae: 4.4277 - val_loss: 35.5341 - val_mae: 4.4231\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5430 - mae: 4.4454 - val_loss: 35.5657 - val_mae: 4.4101\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4750 - mae: 4.4382 - val_loss: 35.5327 - val_mae: 4.4086\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.4287 - mae: 4.4378 - val_loss: 35.4825 - val_mae: 4.4069\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4071 - mae: 4.4539 - val_loss: 35.4717 - val_mae: 4.3976\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4017 - mae: 4.4062 - val_loss: 35.3331 - val_mae: 4.4049\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3424 - mae: 4.4539 - val_loss: 35.3719 - val_mae: 4.3923\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3676 - mae: 4.4269 - val_loss: 35.3311 - val_mae: 4.3926\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4252 - mae: 4.4555 - val_loss: 35.1933 - val_mae: 4.4137\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2612 - mae: 4.4574 - val_loss: 35.4502 - val_mae: 4.3786\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4407 - mae: 4.4708 - val_loss: 35.3984 - val_mae: 4.3755\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2839 - mae: 4.3991 - val_loss: 35.2686 - val_mae: 4.3774\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4860 - mae: 4.3684 - val_loss: 35.0137 - val_mae: 4.3965\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2950 - mae: 4.4835 - val_loss: 35.0471 - val_mae: 4.3815\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1075 - mae: 4.4544 - val_loss: 35.0155 - val_mae: 4.3786\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1093 - mae: 4.4109 - val_loss: 35.1438 - val_mae: 4.3696\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1585 - mae: 4.4808 - val_loss: 34.9388 - val_mae: 4.3772\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9806 - mae: 4.4112 - val_loss: 35.2268 - val_mae: 4.3614\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.0487 - mae: 4.3715 - val_loss: 34.9129 - val_mae: 4.3691\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1407 - mae: 4.5034 - val_loss: 34.9828 - val_mae: 4.3637\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9574 - mae: 4.4240 - val_loss: 34.8733 - val_mae: 4.3611\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0840 - mae: 4.4224 - val_loss: 34.7490 - val_mae: 4.3621\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9275 - mae: 4.4205 - val_loss: 34.9117 - val_mae: 4.3541\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8854 - mae: 4.4268 - val_loss: 34.6702 - val_mae: 4.3692\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8557 - mae: 4.4401 - val_loss: 34.8858 - val_mae: 4.3482\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8212 - mae: 4.3949 - val_loss: 34.6018 - val_mae: 4.3605\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8213 - mae: 4.4594 - val_loss: 34.7320 - val_mae: 4.3480\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8693 - mae: 4.3910 - val_loss: 34.8167 - val_mae: 4.3409\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8786 - mae: 4.4571 - val_loss: 34.7869 - val_mae: 4.3396\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7784 - mae: 4.3907 - val_loss: 34.5939 - val_mae: 4.3423\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6969 - mae: 4.4134 - val_loss: 34.6645 - val_mae: 4.3360\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8177 - mae: 4.3837 - val_loss: 34.4069 - val_mae: 4.3539\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7103 - mae: 4.4535 - val_loss: 34.6026 - val_mae: 4.3295\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7507 - mae: 4.4425 - val_loss: 34.4427 - val_mae: 4.3343\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5919 - mae: 4.4153 - val_loss: 35.1344 - val_mae: 4.3214\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7044 - mae: 4.3743 - val_loss: 34.3033 - val_mae: 4.3367\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6931 - mae: 4.4156 - val_loss: 34.8038 - val_mae: 4.3182\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6476 - mae: 4.4467 - val_loss: 34.3566 - val_mae: 4.3216\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6427 - mae: 4.4048 - val_loss: 34.4972 - val_mae: 4.3158\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7407 - mae: 4.3279 - val_loss: 34.2694 - val_mae: 4.3230\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6806 - mae: 4.5014 - val_loss: 34.4830 - val_mae: 4.3124\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4918 - mae: 4.3806 - val_loss: 34.5037 - val_mae: 4.3116\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 36.1867 - mae: 4.1213\n",
      "Mean Absolute Error on Test Data: 4.121311664581299\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.07593217657488471\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all\n",
    "df_test = dict_test_merged\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "output_var = df\n",
    "target = 'Bildirimsiz_sum'\n",
    "# ilceler = []\n",
    "\n",
    "# NN 3\n",
    "# ilceler = ['izmir-konak','izmir-kinik']\n",
    "all_submissions = []\n",
    "for ilce in ilceler:\n",
    "    df = merged_all[ilce]\n",
    "    df_test = dict_test_merged[ilce]\n",
    "    output_var = df['Bildirimsiz_sum']\n",
    "\n",
    "    # ilcelerin numerizasyonu\n",
    "    columns_tonumerate = ['Bayram_Flag']\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "    # test csv dosyasi numerizasyon\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_transform = scaler.fit_transform(df[features])\n",
    "    feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "    feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "    feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    feature_test = scaler2.fit_transform(df_test[features])\n",
    "    feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)\n",
    "\n",
    "\n",
    "\n",
    "    X = feature_transform\n",
    "    y = output_var\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Mean Absolute Error on Test Data:\", mae)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"R-squared:\", r2)\n",
    "\n",
    "    predictions_new = model.predict(feature_test)\n",
    "    predictions_new = np.round(predictions_new).astype(int)\n",
    "    df_test['bildirimsiz_sum'] = predictions_new\n",
    "    df_test.to_csv('test_with_predictions.csv', index=False)\n",
    "\n",
    "    df_test.rename(columns={'Ilce': 'ilce'}, inplace=True)\n",
    "    df_test.rename(columns={'Tarih': 'tarih'}, inplace=True)\n",
    "    df_test.rename(columns={'Bildirimli_sum': 'bildirimli_sum'}, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    all_submissions.append(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        tarih              ilce  bildirimli_sum  Sicaklik_max  Sicaklik_min  \\\n",
      "0  2024-02-01      izmir-aliaga               0          12.0           5.0   \n",
      "1  2024-02-02      izmir-aliaga               1          12.1           2.6   \n",
      "2  2024-02-03      izmir-aliaga               0          11.9           5.9   \n",
      "3  2024-02-04      izmir-aliaga               1          13.5           4.8   \n",
      "4  2024-02-05      izmir-aliaga               0          17.4           8.0   \n",
      "..        ...               ...             ...           ...           ...   \n",
      "24 2024-02-25  manisa-yunusemre               4          19.5           9.3   \n",
      "25 2024-02-26  manisa-yunusemre               0          18.4           9.7   \n",
      "26 2024-02-27  manisa-yunusemre               0          20.0          10.6   \n",
      "27 2024-02-28  manisa-yunusemre               1          20.6          10.1   \n",
      "28 2024-02-29  manisa-yunusemre               1          22.3           9.7   \n",
      "\n",
      "    Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "0            85.3           49.9              7.0              7.0        1.0   \n",
      "1            90.3           49.1              5.9              5.9        1.0   \n",
      "2            86.0           47.6              6.1              6.1        1.0   \n",
      "3            86.7           58.1              3.0              3.0        4.1   \n",
      "4            92.8           57.3              4.7              4.7        1.0   \n",
      "..            ...            ...              ...              ...        ...   \n",
      "24           94.6           47.8              1.2              1.2       18.1   \n",
      "25           90.1           48.6              1.4              1.4       42.0   \n",
      "26          100.0           48.7              2.6              2.6        1.0   \n",
      "27          100.0           51.1              1.5              1.5        1.0   \n",
      "28           87.7           48.2              1.7              1.7       73.8   \n",
      "\n",
      "    Yagis_min  bildirimsiz_sum  \n",
      "0         1.0                7  \n",
      "1         1.0                3  \n",
      "2         1.0                5  \n",
      "3         1.0                3  \n",
      "4         1.0                8  \n",
      "..        ...              ...  \n",
      "24        1.0                9  \n",
      "25        1.0                9  \n",
      "26        1.0               10  \n",
      "27        1.0               10  \n",
      "28        1.0               11  \n",
      "\n",
      "[1363 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "birlesmisunited = all_submissions[0]\n",
    "for i in range(1,len(ilceler)): # 1 2 3 4   47\n",
    "    birlesmisunited = pd.concat([birlesmisunited,all_submissions[i]])\n",
    "\n",
    "# birlesmisunited.drop('Gun', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Bayram_Flag', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Sicaklik', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Bagil_nem', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Ruzgar_hizi', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Yagis', axis=1, inplace=True)\n",
    "birlesmisunited.drop('Gün', axis=1, inplace=True)\n",
    "\n",
    "print(birlesmisunited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited['tarih'] = pd.to_datetime(birlesmisunited['tarih'])\n",
    "birlesmisunited = birlesmisunited.sort_values(by='tarih')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique id olusturulmali\n",
    "# her tarih kendi icinde ilce adina gore siralanmali\n",
    "birlesmisunited.drop('bildirimli_sum', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited = birlesmisunited.sort_values(by=['tarih', 'ilce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited['unique_id'] = birlesmisunited['tarih'].astype(str) + '-' + birlesmisunited['ilce']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.drop(['tarih', 'ilce'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.insert(0, 'unique_id', birlesmisunited.pop('unique_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(birlesmisunited.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in birlesmisunited.columns:\n",
    "    if column == 'unique_id':\n",
    "        pass\n",
    "    elif column == 'bildirimsiz_sum':\n",
    "        pass\n",
    "    else:\n",
    "        birlesmisunited.drop(column, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>bildirimsiz_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01-izmir-aliaga</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01-izmir-balcova</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01-izmir-bayindir</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01-izmir-bayrakli</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-02-01-izmir-bergama</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-02-29-manisa-sehzadeler</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-02-29-manisa-selendi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-02-29-manisa-soma</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-02-29-manisa-turgutlu</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2024-02-29-manisa-yunusemre</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       unique_id  bildirimsiz_sum\n",
       "0        2024-02-01-izmir-aliaga                7\n",
       "0       2024-02-01-izmir-balcova                1\n",
       "0      2024-02-01-izmir-bayindir                3\n",
       "0      2024-02-01-izmir-bayrakli                6\n",
       "0       2024-02-01-izmir-bergama                6\n",
       "..                           ...              ...\n",
       "28  2024-02-29-manisa-sehzadeler               13\n",
       "28     2024-02-29-manisa-selendi                2\n",
       "28        2024-02-29-manisa-soma                5\n",
       "28    2024-02-29-manisa-turgutlu               10\n",
       "28   2024-02-29-manisa-yunusemre               11\n",
       "\n",
       "[1363 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birlesmisunited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited.to_csv('subbb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
