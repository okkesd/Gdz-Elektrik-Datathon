{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\\n\\n    Yapilmasi gerekenler:\\n        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\\n        - Farkli grafikler cikartmaya calis.\\n        - ML.\\n        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\\n        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\\n\\n    Sorunlar:\\n        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\\n        ortalamasi az olur. Burada farkli bir yontem bul.\\n        Gunluk maks min alinabilir\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1-) Notlar\n",
    "\"\"\"\n",
    "    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\n",
    "\n",
    "    Yapilmasi gerekenler:\n",
    "        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\n",
    "        - Farkli grafikler cikartmaya calis.\n",
    "        - ML.\n",
    "        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\n",
    "        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\n",
    "\n",
    "    Sorunlar:\n",
    "        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\n",
    "        ortalamasi az olur. Burada farkli bir yontem bul.\n",
    "        Gunluk maks min alinabilir\n",
    "\"\"\"\n",
    "# 1-) read and preproccess train.csv\n",
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "# 3-) read and preprocess weather.csv\n",
    "# 4-) read and preprocess holidays.csv\n",
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi\n",
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği\n",
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli ve gerekli degil)\n",
    "# 10-) test icin birlestirme islemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-) Import required moduls and libraries\n",
    "\n",
    "# bildirimisiz_sum tahmin edilecek\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import os\n",
    "from unidecode import unidecode # to convert Turkish characters to English\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sm\n",
    "import statsmodels.api as sa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tarih         ilce  bildirimsiz_sum  bildirimli_sum\n",
      "19236 2021-01-01  izmir-konak                9               0\n",
      "19237 2021-01-02  izmir-konak               20               0\n",
      "19238 2021-01-03  izmir-konak                7               1\n",
      "19239 2021-01-04  izmir-konak               16               1\n",
      "19240 2021-01-05  izmir-konak                3               0\n",
      "...          ...          ...              ...             ...\n",
      "20355 2024-01-27  izmir-konak               12               3\n",
      "20356 2024-01-28  izmir-konak               13               1\n",
      "20357 2024-01-29  izmir-konak               22               0\n",
      "20358 2024-01-30  izmir-konak               28               1\n",
      "20359 2024-01-31  izmir-konak               16               0\n",
      "\n",
      "[1124 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-) read and preproccess train.csv\n",
    "train = pd.read_csv(\"./train.csv\", low_memory=False) # 46.944 satir, 4 kolon\n",
    "\n",
    "#print(train[\"tarih\"]) # 1.098 farkli tarih var, 47 farkli ilce var\n",
    "\n",
    "tarihler = []\n",
    "for i in train[\"tarih\"]:\n",
    "    tarihler.append(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "train[\"tarih\"] = tarihler\n",
    "\n",
    "# print(train.dtypes)\n",
    "\n",
    "dict :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak o ilcenin verisi (1096 gun) df olarak tutar\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "print(dict[\"izmir-konak\"])\n",
    "ilceler = (list(dict.keys()))\n",
    "#print(dict.keys()) # keys olarak her ilceyi, values olarak o ilcelerin bulundugu satirlari icerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'izmir-aliaga': 1106, 'izmir-balcova': 698, 'izmir-bayindir': 1105, 'izmir-bayrakli': 1086, 'izmir-bergama': 1120, 'izmir-beydag': 673, 'izmir-bornova': 1124, 'izmir-buca': 1115, 'izmir-cesme': 1125, 'izmir-cigli': 1071, 'izmir-dikili': 1119, 'izmir-foca': 1086, 'izmir-gaziemir': 920, 'izmir-guzelbahce': 856, 'izmir-karabaglar': 1100, 'izmir-karaburun': 1089, 'izmir-karsiyaka': 1085, 'izmir-kemalpasa': 1118, 'izmir-kinik': 914, 'izmir-kiraz': 1097, 'izmir-konak': 1124, 'izmir-menderes': 1125, 'izmir-menemen': 1119, 'izmir-narlidere': 783, 'izmir-odemis': 1124, 'izmir-seferihisar': 1111, 'izmir-selcuk': 872, 'izmir-tire': 1107, 'izmir-torbali': 1124, 'izmir-urla': 1122, 'manisa-ahmetli': 622, 'manisa-akhisar': 1126, 'manisa-alasehir': 1119, 'manisa-demirci': 938, 'manisa-golmarmara': 566, 'manisa-gordes': 1059, 'manisa-kirkagac': 950, 'manisa-koprubasi': 805, 'manisa-kula': 1039, 'manisa-salihli': 1126, 'manisa-sarigol': 1027, 'manisa-saruhanli': 1105, 'manisa-sehzadeler': 1123, 'manisa-selendi': 993, 'manisa-soma': 1086, 'manisa-turgutlu': 1121, 'manisa-yunusemre': 1125}\n"
     ]
    }
   ],
   "source": [
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "\"\"\"\n",
    "for label in dict.keys(): # her ilce icin bildirimsiz ve bildirimli olarak grafiklerini cikart\n",
    "    print(dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.bar(dict[label][\"tarih\"],dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.title(label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(dict[\"izmir-konak\"][\"tarih\"],dict[\"izmir-konak\"][\"bildirimsiz_sum\"])\n",
    "plt.title(label)\n",
    "plt.margins(0.01)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# ilce tarih sayilarini al hepsinde esit veri yok\n",
    "ilce_tarih_sayilari = {}\n",
    "for name in dict.keys():\n",
    "    ilce_tarih_sayilari[name] = len(list(dict[name][\"tarih\"].to_dict().values()))\n",
    "\n",
    "print(ilce_tarih_sayilari)\n",
    "for name in dict.keys():\n",
    "    dict[name].set_index(\"tarih\", inplace=True)\n",
    "\n",
    "# train.set_index(\"tarih\", inplace=True) # train'in tarih kolonunu indexe cevir\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'lat', 'lon', 't_2m:C', 'effective_cloud_cover:p',\n",
      "       'global_rad:W', 'relative_humidity_2m:p', 'wind_dir_10m:d',\n",
      "       'wind_speed_10m:ms', 'prob_precip_1h:p', 't_apparent:C', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3-) read and preprocess weather.csv  (runtime: 9s)\n",
    "\n",
    "weather = pd.read_csv(\"./weather.csv\", low_memory=False)\n",
    "print(weather.columns) # onemli kolonlar: date, t_apparent:C (hissedilen sicaklik), wind_dir_10m:d (ruzgar yonu),\n",
    "# wind_speed_10m:ms (ruzgar hizi), prob_precip_1h:p (yagis), ilce\n",
    "\n",
    "# ilceleri ayir\n",
    "ilce_weather = {} # keys olarak ilceleri, values olarak o ilcelerin saatlik (1165 gun) hava durumlarini tutar\n",
    "for label, group in weather.groupby(\"name\"):\n",
    "    ilce_weather[label.lower()] = group\n",
    "\n",
    "# tarihleri tarih formatina cevir\n",
    "#print(ilce_weather[\"izmir-konak\"].dtypes)\n",
    "for name in ilce_weather.keys():\n",
    "\n",
    "    tarihler = [] # duzenli tarihleri burada tut\n",
    "    for date in ilce_weather[name][\"date\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ilce_weather[name][\"date\"] = tarihler # duzenli tarihleri date kolonuna ata\n",
    "    ilce_weather[name].set_index(\"date\", inplace=True) # tarihleri indexe cevir\n",
    "    ilce_weather[name][\"tarih\"] = ilce_weather[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "ilce_weather_day = {} # ilce hava durumu verilerini gunluk olarak tut\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_day[name] = ilce_weather[name].resample(\"D\").mean(numeric_only=True)# index'teki tarihleri gune cevir\n",
    "\n",
    "#print(ilce_weather[\"izmir-konak\"][\"tarih\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "lat                               float64\n",
      "lon                               float64\n",
      "t_2m:C                            float64\n",
      "effective_cloud_cover:p           float64\n",
      "global_rad:W                      float64\n",
      "relative_humidity_2m:p            float64\n",
      "wind_dir_10m:d                    float64\n",
      "wind_speed_10m:ms                 float64\n",
      "prob_precip_1h:p                  float64\n",
      "t_apparent:C                      float64\n",
      "name                               object\n",
      "tarih                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3.1-) her ilcenin hava durumunda her gununu ayri ayri df lere koyup dict te tut (runtime: 3m 45s)\n",
    "\n",
    "ilce_weather_detailed = {} \n",
    "# {izmir-konak: {2021-01-01 : df , 2021-01-02 : df ,...} , manisa-akhisar: {2021-01-01 : df , 2021-01-02 : df ,...} }\n",
    "\n",
    "\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_detailed[name] = {}\n",
    "    for label,group in ilce_weather[name].groupby(\"date\"):\n",
    "\n",
    "        gun = label.strftime('%Y-%m-%d')\n",
    "        if gun in ilce_weather_detailed[name]:\n",
    "            ilce_weather_detailed[name][gun] = pd.concat([ilce_weather_detailed[name][gun], group], ignore_index=True)\n",
    "        else:\n",
    "            ilce_weather_detailed[name][gun] = group.copy()\n",
    "\n",
    "\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].keys())[0]))\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].values())[0]).dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat      lon  Sicaklik_max  Sicaklik_min  Bulutluluk_max  \\\n",
      "2021-01-01  38.4177  27.1283          15.3          11.9            90.0   \n",
      "2021-01-02  38.4177  27.1283          17.4          11.0            57.5   \n",
      "2021-01-03  38.4177  27.1283          15.3          11.2            99.8   \n",
      "2021-01-04  38.4177  27.1283          17.7          10.5            97.4   \n",
      "2021-01-05  38.4177  27.1283          16.7          11.2            99.7   \n",
      "\n",
      "            Bulutluluk_min  Guneslilik_max  Guneslilik_min  Bagil_nem_max  \\\n",
      "2021-01-01            28.2           275.4             0.0           93.5   \n",
      "2021-01-02            10.4           374.0             0.0           90.9   \n",
      "2021-01-03            12.4           151.9             0.0           84.6   \n",
      "2021-01-04             9.2           357.0             0.0           85.6   \n",
      "2021-01-05             5.4           362.3             0.0          100.0   \n",
      "\n",
      "            Bagil_nem_min  ...         Ilce      Tarih   Sicaklik  Bulutluluk  \\\n",
      "2021-01-01           82.3  ...  izmir-konak 2021-01-01  13.095833   59.033333   \n",
      "2021-01-02           64.9  ...  izmir-konak 2021-01-02  13.379167   29.912500   \n",
      "2021-01-03           72.9  ...  izmir-konak 2021-01-03  12.587500   69.916667   \n",
      "2021-01-04           55.8  ...  izmir-konak 2021-01-04  13.783333   45.604167   \n",
      "2021-01-05           59.6  ...  izmir-konak 2021-01-05  13.895833   35.670833   \n",
      "\n",
      "            Guneslilik  Bagil_nem  Ruzgar_yonu  Ruzgar_hizi      Yagis  \\\n",
      "2021-01-01   65.212500  87.962500   137.558333     3.129167   1.137500   \n",
      "2021-01-02   91.225000  80.720833   134.820833     2.158333   1.000000   \n",
      "2021-01-03   34.962500  79.725000   142.316667     2.300000   2.520833   \n",
      "2021-01-04   79.400000  71.362500   138.641667     3.979167   1.000000   \n",
      "2021-01-05   92.666667  82.308333   161.516667     2.591667  12.279167   \n",
      "\n",
      "           Hissedilen_sicaklik  \n",
      "2021-01-01           13.891667  \n",
      "2021-01-02           14.250000  \n",
      "2021-01-03           12.937500  \n",
      "2021-01-04           13.787500  \n",
      "2021-01-05           14.850000  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3.2-) 3.1'de ayrilan ilce gunlerini simdi her gun icin degerlerin min max'ini bulup ilce df'lerini tekrar olustur\n",
    "\n",
    "# runtime: 1m 4s\n",
    "weather_last = {} # key olarak ilceleri, value olarak da o ilcelerin hava durumu degerlerini min-max ile tutar\n",
    "for name in ilce_weather_detailed.keys():\n",
    "    weather_last[name] = pd.DataFrame()\n",
    "\n",
    "    for date, day_df in ilce_weather_detailed[name].items():\n",
    "        \n",
    "        # max min leri al\n",
    "        max_values = day_df.max()\n",
    "        min_values = day_df.min()\n",
    "\n",
    "        # satır oluştur\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Tarih\": [datetime.strptime(date, \"%Y-%m-%d\")],\n",
    "            \"lat\": [day_df[\"lat\"].iloc[0]],\n",
    "            \"lon\": [day_df[\"lon\"].iloc[0]],\n",
    "            \"Sicaklik_max\": [max_values[\"t_2m:C\"]],\n",
    "            \"Sicaklik_min\": [min_values[\"t_2m:C\"]],\n",
    "            \"Bulutluluk_max\": [max_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Bulutluluk_min\": [min_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Guneslilik_max\": [max_values.get(\"global_rad:W\", None)],  \n",
    "            \"Guneslilik_min\": [min_values.get(\"global_rad:W\", None)],  \n",
    "            \"Bagil_nem_max\": [max_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Bagil_nem_min\": [min_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Ruzgar_yonu_max\": [max_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_yonu_min\": [min_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_hizi_max\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Ruzgar_hizi_min\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Yagis_max\": [max_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Yagis_min\": [min_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Hissedilen_sicaklik_max\": [max_values.get(\"t_apparent:C\", None)],\n",
    "            \"Hissedilen_sicaklik_min\": [min_values.get(\"t_apparent:C\", None)],\n",
    "            \"Ilce\": [day_df[\"name\"].iloc[0].lower()]  # Ilce ekle\n",
    "        })\n",
    "\n",
    "        # her gunu o ilcenin df ine ekle\n",
    "        weather_last[name] = pd.concat([weather_last[name], new_row], ignore_index=True)\n",
    "\n",
    "  \n",
    "new_column_names = {\n",
    "    \"lat\" : \"lat\", \"lot\" : \"lot\", \"Sicaklik_max\" : \"Sicaklik_max\", \"Sicaklik_min\" : \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\" : \"Bulutluluk_max\", \"Bulutluluk_min\" : \"Bulutluluk_min\", \"Guneslilik_max\" : \"Guneslilik_max\",\n",
    "    \"Guneslilik_min\" : \"Guneslilik_min\", \"Bagil_nem_max\" : \"Bagil_nem_max\", \"Bagil_nem_min\" : \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\" : \"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\" : \"Ruzgar_yonu_min\", \"Ruzgar_hizi_max\" : \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi_min\" : \"Ruzgar_hizi_min\", \"Yagis_max\" : \"Yagis_max\", \"Yagis_min\" : \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\" : \"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\" : \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\" : \"Ilce\", \"t_2m:C\" : \"Sicaklik\", \"effective_cloud_cover:p\" : \"Bulutluluk\", \"global_rad:W\" : \"Guneslilik\",\n",
    "    \"relative_humidity_2m:p\" : \"Bagil_nem\", \"wind_dir_10m:d\" : \"Ruzgar_yonu\", \"wind_speed_10m:ms\" : \"Ruzgar_hizi\",\n",
    "    \"prob_precip_1h:p\" : \"Yagis\", \"t_apparent:C\" : \"Hissedilen_sicaklik\", \"Tarih\" : \"Tarih\"\n",
    "}\n",
    "\n",
    "for name in weather_last.keys():\n",
    "    weather_last[name].set_index(\"Tarih\", inplace=True) # tarih kolonunu indexe ata\n",
    "    weather_last[name][\"Tarih\"] = weather_last[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "    weather_last[name] = pd.concat([weather_last[name], ilce_weather_day[name][[\"t_2m:C\",\"effective_cloud_cover:p\",\n",
    "    \"global_rad:W\", \"relative_humidity_2m:p\",\"wind_dir_10m:d\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\",\n",
    "    \"t_apparent:C\"]]], axis=1) # mean leri ekle\n",
    "    \n",
    "    weather_last[name] = weather_last[name].rename(columns=new_column_names) # kolonlari tekrar isimlendir\n",
    "    \n",
    "\n",
    "print(weather_last[\"izmir-konak\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tatil Adı'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4-) read and preprocess holidays.csv\n",
    "\n",
    "holiday = pd.read_csv(\"./holidays.csv\", low_memory=False)\n",
    "\n",
    "# print(holiday.head())\n",
    "\n",
    "holiday[\"tarih\"] = holiday['Yıl'].astype(str) + '-' + holiday['Ay'].astype(str) + '-' + holiday['Gün'].astype(str)\n",
    "holiday['tarih'] = pd.to_datetime(holiday['tarih'], format='%Y-%m-%d')\n",
    "holiday.set_index(\"tarih\", inplace=True)\n",
    "holiday = holiday.drop(columns=[\"Yıl\", \"Ay\", \"Gün\"])\n",
    "\n",
    "print(holiday.columns) # index olarak tarihi (YY-AA-GG), Bayram_Flag olarak da bayram ismini tutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tarih         ilce  bildirimsiz_sum  bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "                 Tatil Adı  \n",
      "tarih                       \n",
      "2021-01-01  New Year's Day  \n",
      "2021-01-02             NaN  \n",
      "2021-01-03             NaN  \n",
      "2021-01-04             NaN  \n",
      "2021-01-05             NaN  \n",
      "...                    ...  \n",
      "2024-01-27             NaN  \n",
      "2024-01-28             NaN  \n",
      "2024-01-29             NaN  \n",
      "2024-01-30             NaN  \n",
      "2024-01-31             NaN  \n",
      "\n",
      "[1124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "\n",
    "def merge_holiday(df1, df2=holiday):\n",
    "    merged_df = pd.merge(df1, df2[\"Tatil Adı\"], left_index=True, right_index=True, how=\"left\")\n",
    "    #df1[\"Bayramlar\"] = df2[\"Bayram_Flag\"]\n",
    "    return merged_df\n",
    "\n",
    "dict_holiday = {}\n",
    "for name in dict.keys():\n",
    "    dict_holiday[name] = merge_holiday(dict[name],holiday)\n",
    "    dict_holiday[name]['tarih'] = dict_holiday[name].index\n",
    "    dict_holiday[name] = dict_holiday[name].reindex(columns=[\"tarih\", \"ilce\", \"bildirimsiz_sum\", \"bildirimli_sum\", \"Tatil Adı\"])\n",
    "    \n",
    "print(dict_holiday[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                                                     \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500    1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000    2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833    3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000    4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167    5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okkes\\AppData\\Local\\Temp\\ipykernel_22020\\3102189797.py:35: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n"
     ]
    }
   ],
   "source": [
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "\n",
    "def merge_weather(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\n",
    "    \"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "\"\"\" ekstra eklenebilecek kolonlar : (bunlari ustteki diger kolonarin arkasina ekleyebilirsin, ayni sekilde alttaki isimlendirmeye de eklemeyi unutma)\n",
    "\"Bulutluluk_max\", \"Bulutluluk_min\", \"Guneslilik_max\", \"Guneslilik_min\",\"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\",\"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\"\n",
    ",\"Bulutluluk\",\"Guneslilik\",\"Ruzgar_yonu\",\"Hissedilen_sicaklik\"\n",
    "\"\"\"\n",
    "merged_all = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all[name] = merge_weather(dict_holiday[name], weather_last[name])\n",
    "\n",
    "    merged_all[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "    \n",
    "    merged_all[name]['Gün'] = range(1, len(merged_all[name]) + 1)\n",
    "\n",
    "print(merged_all[\"izmir-konak\"].head())\n",
    "\n",
    "all_in_one = pd.concat(merged_all.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "#print(\"\\nall_in_one: \\n\\n\",all_in_one.dtypes)\n",
    "\n",
    "merged_all_week = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_week[name] = merged_all[name].resample(\"W\").sum(numeric_only=True)\n",
    "#print(merged_all_week[\"izmir-konak\"])\n",
    "\n",
    "merged_all_month = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n",
    "#print(merged_all_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if not os.path.exists(\"graphs\"):\\n    os.makedirs(\"graphs\")\\n    print(\"images klasörü olustu\")\\nif not os.path.exists(\"./graphs/bildirimli_siz\"):\\n    os.makedirs(\"./graphs/bildirimli_siz\")\\n    print(\"bildirimli_siz klasoru olustu\")\\n\\n\\nfor name in merged_all_week.keys():\\n    plt.figure(figsize=(17,8))\\n    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\\n    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\\n    plt.xticks(rotation=90)\\n    plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\\n    plt.xlabel(\"Tarih\", fontsize=13)\\n    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\\n\\n    plt.margins(0.01)\\n    plt.legend()\\n    plt.grid()\\n    plt.subplots_adjust(bottom=0.15)\\n    plt.tight_layout()\\n    #plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\\n    #plt.show()'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi (runtime: 19s)\n",
    "\n",
    "\"\"\"if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimli_siz\"):\n",
    "    os.makedirs(\"./graphs/bildirimli_siz\")\n",
    "    print(\"bildirimli_siz klasoru olustu\")\n",
    "\n",
    "\n",
    "for name in merged_all_week.keys():\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    plt.margins(0.01)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\n",
    "    #plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\\n\\nanaliz = pd.concat([\\n    ayristirma2.observed,\\n    ayristirma2.trend,\\n    ayristirma2.seasonal,\\n    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\\n], axis=1)\\nanaliz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\\n\\nindeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\\n\\n\\nX = sa.add_constant(indeks)\\nmodel = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\\nsonuc = model.fit()\\nrsquared_value = sonuc.rsquared\\ny = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\\n\\nyeni_satirlar = pd.DataFrame(index=y)\\nanaliz = pd.concat([analiz, yeni_satirlar])\\n\\n# not: bu degerleri ayarla\\nmev = [\\n    1.038656,\\n    0.973940,\\n    0.987404,\\n    1.038656\\n]\\n\\nnan_indices = analiz.index[analiz[\\'Mevsimsellik\\'].isna()]\\nfor i, index in enumerate(nan_indices):\\n    if i < len(mev):\\n        analiz.at[index, \\'Mevsimsellik\\'] = mev[i]\\n#print(analiz[\"Mevsimsellik\"])\\n\\ngirdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\\nregmodel = sonuc.predict(sa.add_constant(girdi))\\n\\nanaliz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\\n\\n\\nprint(analiz.head())\\n\\nplt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\')\\n#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\', horizontalalignment=\\'left\\')\\nplt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\\n#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\\nplt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği (runtime: 18s)\n",
    "\n",
    "\"\"\"if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimsiz_detailed\"):\n",
    "    os.makedirs(\"./graphs/bildirimsiz_detailed\")\n",
    "    print(\"bildirimsiz_detailed klasoru olustu\")\n",
    "\n",
    "for name in merged_all_week.keys():\n",
    "\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    plt.title(\"{} - Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    window_size = 3  # Hareketli ortalama penceresi\n",
    "    merged_all_week[name]['Moving_Average'] = merged_all_week[name][\"Bildirimsiz_sum\"].rolling(window=window_size, center=True).mean()\n",
    "    #plt.plot(merged_all_week[name]['Moving_Average'], label=\"MHO\", color=\"black\")\n",
    "\n",
    "    ortalama = merged_all_week[name][\"Bildirimsiz_sum\"].mean()\n",
    "    plt.axhline(y=ortalama, color='orange', linestyle='--', label='Ortalama %{:.1f}'.format(ortalama),linewidth=2.2)\n",
    "    alpha = 0.2  # Yumuşatma parametresi \n",
    "    # formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "    merged_all_week[name]['EWMA'] = merged_all_week[name][\"Bildirimsiz_sum\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    plt.plot(merged_all_week[name]['EWMA'], label=\"EWMA\", color=\"red\", lw=2.9)\n",
    "\n",
    "\n",
    "    plt.margins(0.01)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"./graphs/bildirimsiz_detailed/{}.png\".format(name))\n",
    "    #plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "ayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\n",
    "\n",
    "analiz = pd.concat([\n",
    "    ayristirma2.observed,\n",
    "    ayristirma2.trend,\n",
    "    ayristirma2.seasonal,\n",
    "    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\n",
    "], axis=1)\n",
    "analiz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\n",
    "\n",
    "indeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\n",
    "\n",
    "\n",
    "X = sa.add_constant(indeks)\n",
    "model = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\n",
    "sonuc = model.fit()\n",
    "rsquared_value = sonuc.rsquared\n",
    "y = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\n",
    "\n",
    "yeni_satirlar = pd.DataFrame(index=y)\n",
    "analiz = pd.concat([analiz, yeni_satirlar])\n",
    "\n",
    "# not: bu degerleri ayarla\n",
    "mev = [\n",
    "    1.038656,\n",
    "    0.973940,\n",
    "    0.987404,\n",
    "    1.038656\n",
    "]\n",
    "\n",
    "nan_indices = analiz.index[analiz['Mevsimsellik'].isna()]\n",
    "for i, index in enumerate(nan_indices):\n",
    "    if i < len(mev):\n",
    "        analiz.at[index, 'Mevsimsellik'] = mev[i]\n",
    "#print(analiz[\"Mevsimsellik\"])\n",
    "\n",
    "girdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\n",
    "regmodel = sonuc.predict(sa.add_constant(girdi))\n",
    "\n",
    "analiz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\n",
    "\n",
    "\n",
    "print(analiz.head())\n",
    "\n",
    "plt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top')\n",
    "#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "plt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\n",
    "#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\n",
    "plt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.0: 30.0, 6.0: 26.285714285714285, 7.0: 34.170212765957444, 8.2: 31.0, 8.5: 36.0, 8.6: 34.0, 8.9: 37.0, 10.3: 26.0, 11.6: 26.0, 12.7: 35.0, 16.6: 37.0, 18.2: 44.0, 18.9: 28.0, 20.7: 35.0, 22.1: 26.0, 23.7: 33.0, 27.7: 25.0, 28.8: 52.5, 29.0: 32.0, 41.3: 35.0, 44.0: 69.0, 44.3: 27.0, 52.6: 22.0, 54.3: 32.0, 54.5: 13.0, 55.0: 20.0, 56.5: 18.0, 57.2: 43.0, 58.5: 37.0, 60.0: 44.0, 60.1: 28.0, 61.6: 29.0, 67.1: 32.0, 69.3: 45.0, 73.1: 22.0, 73.7: 32.0, 77.0: 61.0, 80.7: 41.0, 85.5: 23.0, 88.3: 38.0, 88.4: 37.0, 91.3: 49.0, 98.6: 72.0, 99.9: 41.0, 100.8: 54.0, 101.0: 31.0, 101.3: 27.0, 102.4: 28.0, 102.9: 18.0, 104.2: 40.0, 105.8: 53.0, 106.6: 30.0, 107.8: 20.0, 108.7: 35.0, 120.2: 40.0, 120.4: 28.0, 122.3: 31.0, 125.6: 54.0, 127.6: 39.0, 139.2: 53.0, 141.70000000000002: 35.0, 143.8: 68.0, 154.6: 27.0, 157.2: 41.0, 157.60000000000002: 31.0, 159.3: 47.0, 162.0: 58.0, 164.8: 32.0, 174.4: 29.0, 175.5: 37.0, 175.7: 33.0, 178.0: 27.0, 178.5: 53.0, 178.6: 60.0, 179.1: 53.0, 180.0: 30.0, 180.6: 28.0, 184.0: 43.0, 186.3: 68.0, 190.4: 35.0, 191.2: 84.0, 191.5: 33.0, 191.6: 32.0, 197.8: 66.0, 202.89999999999998: 35.0, 209.8: 45.0, 212.5: 40.0, 215.6: 59.0, 216.9: 37.0, 219.0: 30.0, 223.6: 25.0, 233.6: 35.0, 237.3: 64.0, 241.2: 35.0, 244.1: 39.0, 246.5: 62.0, 252.1: 52.0, 263.8: 64.0, 272.5: 27.0, 280.1: 60.0, 300.6: 35.0, 308.1: 40.0, 323.4: 76.0, 327.6: 65.0, 335.0: 35.0, 352.9: 85.0, 393.6: 48.0, 413.7: 40.0}\n",
      "     Ort. Yagis  Ort. Kesinti\n",
      "0           5.0     30.000000\n",
      "1           6.0     26.285714\n",
      "2           7.0     34.170213\n",
      "3           8.2     31.000000\n",
      "4           8.5     36.000000\n",
      "..          ...           ...\n",
      "103       327.6     65.000000\n",
      "104       335.0     35.000000\n",
      "105       352.9     85.000000\n",
      "106       393.6     48.000000\n",
      "107       413.7     40.000000\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli, gerekli degil)\n",
    "\n",
    "yagis_dict = {}\n",
    "for label,group in merged_all_week[\"izmir-aliaga\"].groupby(\"Yagis_max\"):\n",
    "    yagis_dict[label] = group\n",
    "\n",
    "yagis_dict_toplamlari = {}\n",
    "for deger in yagis_dict.keys():\n",
    "    yagis_dict_toplamlari[deger] = yagis_dict[deger][\"Bildirimsiz_sum\"].mean()\n",
    "print(yagis_dict_toplamlari)\n",
    "\n",
    "hesaplamalar = pd.DataFrame(list(yagis_dict_toplamlari.items()), columns=['Ort. Yagis', 'Ort. Kesinti'])\n",
    "print(hesaplamalar)\n",
    "window_size = 3  # Hareketli ortalama penceresi\n",
    "hesaplamalar['Moving_Average'] = hesaplamalar[\"Ort. Kesinti\"].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "alpha = 0.3  # Yumuşatma parametresi \n",
    "# formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "hesaplamalar['EWMA'] = hesaplamalar[\"Ort. Kesinti\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "#plt.bar(merged_all_week[\"izmir-aliaga\"][\"Yagis\"],merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], width=0.05, label=\"Bildirimsiz\")\n",
    "plt.scatter(yagis_dict_toplamlari.keys(), yagis_dict_toplamlari.values(), label=\"Ort. Kesinti\")\n",
    "#plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['Moving_Average'], label=\"MHO\", color=\"red\")\n",
    "plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['EWMA'], label=\"EWMA\", color=\"red\")\n",
    "plt.margins(0.01)\n",
    "plt.title(\"Izmir_Aliaga Yagis - Bildirimsiz (Haftalik)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Ortalama Yagis Miktari\")\n",
    "plt.ylabel(\"Ortalama Elektrik Kesintisi\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tarih         ilce  bildirimli_sum\n",
      "18   2024-02-01  izmir-konak               4\n",
      "65   2024-02-02  izmir-konak               1\n",
      "112  2024-02-03  izmir-konak               1\n",
      "159  2024-02-04  izmir-konak               0\n",
      "206  2024-02-05  izmir-konak               2\n",
      "Index(['Tarih', 'Ilce', 'Bildirimli_sum'], dtype='object')\n",
      "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
      "tarih                                                                          \n",
      "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
      "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
      "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
      "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
      "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
      "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
      "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
      "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
      "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
      "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
      "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
      "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
      "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
      "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
      "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
      "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
      "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
      "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
      "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
      "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
      "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
      "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
      "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
      "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
      "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
      "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
      "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
      "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
      "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
      "\n",
      "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
      "tarih                                                                     \n",
      "2024-02-01           4.6           89.5           50.8              2.9   \n",
      "2024-02-02           3.7           92.1           48.5              3.3   \n",
      "2024-02-03           5.6           88.7           49.0              4.7   \n",
      "2024-02-04           5.9           86.1           56.0              1.6   \n",
      "2024-02-05           7.1           95.7           59.0              2.7   \n",
      "2024-02-06           9.3           95.3           60.8              3.0   \n",
      "2024-02-07          12.0           95.7           67.3              6.9   \n",
      "2024-02-08          12.7           89.8           62.3              6.5   \n",
      "2024-02-09          10.8           94.2           60.1              3.5   \n",
      "2024-02-10          11.0           93.2           66.3              6.2   \n",
      "2024-02-11          13.8           82.0           54.5              8.0   \n",
      "2024-02-12          12.1           90.0           65.2              7.9   \n",
      "2024-02-13           8.9           96.1           52.5              3.9   \n",
      "2024-02-14           8.0           99.9           47.6              5.6   \n",
      "2024-02-15           8.2           80.6           59.9              5.3   \n",
      "2024-02-16           7.0           84.0           53.4              4.2   \n",
      "2024-02-17           7.4           87.0           53.2              4.0   \n",
      "2024-02-18           6.8           87.2           53.2              4.0   \n",
      "2024-02-19           6.3           84.7           49.8              3.7   \n",
      "2024-02-20           5.1           89.9           52.6              2.9   \n",
      "2024-02-21           6.1           88.8           44.0              3.4   \n",
      "2024-02-22           8.0           75.4           40.9              2.5   \n",
      "2024-02-23           8.6           95.3           62.4              4.3   \n",
      "2024-02-24           9.7           97.4           55.6              2.8   \n",
      "2024-02-25          10.6           90.8           48.1              3.1   \n",
      "2024-02-26          10.9           84.1           47.7              5.4   \n",
      "2024-02-27          11.8           87.6           50.0              2.6   \n",
      "2024-02-28          10.7           91.7           44.1              2.4   \n",
      "2024-02-29           9.9           92.8           41.2              2.6   \n",
      "\n",
      "            Ruzgar_hizi_min  Yagis_max  Yagis_min   Sicaklik  Bagil_nem  \\\n",
      "tarih                                                                     \n",
      "2024-02-01              2.9        1.0        1.0   7.416667  76.075000   \n",
      "2024-02-02              3.3        1.0        1.0   7.537500  76.600000   \n",
      "2024-02-03              4.7        1.0        1.0   8.354167  70.429167   \n",
      "2024-02-04              1.6        8.7        1.0   9.300000  72.108333   \n",
      "2024-02-05              2.7        1.0        1.0  11.412500  81.879167   \n",
      "2024-02-06              3.0        1.0        1.0  13.341667  82.562500   \n",
      "2024-02-07              6.9        4.6        1.0  14.958333  84.025000   \n",
      "2024-02-08              6.5        1.0        1.0  14.925000  80.550000   \n",
      "2024-02-09              3.5       57.1        1.0  13.379167  83.950000   \n",
      "2024-02-10              6.2       14.1        1.0  14.108333  79.545833   \n",
      "2024-02-11              8.0       94.2        1.0  15.645833  70.191667   \n",
      "2024-02-12              7.9       95.0        1.0  14.679167  78.145833   \n",
      "2024-02-13              3.9       31.7        1.0  11.904167  82.216667   \n",
      "2024-02-14              5.6        1.0        1.0  11.454167  76.887500   \n",
      "2024-02-15              5.3       68.7        1.0   9.841667  72.545833   \n",
      "2024-02-16              4.2        1.0        1.0   9.875000  72.495833   \n",
      "2024-02-17              4.0        1.0        1.0  10.062500  73.662500   \n",
      "2024-02-18              4.0        1.0        1.0  10.137500  73.129167   \n",
      "2024-02-19              3.7        1.0        1.0   9.700000  70.825000   \n",
      "2024-02-20              2.9        1.0        1.0   9.154167  75.720833   \n",
      "2024-02-21              3.4        8.3        1.0   9.941667  69.854167   \n",
      "2024-02-22              2.5        1.0        1.0  11.979167  60.583333   \n",
      "2024-02-23              4.3        1.0        1.0  12.425000  79.912500   \n",
      "2024-02-24              2.8        1.0        1.0  13.795833  81.883333   \n",
      "2024-02-25              3.1       25.7        1.0  14.058333  71.991667   \n",
      "2024-02-26              5.4       31.1        1.0  14.300000  67.937500   \n",
      "2024-02-27              2.6        1.0        1.0  14.954167  75.770833   \n",
      "2024-02-28              2.4        1.0        1.0  15.362500  73.537500   \n",
      "2024-02-29              2.6       85.1        1.0  15.991667  68.441667   \n",
      "\n",
      "            Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                    \n",
      "2024-02-01     2.054167   1.000000    1  \n",
      "2024-02-02     1.691667   1.000000    2  \n",
      "2024-02-03     2.483333   1.000000    3  \n",
      "2024-02-04     1.083333   1.616667    4  \n",
      "2024-02-05     2.004167   1.000000    5  \n",
      "2024-02-06     2.337500   1.000000    6  \n",
      "2024-02-07     4.529167   1.245833    7  \n",
      "2024-02-08     4.483333   1.000000    8  \n",
      "2024-02-09     2.491667   5.058333    9  \n",
      "2024-02-10     4.133333   2.512500   10  \n",
      "2024-02-11     6.191667  15.691667   11  \n",
      "2024-02-12     6.387500  37.950000   12  \n",
      "2024-02-13     2.108333   3.087500   13  \n",
      "2024-02-14     2.820833   1.000000   14  \n",
      "2024-02-15     4.150000   7.166667   15  \n",
      "2024-02-16     2.641667   1.000000   16  \n",
      "2024-02-17     2.487500   1.000000   17  \n",
      "2024-02-18     2.637500   1.000000   18  \n",
      "2024-02-19     2.166667   1.000000   19  \n",
      "2024-02-20     1.504167   1.000000   20  \n",
      "2024-02-21     1.695833   1.562500   21  \n",
      "2024-02-22     1.137500   1.000000   22  \n",
      "2024-02-23     2.583333   1.000000   23  \n",
      "2024-02-24     2.112500   1.000000   24  \n",
      "2024-02-25     2.170833   2.216667   25  \n",
      "2024-02-26     2.712500   5.658333   26  \n",
      "2024-02-27     1.112500   1.000000   27  \n",
      "2024-02-28     1.566667   1.000000   28  \n",
      "2024-02-29     1.633333  14.195833   29  \n"
     ]
    }
   ],
   "source": [
    "# 10-) test icin birlestirme islemleri\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\", low_memory=False) # 47 ilce icin 28 gunluk veriler var. (tarih, ilce, bildirimli_sum)\n",
    "#print(test)\n",
    "\n",
    "dict_test :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak ilcelerin 4 ocak - 31 ocak arasi verilerini df olarak tutar\n",
    "for label, group in test.groupby(\"ilce\"):\n",
    "    dict_test[label] = group\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].head())\n",
    "\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    tarihler = [] # duzgun tarihleri tutacak\n",
    "    for date in dict_test[name][\"tarih\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d\")) \n",
    "\n",
    "    dict_test[name][\"tarih\"] = tarihler # duzeltilmis tarihleri ata\n",
    "\n",
    "    dict_test[name].set_index(\"tarih\", inplace=True) # tarih kolonunu index'e ata\n",
    "    dict_test[name][\"Tarih\"] = dict_test[name].index # tarih kolonunu yeniden olustur\n",
    "    dict_test[name] = dict_test[name].iloc[:, [2, 0, 1]] # kolon siralarini duzenle\n",
    "    dict_test[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\"] # kolon isimlerini duzenle\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].columns)\n",
    "\n",
    "\n",
    "dict_test_merged = {} # birlestirilenleri tutacak dict\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    gecici = merge_holiday(dict_test[name], holiday) # test'e holiday ekle\n",
    "    dict_test_merged[name] = merge_weather(gecici, weather_last[name]) # sonra weather'i ekle\n",
    "\n",
    "    dict_test_merged[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "\n",
    "    dict_test_merged[name]['Gün'] = range(1, len(dict_test_merged[name]) + 1) # gun kolonu ekle (1-28 arasi oluyor)\n",
    "\n",
    "print(dict_test_merged[\"izmir-konak\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKINE OGRENMESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#print((gpus), \"Physical GPU,\\n\", (logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config islemleri\n",
    "\n",
    "\n",
    "#tf.config.set_soft_device_placement(True) # otomatik dogru cihaz kullanimi\n",
    "#strategy = tf.distribute.MirroredStrategy() # birden fazla gpu kullanimi\n",
    "#tf.debugging.set_log_device_placement(True) # hangi cihazlarin kullanildigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 6ms/step - loss: 41.8500 - mae: 4.9652 - val_loss: 32.3147 - val_mae: 4.0520\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.4416 - mae: 3.4929 - val_loss: 17.8729 - val_mae: 2.6940\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 17.2376 - mae: 2.8093 - val_loss: 15.6158 - val_mae: 2.8183\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 16.3186 - mae: 2.8718 - val_loss: 15.1613 - val_mae: 2.6840\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 16.1604 - mae: 2.7794 - val_loss: 14.9537 - val_mae: 2.6648\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.9244 - mae: 2.7660 - val_loss: 14.7289 - val_mae: 2.6641\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.7343 - mae: 2.7449 - val_loss: 14.5521 - val_mae: 2.6913\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.6197 - mae: 2.7788 - val_loss: 14.4465 - val_mae: 2.6729\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.5405 - mae: 2.7467 - val_loss: 14.3446 - val_mae: 2.6607\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.4810 - mae: 2.7592 - val_loss: 14.2795 - val_mae: 2.6660\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.4062 - mae: 2.7826 - val_loss: 14.1832 - val_mae: 2.6673\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.3639 - mae: 2.7504 - val_loss: 14.1881 - val_mae: 2.6211\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.2874 - mae: 2.7259 - val_loss: 13.9716 - val_mae: 2.6785\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.2383 - mae: 2.7826 - val_loss: 13.9293 - val_mae: 2.6494\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.1579 - mae: 2.7196 - val_loss: 13.8463 - val_mae: 2.6739\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.1620 - mae: 2.7830 - val_loss: 13.7993 - val_mae: 2.6532\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.1053 - mae: 2.7790 - val_loss: 13.7705 - val_mae: 2.6043\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0489 - mae: 2.6963 - val_loss: 13.6280 - val_mae: 2.6275\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9490 - mae: 2.7310 - val_loss: 13.5457 - val_mae: 2.6426\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.8903 - mae: 2.7227 - val_loss: 13.5829 - val_mae: 2.6085\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9168 - mae: 2.7007 - val_loss: 13.4729 - val_mae: 2.6503\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9492 - mae: 2.7670 - val_loss: 13.4538 - val_mae: 2.5971\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.7143 - mae: 2.7080 - val_loss: 13.4259 - val_mae: 2.6901\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.9224 - mae: 2.7858 - val_loss: 13.4260 - val_mae: 2.5779\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.8704 - mae: 2.6788 - val_loss: 13.2416 - val_mae: 2.6587\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6195 - mae: 2.7274 - val_loss: 13.3484 - val_mae: 2.5616\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.7359 - mae: 2.7109 - val_loss: 13.2046 - val_mae: 2.5687\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6081 - mae: 2.6813 - val_loss: 13.0044 - val_mae: 2.6281\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5696 - mae: 2.7378 - val_loss: 13.1804 - val_mae: 2.5590\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5150 - mae: 2.6839 - val_loss: 12.8875 - val_mae: 2.5956\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3697 - mae: 2.6750 - val_loss: 12.9204 - val_mae: 2.5555\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5222 - mae: 2.6787 - val_loss: 12.7983 - val_mae: 2.6044\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3710 - mae: 2.6974 - val_loss: 12.7955 - val_mae: 2.5517\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6251 - mae: 2.7882 - val_loss: 12.7860 - val_mae: 2.5561\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3991 - mae: 2.6921 - val_loss: 12.6403 - val_mae: 2.5755\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4236 - mae: 2.7345 - val_loss: 12.5812 - val_mae: 2.5645\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2516 - mae: 2.7026 - val_loss: 12.5887 - val_mae: 2.5522\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1400 - mae: 2.6592 - val_loss: 12.5512 - val_mae: 2.5501\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2710 - mae: 2.6911 - val_loss: 12.4885 - val_mae: 2.5545\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1795 - mae: 2.6425 - val_loss: 12.4966 - val_mae: 2.5687\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0460 - mae: 2.7019 - val_loss: 12.5398 - val_mae: 2.5278\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1347 - mae: 2.6454 - val_loss: 12.4121 - val_mae: 2.5434\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0050 - mae: 2.6502 - val_loss: 12.3595 - val_mae: 2.5528\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9601 - mae: 2.6737 - val_loss: 12.3690 - val_mae: 2.5406\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9595 - mae: 2.6754 - val_loss: 12.3417 - val_mae: 2.5431\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9620 - mae: 2.6810 - val_loss: 12.4220 - val_mae: 2.5137\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9194 - mae: 2.6403 - val_loss: 12.3008 - val_mae: 2.5621\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9881 - mae: 2.6846 - val_loss: 12.3171 - val_mae: 2.5323\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9790 - mae: 2.6216 - val_loss: 12.2929 - val_mae: 2.5612\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8831 - mae: 2.6674 - val_loss: 12.2325 - val_mae: 2.5360\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12.0037 - mae: 2.6614\n",
      "Mean Absolute Error on Test Data: 2.661430597305298\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0032471524111126193\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 4.5253 - mae: 1.6415 - val_loss: 2.3266 - val_mae: 1.1243\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 2.7326 - mae: 1.0938 - val_loss: 1.3858 - val_mae: 0.8597\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.9998 - mae: 1.0382 - val_loss: 1.4072 - val_mae: 0.9607\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.9872 - mae: 1.0817 - val_loss: 1.3823 - val_mae: 0.9512\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.9324 - mae: 1.0373 - val_loss: 1.3012 - val_mae: 0.9057\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.9183 - mae: 1.0274 - val_loss: 1.3006 - val_mae: 0.9080\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.9084 - mae: 1.0298 - val_loss: 1.2964 - val_mae: 0.9080\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8996 - mae: 1.0269 - val_loss: 1.2841 - val_mae: 0.9033\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8926 - mae: 1.0221 - val_loss: 1.2748 - val_mae: 0.8993\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8948 - mae: 1.0358 - val_loss: 1.3087 - val_mae: 0.9259\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.9030 - mae: 1.0229 - val_loss: 1.2487 - val_mae: 0.8862\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8666 - mae: 1.0164 - val_loss: 1.2697 - val_mae: 0.9065\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8671 - mae: 1.0273 - val_loss: 1.2557 - val_mae: 0.8997\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8601 - mae: 1.0231 - val_loss: 1.2607 - val_mae: 0.9060\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8553 - mae: 1.0071 - val_loss: 1.2335 - val_mae: 0.8872\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8454 - mae: 1.0159 - val_loss: 1.2452 - val_mae: 0.9016\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8439 - mae: 1.0162 - val_loss: 1.2203 - val_mae: 0.8844\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8334 - mae: 1.0034 - val_loss: 1.2345 - val_mae: 0.8985\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8350 - mae: 1.0092 - val_loss: 1.2296 - val_mae: 0.8983\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8284 - mae: 1.0176 - val_loss: 1.2300 - val_mae: 0.8997\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8312 - mae: 1.0253 - val_loss: 1.2137 - val_mae: 0.8879\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8252 - mae: 0.9939 - val_loss: 1.2004 - val_mae: 0.8770\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8098 - mae: 1.0103 - val_loss: 1.2456 - val_mae: 0.9179\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8061 - mae: 1.0164 - val_loss: 1.2179 - val_mae: 0.8963\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8284 - mae: 0.9993 - val_loss: 1.2267 - val_mae: 0.9055\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8017 - mae: 1.0107 - val_loss: 1.2129 - val_mae: 0.8956\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7990 - mae: 0.9977 - val_loss: 1.2053 - val_mae: 0.8903\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7979 - mae: 1.0052 - val_loss: 1.2127 - val_mae: 0.8979\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7926 - mae: 1.0060 - val_loss: 1.2042 - val_mae: 0.8900\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7863 - mae: 0.9992 - val_loss: 1.2203 - val_mae: 0.9068\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.8005 - mae: 1.0107 - val_loss: 1.1866 - val_mae: 0.8737\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7838 - mae: 0.9970 - val_loss: 1.2430 - val_mae: 0.9258\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7777 - mae: 1.0054 - val_loss: 1.1928 - val_mae: 0.8822\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7890 - mae: 0.9864 - val_loss: 1.2029 - val_mae: 0.8905\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7830 - mae: 1.0162 - val_loss: 1.2129 - val_mae: 0.9038\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7743 - mae: 1.0086 - val_loss: 1.2022 - val_mae: 0.8957\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7669 - mae: 0.9882 - val_loss: 1.1882 - val_mae: 0.8771\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7643 - mae: 1.0022 - val_loss: 1.2176 - val_mae: 0.9081\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7717 - mae: 0.9867 - val_loss: 1.1883 - val_mae: 0.8803\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7616 - mae: 1.0117 - val_loss: 1.2252 - val_mae: 0.9145\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7566 - mae: 1.0016 - val_loss: 1.2052 - val_mae: 0.8950\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7569 - mae: 0.9929 - val_loss: 1.1953 - val_mae: 0.8895\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7502 - mae: 0.9825 - val_loss: 1.1888 - val_mae: 0.8829\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7555 - mae: 1.0136 - val_loss: 1.2141 - val_mae: 0.9047\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7419 - mae: 0.9852 - val_loss: 1.1912 - val_mae: 0.8780\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7430 - mae: 1.0000 - val_loss: 1.2121 - val_mae: 0.9039\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7468 - mae: 0.9967 - val_loss: 1.2143 - val_mae: 0.9072\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7355 - mae: 0.9819 - val_loss: 1.2004 - val_mae: 0.8919\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7336 - mae: 1.0017 - val_loss: 1.2056 - val_mae: 0.8993\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 1.7453 - mae: 0.9732 - val_loss: 1.2106 - val_mae: 0.9012\n",
      "5/5 [==============================] - 0s 757us/step - loss: 1.4617 - mae: 0.9489\n",
      "Mean Absolute Error on Test Data: 0.9489347338676453\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.061140889500682305\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 34.6692 - mae: 4.4506 - val_loss: 31.3049 - val_mae: 4.1654\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6675 - mae: 3.3061 - val_loss: 18.9114 - val_mae: 3.0527\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.5761 - mae: 2.7196 - val_loss: 14.9934 - val_mae: 2.9538\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.0238 - mae: 2.8845 - val_loss: 14.9706 - val_mae: 2.9101\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.8153 - mae: 2.7278 - val_loss: 15.0609 - val_mae: 2.8791\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6650 - mae: 2.7391 - val_loss: 14.8271 - val_mae: 2.8949\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5785 - mae: 2.7611 - val_loss: 14.7831 - val_mae: 2.8844\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5228 - mae: 2.7557 - val_loss: 14.7634 - val_mae: 2.8730\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4532 - mae: 2.7296 - val_loss: 14.7270 - val_mae: 2.8678\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4026 - mae: 2.7381 - val_loss: 14.5965 - val_mae: 2.8823\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3662 - mae: 2.7411 - val_loss: 14.5664 - val_mae: 2.8777\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3499 - mae: 2.7590 - val_loss: 14.5911 - val_mae: 2.8617\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3143 - mae: 2.7159 - val_loss: 14.5213 - val_mae: 2.8677\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2757 - mae: 2.7151 - val_loss: 14.4538 - val_mae: 2.8837\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2904 - mae: 2.7320 - val_loss: 14.4377 - val_mae: 2.8825\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2364 - mae: 2.7473 - val_loss: 14.5096 - val_mae: 2.8558\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2249 - mae: 2.7416 - val_loss: 14.5129 - val_mae: 2.8514\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1925 - mae: 2.7041 - val_loss: 14.4830 - val_mae: 2.8520\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1515 - mae: 2.7163 - val_loss: 14.4108 - val_mae: 2.8629\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2237 - mae: 2.6868 - val_loss: 14.3688 - val_mae: 2.8759\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1332 - mae: 2.7509 - val_loss: 14.4039 - val_mae: 2.8632\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0895 - mae: 2.7015 - val_loss: 14.4061 - val_mae: 2.8571\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0780 - mae: 2.7070 - val_loss: 14.3816 - val_mae: 2.8609\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1017 - mae: 2.7346 - val_loss: 14.4287 - val_mae: 2.8524\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1788 - mae: 2.6705 - val_loss: 14.3617 - val_mae: 2.8695\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1409 - mae: 2.7698 - val_loss: 14.4774 - val_mae: 2.8437\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1946 - mae: 2.6739 - val_loss: 14.3302 - val_mae: 2.8863\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0547 - mae: 2.7571 - val_loss: 14.4423 - val_mae: 2.8451\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1086 - mae: 2.6514 - val_loss: 14.3619 - val_mae: 2.8674\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0674 - mae: 2.7444 - val_loss: 14.3422 - val_mae: 2.8794\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0374 - mae: 2.7394 - val_loss: 14.3862 - val_mae: 2.8622\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9711 - mae: 2.7115 - val_loss: 14.4996 - val_mae: 2.8402\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0138 - mae: 2.7004 - val_loss: 14.4184 - val_mae: 2.8527\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9753 - mae: 2.6875 - val_loss: 14.4832 - val_mae: 2.8461\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0205 - mae: 2.6917 - val_loss: 14.3847 - val_mae: 2.8709\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9977 - mae: 2.7089 - val_loss: 14.4352 - val_mae: 2.8571\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9755 - mae: 2.7397 - val_loss: 14.4155 - val_mae: 2.8778\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9781 - mae: 2.6841 - val_loss: 14.5036 - val_mae: 2.8538\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9699 - mae: 2.7406 - val_loss: 14.4526 - val_mae: 2.8661\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9441 - mae: 2.7174 - val_loss: 14.4494 - val_mae: 2.8700\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9980 - mae: 2.7404 - val_loss: 14.5808 - val_mae: 2.8477\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9107 - mae: 2.6842 - val_loss: 14.4725 - val_mae: 2.8701\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9640 - mae: 2.7246 - val_loss: 14.6156 - val_mae: 2.8400\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9405 - mae: 2.7157 - val_loss: 14.4671 - val_mae: 2.8697\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9892 - mae: 2.6822 - val_loss: 14.5176 - val_mae: 2.8505\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8957 - mae: 2.7271 - val_loss: 14.4252 - val_mae: 2.8909\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0198 - mae: 2.7636 - val_loss: 14.5115 - val_mae: 2.8623\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9028 - mae: 2.6886 - val_loss: 14.5115 - val_mae: 2.8679\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9022 - mae: 2.7015 - val_loss: 14.4721 - val_mae: 2.8769\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8902 - mae: 2.7205 - val_loss: 14.6001 - val_mae: 2.8494\n",
      "7/7 [==============================] - 0s 833us/step - loss: 13.7543 - mae: 2.5552\n",
      "Mean Absolute Error on Test Data: 2.5551838874816895\n",
      "7/7 [==============================] - 0s 843us/step\n",
      "R-squared: 0.008078525828138172\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 32.4198 - mae: 3.8824 - val_loss: 44.3341 - val_mae: 3.6976\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 24.8214 - mae: 3.0012 - val_loss: 35.8289 - val_mae: 2.9017\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 18.7039 - mae: 2.5724 - val_loss: 31.5318 - val_mae: 2.7996\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.4418 - mae: 2.6981 - val_loss: 31.4134 - val_mae: 2.8279\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.3903 - mae: 2.6110 - val_loss: 31.4748 - val_mae: 2.7814\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.3176 - mae: 2.6300 - val_loss: 31.3233 - val_mae: 2.7929\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.3170 - mae: 2.6473 - val_loss: 31.3286 - val_mae: 2.7802\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.2793 - mae: 2.6344 - val_loss: 31.1496 - val_mae: 2.8071\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.2523 - mae: 2.5923 - val_loss: 31.1691 - val_mae: 2.7809\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.2686 - mae: 2.6548 - val_loss: 31.0682 - val_mae: 2.7869\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1756 - mae: 2.5876 - val_loss: 31.0084 - val_mae: 2.7762\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1389 - mae: 2.6181 - val_loss: 30.9227 - val_mae: 2.7837\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1257 - mae: 2.5892 - val_loss: 30.8961 - val_mae: 2.7675\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.0816 - mae: 2.6237 - val_loss: 30.6828 - val_mae: 2.7931\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 17.0652 - mae: 2.5830 - val_loss: 30.7105 - val_mae: 2.7650\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0181 - mae: 2.5944 - val_loss: 30.5564 - val_mae: 2.7716\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.0108 - mae: 2.5909 - val_loss: 30.4421 - val_mae: 2.7722\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9642 - mae: 2.5894 - val_loss: 30.3984 - val_mae: 2.7571\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9333 - mae: 2.5885 - val_loss: 30.2967 - val_mae: 2.7596\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9274 - mae: 2.6041 - val_loss: 30.2528 - val_mae: 2.7543\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8527 - mae: 2.5791 - val_loss: 30.1511 - val_mae: 2.7534\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8604 - mae: 2.5929 - val_loss: 30.0478 - val_mae: 2.7495\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7864 - mae: 2.5617 - val_loss: 30.0397 - val_mae: 2.7378\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7743 - mae: 2.5410 - val_loss: 29.8737 - val_mae: 2.7512\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7444 - mae: 2.5761 - val_loss: 29.8465 - val_mae: 2.7340\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7439 - mae: 2.5976 - val_loss: 29.7317 - val_mae: 2.7417\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7323 - mae: 2.5192 - val_loss: 29.7546 - val_mae: 2.7104\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7264 - mae: 2.6082 - val_loss: 29.5666 - val_mae: 2.7341\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6755 - mae: 2.5118 - val_loss: 29.4979 - val_mae: 2.7305\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5929 - mae: 2.5783 - val_loss: 29.3549 - val_mae: 2.7323\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5621 - mae: 2.5455 - val_loss: 29.3710 - val_mae: 2.7180\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5013 - mae: 2.5366 - val_loss: 29.1753 - val_mae: 2.7236\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4682 - mae: 2.5365 - val_loss: 29.1438 - val_mae: 2.7146\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4859 - mae: 2.5543 - val_loss: 29.0102 - val_mae: 2.7290\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4209 - mae: 2.5234 - val_loss: 29.0018 - val_mae: 2.6986\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4708 - mae: 2.5428 - val_loss: 28.9806 - val_mae: 2.6977\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3712 - mae: 2.5341 - val_loss: 28.8281 - val_mae: 2.7162\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3534 - mae: 2.5076 - val_loss: 28.7270 - val_mae: 2.7080\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3341 - mae: 2.5239 - val_loss: 28.7745 - val_mae: 2.6842\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2148 - mae: 2.5010 - val_loss: 28.5261 - val_mae: 2.7430\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2688 - mae: 2.5653 - val_loss: 28.5305 - val_mae: 2.7035\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1923 - mae: 2.5218 - val_loss: 28.4758 - val_mae: 2.6877\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1502 - mae: 2.5115 - val_loss: 28.2724 - val_mae: 2.7071\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1512 - mae: 2.5437 - val_loss: 28.2860 - val_mae: 2.6940\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1241 - mae: 2.4861 - val_loss: 28.2626 - val_mae: 2.6855\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1101 - mae: 2.4926 - val_loss: 27.9979 - val_mae: 2.7177\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0924 - mae: 2.5287 - val_loss: 28.2340 - val_mae: 2.6863\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1005 - mae: 2.5296 - val_loss: 27.9718 - val_mae: 2.6942\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1834 - mae: 2.4837 - val_loss: 27.9323 - val_mae: 2.7300\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.9853 - mae: 2.5016 - val_loss: 27.8212 - val_mae: 2.6960\n",
      "7/7 [==============================] - 0s 834us/step - loss: 16.5074 - mae: 2.7622\n",
      "Mean Absolute Error on Test Data: 2.762237310409546\n",
      "7/7 [==============================] - 0s 836us/step\n",
      "R-squared: 0.060265301001375615\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 58.9404 - mae: 5.9505 - val_loss: 50.6581 - val_mae: 5.2762\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 38.3179 - mae: 4.2738 - val_loss: 28.2646 - val_mae: 3.4566\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8444 - mae: 3.2457 - val_loss: 22.9128 - val_mae: 3.4567\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6765 - mae: 3.3237 - val_loss: 22.6461 - val_mae: 3.3405\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.5650 - mae: 3.2210 - val_loss: 22.5359 - val_mae: 3.3279\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.4625 - mae: 3.2599 - val_loss: 22.3764 - val_mae: 3.3555\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.3756 - mae: 3.2350 - val_loss: 22.3395 - val_mae: 3.3167\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.3274 - mae: 3.2520 - val_loss: 22.1772 - val_mae: 3.3352\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.3078 - mae: 3.2648 - val_loss: 22.0941 - val_mae: 3.3266\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.3259 - mae: 3.2128 - val_loss: 22.0021 - val_mae: 3.3500\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2579 - mae: 3.2376 - val_loss: 21.9543 - val_mae: 3.3557\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1526 - mae: 3.2507 - val_loss: 21.9381 - val_mae: 3.3298\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1425 - mae: 3.2415 - val_loss: 21.8501 - val_mae: 3.3470\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.2183 - mae: 3.2189 - val_loss: 21.7931 - val_mae: 3.3579\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1132 - mae: 3.2710 - val_loss: 21.7243 - val_mae: 3.3517\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1046 - mae: 3.2252 - val_loss: 21.6839 - val_mae: 3.3518\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0186 - mae: 3.2242 - val_loss: 21.6512 - val_mae: 3.3498\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9308 - mae: 3.2416 - val_loss: 21.6187 - val_mae: 3.3255\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9121 - mae: 3.2374 - val_loss: 21.5412 - val_mae: 3.3591\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8714 - mae: 3.2407 - val_loss: 21.5919 - val_mae: 3.3044\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9490 - mae: 3.2586 - val_loss: 21.6276 - val_mae: 3.2852\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8879 - mae: 3.1915 - val_loss: 21.4253 - val_mae: 3.3620\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8277 - mae: 3.2789 - val_loss: 21.4014 - val_mae: 3.3320\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9544 - mae: 3.2213 - val_loss: 21.3287 - val_mae: 3.3686\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6646 - mae: 3.2330 - val_loss: 21.3664 - val_mae: 3.3124\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6721 - mae: 3.2338 - val_loss: 21.3192 - val_mae: 3.3219\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6130 - mae: 3.2212 - val_loss: 21.3526 - val_mae: 3.3001\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6053 - mae: 3.2021 - val_loss: 21.2365 - val_mae: 3.3419\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6607 - mae: 3.2545 - val_loss: 21.2057 - val_mae: 3.3213\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5332 - mae: 3.1988 - val_loss: 21.1396 - val_mae: 3.3441\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6136 - mae: 3.2180 - val_loss: 21.2579 - val_mae: 3.2864\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5899 - mae: 3.2659 - val_loss: 21.1321 - val_mae: 3.3103\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4594 - mae: 3.1869 - val_loss: 21.0785 - val_mae: 3.3407\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4041 - mae: 3.2287 - val_loss: 21.0453 - val_mae: 3.3359\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6681 - mae: 3.2625 - val_loss: 21.2117 - val_mae: 3.2681\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3717 - mae: 3.2038 - val_loss: 20.9805 - val_mae: 3.3410\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3544 - mae: 3.2014 - val_loss: 21.0069 - val_mae: 3.3111\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4216 - mae: 3.2598 - val_loss: 21.0042 - val_mae: 3.2862\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3097 - mae: 3.1864 - val_loss: 20.9044 - val_mae: 3.3460\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3400 - mae: 3.2207 - val_loss: 20.9170 - val_mae: 3.3020\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3326 - mae: 3.2574 - val_loss: 20.9089 - val_mae: 3.2891\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1641 - mae: 3.1737 - val_loss: 20.7986 - val_mae: 3.3324\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3933 - mae: 3.2614 - val_loss: 21.0851 - val_mae: 3.2521\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3980 - mae: 3.2340 - val_loss: 20.8496 - val_mae: 3.2726\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2383 - mae: 3.1606 - val_loss: 20.7195 - val_mae: 3.3213\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1529 - mae: 3.2588 - val_loss: 20.7694 - val_mae: 3.2794\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1367 - mae: 3.1914 - val_loss: 20.7614 - val_mae: 3.2690\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0030 - mae: 3.1779 - val_loss: 20.6439 - val_mae: 3.3081\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 20.9616 - mae: 3.2148 - val_loss: 20.7261 - val_mae: 3.2663\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.0293 - mae: 3.1811 - val_loss: 20.6125 - val_mae: 3.2994\n",
      "7/7 [==============================] - 0s 822us/step - loss: 24.5207 - mae: 3.4188\n",
      "Mean Absolute Error on Test Data: 3.4188129901885986\n",
      "7/7 [==============================] - 0s 820us/step\n",
      "R-squared: 0.14234433071569874\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 7.8698 - mae: 1.8326 - val_loss: 3.2124 - val_mae: 1.2316\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 5.7990 - mae: 1.3144 - val_loss: 2.0283 - val_mae: 0.9788\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.8694 - mae: 1.3147 - val_loss: 1.8580 - val_mae: 1.0808\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.6049 - mae: 1.3818 - val_loss: 1.9059 - val_mae: 1.1302\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5729 - mae: 1.3880 - val_loss: 1.8929 - val_mae: 1.1198\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.5504 - mae: 1.3720 - val_loss: 1.8808 - val_mae: 1.1078\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5323 - mae: 1.3585 - val_loss: 1.8736 - val_mae: 1.1012\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.5464 - mae: 1.3990 - val_loss: 1.9193 - val_mae: 1.1395\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.5154 - mae: 1.3506 - val_loss: 1.8478 - val_mae: 1.0801\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.4902 - mae: 1.3517 - val_loss: 1.9052 - val_mae: 1.1281\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4771 - mae: 1.3620 - val_loss: 1.8845 - val_mae: 1.1128\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.4723 - mae: 1.3434 - val_loss: 1.8906 - val_mae: 1.1180\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4429 - mae: 1.3694 - val_loss: 1.9116 - val_mae: 1.1331\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4323 - mae: 1.3564 - val_loss: 1.9161 - val_mae: 1.1350\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4276 - mae: 1.3751 - val_loss: 1.9248 - val_mae: 1.1399\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.4037 - mae: 1.3448 - val_loss: 1.8946 - val_mae: 1.1174\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.3840 - mae: 1.3615 - val_loss: 1.9696 - val_mae: 1.1652\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.3652 - mae: 1.3695 - val_loss: 1.9650 - val_mae: 1.1614\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3504 - mae: 1.3562 - val_loss: 1.9535 - val_mae: 1.1536\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.3395 - mae: 1.3554 - val_loss: 1.9477 - val_mae: 1.1484\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.3421 - mae: 1.3148 - val_loss: 1.9358 - val_mae: 1.1385\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.3078 - mae: 1.3442 - val_loss: 2.0239 - val_mae: 1.1937\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3168 - mae: 1.3621 - val_loss: 2.0827 - val_mae: 1.2220\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3093 - mae: 1.3745 - val_loss: 1.9809 - val_mae: 1.1593\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2840 - mae: 1.3330 - val_loss: 2.0395 - val_mae: 1.1958\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2933 - mae: 1.3810 - val_loss: 2.0747 - val_mae: 1.2142\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2970 - mae: 1.3394 - val_loss: 2.0509 - val_mae: 1.1983\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2743 - mae: 1.3420 - val_loss: 2.0376 - val_mae: 1.1885\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.3140 - mae: 1.3937 - val_loss: 2.1121 - val_mae: 1.2269\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2609 - mae: 1.3224 - val_loss: 2.0243 - val_mae: 1.1756\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2454 - mae: 1.3420 - val_loss: 2.1319 - val_mae: 1.2352\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2774 - mae: 1.3415 - val_loss: 2.1139 - val_mae: 1.2259\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2370 - mae: 1.3541 - val_loss: 2.1387 - val_mae: 1.2357\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2388 - mae: 1.3373 - val_loss: 2.0819 - val_mae: 1.2046\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2373 - mae: 1.3687 - val_loss: 2.2026 - val_mae: 1.2645\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2289 - mae: 1.3396 - val_loss: 2.1087 - val_mae: 1.2156\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2735 - mae: 1.2977 - val_loss: 2.1127 - val_mae: 1.2170\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2971 - mae: 1.4136 - val_loss: 2.2728 - val_mae: 1.2938\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2329 - mae: 1.3197 - val_loss: 2.0681 - val_mae: 1.1834\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2260 - mae: 1.3157 - val_loss: 2.1944 - val_mae: 1.2561\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2077 - mae: 1.3561 - val_loss: 2.1679 - val_mae: 1.2426\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2033 - mae: 1.3307 - val_loss: 2.1597 - val_mae: 1.2372\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1967 - mae: 1.3460 - val_loss: 2.1910 - val_mae: 1.2527\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2376 - mae: 1.3991 - val_loss: 2.2046 - val_mae: 1.2579\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1879 - mae: 1.3364 - val_loss: 2.1560 - val_mae: 1.2280\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2074 - mae: 1.3558 - val_loss: 2.2418 - val_mae: 1.2714\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.1852 - mae: 1.3565 - val_loss: 2.1455 - val_mae: 1.2149\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1986 - mae: 1.3102 - val_loss: 2.2004 - val_mae: 1.2480\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.2298 - mae: 1.3895 - val_loss: 2.2146 - val_mae: 1.2564\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 2ms/step - loss: 4.1773 - mae: 1.3604 - val_loss: 2.2687 - val_mae: 1.2763\n",
      "5/5 [==============================] - 0s 927us/step - loss: 5.4038 - mae: 1.4297\n",
      "Mean Absolute Error on Test Data: 1.4297144412994385\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.19516088257312925\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 120.3593 - mae: 8.6977 - val_loss: 103.3767 - val_mae: 7.8380\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 89.4833 - mae: 6.8153 - val_loss: 67.2623 - val_mae: 5.5521\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 54.7587 - mae: 4.6410 - val_loss: 42.8246 - val_mae: 4.4241\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1357 - mae: 4.4461 - val_loss: 42.2612 - val_mae: 4.6112\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6839 - mae: 4.4248 - val_loss: 41.9698 - val_mae: 4.4920\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7335 - mae: 4.3520 - val_loss: 41.8350 - val_mae: 4.5196\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5055 - mae: 4.3693 - val_loss: 41.7266 - val_mae: 4.5090\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3148 - mae: 4.3957 - val_loss: 41.5979 - val_mae: 4.4979\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2209 - mae: 4.3535 - val_loss: 41.4994 - val_mae: 4.4803\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1850 - mae: 4.3494 - val_loss: 41.3922 - val_mae: 4.4643\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9548 - mae: 4.3633 - val_loss: 41.2786 - val_mae: 4.4820\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9272 - mae: 4.3402 - val_loss: 41.1665 - val_mae: 4.4609\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.7995 - mae: 4.3645 - val_loss: 41.0849 - val_mae: 4.5040\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.7282 - mae: 4.3508 - val_loss: 40.9936 - val_mae: 4.4305\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4971 - mae: 4.3306 - val_loss: 40.8436 - val_mae: 4.4471\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4399 - mae: 4.3142 - val_loss: 40.7707 - val_mae: 4.4471\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3382 - mae: 4.3717 - val_loss: 40.6831 - val_mae: 4.4431\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3903 - mae: 4.2971 - val_loss: 40.5965 - val_mae: 4.4330\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3751 - mae: 4.2917 - val_loss: 40.5264 - val_mae: 4.4449\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1770 - mae: 4.3765 - val_loss: 40.4519 - val_mae: 4.4370\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.9291 - mae: 4.2945 - val_loss: 40.4431 - val_mae: 4.3909\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8846 - mae: 4.3016 - val_loss: 40.3092 - val_mae: 4.4155\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7738 - mae: 4.2997 - val_loss: 40.2772 - val_mae: 4.3786\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7917 - mae: 4.3146 - val_loss: 40.1800 - val_mae: 4.4122\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6550 - mae: 4.2775 - val_loss: 40.1661 - val_mae: 4.3834\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6695 - mae: 4.3224 - val_loss: 40.1224 - val_mae: 4.3993\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4288 - mae: 4.2611 - val_loss: 40.1653 - val_mae: 4.3505\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7176 - mae: 4.2806 - val_loss: 40.1811 - val_mae: 4.3393\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.3263 - mae: 4.2855 - val_loss: 39.9879 - val_mae: 4.3958\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.2679 - mae: 4.2747 - val_loss: 40.0310 - val_mae: 4.3469\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.2693 - mae: 4.2409 - val_loss: 39.9876 - val_mae: 4.3516\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4030 - mae: 4.3482 - val_loss: 40.0062 - val_mae: 4.3333\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.1373 - mae: 4.2323 - val_loss: 39.9635 - val_mae: 4.3351\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.0538 - mae: 4.2505 - val_loss: 39.8616 - val_mae: 4.3513\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.0092 - mae: 4.2620 - val_loss: 39.8375 - val_mae: 4.3573\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.3453 - mae: 4.2268 - val_loss: 39.8620 - val_mae: 4.3845\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.0523 - mae: 4.3265 - val_loss: 39.8387 - val_mae: 4.3359\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.8342 - mae: 4.2745 - val_loss: 39.8970 - val_mae: 4.3134\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.6827 - mae: 4.2350 - val_loss: 39.7764 - val_mae: 4.3390\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.7030 - mae: 4.2334 - val_loss: 39.7688 - val_mae: 4.3621\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.5989 - mae: 4.2742 - val_loss: 39.7488 - val_mae: 4.3233\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.5801 - mae: 4.2209 - val_loss: 39.7395 - val_mae: 4.3302\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.5362 - mae: 4.2562 - val_loss: 39.7876 - val_mae: 4.3142\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.4263 - mae: 4.2263 - val_loss: 39.7042 - val_mae: 4.3428\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.3232 - mae: 4.2071 - val_loss: 39.6786 - val_mae: 4.3222\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.1749 - mae: 4.2539 - val_loss: 39.7140 - val_mae: 4.3372\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.1172 - mae: 4.2285 - val_loss: 39.7170 - val_mae: 4.3143\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.0956 - mae: 4.2065 - val_loss: 39.7121 - val_mae: 4.3037\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.2598 - mae: 4.2593 - val_loss: 39.8095 - val_mae: 4.2801\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 40.0193 - mae: 4.1731 - val_loss: 39.6424 - val_mae: 4.3327\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 34.3787 - mae: 4.2905\n",
      "Mean Absolute Error on Test Data: 4.29053258895874\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.02427782693117808\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 69.1395 - mae: 6.5845 - val_loss: 63.9365 - val_mae: 6.2795\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 53.4882 - mae: 5.3568 - val_loss: 43.5495 - val_mae: 4.5762\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 34.0790 - mae: 3.6901 - val_loss: 25.9909 - val_mae: 3.4070\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1148 - mae: 3.3680 - val_loss: 24.5215 - val_mae: 3.5515\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.0841 - mae: 3.3545 - val_loss: 24.5597 - val_mae: 3.4625\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.7887 - mae: 3.3675 - val_loss: 24.3629 - val_mae: 3.4840\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.7625 - mae: 3.3405 - val_loss: 24.3080 - val_mae: 3.4772\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.6907 - mae: 3.3640 - val_loss: 24.2607 - val_mae: 3.4611\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.7108 - mae: 3.3166 - val_loss: 24.1823 - val_mae: 3.4686\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.6056 - mae: 3.3582 - val_loss: 24.1295 - val_mae: 3.4603\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.5274 - mae: 3.3436 - val_loss: 24.0756 - val_mae: 3.4562\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.5007 - mae: 3.3400 - val_loss: 24.0483 - val_mae: 3.4489\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.4737 - mae: 3.3466 - val_loss: 23.9835 - val_mae: 3.4476\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.4109 - mae: 3.3463 - val_loss: 23.9095 - val_mae: 3.4481\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.3951 - mae: 3.3549 - val_loss: 23.7920 - val_mae: 3.4540\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.4212 - mae: 3.3228 - val_loss: 23.7600 - val_mae: 3.4565\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.3847 - mae: 3.4272 - val_loss: 23.7728 - val_mae: 3.4412\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.5006 - mae: 3.3016 - val_loss: 23.6786 - val_mae: 3.4443\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.2613 - mae: 3.3196 - val_loss: 23.5779 - val_mae: 3.4535\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.1926 - mae: 3.3189 - val_loss: 23.5976 - val_mae: 3.4308\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.1101 - mae: 3.3462 - val_loss: 23.5753 - val_mae: 3.4202\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.0602 - mae: 3.3053 - val_loss: 23.5631 - val_mae: 3.4144\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.0458 - mae: 3.3437 - val_loss: 23.5175 - val_mae: 3.4177\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.0258 - mae: 3.2906 - val_loss: 23.3985 - val_mae: 3.4219\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9554 - mae: 3.3554 - val_loss: 23.4971 - val_mae: 3.4158\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8786 - mae: 3.2989 - val_loss: 23.4357 - val_mae: 3.4108\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8746 - mae: 3.3140 - val_loss: 23.5526 - val_mae: 3.3864\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7917 - mae: 3.2871 - val_loss: 23.3318 - val_mae: 3.4188\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7910 - mae: 3.2800 - val_loss: 23.3865 - val_mae: 3.4112\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7652 - mae: 3.3828 - val_loss: 23.3185 - val_mae: 3.4219\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7445 - mae: 3.2712 - val_loss: 23.2858 - val_mae: 3.4166\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6866 - mae: 3.3329 - val_loss: 23.3799 - val_mae: 3.3967\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6453 - mae: 3.2904 - val_loss: 23.2240 - val_mae: 3.4069\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5152 - mae: 3.2809 - val_loss: 23.2320 - val_mae: 3.4029\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4959 - mae: 3.3001 - val_loss: 23.1675 - val_mae: 3.4165\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5393 - mae: 3.2543 - val_loss: 23.1573 - val_mae: 3.4246\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4131 - mae: 3.2995 - val_loss: 23.2358 - val_mae: 3.3962\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4468 - mae: 3.2842 - val_loss: 23.1092 - val_mae: 3.4360\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3143 - mae: 3.3174 - val_loss: 23.3091 - val_mae: 3.3911\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3306 - mae: 3.2603 - val_loss: 23.3124 - val_mae: 3.3839\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2773 - mae: 3.2646 - val_loss: 23.1471 - val_mae: 3.4111\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2133 - mae: 3.2446 - val_loss: 23.1194 - val_mae: 3.4157\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3051 - mae: 3.2903 - val_loss: 23.5455 - val_mae: 3.3739\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0587 - mae: 3.2367 - val_loss: 23.0868 - val_mae: 3.4717\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0503 - mae: 3.2766 - val_loss: 23.2549 - val_mae: 3.3955\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0508 - mae: 3.2680 - val_loss: 23.0988 - val_mae: 3.4198\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0435 - mae: 3.2145 - val_loss: 23.2018 - val_mae: 3.4128\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0257 - mae: 3.2992 - val_loss: 23.3726 - val_mae: 3.3976\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9629 - mae: 3.2148 - val_loss: 23.0197 - val_mae: 3.4455\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.0018 - mae: 3.3067 - val_loss: 23.1397 - val_mae: 3.4154\n",
      "7/7 [==============================] - 0s 841us/step - loss: 18.1157 - mae: 3.2669\n",
      "Mean Absolute Error on Test Data: 3.266922950744629\n",
      "7/7 [==============================] - 0s 942us/step\n",
      "R-squared: 0.021682051447897255\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 198.4521 - mae: 12.1958 - val_loss: 158.1399 - val_mae: 10.5000\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 152.9247 - mae: 10.1888 - val_loss: 106.7260 - val_mae: 7.9743\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 90.2176 - mae: 6.9834 - val_loss: 56.7274 - val_mae: 5.4815\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.5508 - mae: 5.2835 - val_loss: 52.6767 - val_mae: 5.6088\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.2942 - mae: 5.3797 - val_loss: 51.7053 - val_mae: 5.5156\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.8621 - mae: 5.2820 - val_loss: 51.0812 - val_mae: 5.4464\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.6961 - mae: 5.2718 - val_loss: 51.3793 - val_mae: 5.4880\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.5828 - mae: 5.2982 - val_loss: 51.0909 - val_mae: 5.4578\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.4736 - mae: 5.2501 - val_loss: 50.7769 - val_mae: 5.4221\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.4466 - mae: 5.2933 - val_loss: 50.8125 - val_mae: 5.4365\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.2338 - mae: 5.2457 - val_loss: 50.5479 - val_mae: 5.4127\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.0863 - mae: 5.2553 - val_loss: 50.4931 - val_mae: 5.4220\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 49.0188 - mae: 5.2322 - val_loss: 50.3109 - val_mae: 5.4129\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.8503 - mae: 5.2615 - val_loss: 50.2834 - val_mae: 5.4182\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.7802 - mae: 5.2308 - val_loss: 50.3077 - val_mae: 5.4340\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.7242 - mae: 5.2476 - val_loss: 50.0374 - val_mae: 5.4179\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.6889 - mae: 5.2143 - val_loss: 49.9599 - val_mae: 5.4177\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.4495 - mae: 5.2458 - val_loss: 50.1120 - val_mae: 5.4456\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.4289 - mae: 5.1982 - val_loss: 49.4768 - val_mae: 5.3813\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.2219 - mae: 5.2315 - val_loss: 50.1288 - val_mae: 5.4658\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1391 - mae: 5.2372 - val_loss: 49.2522 - val_mae: 5.3702\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.1912 - mae: 5.1813 - val_loss: 49.4534 - val_mae: 5.4062\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9514 - mae: 5.2268 - val_loss: 49.2887 - val_mae: 5.3955\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9005 - mae: 5.1968 - val_loss: 48.5911 - val_mae: 5.3033\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.2536 - mae: 5.2810 - val_loss: 48.6489 - val_mae: 5.3246\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.1184 - mae: 5.1926 - val_loss: 48.4146 - val_mae: 5.3013\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.8822 - mae: 5.1621 - val_loss: 48.5659 - val_mae: 5.3413\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.6199 - mae: 5.1761 - val_loss: 48.4925 - val_mae: 5.3469\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.6077 - mae: 5.1914 - val_loss: 48.4644 - val_mae: 5.3505\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.4903 - mae: 5.1938 - val_loss: 48.1291 - val_mae: 5.3174\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.5939 - mae: 5.1810 - val_loss: 48.8531 - val_mae: 5.4148\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.3916 - mae: 5.2109 - val_loss: 47.9233 - val_mae: 5.3181\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.3533 - mae: 5.1410 - val_loss: 47.9957 - val_mae: 5.3362\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.2958 - mae: 5.1902 - val_loss: 47.9883 - val_mae: 5.3501\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.2452 - mae: 5.1601 - val_loss: 47.8328 - val_mae: 5.3361\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.2258 - mae: 5.2168 - val_loss: 47.0347 - val_mae: 5.2425\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.2574 - mae: 5.1638 - val_loss: 47.1054 - val_mae: 5.2721\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.1139 - mae: 5.1307 - val_loss: 47.5922 - val_mae: 5.3469\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.9630 - mae: 5.1938 - val_loss: 46.8881 - val_mae: 5.2648\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.0017 - mae: 5.1355 - val_loss: 46.9180 - val_mae: 5.2791\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.9075 - mae: 5.1440 - val_loss: 46.7595 - val_mae: 5.2693\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.8927 - mae: 5.1978 - val_loss: 47.3483 - val_mae: 5.3478\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.8862 - mae: 5.1176 - val_loss: 46.5525 - val_mae: 5.2670\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.9642 - mae: 5.2199 - val_loss: 46.2968 - val_mae: 5.2449\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.1101 - mae: 5.0973 - val_loss: 47.4284 - val_mae: 5.3836\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.8711 - mae: 5.1896 - val_loss: 46.3802 - val_mae: 5.2789\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.8921 - mae: 5.1837 - val_loss: 46.3104 - val_mae: 5.2854\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.5489 - mae: 5.1680 - val_loss: 45.6032 - val_mae: 5.1982\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.4252 - mae: 5.1120 - val_loss: 46.1481 - val_mae: 5.2856\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 46.4550 - mae: 5.1593 - val_loss: 45.8242 - val_mae: 5.2555\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 40.7088 - mae: 5.0327\n",
      "Mean Absolute Error on Test Data: 5.032690048217773\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.044980915290053214\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 24.4581 - mae: 3.8093 - val_loss: 16.1398 - val_mae: 2.9672\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.4089 - mae: 2.7168 - val_loss: 8.7876 - val_mae: 2.2473\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.5209 - mae: 2.3902 - val_loss: 8.7929 - val_mae: 2.4308\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.0331 - mae: 2.3934 - val_loss: 8.3864 - val_mae: 2.3188\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.0610 - mae: 2.3342 - val_loss: 8.4310 - val_mae: 2.3349\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.0408 - mae: 2.3599 - val_loss: 8.4121 - val_mae: 2.3266\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.0325 - mae: 2.3689 - val_loss: 8.4387 - val_mae: 2.3361\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.0054 - mae: 2.3459 - val_loss: 8.3956 - val_mae: 2.3206\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9993 - mae: 2.3551 - val_loss: 8.4748 - val_mae: 2.3496\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0147 - mae: 2.3307 - val_loss: 8.3635 - val_mae: 2.3070\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9870 - mae: 2.3643 - val_loss: 8.5079 - val_mae: 2.3543\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9595 - mae: 2.3484 - val_loss: 8.4769 - val_mae: 2.3478\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9693 - mae: 2.3811 - val_loss: 8.4478 - val_mae: 2.3367\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9323 - mae: 2.3308 - val_loss: 8.4011 - val_mae: 2.3203\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9230 - mae: 2.3197 - val_loss: 8.4435 - val_mae: 2.3357\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9136 - mae: 2.3428 - val_loss: 8.4089 - val_mae: 2.3290\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9338 - mae: 2.3785 - val_loss: 8.3814 - val_mae: 2.3182\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.9242 - mae: 2.3216 - val_loss: 8.4565 - val_mae: 2.3339\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8899 - mae: 2.3482 - val_loss: 8.4545 - val_mae: 2.3286\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8577 - mae: 2.3152 - val_loss: 8.4493 - val_mae: 2.3318\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8458 - mae: 2.3349 - val_loss: 8.4798 - val_mae: 2.3418\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8525 - mae: 2.3274 - val_loss: 8.4974 - val_mae: 2.3519\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8572 - mae: 2.3536 - val_loss: 8.3685 - val_mae: 2.3161\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8216 - mae: 2.3343 - val_loss: 8.4602 - val_mae: 2.3410\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8032 - mae: 2.3475 - val_loss: 8.3625 - val_mae: 2.3046\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8388 - mae: 2.2881 - val_loss: 8.4151 - val_mae: 2.3332\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8987 - mae: 2.3838 - val_loss: 8.3253 - val_mae: 2.2916\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8476 - mae: 2.3241 - val_loss: 8.4924 - val_mae: 2.3476\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8266 - mae: 2.2925 - val_loss: 8.4252 - val_mae: 2.3281\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8469 - mae: 2.3504 - val_loss: 8.6382 - val_mae: 2.3804\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.8281 - mae: 2.3016 - val_loss: 8.4069 - val_mae: 2.3183\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7705 - mae: 2.3557 - val_loss: 8.3839 - val_mae: 2.3140\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7120 - mae: 2.3005 - val_loss: 8.4117 - val_mae: 2.3149\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7030 - mae: 2.3211 - val_loss: 8.4556 - val_mae: 2.3235\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7191 - mae: 2.2928 - val_loss: 8.4388 - val_mae: 2.3263\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7458 - mae: 2.3708 - val_loss: 8.3444 - val_mae: 2.2795\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7013 - mae: 2.2848 - val_loss: 8.4614 - val_mae: 2.3233\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6749 - mae: 2.3050 - val_loss: 8.4921 - val_mae: 2.3204\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6452 - mae: 2.2918 - val_loss: 8.4400 - val_mae: 2.3107\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6513 - mae: 2.3260 - val_loss: 8.3170 - val_mae: 2.2731\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.7315 - mae: 2.3250 - val_loss: 8.4578 - val_mae: 2.3085\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6616 - mae: 2.2713 - val_loss: 8.4811 - val_mae: 2.3248\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6487 - mae: 2.3259 - val_loss: 8.3780 - val_mae: 2.2963\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6287 - mae: 2.3178 - val_loss: 8.4350 - val_mae: 2.2918\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.6230 - mae: 2.3163 - val_loss: 8.5690 - val_mae: 2.3441\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5713 - mae: 2.2854 - val_loss: 8.3464 - val_mae: 2.2757\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.5714 - mae: 2.2864 - val_loss: 8.4920 - val_mae: 2.3221\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.5601 - mae: 2.2869 - val_loss: 8.4993 - val_mae: 2.3239\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.5455 - mae: 2.2811 - val_loss: 8.4680 - val_mae: 2.3178\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.5346 - mae: 2.2844 - val_loss: 8.5320 - val_mae: 2.3387\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.0275 - mae: 2.2846\n",
      "Mean Absolute Error on Test Data: 2.284630298614502\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.02978264590287938\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 37.2915 - mae: 4.9828 - val_loss: 32.8360 - val_mae: 4.3574\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0506 - mae: 3.4108 - val_loss: 17.4718 - val_mae: 2.8217\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.7092 - mae: 2.6153 - val_loss: 14.4341 - val_mae: 2.7930\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.4543 - mae: 2.6710 - val_loss: 14.3963 - val_mae: 2.7045\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.3204 - mae: 2.6021 - val_loss: 14.3129 - val_mae: 2.7140\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.2721 - mae: 2.6238 - val_loss: 14.2650 - val_mae: 2.7113\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.2464 - mae: 2.6039 - val_loss: 14.2416 - val_mae: 2.7014\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.2076 - mae: 2.6102 - val_loss: 14.1931 - val_mae: 2.7006\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.2041 - mae: 2.6002 - val_loss: 14.2107 - val_mae: 2.6858\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.1741 - mae: 2.6239 - val_loss: 14.0936 - val_mae: 2.7045\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.1825 - mae: 2.5913 - val_loss: 14.1227 - val_mae: 2.6874\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.1434 - mae: 2.6226 - val_loss: 14.0429 - val_mae: 2.6965\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.1060 - mae: 2.6211 - val_loss: 14.0321 - val_mae: 2.6877\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0730 - mae: 2.5955 - val_loss: 13.9704 - val_mae: 2.6964\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0639 - mae: 2.6175 - val_loss: 13.9684 - val_mae: 2.6852\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0333 - mae: 2.5916 - val_loss: 13.9320 - val_mae: 2.6880\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0041 - mae: 2.6000 - val_loss: 13.9120 - val_mae: 2.6874\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0019 - mae: 2.6200 - val_loss: 13.9436 - val_mae: 2.6730\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0759 - mae: 2.5803 - val_loss: 13.8537 - val_mae: 2.6993\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9962 - mae: 2.6092 - val_loss: 13.8898 - val_mae: 2.6771\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0522 - mae: 2.6473 - val_loss: 14.0268 - val_mae: 2.6534\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 12.0935 - mae: 2.5758 - val_loss: 13.8069 - val_mae: 2.7049\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9714 - mae: 2.6214 - val_loss: 13.9445 - val_mae: 2.6583\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9103 - mae: 2.5828 - val_loss: 13.7881 - val_mae: 2.7035\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9649 - mae: 2.6159 - val_loss: 14.0108 - val_mae: 2.6456\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9703 - mae: 2.6067 - val_loss: 13.8033 - val_mae: 2.6734\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8749 - mae: 2.5772 - val_loss: 13.7680 - val_mae: 2.6789\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8943 - mae: 2.6096 - val_loss: 13.7874 - val_mae: 2.6693\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8885 - mae: 2.5936 - val_loss: 13.7406 - val_mae: 2.6777\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8840 - mae: 2.6292 - val_loss: 13.7750 - val_mae: 2.6628\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8327 - mae: 2.5780 - val_loss: 13.7856 - val_mae: 2.6607\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8838 - mae: 2.5819 - val_loss: 13.7160 - val_mae: 2.7289\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9469 - mae: 2.6459 - val_loss: 13.8725 - val_mae: 2.6435\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8985 - mae: 2.6012 - val_loss: 13.6847 - val_mae: 2.6792\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.9115 - mae: 2.5819 - val_loss: 13.6595 - val_mae: 2.6857\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7522 - mae: 2.6010 - val_loss: 13.7297 - val_mae: 2.6646\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.8134 - mae: 2.5952 - val_loss: 13.6662 - val_mae: 2.6875\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7852 - mae: 2.5871 - val_loss: 13.6636 - val_mae: 2.6777\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7864 - mae: 2.5866 - val_loss: 13.6572 - val_mae: 2.6857\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7263 - mae: 2.5801 - val_loss: 13.7156 - val_mae: 2.6633\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7401 - mae: 2.6104 - val_loss: 13.6827 - val_mae: 2.6693\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7605 - mae: 2.5677 - val_loss: 13.6506 - val_mae: 2.6823\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7585 - mae: 2.6244 - val_loss: 13.6837 - val_mae: 2.6541\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7202 - mae: 2.5625 - val_loss: 13.6181 - val_mae: 2.6852\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7456 - mae: 2.6143 - val_loss: 13.7014 - val_mae: 2.6583\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7123 - mae: 2.6141 - val_loss: 13.6459 - val_mae: 2.6697\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7328 - mae: 2.5539 - val_loss: 13.6111 - val_mae: 2.6803\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6736 - mae: 2.6238 - val_loss: 13.5935 - val_mae: 2.6695\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.7147 - mae: 2.5724 - val_loss: 13.6068 - val_mae: 2.6747\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 11.6795 - mae: 2.5969 - val_loss: 13.6671 - val_mae: 2.6496\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.2123 - mae: 2.9703\n",
      "Mean Absolute Error on Test Data: 2.970276355743408\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.10344053638128436\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 25.3609 - mae: 4.1474 - val_loss: 24.1390 - val_mae: 3.7866\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7974 - mae: 3.0740 - val_loss: 14.3385 - val_mae: 2.7141\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 9.3226 - mae: 2.1760 - val_loss: 9.8607 - val_mae: 2.4094\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.1447 - mae: 2.1791 - val_loss: 9.8701 - val_mae: 2.3951\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0766 - mae: 2.1237 - val_loss: 9.8860 - val_mae: 2.3863\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0670 - mae: 2.1425 - val_loss: 9.8914 - val_mae: 2.3813\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0378 - mae: 2.1240 - val_loss: 9.8772 - val_mae: 2.3830\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0369 - mae: 2.1260 - val_loss: 9.8542 - val_mae: 2.3880\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0154 - mae: 2.1291 - val_loss: 9.8724 - val_mae: 2.3783\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0305 - mae: 2.1411 - val_loss: 9.9042 - val_mae: 2.3724\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 8.0132 - mae: 2.1093 - val_loss: 9.8725 - val_mae: 2.3766\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0168 - mae: 2.1377 - val_loss: 9.8769 - val_mae: 2.3736\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9877 - mae: 2.1217 - val_loss: 9.8823 - val_mae: 2.3726\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9806 - mae: 2.1205 - val_loss: 9.8535 - val_mae: 2.3771\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9781 - mae: 2.1166 - val_loss: 9.8783 - val_mae: 2.3715\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9784 - mae: 2.1299 - val_loss: 9.8764 - val_mae: 2.3704\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9846 - mae: 2.1177 - val_loss: 9.8476 - val_mae: 2.3818\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9771 - mae: 2.1157 - val_loss: 9.8480 - val_mae: 2.3774\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9538 - mae: 2.1216 - val_loss: 9.8848 - val_mae: 2.3669\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9655 - mae: 2.1287 - val_loss: 9.9151 - val_mae: 2.3653\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9593 - mae: 2.1235 - val_loss: 9.9310 - val_mae: 2.3619\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9409 - mae: 2.1122 - val_loss: 9.8682 - val_mae: 2.3711\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9448 - mae: 2.1143 - val_loss: 9.8805 - val_mae: 2.3717\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9471 - mae: 2.1283 - val_loss: 9.8657 - val_mae: 2.3693\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9675 - mae: 2.0990 - val_loss: 9.8772 - val_mae: 2.3685\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9465 - mae: 2.1341 - val_loss: 9.9022 - val_mae: 2.3632\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9304 - mae: 2.1071 - val_loss: 9.8941 - val_mae: 2.3625\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9583 - mae: 2.1149 - val_loss: 9.9037 - val_mae: 2.3620\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9285 - mae: 2.1216 - val_loss: 9.8756 - val_mae: 2.3655\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9506 - mae: 2.1176 - val_loss: 9.8562 - val_mae: 2.3733\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9213 - mae: 2.1057 - val_loss: 9.9485 - val_mae: 2.3543\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9287 - mae: 2.1214 - val_loss: 9.8587 - val_mae: 2.3754\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9481 - mae: 2.1013 - val_loss: 9.8593 - val_mae: 2.3720\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9322 - mae: 2.1204 - val_loss: 9.9107 - val_mae: 2.3576\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9039 - mae: 2.1154 - val_loss: 9.8652 - val_mae: 2.3678\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9122 - mae: 2.1164 - val_loss: 9.8750 - val_mae: 2.3608\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9569 - mae: 2.1018 - val_loss: 9.8434 - val_mae: 2.3859\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9079 - mae: 2.1345 - val_loss: 9.8946 - val_mae: 2.3591\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9518 - mae: 2.0847 - val_loss: 9.8712 - val_mae: 2.3660\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9563 - mae: 2.1390 - val_loss: 9.8792 - val_mae: 2.3582\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8856 - mae: 2.1087 - val_loss: 9.8611 - val_mae: 2.3659\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8878 - mae: 2.1096 - val_loss: 9.8769 - val_mae: 2.3599\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9207 - mae: 2.1366 - val_loss: 9.9784 - val_mae: 2.3468\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9739 - mae: 2.1067 - val_loss: 9.9451 - val_mae: 2.3485\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8962 - mae: 2.0805 - val_loss: 9.8478 - val_mae: 2.3733\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9310 - mae: 2.1532 - val_loss: 9.9814 - val_mae: 2.3474\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9661 - mae: 2.0795 - val_loss: 9.8256 - val_mae: 2.3798\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.9254 - mae: 2.1444 - val_loss: 9.8919 - val_mae: 2.3516\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8963 - mae: 2.0934 - val_loss: 9.8334 - val_mae: 2.3680\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 7.8938 - mae: 2.1322 - val_loss: 9.9577 - val_mae: 2.3462\n",
      "7/7 [==============================] - 0s 833us/step - loss: 10.3632 - mae: 2.2900\n",
      "Mean Absolute Error on Test Data: 2.290024995803833\n",
      "7/7 [==============================] - 0s 894us/step\n",
      "R-squared: -0.002890469337382795\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 7.5957 - mae: 1.8710 - val_loss: 5.8982 - val_mae: 1.6765\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.8492 - mae: 1.4842 - val_loss: 4.3856 - val_mae: 1.5572\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4612 - mae: 1.5414 - val_loss: 4.3905 - val_mae: 1.5482\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4306 - mae: 1.4967 - val_loss: 4.4396 - val_mae: 1.5320\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4435 - mae: 1.4775 - val_loss: 4.4140 - val_mae: 1.5394\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4276 - mae: 1.5185 - val_loss: 4.3922 - val_mae: 1.5482\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4159 - mae: 1.5050 - val_loss: 4.4200 - val_mae: 1.5378\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4279 - mae: 1.4854 - val_loss: 4.4012 - val_mae: 1.5422\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4120 - mae: 1.5053 - val_loss: 4.4026 - val_mae: 1.5382\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4055 - mae: 1.4867 - val_loss: 4.4155 - val_mae: 1.5347\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4003 - mae: 1.4824 - val_loss: 4.4240 - val_mae: 1.5323\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4007 - mae: 1.5127 - val_loss: 4.3781 - val_mae: 1.5423\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3937 - mae: 1.4931 - val_loss: 4.4055 - val_mae: 1.5378\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3870 - mae: 1.4983 - val_loss: 4.4099 - val_mae: 1.5344\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3854 - mae: 1.5009 - val_loss: 4.4175 - val_mae: 1.5340\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3814 - mae: 1.4906 - val_loss: 4.4097 - val_mae: 1.5356\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3812 - mae: 1.4941 - val_loss: 4.4119 - val_mae: 1.5364\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3756 - mae: 1.4965 - val_loss: 4.4001 - val_mae: 1.5386\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3700 - mae: 1.4877 - val_loss: 4.4026 - val_mae: 1.5367\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3698 - mae: 1.4960 - val_loss: 4.3675 - val_mae: 1.5498\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3761 - mae: 1.5012 - val_loss: 4.4315 - val_mae: 1.5313\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3695 - mae: 1.5000 - val_loss: 4.4095 - val_mae: 1.5363\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3582 - mae: 1.4913 - val_loss: 4.3995 - val_mae: 1.5401\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3572 - mae: 1.5051 - val_loss: 4.4139 - val_mae: 1.5370\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3554 - mae: 1.4820 - val_loss: 4.4168 - val_mae: 1.5354\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3517 - mae: 1.5014 - val_loss: 4.4196 - val_mae: 1.5361\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3602 - mae: 1.4773 - val_loss: 4.3928 - val_mae: 1.5437\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3649 - mae: 1.5352 - val_loss: 4.4432 - val_mae: 1.5323\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3505 - mae: 1.4645 - val_loss: 4.4594 - val_mae: 1.5276\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3390 - mae: 1.4884 - val_loss: 4.3803 - val_mae: 1.5455\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3385 - mae: 1.5065 - val_loss: 4.4365 - val_mae: 1.5323\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3417 - mae: 1.5042 - val_loss: 4.4220 - val_mae: 1.5373\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3731 - mae: 1.4556 - val_loss: 4.4040 - val_mae: 1.5406\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3401 - mae: 1.5025 - val_loss: 4.3939 - val_mae: 1.5403\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3488 - mae: 1.5143 - val_loss: 4.4050 - val_mae: 1.5399\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3534 - mae: 1.4624 - val_loss: 4.4111 - val_mae: 1.5333\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3440 - mae: 1.5252 - val_loss: 4.3704 - val_mae: 1.5544\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4499 - mae: 1.4649 - val_loss: 4.3956 - val_mae: 1.5376\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 4.4070 - mae: 1.5639 - val_loss: 4.4485 - val_mae: 1.5292\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3447 - mae: 1.4654 - val_loss: 4.4063 - val_mae: 1.5351\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3164 - mae: 1.4849 - val_loss: 4.4220 - val_mae: 1.5355\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3461 - mae: 1.4715 - val_loss: 4.3808 - val_mae: 1.5474\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3430 - mae: 1.5063 - val_loss: 4.3882 - val_mae: 1.5453\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3177 - mae: 1.5124 - val_loss: 4.4467 - val_mae: 1.5324\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3006 - mae: 1.4811 - val_loss: 4.4035 - val_mae: 1.5405\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3029 - mae: 1.4838 - val_loss: 4.4246 - val_mae: 1.5360\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.2950 - mae: 1.4857 - val_loss: 4.4083 - val_mae: 1.5402\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3037 - mae: 1.5003 - val_loss: 4.4044 - val_mae: 1.5415\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.2985 - mae: 1.5022 - val_loss: 4.4690 - val_mae: 1.5287\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.3091 - mae: 1.4832 - val_loss: 4.3858 - val_mae: 1.5473\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.3127 - mae: 1.4449\n",
      "Mean Absolute Error on Test Data: 1.4448716640472412\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.015883510069415352\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 7.0226 - mae: 1.8856 - val_loss: 5.1514 - val_mae: 1.4863\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.3344 - mae: 1.3830 - val_loss: 3.4529 - val_mae: 1.3009\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.7309 - mae: 1.4184 - val_loss: 3.3980 - val_mae: 1.3564\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.7030 - mae: 1.4115 - val_loss: 3.4051 - val_mae: 1.3151\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.7107 - mae: 1.3560 - val_loss: 3.4261 - val_mae: 1.2999\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6484 - mae: 1.3742 - val_loss: 3.3876 - val_mae: 1.3361\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6369 - mae: 1.3798 - val_loss: 3.4042 - val_mae: 1.3152\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6233 - mae: 1.3817 - val_loss: 3.4068 - val_mae: 1.3225\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6211 - mae: 1.3529 - val_loss: 3.4258 - val_mae: 1.3141\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5991 - mae: 1.3655 - val_loss: 3.4242 - val_mae: 1.3177\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.6053 - mae: 1.3499 - val_loss: 3.4521 - val_mae: 1.3123\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5894 - mae: 1.3696 - val_loss: 3.4313 - val_mae: 1.3277\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5803 - mae: 1.3811 - val_loss: 3.4363 - val_mae: 1.3310\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.5740 - mae: 1.3710 - val_loss: 3.4496 - val_mae: 1.3188\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5762 - mae: 1.3379 - val_loss: 3.4575 - val_mae: 1.3145\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5647 - mae: 1.3784 - val_loss: 3.4444 - val_mae: 1.3381\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5770 - mae: 1.3506 - val_loss: 3.4560 - val_mae: 1.3272\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5578 - mae: 1.3769 - val_loss: 3.4494 - val_mae: 1.3377\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5719 - mae: 1.3502 - val_loss: 3.4436 - val_mae: 1.3574\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5692 - mae: 1.4059 - val_loss: 3.4404 - val_mae: 1.3389\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5416 - mae: 1.3708 - val_loss: 3.4405 - val_mae: 1.3266\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5456 - mae: 1.3463 - val_loss: 3.4387 - val_mae: 1.3307\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.5357 - mae: 1.3478 - val_loss: 3.4351 - val_mae: 1.3356\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5399 - mae: 1.3570 - val_loss: 3.4302 - val_mae: 1.3376\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5460 - mae: 1.3886 - val_loss: 3.4326 - val_mae: 1.3625\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5289 - mae: 1.3854 - val_loss: 3.4328 - val_mae: 1.3310\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5390 - mae: 1.3778 - val_loss: 3.4269 - val_mae: 1.3368\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5249 - mae: 1.3447 - val_loss: 3.4456 - val_mae: 1.3171\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5144 - mae: 1.3484 - val_loss: 3.4359 - val_mae: 1.3314\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5172 - mae: 1.3693 - val_loss: 3.4520 - val_mae: 1.3209\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5188 - mae: 1.3313 - val_loss: 3.4448 - val_mae: 1.3205\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5210 - mae: 1.3781 - val_loss: 3.4325 - val_mae: 1.3347\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5098 - mae: 1.3432 - val_loss: 3.4521 - val_mae: 1.3125\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5076 - mae: 1.3459 - val_loss: 3.4385 - val_mae: 1.3267\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5005 - mae: 1.3578 - val_loss: 3.4234 - val_mae: 1.3585\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5423 - mae: 1.4154 - val_loss: 3.4132 - val_mae: 1.3608\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5139 - mae: 1.3414 - val_loss: 3.4245 - val_mae: 1.3163\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4975 - mae: 1.3585 - val_loss: 3.4062 - val_mae: 1.3500\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4976 - mae: 1.3740 - val_loss: 3.4121 - val_mae: 1.3278\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.5254 - mae: 1.3211 - val_loss: 3.4084 - val_mae: 1.3270\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4820 - mae: 1.3691 - val_loss: 3.4015 - val_mae: 1.3465\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4890 - mae: 1.3437 - val_loss: 3.4172 - val_mae: 1.3143\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4845 - mae: 1.3276 - val_loss: 3.4108 - val_mae: 1.3266\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4843 - mae: 1.3433 - val_loss: 3.4021 - val_mae: 1.3369\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4755 - mae: 1.3679 - val_loss: 3.4169 - val_mae: 1.3163\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4968 - mae: 1.3150 - val_loss: 3.4067 - val_mae: 1.3282\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4822 - mae: 1.3803 - val_loss: 3.4020 - val_mae: 1.3336\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4661 - mae: 1.3393 - val_loss: 3.3983 - val_mae: 1.3310\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4637 - mae: 1.3656 - val_loss: 3.4023 - val_mae: 1.3567\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 3.4844 - mae: 1.3884 - val_loss: 3.4308 - val_mae: 1.3129\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.2489 - mae: 1.5411\n",
      "Mean Absolute Error on Test Data: 1.541091799736023\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.01876634850857639\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 42.5353 - mae: 5.0681 - val_loss: 49.2486 - val_mae: 4.4674\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 29.4010 - mae: 3.8618 - val_loss: 35.6111 - val_mae: 3.4154\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 18.7196 - mae: 3.0635 - val_loss: 29.5168 - val_mae: 3.4612\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.4260 - mae: 3.1946 - val_loss: 29.4309 - val_mae: 3.4711\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.2519 - mae: 3.0982 - val_loss: 29.3876 - val_mae: 3.4131\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.2110 - mae: 3.1109 - val_loss: 29.2770 - val_mae: 3.4132\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1625 - mae: 3.0985 - val_loss: 29.2091 - val_mae: 3.4309\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1302 - mae: 3.0901 - val_loss: 29.1732 - val_mae: 3.3979\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.1304 - mae: 3.1053 - val_loss: 29.0713 - val_mae: 3.4135\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.0951 - mae: 3.0771 - val_loss: 28.9912 - val_mae: 3.4174\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0375 - mae: 3.1023 - val_loss: 28.9425 - val_mae: 3.4098\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 17.0066 - mae: 3.0759 - val_loss: 28.9034 - val_mae: 3.3970\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9845 - mae: 3.1008 - val_loss: 28.8293 - val_mae: 3.3937\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9393 - mae: 3.0859 - val_loss: 28.8517 - val_mae: 3.3785\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9219 - mae: 3.0473 - val_loss: 28.7968 - val_mae: 3.3799\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8719 - mae: 3.0815 - val_loss: 28.6684 - val_mae: 3.3898\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8422 - mae: 3.0591 - val_loss: 28.6582 - val_mae: 3.3870\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.9451 - mae: 3.1159 - val_loss: 28.7032 - val_mae: 3.3333\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8838 - mae: 3.0654 - val_loss: 28.5826 - val_mae: 3.3909\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.7562 - mae: 3.0511 - val_loss: 28.6186 - val_mae: 3.3340\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6910 - mae: 3.0336 - val_loss: 28.5520 - val_mae: 3.3885\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.6949 - mae: 3.0703 - val_loss: 28.4611 - val_mae: 3.3621\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.6628 - mae: 3.0628 - val_loss: 28.4388 - val_mae: 3.3847\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6063 - mae: 3.0393 - val_loss: 28.4599 - val_mae: 3.3399\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.6451 - mae: 3.0474 - val_loss: 28.3245 - val_mae: 3.3441\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5893 - mae: 3.0444 - val_loss: 28.3371 - val_mae: 3.3436\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5992 - mae: 3.0469 - val_loss: 28.2732 - val_mae: 3.3657\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.5264 - mae: 3.0167 - val_loss: 28.2464 - val_mae: 3.3498\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4715 - mae: 3.0334 - val_loss: 28.2431 - val_mae: 3.3813\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4397 - mae: 3.0445 - val_loss: 28.2709 - val_mae: 3.3459\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4572 - mae: 2.9894 - val_loss: 28.1411 - val_mae: 3.3575\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.4569 - mae: 3.0730 - val_loss: 28.1774 - val_mae: 3.3234\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3957 - mae: 2.9938 - val_loss: 28.1510 - val_mae: 3.3534\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3388 - mae: 3.0135 - val_loss: 28.2098 - val_mae: 3.3322\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.3364 - mae: 3.0178 - val_loss: 28.1376 - val_mae: 3.3674\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2640 - mae: 3.0225 - val_loss: 28.0527 - val_mae: 3.3262\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2585 - mae: 2.9788 - val_loss: 28.1077 - val_mae: 3.3097\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2429 - mae: 3.0316 - val_loss: 27.9800 - val_mae: 3.3642\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.2453 - mae: 2.9728 - val_loss: 28.1371 - val_mae: 3.3490\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1557 - mae: 3.0268 - val_loss: 28.0604 - val_mae: 3.3532\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.1471 - mae: 2.9828 - val_loss: 28.0485 - val_mae: 3.2830\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0690 - mae: 3.0000 - val_loss: 27.9560 - val_mae: 3.3499\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0473 - mae: 2.9915 - val_loss: 28.0491 - val_mae: 3.3304\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0062 - mae: 2.9974 - val_loss: 28.0831 - val_mae: 3.2909\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0108 - mae: 2.9588 - val_loss: 28.0495 - val_mae: 3.3547\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0257 - mae: 2.9668 - val_loss: 28.0411 - val_mae: 3.3453\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.9842 - mae: 3.0102 - val_loss: 28.0576 - val_mae: 3.3120\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.0082 - mae: 3.0042 - val_loss: 28.1081 - val_mae: 3.2935\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.9735 - mae: 2.9341 - val_loss: 28.0866 - val_mae: 3.3421\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.8809 - mae: 3.0076 - val_loss: 28.1649 - val_mae: 3.2720\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 19.1975 - mae: 3.2764\n",
      "Mean Absolute Error on Test Data: 3.276385545730591\n",
      "7/7 [==============================] - 0s 948us/step\n",
      "R-squared: 0.06833088195790105\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 43.3119 - mae: 4.4197 - val_loss: 33.1807 - val_mae: 3.7815\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.9601 - mae: 3.4671 - val_loss: 23.2340 - val_mae: 2.9080\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 25.5089 - mae: 3.0911 - val_loss: 20.7388 - val_mae: 3.1247\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 24.2279 - mae: 3.1762 - val_loss: 20.2513 - val_mae: 3.0605\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 24.0078 - mae: 3.0971 - val_loss: 19.8752 - val_mae: 2.9868\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 23.6199 - mae: 3.1347 - val_loss: 19.6193 - val_mae: 3.0596\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 23.2917 - mae: 3.0963 - val_loss: 19.1290 - val_mae: 2.9838\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.0885 - mae: 3.0220 - val_loss: 18.8922 - val_mae: 3.0041\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.8023 - mae: 3.0658 - val_loss: 18.5444 - val_mae: 2.9629\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.6311 - mae: 3.0456 - val_loss: 18.2471 - val_mae: 2.9171\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.4805 - mae: 2.9871 - val_loss: 18.0304 - val_mae: 2.9155\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.3125 - mae: 2.9948 - val_loss: 17.8340 - val_mae: 2.9179\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.2511 - mae: 2.9688 - val_loss: 17.6422 - val_mae: 2.8984\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 22.1110 - mae: 2.9574 - val_loss: 17.4682 - val_mae: 2.9040\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.9549 - mae: 2.9678 - val_loss: 17.2340 - val_mae: 2.8729\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.9277 - mae: 2.9500 - val_loss: 17.1518 - val_mae: 2.9192\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.8476 - mae: 2.9423 - val_loss: 16.8748 - val_mae: 2.8104\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.7242 - mae: 2.9790 - val_loss: 16.8372 - val_mae: 2.8976\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.7173 - mae: 2.9286 - val_loss: 16.5819 - val_mae: 2.8223\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.7187 - mae: 2.9509 - val_loss: 16.7681 - val_mae: 2.9419\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.5819 - mae: 2.9437 - val_loss: 16.4525 - val_mae: 2.8288\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.5584 - mae: 2.9269 - val_loss: 16.3213 - val_mae: 2.7833\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.5270 - mae: 2.8858 - val_loss: 16.5197 - val_mae: 2.9286\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.4951 - mae: 2.9935 - val_loss: 16.1804 - val_mae: 2.7754\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.4907 - mae: 2.8877 - val_loss: 16.2183 - val_mae: 2.8594\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.3859 - mae: 2.9274 - val_loss: 16.1345 - val_mae: 2.8460\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.3932 - mae: 2.8686 - val_loss: 16.0553 - val_mae: 2.8202\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2940 - mae: 2.9003 - val_loss: 16.2241 - val_mae: 2.9059\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.3551 - mae: 2.9613 - val_loss: 15.9512 - val_mae: 2.7779\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.2497 - mae: 2.9052 - val_loss: 15.9873 - val_mae: 2.8436\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.2253 - mae: 2.8851 - val_loss: 15.8321 - val_mae: 2.7819\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.2698 - mae: 2.9122 - val_loss: 15.8439 - val_mae: 2.7986\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1304 - mae: 2.8804 - val_loss: 15.9246 - val_mae: 2.8578\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1803 - mae: 2.9319 - val_loss: 15.7536 - val_mae: 2.7599\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1318 - mae: 2.8539 - val_loss: 15.9528 - val_mae: 2.8838\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1520 - mae: 2.9114 - val_loss: 15.8993 - val_mae: 2.8803\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0332 - mae: 2.9214 - val_loss: 15.7064 - val_mae: 2.8005\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1091 - mae: 2.8583 - val_loss: 15.9219 - val_mae: 2.9074\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.0602 - mae: 2.8847 - val_loss: 15.7070 - val_mae: 2.8275\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.0587 - mae: 2.9169 - val_loss: 15.6524 - val_mae: 2.7853\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.0068 - mae: 2.8751 - val_loss: 15.6724 - val_mae: 2.7841\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.1095 - mae: 2.9332 - val_loss: 15.5902 - val_mae: 2.7772\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 21.0053 - mae: 2.8368 - val_loss: 15.7917 - val_mae: 2.8699\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.9640 - mae: 2.8735 - val_loss: 15.7263 - val_mae: 2.8529\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.8739 - mae: 2.8976 - val_loss: 15.6841 - val_mae: 2.8205\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9043 - mae: 2.9023 - val_loss: 15.6307 - val_mae: 2.7997\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.8608 - mae: 2.8300 - val_loss: 15.7323 - val_mae: 2.8631\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.8343 - mae: 2.9020 - val_loss: 15.7364 - val_mae: 2.8503\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.9207 - mae: 2.9018 - val_loss: 15.7271 - val_mae: 2.8451\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 20.9105 - mae: 2.8198 - val_loss: 15.8322 - val_mae: 2.9015\n",
      "7/7 [==============================] - 0s 788us/step - loss: 20.6523 - mae: 3.2216\n",
      "Mean Absolute Error on Test Data: 3.221647024154663\n",
      "7/7 [==============================] - 0s 753us/step\n",
      "R-squared: 0.06507877403122864\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 33.4114 - mae: 4.3580 - val_loss: 48.6810 - val_mae: 4.4726\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.0225 - mae: 3.0578 - val_loss: 33.5841 - val_mae: 3.3973\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.2184 - mae: 2.9454 - val_loss: 30.8254 - val_mae: 3.5360\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.1359 - mae: 2.9147 - val_loss: 31.7466 - val_mae: 3.4092\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.9151 - mae: 2.8934 - val_loss: 31.1718 - val_mae: 3.4435\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8931 - mae: 2.9085 - val_loss: 31.2402 - val_mae: 3.4267\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8734 - mae: 2.9057 - val_loss: 31.3187 - val_mae: 3.4083\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8181 - mae: 2.8809 - val_loss: 31.1404 - val_mae: 3.4185\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8836 - mae: 2.8573 - val_loss: 30.9516 - val_mae: 3.4287\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7600 - mae: 2.9131 - val_loss: 30.9064 - val_mae: 3.4212\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6952 - mae: 2.8878 - val_loss: 31.0185 - val_mae: 3.4005\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7199 - mae: 2.8433 - val_loss: 30.7666 - val_mae: 3.4234\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6735 - mae: 2.9021 - val_loss: 31.1212 - val_mae: 3.3899\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6402 - mae: 2.8321 - val_loss: 30.7393 - val_mae: 3.4052\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5991 - mae: 2.8903 - val_loss: 30.9857 - val_mae: 3.3813\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6339 - mae: 2.8087 - val_loss: 30.5618 - val_mae: 3.4098\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5353 - mae: 2.8411 - val_loss: 30.6582 - val_mae: 3.3952\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5656 - mae: 2.8828 - val_loss: 30.8160 - val_mae: 3.3770\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.4823 - mae: 2.8511 - val_loss: 30.6124 - val_mae: 3.3889\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.4163 - mae: 2.8214 - val_loss: 30.6408 - val_mae: 3.3795\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.3853 - mae: 2.8228 - val_loss: 30.4387 - val_mae: 3.3914\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3498 - mae: 2.8241 - val_loss: 30.2619 - val_mae: 3.3959\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.3246 - mae: 2.8468 - val_loss: 30.6734 - val_mae: 3.3617\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.2849 - mae: 2.8166 - val_loss: 30.5257 - val_mae: 3.3642\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.2561 - mae: 2.7927 - val_loss: 30.0533 - val_mae: 3.3953\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.2155 - mae: 2.8224 - val_loss: 30.1705 - val_mae: 3.3743\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.1674 - mae: 2.8121 - val_loss: 30.4457 - val_mae: 3.3484\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.1802 - mae: 2.8069 - val_loss: 30.4500 - val_mae: 3.3386\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.1623 - mae: 2.7926 - val_loss: 30.0879 - val_mae: 3.3748\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0668 - mae: 2.7952 - val_loss: 30.1060 - val_mae: 3.3571\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.0577 - mae: 2.7764 - val_loss: 29.7406 - val_mae: 3.3946\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.0573 - mae: 2.8102 - val_loss: 30.2536 - val_mae: 3.3409\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.9589 - mae: 2.7674 - val_loss: 29.7806 - val_mae: 3.3730\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.9586 - mae: 2.7961 - val_loss: 30.2727 - val_mae: 3.3362\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.9341 - mae: 2.7611 - val_loss: 29.6793 - val_mae: 3.3625\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.9704 - mae: 2.8245 - val_loss: 30.1597 - val_mae: 3.3308\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.9328 - mae: 2.7495 - val_loss: 29.9100 - val_mae: 3.3574\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.8328 - mae: 2.7679 - val_loss: 30.2547 - val_mae: 3.3274\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8297 - mae: 2.7562 - val_loss: 30.1285 - val_mae: 3.3311\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.8092 - mae: 2.7666 - val_loss: 29.6949 - val_mae: 3.3458\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.8269 - mae: 2.7843 - val_loss: 30.7813 - val_mae: 3.3116\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.7525 - mae: 2.7310 - val_loss: 29.3893 - val_mae: 3.3726\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.7785 - mae: 2.7891 - val_loss: 30.7170 - val_mae: 3.3020\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.8543 - mae: 2.7654 - val_loss: 29.9622 - val_mae: 3.3472\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.7078 - mae: 2.7513 - val_loss: 30.3204 - val_mae: 3.3139\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.7090 - mae: 2.7478 - val_loss: 29.5347 - val_mae: 3.3750\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.7530 - mae: 2.7553 - val_loss: 30.0383 - val_mae: 3.3339\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6627 - mae: 2.7625 - val_loss: 29.9030 - val_mae: 3.3455\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.6227 - mae: 2.7482 - val_loss: 29.8756 - val_mae: 3.3530\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.6261 - mae: 2.7285 - val_loss: 29.7144 - val_mae: 3.3588\n",
      "7/7 [==============================] - 0s 951us/step - loss: 11.6252 - mae: 2.7046\n",
      "Mean Absolute Error on Test Data: 2.7046191692352295\n",
      "7/7 [==============================] - 0s 833us/step\n",
      "R-squared: 0.02599290325363357\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 70.9636 - mae: 6.7094 - val_loss: 50.7787 - val_mae: 5.6126\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.4287 - mae: 5.1822 - val_loss: 28.6116 - val_mae: 3.5767\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.3118 - mae: 3.5601 - val_loss: 19.6500 - val_mae: 3.2937\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0874 - mae: 3.6023 - val_loss: 19.6858 - val_mae: 3.3135\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9680 - mae: 3.5535 - val_loss: 19.3610 - val_mae: 3.2065\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8548 - mae: 3.4597 - val_loss: 19.5334 - val_mae: 3.2649\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7920 - mae: 3.5427 - val_loss: 19.4022 - val_mae: 3.2217\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.7686 - mae: 3.4507 - val_loss: 19.4402 - val_mae: 3.2349\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.9035 - mae: 3.5895 - val_loss: 19.5427 - val_mae: 3.2577\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6825 - mae: 3.4479 - val_loss: 19.3847 - val_mae: 3.1977\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6226 - mae: 3.4800 - val_loss: 19.4454 - val_mae: 3.2168\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5607 - mae: 3.4910 - val_loss: 19.5113 - val_mae: 3.2308\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5427 - mae: 3.4509 - val_loss: 19.5409 - val_mae: 3.2267\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6107 - mae: 3.5475 - val_loss: 19.5488 - val_mae: 3.2227\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5035 - mae: 3.4708 - val_loss: 19.5537 - val_mae: 3.2215\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4880 - mae: 3.4938 - val_loss: 19.4746 - val_mae: 3.1824\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5474 - mae: 3.4363 - val_loss: 19.6771 - val_mae: 3.2452\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.6743 - mae: 3.4619 - val_loss: 19.4263 - val_mae: 3.1430\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4586 - mae: 3.5190 - val_loss: 19.7215 - val_mae: 3.2540\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4184 - mae: 3.4613 - val_loss: 19.5719 - val_mae: 3.1991\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.5079 - mae: 3.4650 - val_loss: 19.5928 - val_mae: 3.2049\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4084 - mae: 3.4552 - val_loss: 19.6925 - val_mae: 3.2289\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.4444 - mae: 3.5402 - val_loss: 19.7472 - val_mae: 3.2390\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3998 - mae: 3.4438 - val_loss: 19.5779 - val_mae: 3.1758\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4813 - mae: 3.4773 - val_loss: 19.6692 - val_mae: 3.2234\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4883 - mae: 3.5583 - val_loss: 19.7266 - val_mae: 3.2227\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3743 - mae: 3.4703 - val_loss: 19.6710 - val_mae: 3.2063\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3736 - mae: 3.4535 - val_loss: 19.7097 - val_mae: 3.2100\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3576 - mae: 3.4956 - val_loss: 19.9165 - val_mae: 3.2702\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3936 - mae: 3.4746 - val_loss: 19.6900 - val_mae: 3.1830\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4162 - mae: 3.5128 - val_loss: 19.8763 - val_mae: 3.2512\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4338 - mae: 3.4345 - val_loss: 19.8340 - val_mae: 3.2516\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3232 - mae: 3.4870 - val_loss: 19.6635 - val_mae: 3.1904\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3528 - mae: 3.4276 - val_loss: 19.6761 - val_mae: 3.1936\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.4014 - mae: 3.4588 - val_loss: 19.8627 - val_mae: 3.2563\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3408 - mae: 3.4620 - val_loss: 19.8499 - val_mae: 3.2498\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2732 - mae: 3.5046 - val_loss: 19.8697 - val_mae: 3.2492\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2518 - mae: 3.4526 - val_loss: 19.6421 - val_mae: 3.1525\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2147 - mae: 3.4254 - val_loss: 19.9988 - val_mae: 3.2900\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2563 - mae: 3.5143 - val_loss: 19.8409 - val_mae: 3.2274\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2637 - mae: 3.4342 - val_loss: 19.8957 - val_mae: 3.2365\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2208 - mae: 3.4695 - val_loss: 19.7773 - val_mae: 3.2121\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2834 - mae: 3.4556 - val_loss: 19.7662 - val_mae: 3.1851\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.2056 - mae: 3.4246 - val_loss: 20.0067 - val_mae: 3.2766\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2289 - mae: 3.4905 - val_loss: 19.9137 - val_mae: 3.2473\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2460 - mae: 3.4566 - val_loss: 20.0204 - val_mae: 3.2747\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2215 - mae: 3.4779 - val_loss: 19.7990 - val_mae: 3.2084\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1691 - mae: 3.4295 - val_loss: 19.8085 - val_mae: 3.2152\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.2113 - mae: 3.4676 - val_loss: 19.8349 - val_mae: 3.2047\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.3245 - mae: 3.4311 - val_loss: 20.2217 - val_mae: 3.3291\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.5201 - mae: 3.4146\n",
      "Mean Absolute Error on Test Data: 3.4145970344543457\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.07316630521551903\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 6.4901 - mae: 1.8704 - val_loss: 3.9235 - val_mae: 1.3546\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.4584 - mae: 1.3031 - val_loss: 2.9770 - val_mae: 1.3417\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.1202 - mae: 1.3714 - val_loss: 2.8959 - val_mae: 1.3075\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.0477 - mae: 1.2924 - val_loss: 2.8327 - val_mae: 1.2670\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.0261 - mae: 1.3072 - val_loss: 2.8357 - val_mae: 1.2848\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9987 - mae: 1.2950 - val_loss: 2.8101 - val_mae: 1.2654\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9932 - mae: 1.2866 - val_loss: 2.7999 - val_mae: 1.2659\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9797 - mae: 1.2871 - val_loss: 2.7975 - val_mae: 1.2676\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9783 - mae: 1.2872 - val_loss: 2.7998 - val_mae: 1.2757\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9657 - mae: 1.2811 - val_loss: 2.7828 - val_mae: 1.2601\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.9652 - mae: 1.2809 - val_loss: 2.7891 - val_mae: 1.2720\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9752 - mae: 1.2727 - val_loss: 2.7761 - val_mae: 1.2600\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9623 - mae: 1.2681 - val_loss: 2.7771 - val_mae: 1.2647\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9515 - mae: 1.2787 - val_loss: 2.7764 - val_mae: 1.2679\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9514 - mae: 1.2848 - val_loss: 2.7756 - val_mae: 1.2701\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9499 - mae: 1.2926 - val_loss: 2.7644 - val_mae: 1.2640\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9407 - mae: 1.2751 - val_loss: 2.7551 - val_mae: 1.2580\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9329 - mae: 1.2807 - val_loss: 2.7693 - val_mae: 1.2763\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9340 - mae: 1.2754 - val_loss: 2.7484 - val_mae: 1.2548\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9345 - mae: 1.2708 - val_loss: 2.7541 - val_mae: 1.2625\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9311 - mae: 1.2874 - val_loss: 2.7493 - val_mae: 1.2633\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9236 - mae: 1.2613 - val_loss: 2.7266 - val_mae: 1.2298\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9406 - mae: 1.2782 - val_loss: 2.7373 - val_mae: 1.2562\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9246 - mae: 1.2597 - val_loss: 2.7363 - val_mae: 1.2565\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9145 - mae: 1.2756 - val_loss: 2.7158 - val_mae: 1.2347\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9231 - mae: 1.2703 - val_loss: 2.7340 - val_mae: 1.2598\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9122 - mae: 1.2720 - val_loss: 2.7117 - val_mae: 1.2236\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2.9177 - mae: 1.2650 - val_loss: 2.7211 - val_mae: 1.2495\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9029 - mae: 1.2646 - val_loss: 2.7156 - val_mae: 1.2458\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9013 - mae: 1.2566 - val_loss: 2.7112 - val_mae: 1.2346\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9043 - mae: 1.2752 - val_loss: 2.7271 - val_mae: 1.2580\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9069 - mae: 1.2614 - val_loss: 2.7152 - val_mae: 1.2506\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.9003 - mae: 1.2806 - val_loss: 2.7039 - val_mae: 1.2297\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8958 - mae: 1.2533 - val_loss: 2.7204 - val_mae: 1.2579\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8878 - mae: 1.2707 - val_loss: 2.7123 - val_mae: 1.2444\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.8823 - mae: 1.2627 - val_loss: 2.7098 - val_mae: 1.2479\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8807 - mae: 1.2523 - val_loss: 2.7051 - val_mae: 1.2421\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8779 - mae: 1.2696 - val_loss: 2.7132 - val_mae: 1.2506\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8768 - mae: 1.2655 - val_loss: 2.7139 - val_mae: 1.2553\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8690 - mae: 1.2692 - val_loss: 2.6913 - val_mae: 1.2199\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8719 - mae: 1.2470 - val_loss: 2.6988 - val_mae: 1.2427\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8701 - mae: 1.2738 - val_loss: 2.6969 - val_mae: 1.2359\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8856 - mae: 1.2651 - val_loss: 2.7182 - val_mae: 1.2602\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8729 - mae: 1.2485 - val_loss: 2.6948 - val_mae: 1.2395\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8594 - mae: 1.2557 - val_loss: 2.6886 - val_mae: 1.2264\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8666 - mae: 1.2392 - val_loss: 2.7253 - val_mae: 1.2681\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8848 - mae: 1.2893 - val_loss: 2.6824 - val_mae: 1.2077\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8628 - mae: 1.2457 - val_loss: 2.7016 - val_mae: 1.2523\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8506 - mae: 1.2582 - val_loss: 2.7016 - val_mae: 1.2518\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 2.8511 - mae: 1.2601 - val_loss: 2.6964 - val_mae: 1.2450\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.4714 - mae: 1.1864\n",
      "Mean Absolute Error on Test Data: 1.1864274740219116\n",
      "6/6 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.03867880423813386\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 61.3287 - mae: 5.0469 - val_loss: 58.1650 - val_mae: 4.9361\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 49.7502 - mae: 4.0675 - val_loss: 44.0829 - val_mae: 4.0005\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 38.0201 - mae: 3.5442 - val_loss: 35.0349 - val_mae: 3.9283\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 34.9295 - mae: 3.9215 - val_loss: 34.4782 - val_mae: 4.0195\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 34.4467 - mae: 3.7866 - val_loss: 34.5415 - val_mae: 3.9093\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 34.1225 - mae: 3.7794 - val_loss: 34.1409 - val_mae: 3.9262\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.8420 - mae: 3.7594 - val_loss: 33.9302 - val_mae: 3.9142\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.6779 - mae: 3.8435 - val_loss: 33.7070 - val_mae: 3.8944\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.4086 - mae: 3.7197 - val_loss: 33.7376 - val_mae: 3.8469\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.3816 - mae: 3.8287 - val_loss: 33.3753 - val_mae: 3.8710\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 33.0449 - mae: 3.7160 - val_loss: 33.4519 - val_mae: 3.8301\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.9828 - mae: 3.7102 - val_loss: 33.2057 - val_mae: 3.8317\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.8342 - mae: 3.7391 - val_loss: 32.9561 - val_mae: 3.8378\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.7350 - mae: 3.7150 - val_loss: 32.8296 - val_mae: 3.8305\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.7997 - mae: 3.8231 - val_loss: 32.6765 - val_mae: 3.8352\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.5737 - mae: 3.6418 - val_loss: 32.9794 - val_mae: 3.7670\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.5098 - mae: 3.7114 - val_loss: 32.3331 - val_mae: 3.8430\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.4687 - mae: 3.7366 - val_loss: 32.3412 - val_mae: 3.8163\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.5103 - mae: 3.6502 - val_loss: 32.2949 - val_mae: 3.7939\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.3260 - mae: 3.7512 - val_loss: 32.1600 - val_mae: 3.8062\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.2100 - mae: 3.7156 - val_loss: 32.0443 - val_mae: 3.7989\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.2315 - mae: 3.7616 - val_loss: 32.1337 - val_mae: 3.7680\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.1991 - mae: 3.6284 - val_loss: 32.1627 - val_mae: 3.7491\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.0449 - mae: 3.6667 - val_loss: 31.8398 - val_mae: 3.7612\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 31.9540 - mae: 3.6242 - val_loss: 31.6784 - val_mae: 3.7776\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.0268 - mae: 3.7918 - val_loss: 31.6902 - val_mae: 3.7620\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.8818 - mae: 3.5980 - val_loss: 31.6571 - val_mae: 3.7559\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 32.0286 - mae: 3.7380 - val_loss: 31.7807 - val_mae: 3.7257\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.7571 - mae: 3.6591 - val_loss: 31.5064 - val_mae: 3.7469\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.7781 - mae: 3.6803 - val_loss: 31.4759 - val_mae: 3.7323\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.6969 - mae: 3.6460 - val_loss: 31.5589 - val_mae: 3.7159\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.5980 - mae: 3.6356 - val_loss: 31.0335 - val_mae: 3.7925\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.5635 - mae: 3.6618 - val_loss: 31.3790 - val_mae: 3.7169\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.4548 - mae: 3.6669 - val_loss: 31.0814 - val_mae: 3.7545\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.5039 - mae: 3.6160 - val_loss: 31.0872 - val_mae: 3.7514\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.4686 - mae: 3.6352 - val_loss: 30.9883 - val_mae: 3.7389\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.5810 - mae: 3.7147 - val_loss: 30.9772 - val_mae: 3.7342\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.2802 - mae: 3.5912 - val_loss: 30.8210 - val_mae: 3.7565\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.3319 - mae: 3.6866 - val_loss: 30.7282 - val_mae: 3.7625\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.1755 - mae: 3.6359 - val_loss: 30.8093 - val_mae: 3.7355\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.0891 - mae: 3.6612 - val_loss: 30.7139 - val_mae: 3.7379\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 31.0491 - mae: 3.6100 - val_loss: 30.7290 - val_mae: 3.7282\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.9994 - mae: 3.6044 - val_loss: 30.8071 - val_mae: 3.7030\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.9389 - mae: 3.6624 - val_loss: 30.5051 - val_mae: 3.7419\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.9191 - mae: 3.5793 - val_loss: 30.6259 - val_mae: 3.7129\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.9101 - mae: 3.6868 - val_loss: 30.5203 - val_mae: 3.7133\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.9574 - mae: 3.5099 - val_loss: 30.5089 - val_mae: 3.7148\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.8551 - mae: 3.6919 - val_loss: 30.4521 - val_mae: 3.7129\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6995 - mae: 3.5727 - val_loss: 30.4683 - val_mae: 3.7123\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 30.6074 - mae: 3.6363 - val_loss: 30.4179 - val_mae: 3.7101\n",
      "7/7 [==============================] - 0s 921us/step - loss: 20.8086 - mae: 3.2418\n",
      "Mean Absolute Error on Test Data: 3.2418251037597656\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09136827269437131\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 161.4827 - mae: 9.2736 - val_loss: 93.4812 - val_mae: 7.6050\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 130.4355 - mae: 7.5262 - val_loss: 59.7204 - val_mae: 5.3915\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 92.1278 - mae: 5.5427 - val_loss: 36.5256 - val_mae: 4.5055\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 76.0273 - mae: 5.4719 - val_loss: 37.3273 - val_mae: 4.9484\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 74.6812 - mae: 5.4991 - val_loss: 35.7677 - val_mae: 4.7644\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 73.3704 - mae: 5.3691 - val_loss: 35.4604 - val_mae: 4.7665\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 72.4708 - mae: 5.3151 - val_loss: 34.9520 - val_mae: 4.7231\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 71.6744 - mae: 5.2472 - val_loss: 34.5441 - val_mae: 4.6871\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 71.0409 - mae: 5.2780 - val_loss: 33.9268 - val_mae: 4.5978\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.9671 - mae: 5.2424 - val_loss: 34.1998 - val_mae: 4.6654\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.3985 - mae: 5.1664 - val_loss: 33.8813 - val_mae: 4.6198\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 68.9285 - mae: 5.2310 - val_loss: 33.5245 - val_mae: 4.5581\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.8720 - mae: 5.0611 - val_loss: 34.0141 - val_mae: 4.6304\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 68.0555 - mae: 5.1970 - val_loss: 33.9915 - val_mae: 4.6211\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.6490 - mae: 5.0839 - val_loss: 33.5535 - val_mae: 4.5581\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.2205 - mae: 5.0683 - val_loss: 33.7292 - val_mae: 4.5763\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.8916 - mae: 5.0448 - val_loss: 33.4438 - val_mae: 4.5332\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.2559 - mae: 5.1931 - val_loss: 32.7653 - val_mae: 4.4069\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.4731 - mae: 4.9861 - val_loss: 33.5202 - val_mae: 4.5338\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.4393 - mae: 5.1683 - val_loss: 32.9012 - val_mae: 4.4340\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.1261 - mae: 4.9209 - val_loss: 32.9165 - val_mae: 4.4479\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.8623 - mae: 5.0452 - val_loss: 32.9519 - val_mae: 4.4527\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.6902 - mae: 4.9401 - val_loss: 33.5351 - val_mae: 4.5331\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.5079 - mae: 5.0785 - val_loss: 33.0958 - val_mae: 4.4682\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.1416 - mae: 4.9730 - val_loss: 32.9010 - val_mae: 4.4451\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.0047 - mae: 4.9760 - val_loss: 32.8893 - val_mae: 4.4454\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.4713 - mae: 4.9819 - val_loss: 33.3866 - val_mae: 4.5093\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.7601 - mae: 4.9567 - val_loss: 32.4781 - val_mae: 4.3818\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.8040 - mae: 4.9196 - val_loss: 33.2867 - val_mae: 4.5012\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.5267 - mae: 5.0080 - val_loss: 32.7805 - val_mae: 4.4197\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.5999 - mae: 4.8888 - val_loss: 32.3540 - val_mae: 4.3688\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.5155 - mae: 5.0152 - val_loss: 33.2261 - val_mae: 4.4851\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.4625 - mae: 4.8574 - val_loss: 32.8070 - val_mae: 4.4367\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.3128 - mae: 5.0120 - val_loss: 33.1669 - val_mae: 4.4772\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.1552 - mae: 4.8863 - val_loss: 32.8683 - val_mae: 4.4595\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.0435 - mae: 4.9169 - val_loss: 32.8795 - val_mae: 4.4495\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.3264 - mae: 4.9157 - val_loss: 32.8337 - val_mae: 4.4430\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 64.0407 - mae: 4.9937 - val_loss: 33.1570 - val_mae: 4.4636\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.8485 - mae: 4.9326 - val_loss: 32.5778 - val_mae: 4.3880\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.9561 - mae: 4.8162 - val_loss: 32.4324 - val_mae: 4.3802\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.5310 - mae: 4.8962 - val_loss: 33.5693 - val_mae: 4.5485\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.8995 - mae: 4.9714 - val_loss: 32.5632 - val_mae: 4.3874\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 63.9282 - mae: 4.9096 - val_loss: 32.3264 - val_mae: 4.3366\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.4363 - mae: 4.8459 - val_loss: 33.2931 - val_mae: 4.5178\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.6551 - mae: 5.0128 - val_loss: 32.6397 - val_mae: 4.3914\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.5736 - mae: 4.7946 - val_loss: 32.5950 - val_mae: 4.4125\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.4038 - mae: 4.9377 - val_loss: 32.7385 - val_mae: 4.4368\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.7581 - mae: 4.7869 - val_loss: 32.6987 - val_mae: 4.4533\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.5826 - mae: 4.9788 - val_loss: 32.6844 - val_mae: 4.4109\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 63.2114 - mae: 4.8285 - val_loss: 32.6360 - val_mae: 4.4044\n",
      "8/8 [==============================] - 0s 951us/step - loss: 58.7366 - mae: 5.3383\n",
      "Mean Absolute Error on Test Data: 5.338253974914551\n",
      "8/8 [==============================] - 0s 850us/step\n",
      "R-squared: 0.1869144144007282\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 91.6147 - mae: 7.8528 - val_loss: 83.6285 - val_mae: 7.3716\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 69.7052 - mae: 6.3553 - val_loss: 56.1860 - val_mae: 5.5140\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4294 - mae: 4.3357 - val_loss: 31.2107 - val_mae: 4.0424\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.3232 - mae: 4.0690 - val_loss: 29.5294 - val_mae: 4.0844\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.3998 - mae: 3.9592 - val_loss: 29.4101 - val_mae: 4.0327\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.1865 - mae: 3.9228 - val_loss: 29.1231 - val_mae: 4.0251\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.0401 - mae: 3.9450 - val_loss: 28.9113 - val_mae: 4.0158\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.9669 - mae: 3.9612 - val_loss: 28.7903 - val_mae: 3.9982\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.8697 - mae: 3.8818 - val_loss: 28.6329 - val_mae: 3.9971\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.7572 - mae: 3.9156 - val_loss: 28.3981 - val_mae: 3.9985\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.8487 - mae: 3.9007 - val_loss: 28.2470 - val_mae: 3.9980\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.7811 - mae: 3.9469 - val_loss: 28.3532 - val_mae: 3.9655\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.6029 - mae: 3.9048 - val_loss: 28.1512 - val_mae: 3.9660\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5509 - mae: 3.8748 - val_loss: 28.1632 - val_mae: 3.9575\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.6065 - mae: 3.9262 - val_loss: 28.1165 - val_mae: 3.9518\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5445 - mae: 3.8421 - val_loss: 27.9683 - val_mae: 3.9561\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5597 - mae: 3.9301 - val_loss: 27.9353 - val_mae: 3.9493\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4868 - mae: 3.8837 - val_loss: 27.9361 - val_mae: 3.9449\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3902 - mae: 3.8709 - val_loss: 27.8386 - val_mae: 3.9450\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.4670 - mae: 3.8955 - val_loss: 27.8433 - val_mae: 3.9386\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3713 - mae: 3.8798 - val_loss: 27.6810 - val_mae: 3.9455\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3559 - mae: 3.8951 - val_loss: 27.8537 - val_mae: 3.9317\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 28.5693 - mae: 3.8506 - val_loss: 27.5747 - val_mae: 3.9484\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3448 - mae: 3.8633 - val_loss: 27.6167 - val_mae: 3.9384\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3203 - mae: 3.8897 - val_loss: 27.5442 - val_mae: 3.9446\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3158 - mae: 3.8812 - val_loss: 27.6151 - val_mae: 3.9312\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 28.3331 - mae: 3.8653 - val_loss: 27.5119 - val_mae: 3.9350\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2693 - mae: 3.8567 - val_loss: 27.6208 - val_mae: 3.9256\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2879 - mae: 3.8731 - val_loss: 27.5354 - val_mae: 3.9270\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2986 - mae: 3.8689 - val_loss: 27.4754 - val_mae: 3.9291\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2473 - mae: 3.8732 - val_loss: 27.6527 - val_mae: 3.9196\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3008 - mae: 3.8753 - val_loss: 27.5884 - val_mae: 3.9198\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.2917 - mae: 3.8901 - val_loss: 27.6063 - val_mae: 3.9175\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2565 - mae: 3.8399 - val_loss: 27.4719 - val_mae: 3.9198\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2799 - mae: 3.9074 - val_loss: 27.4845 - val_mae: 3.9192\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.2701 - mae: 3.8417 - val_loss: 27.3684 - val_mae: 3.9223\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4304 - mae: 3.9475 - val_loss: 27.8429 - val_mae: 3.9080\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.4481 - mae: 3.8111 - val_loss: 27.3113 - val_mae: 3.9230\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2279 - mae: 3.8498 - val_loss: 27.3302 - val_mae: 3.9185\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3531 - mae: 3.9295 - val_loss: 27.5963 - val_mae: 3.9085\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2804 - mae: 3.8191 - val_loss: 27.3511 - val_mae: 3.9159\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2063 - mae: 3.8926 - val_loss: 27.3036 - val_mae: 3.9200\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3341 - mae: 3.8599 - val_loss: 27.1976 - val_mae: 3.9291\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.3519 - mae: 3.8425 - val_loss: 27.3869 - val_mae: 3.9104\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2505 - mae: 3.9075 - val_loss: 27.3688 - val_mae: 3.9111\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.1511 - mae: 3.8389 - val_loss: 27.3168 - val_mae: 3.9119\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.2170 - mae: 3.8947 - val_loss: 27.2712 - val_mae: 3.9157\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1627 - mae: 3.8398 - val_loss: 27.2980 - val_mae: 3.9126\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1341 - mae: 3.8726 - val_loss: 27.2248 - val_mae: 3.9140\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1899 - mae: 3.8660 - val_loss: 27.2592 - val_mae: 3.9110\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 31.0454 - mae: 4.0367\n",
      "Mean Absolute Error on Test Data: 4.036722183227539\n",
      "8/8 [==============================] - 0s 945us/step\n",
      "R-squared: 0.0619045358260526\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 71.5180 - mae: 6.9866 - val_loss: 53.6898 - val_mae: 5.5922\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.5442 - mae: 4.9304 - val_loss: 29.9473 - val_mae: 3.6590\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 24.8801 - mae: 3.5390 - val_loss: 22.3503 - val_mae: 3.5161\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2282 - mae: 3.5574 - val_loss: 22.2573 - val_mae: 3.4974\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9946 - mae: 3.4670 - val_loss: 22.0806 - val_mae: 3.4269\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0015 - mae: 3.4698 - val_loss: 22.1199 - val_mae: 3.4739\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9252 - mae: 3.4527 - val_loss: 22.0218 - val_mae: 3.4335\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9544 - mae: 3.4666 - val_loss: 22.0357 - val_mae: 3.4587\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8571 - mae: 3.4653 - val_loss: 22.0001 - val_mae: 3.4483\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8473 - mae: 3.4630 - val_loss: 21.9956 - val_mae: 3.4592\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8592 - mae: 3.4657 - val_loss: 21.9046 - val_mae: 3.4146\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8252 - mae: 3.4497 - val_loss: 21.9252 - val_mae: 3.4416\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8456 - mae: 3.4508 - val_loss: 21.8979 - val_mae: 3.4361\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7713 - mae: 3.4810 - val_loss: 21.9258 - val_mae: 3.4618\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7782 - mae: 3.4705 - val_loss: 21.7980 - val_mae: 3.4143\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7980 - mae: 3.4270 - val_loss: 21.8294 - val_mae: 3.4311\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8779 - mae: 3.5047 - val_loss: 21.7463 - val_mae: 3.3922\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6780 - mae: 3.4389 - val_loss: 21.8172 - val_mae: 3.4562\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8003 - mae: 3.5148 - val_loss: 21.7301 - val_mae: 3.3643\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0020 - mae: 3.4168 - val_loss: 21.8966 - val_mae: 3.4969\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6249 - mae: 3.4644 - val_loss: 21.6890 - val_mae: 3.3936\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6575 - mae: 3.4428 - val_loss: 21.7828 - val_mae: 3.4620\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6319 - mae: 3.4481 - val_loss: 21.6512 - val_mae: 3.4079\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6400 - mae: 3.4829 - val_loss: 21.6594 - val_mae: 3.4074\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5906 - mae: 3.4206 - val_loss: 21.6716 - val_mae: 3.4220\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5684 - mae: 3.4675 - val_loss: 21.6646 - val_mae: 3.4351\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6588 - mae: 3.4585 - val_loss: 21.5902 - val_mae: 3.3590\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5148 - mae: 3.4446 - val_loss: 21.8382 - val_mae: 3.5043\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4652 - mae: 3.4676 - val_loss: 21.5452 - val_mae: 3.3647\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5391 - mae: 3.4055 - val_loss: 21.6403 - val_mae: 3.4439\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5830 - mae: 3.4828 - val_loss: 21.5378 - val_mae: 3.3492\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4780 - mae: 3.4233 - val_loss: 21.5879 - val_mae: 3.4431\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3915 - mae: 3.4586 - val_loss: 21.4959 - val_mae: 3.4029\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3855 - mae: 3.4151 - val_loss: 21.5426 - val_mae: 3.4334\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3855 - mae: 3.4786 - val_loss: 21.4378 - val_mae: 3.3642\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6772 - mae: 3.3966 - val_loss: 21.7758 - val_mae: 3.5179\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4115 - mae: 3.4662 - val_loss: 21.4533 - val_mae: 3.4077\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3360 - mae: 3.4173 - val_loss: 21.5439 - val_mae: 3.4538\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.2704 - mae: 3.4482 - val_loss: 21.4019 - val_mae: 3.3921\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2844 - mae: 3.4235 - val_loss: 21.4872 - val_mae: 3.4447\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2207 - mae: 3.4585 - val_loss: 21.3227 - val_mae: 3.3485\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2232 - mae: 3.3910 - val_loss: 21.4771 - val_mae: 3.4584\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3798 - mae: 3.4857 - val_loss: 21.3382 - val_mae: 3.3232\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.2604 - mae: 3.4116 - val_loss: 21.3569 - val_mae: 3.4155\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1766 - mae: 3.4445 - val_loss: 21.2645 - val_mae: 3.3693\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1339 - mae: 3.4093 - val_loss: 21.2777 - val_mae: 3.3989\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.1618 - mae: 3.4095 - val_loss: 21.4764 - val_mae: 3.4668\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1451 - mae: 3.4239 - val_loss: 21.2657 - val_mae: 3.4031\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1879 - mae: 3.4458 - val_loss: 21.2289 - val_mae: 3.3889\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.1002 - mae: 3.4616 - val_loss: 21.1856 - val_mae: 3.3637\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 26.9420 - mae: 3.4431\n",
      "Mean Absolute Error on Test Data: 3.443101167678833\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08815918037030501\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.1213 - mae: 1.6716 - val_loss: 5.8324 - val_mae: 1.4433\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1625 - mae: 1.2418 - val_loss: 4.1793 - val_mae: 1.2676\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4616 - mae: 1.2249 - val_loss: 4.0097 - val_mae: 1.3080\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4614 - mae: 1.2566 - val_loss: 4.0192 - val_mae: 1.2801\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4425 - mae: 1.2165 - val_loss: 4.0443 - val_mae: 1.2716\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4356 - mae: 1.2228 - val_loss: 4.0074 - val_mae: 1.2817\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4288 - mae: 1.2351 - val_loss: 4.0215 - val_mae: 1.2754\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.4277 - mae: 1.2096 - val_loss: 4.0375 - val_mae: 1.2747\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4165 - mae: 1.2245 - val_loss: 4.0060 - val_mae: 1.2794\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4089 - mae: 1.2334 - val_loss: 3.9801 - val_mae: 1.2856\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4025 - mae: 1.2293 - val_loss: 4.0110 - val_mae: 1.2743\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4021 - mae: 1.2145 - val_loss: 3.9896 - val_mae: 1.2807\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3981 - mae: 1.2268 - val_loss: 3.9780 - val_mae: 1.2894\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4030 - mae: 1.2109 - val_loss: 4.0166 - val_mae: 1.2717\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3857 - mae: 1.2322 - val_loss: 3.9603 - val_mae: 1.3070\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3888 - mae: 1.2317 - val_loss: 4.0175 - val_mae: 1.2750\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3743 - mae: 1.2124 - val_loss: 3.9652 - val_mae: 1.2926\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3831 - mae: 1.2353 - val_loss: 3.9893 - val_mae: 1.2864\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3753 - mae: 1.2330 - val_loss: 3.9790 - val_mae: 1.2988\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3774 - mae: 1.2139 - val_loss: 4.0189 - val_mae: 1.2880\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3832 - mae: 1.2475 - val_loss: 3.9657 - val_mae: 1.3040\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3848 - mae: 1.2154 - val_loss: 3.9851 - val_mae: 1.2873\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3821 - mae: 1.2513 - val_loss: 3.9775 - val_mae: 1.3041\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3531 - mae: 1.2064 - val_loss: 4.0531 - val_mae: 1.2766\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3579 - mae: 1.2090 - val_loss: 3.9547 - val_mae: 1.3045\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3714 - mae: 1.2663 - val_loss: 4.0004 - val_mae: 1.2976\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3595 - mae: 1.2036 - val_loss: 4.0300 - val_mae: 1.2810\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3629 - mae: 1.2495 - val_loss: 3.9608 - val_mae: 1.3075\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3359 - mae: 1.2199 - val_loss: 3.9790 - val_mae: 1.2957\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3251 - mae: 1.2164 - val_loss: 3.9709 - val_mae: 1.2981\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3229 - mae: 1.2334 - val_loss: 3.9898 - val_mae: 1.3067\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3191 - mae: 1.2240 - val_loss: 4.0120 - val_mae: 1.2945\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3212 - mae: 1.2236 - val_loss: 3.9675 - val_mae: 1.3029\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3250 - mae: 1.2360 - val_loss: 4.0388 - val_mae: 1.3003\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3121 - mae: 1.2108 - val_loss: 3.9620 - val_mae: 1.3085\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3097 - mae: 1.2411 - val_loss: 3.9701 - val_mae: 1.3194\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2941 - mae: 1.2274 - val_loss: 4.0072 - val_mae: 1.2969\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2993 - mae: 1.2172 - val_loss: 3.9866 - val_mae: 1.3035\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3031 - mae: 1.2160 - val_loss: 3.9740 - val_mae: 1.3097\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3159 - mae: 1.2659 - val_loss: 4.0492 - val_mae: 1.3044\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2988 - mae: 1.2066 - val_loss: 3.9934 - val_mae: 1.2942\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3022 - mae: 1.2386 - val_loss: 3.9807 - val_mae: 1.3073\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2939 - mae: 1.2164 - val_loss: 3.9599 - val_mae: 1.3079\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2864 - mae: 1.2223 - val_loss: 4.0561 - val_mae: 1.3010\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2891 - mae: 1.2285 - val_loss: 3.9595 - val_mae: 1.3468\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3150 - mae: 1.2248 - val_loss: 4.0100 - val_mae: 1.2906\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2648 - mae: 1.2289 - val_loss: 4.0086 - val_mae: 1.3197\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2721 - mae: 1.2197 - val_loss: 3.9766 - val_mae: 1.3061\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2487 - mae: 1.2252 - val_loss: 3.9821 - val_mae: 1.3113\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2472 - mae: 1.2270 - val_loss: 3.9768 - val_mae: 1.3050\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 2.3354 - mae: 1.0830\n",
      "Mean Absolute Error on Test Data: 1.0830060243606567\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03761749161778838\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 155.2911 - mae: 10.0389 - val_loss: 145.0552 - val_mae: 9.3975\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 129.9148 - mae: 8.7472 - val_loss: 113.0524 - val_mae: 7.6845\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 91.5368 - mae: 6.6442 - val_loss: 71.3305 - val_mae: 5.5474\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 57.8076 - mae: 4.9610 - val_loss: 55.2043 - val_mae: 5.5525\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.2909 - mae: 5.1783 - val_loss: 55.2374 - val_mae: 5.6518\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.1026 - mae: 5.0569 - val_loss: 54.9773 - val_mae: 5.5145\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.0143 - mae: 5.0956 - val_loss: 54.8752 - val_mae: 5.5716\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.9314 - mae: 5.0155 - val_loss: 54.7535 - val_mae: 5.4988\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.8088 - mae: 5.0760 - val_loss: 54.7277 - val_mae: 5.6062\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.6772 - mae: 5.0661 - val_loss: 54.5260 - val_mae: 5.5087\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.6267 - mae: 5.0133 - val_loss: 54.4331 - val_mae: 5.5176\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.6428 - mae: 5.0695 - val_loss: 54.3161 - val_mae: 5.5035\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.5364 - mae: 5.0070 - val_loss: 54.2401 - val_mae: 5.5194\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.4771 - mae: 5.0432 - val_loss: 54.1470 - val_mae: 5.4992\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.5033 - mae: 4.9895 - val_loss: 54.0607 - val_mae: 5.5096\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.4995 - mae: 5.0463 - val_loss: 54.0307 - val_mae: 5.5404\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.3304 - mae: 5.0091 - val_loss: 53.9093 - val_mae: 5.4865\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3016 - mae: 5.0078 - val_loss: 53.8380 - val_mae: 5.4735\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.2670 - mae: 5.0143 - val_loss: 53.7332 - val_mae: 5.4781\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.2849 - mae: 4.9968 - val_loss: 53.6808 - val_mae: 5.4933\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.2335 - mae: 4.9922 - val_loss: 53.6145 - val_mae: 5.4844\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.2105 - mae: 5.0609 - val_loss: 53.5614 - val_mae: 5.4909\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.0866 - mae: 4.9694 - val_loss: 53.5091 - val_mae: 5.4331\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.1558 - mae: 4.9466 - val_loss: 53.4091 - val_mae: 5.4734\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.3186 - mae: 5.0897 - val_loss: 53.3967 - val_mae: 5.4455\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3505 - mae: 4.9324 - val_loss: 53.3819 - val_mae: 5.4660\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.3865 - mae: 5.1043 - val_loss: 53.3338 - val_mae: 5.4819\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.0170 - mae: 4.9513 - val_loss: 53.3327 - val_mae: 5.4042\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.2654 - mae: 5.0126 - val_loss: 53.2173 - val_mae: 5.4540\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8983 - mae: 4.9568 - val_loss: 53.2637 - val_mae: 5.4032\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.0066 - mae: 4.9891 - val_loss: 53.1733 - val_mae: 5.4153\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.0223 - mae: 4.9654 - val_loss: 53.1642 - val_mae: 5.3988\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.9080 - mae: 4.9478 - val_loss: 53.0643 - val_mae: 5.4380\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.9039 - mae: 5.0111 - val_loss: 53.0143 - val_mae: 5.4463\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.9228 - mae: 4.9558 - val_loss: 52.9918 - val_mae: 5.4433\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8622 - mae: 4.9626 - val_loss: 52.9660 - val_mae: 5.4224\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8235 - mae: 4.9547 - val_loss: 52.9666 - val_mae: 5.4825\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8715 - mae: 5.0108 - val_loss: 52.9154 - val_mae: 5.4800\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.7858 - mae: 4.9701 - val_loss: 52.8795 - val_mae: 5.4132\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.8971 - mae: 5.0168 - val_loss: 52.8344 - val_mae: 5.4329\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.7352 - mae: 4.9814 - val_loss: 52.8951 - val_mae: 5.3936\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 51.1344 - mae: 4.9439 - val_loss: 52.8189 - val_mae: 5.4776\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.8121 - mae: 4.9794 - val_loss: 52.7751 - val_mae: 5.4149\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.7506 - mae: 5.0245 - val_loss: 52.7462 - val_mae: 5.4384\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.6773 - mae: 4.9838 - val_loss: 52.7469 - val_mae: 5.4132\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.7978 - mae: 4.9327 - val_loss: 52.6939 - val_mae: 5.4399\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.6625 - mae: 5.0137 - val_loss: 52.6852 - val_mae: 5.4236\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.6147 - mae: 4.9615 - val_loss: 52.7031 - val_mae: 5.4142\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.5889 - mae: 4.9359 - val_loss: 52.6755 - val_mae: 5.3961\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 50.6969 - mae: 4.9629 - val_loss: 52.6313 - val_mae: 5.4188\n",
      "8/8 [==============================] - 0s 991us/step - loss: 40.7574 - mae: 4.6619\n",
      "Mean Absolute Error on Test Data: 4.661898136138916\n",
      "8/8 [==============================] - 0s 840us/step\n",
      "R-squared: 0.11689237419470844\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 40.6998 - mae: 5.1100 - val_loss: 27.6809 - val_mae: 4.0555\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1845 - mae: 3.7494 - val_loss: 15.1484 - val_mae: 2.7491\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.9474 - mae: 2.8287 - val_loss: 12.3517 - val_mae: 2.8186\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.8983 - mae: 2.9122 - val_loss: 11.8330 - val_mae: 2.7188\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6391 - mae: 2.8275 - val_loss: 11.8594 - val_mae: 2.7348\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5905 - mae: 2.8674 - val_loss: 11.8241 - val_mae: 2.7295\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.5048 - mae: 2.8475 - val_loss: 11.7727 - val_mae: 2.7213\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4550 - mae: 2.8122 - val_loss: 11.7403 - val_mae: 2.7173\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.6760 - mae: 2.9041 - val_loss: 11.6915 - val_mae: 2.7065\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.4245 - mae: 2.7853 - val_loss: 11.7269 - val_mae: 2.7108\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3145 - mae: 2.8255 - val_loss: 11.7207 - val_mae: 2.7089\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3198 - mae: 2.7813 - val_loss: 11.7172 - val_mae: 2.7081\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.3098 - mae: 2.8491 - val_loss: 11.7564 - val_mae: 2.7170\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2672 - mae: 2.8264 - val_loss: 11.7389 - val_mae: 2.7112\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2156 - mae: 2.7694 - val_loss: 11.6800 - val_mae: 2.7013\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1908 - mae: 2.7845 - val_loss: 11.8239 - val_mae: 2.7261\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1787 - mae: 2.8035 - val_loss: 11.7156 - val_mae: 2.7097\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1793 - mae: 2.8285 - val_loss: 11.6512 - val_mae: 2.6979\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.2286 - mae: 2.7604 - val_loss: 11.7767 - val_mae: 2.7211\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1286 - mae: 2.7806 - val_loss: 11.6416 - val_mae: 2.6944\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2469 - mae: 2.7418 - val_loss: 11.8049 - val_mae: 2.7247\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2655 - mae: 2.8551 - val_loss: 11.7291 - val_mae: 2.7103\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0975 - mae: 2.7972 - val_loss: 11.7262 - val_mae: 2.7085\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.1009 - mae: 2.8069 - val_loss: 11.7128 - val_mae: 2.7050\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0485 - mae: 2.7895 - val_loss: 11.7687 - val_mae: 2.7167\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0553 - mae: 2.8166 - val_loss: 11.7462 - val_mae: 2.7135\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0560 - mae: 2.7726 - val_loss: 11.8316 - val_mae: 2.7289\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0354 - mae: 2.7930 - val_loss: 11.7061 - val_mae: 2.7021\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9983 - mae: 2.7677 - val_loss: 11.8899 - val_mae: 2.7426\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0037 - mae: 2.7956 - val_loss: 11.7433 - val_mae: 2.7114\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0245 - mae: 2.8096 - val_loss: 11.8008 - val_mae: 2.7230\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9826 - mae: 2.7898 - val_loss: 11.8114 - val_mae: 2.7257\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0866 - mae: 2.7632 - val_loss: 11.9804 - val_mae: 2.7598\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9375 - mae: 2.7926 - val_loss: 11.7727 - val_mae: 2.7155\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0818 - mae: 2.7233 - val_loss: 12.0291 - val_mae: 2.7671\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2139 - mae: 2.9101 - val_loss: 11.7110 - val_mae: 2.6955\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 14.0557 - mae: 2.7511 - val_loss: 11.7932 - val_mae: 2.7210\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9312 - mae: 2.7642 - val_loss: 11.8814 - val_mae: 2.7376\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9682 - mae: 2.7836 - val_loss: 11.8615 - val_mae: 2.7350\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9382 - mae: 2.7512 - val_loss: 11.8644 - val_mae: 2.7381\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9614 - mae: 2.7562 - val_loss: 11.9912 - val_mae: 2.7654\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9358 - mae: 2.7948 - val_loss: 11.7576 - val_mae: 2.7058\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.9418 - mae: 2.7504 - val_loss: 11.9738 - val_mae: 2.7615\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9347 - mae: 2.7664 - val_loss: 11.9138 - val_mae: 2.7522\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8784 - mae: 2.7647 - val_loss: 11.8801 - val_mae: 2.7465\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8419 - mae: 2.7510 - val_loss: 11.8159 - val_mae: 2.7375\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8659 - mae: 2.7768 - val_loss: 11.8365 - val_mae: 2.7498\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8965 - mae: 2.7659 - val_loss: 11.9177 - val_mae: 2.7649\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 13.8448 - mae: 2.8121 - val_loss: 11.6956 - val_mae: 2.7121\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8685 - mae: 2.7497 - val_loss: 11.8457 - val_mae: 2.7538\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 16.7718 - mae: 3.0649\n",
      "Mean Absolute Error on Test Data: 3.0649003982543945\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.02025423117335856\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 8.7844 - mae: 1.8068 - val_loss: 5.2955 - val_mae: 1.3682\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.9698 - mae: 1.4144 - val_loss: 4.3721 - val_mae: 1.4313\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.6478 - mae: 1.5479 - val_loss: 4.3644 - val_mae: 1.4510\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.4931 - mae: 1.4636 - val_loss: 4.3243 - val_mae: 1.3828\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3866 - mae: 1.4371 - val_loss: 4.3231 - val_mae: 1.4137\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.3233 - mae: 1.4736 - val_loss: 4.3306 - val_mae: 1.4329\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.2731 - mae: 1.4408 - val_loss: 4.3228 - val_mae: 1.3865\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2052 - mae: 1.4371 - val_loss: 4.3549 - val_mae: 1.4340\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1696 - mae: 1.4617 - val_loss: 4.3463 - val_mae: 1.4090\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1807 - mae: 1.3979 - val_loss: 4.3624 - val_mae: 1.4145\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1515 - mae: 1.4768 - val_loss: 4.4148 - val_mae: 1.4561\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.1566 - mae: 1.3931 - val_loss: 4.3811 - val_mae: 1.4032\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0956 - mae: 1.4439 - val_loss: 4.4283 - val_mae: 1.4594\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0845 - mae: 1.4223 - val_loss: 4.3907 - val_mae: 1.4010\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0685 - mae: 1.4063 - val_loss: 4.4189 - val_mae: 1.4393\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0797 - mae: 1.4507 - val_loss: 4.4029 - val_mae: 1.4209\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0469 - mae: 1.4307 - val_loss: 4.4472 - val_mae: 1.4324\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0360 - mae: 1.4193 - val_loss: 4.4448 - val_mae: 1.4350\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0803 - mae: 1.4803 - val_loss: 4.4479 - val_mae: 1.4108\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0370 - mae: 1.3962 - val_loss: 4.4498 - val_mae: 1.4329\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0203 - mae: 1.4388 - val_loss: 4.4465 - val_mae: 1.4278\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0137 - mae: 1.4158 - val_loss: 4.4661 - val_mae: 1.4481\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0002 - mae: 1.4320 - val_loss: 4.4415 - val_mae: 1.4133\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0005 - mae: 1.4080 - val_loss: 4.4133 - val_mae: 1.4173\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9769 - mae: 1.4239 - val_loss: 4.4399 - val_mae: 1.4411\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.0094 - mae: 1.4481 - val_loss: 4.4650 - val_mae: 1.4040\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 5.0423 - mae: 1.4100 - val_loss: 4.4829 - val_mae: 1.4672\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9650 - mae: 1.4244 - val_loss: 4.4224 - val_mae: 1.3902\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9497 - mae: 1.4016 - val_loss: 4.4585 - val_mae: 1.4623\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9525 - mae: 1.4157 - val_loss: 4.4350 - val_mae: 1.4338\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9620 - mae: 1.4346 - val_loss: 4.4866 - val_mae: 1.4414\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9674 - mae: 1.4562 - val_loss: 4.4462 - val_mae: 1.4102\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4.9881 - mae: 1.3648 - val_loss: 4.4233 - val_mae: 1.4331\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9421 - mae: 1.4421 - val_loss: 4.4507 - val_mae: 1.4551\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9150 - mae: 1.3991 - val_loss: 4.4174 - val_mae: 1.4131\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9179 - mae: 1.3959 - val_loss: 4.4269 - val_mae: 1.4457\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9566 - mae: 1.4509 - val_loss: 4.4253 - val_mae: 1.3930\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9243 - mae: 1.4141 - val_loss: 4.4068 - val_mae: 1.4213\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8892 - mae: 1.3959 - val_loss: 4.3945 - val_mae: 1.4109\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4.9849 - mae: 1.4706 - val_loss: 4.4738 - val_mae: 1.4220\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9018 - mae: 1.3819 - val_loss: 4.4410 - val_mae: 1.4371\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9071 - mae: 1.4128 - val_loss: 4.3893 - val_mae: 1.4342\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.9099 - mae: 1.4561 - val_loss: 4.4678 - val_mae: 1.4305\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8638 - mae: 1.4018 - val_loss: 4.4187 - val_mae: 1.4321\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8420 - mae: 1.4105 - val_loss: 4.3927 - val_mae: 1.4080\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8433 - mae: 1.4143 - val_loss: 4.4085 - val_mae: 1.4253\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8335 - mae: 1.4045 - val_loss: 4.3947 - val_mae: 1.4167\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8285 - mae: 1.3905 - val_loss: 4.3914 - val_mae: 1.4065\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8131 - mae: 1.4000 - val_loss: 4.4130 - val_mae: 1.4351\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 4.8338 - mae: 1.4193 - val_loss: 4.4250 - val_mae: 1.4273\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.7242 - mae: 1.4839\n",
      "Mean Absolute Error on Test Data: 1.483862280845642\n",
      "6/6 [==============================] - 0s 854us/step\n",
      "R-squared: 0.1776981770765178\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 71.1561 - mae: 6.0384 - val_loss: 47.1507 - val_mae: 4.4501\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.6790 - mae: 4.6590 - val_loss: 31.5262 - val_mae: 3.3064\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 35.6307 - mae: 3.7966 - val_loss: 29.4753 - val_mae: 3.9282\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.7535 - mae: 4.0963 - val_loss: 28.8412 - val_mae: 3.8222\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.5030 - mae: 3.8963 - val_loss: 28.4304 - val_mae: 3.7680\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.3341 - mae: 4.0023 - val_loss: 28.5977 - val_mae: 3.8348\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.2103 - mae: 3.9965 - val_loss: 28.2767 - val_mae: 3.7924\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.1415 - mae: 3.9053 - val_loss: 27.7049 - val_mae: 3.7026\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.0495 - mae: 3.9011 - val_loss: 27.8420 - val_mae: 3.7464\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.0241 - mae: 3.9584 - val_loss: 27.7526 - val_mae: 3.7429\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 33.0848 - mae: 3.8325 - val_loss: 27.6558 - val_mae: 3.7362\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.9798 - mae: 3.9855 - val_loss: 27.5748 - val_mae: 3.7314\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.8274 - mae: 3.8794 - val_loss: 27.5691 - val_mae: 3.7427\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.7342 - mae: 3.9548 - val_loss: 27.5436 - val_mae: 3.7541\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.8811 - mae: 3.8675 - val_loss: 26.8308 - val_mae: 3.6103\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.6650 - mae: 3.8908 - val_loss: 27.6247 - val_mae: 3.7847\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.7929 - mae: 3.8513 - val_loss: 27.2812 - val_mae: 3.7334\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.8443 - mae: 4.0243 - val_loss: 26.8845 - val_mae: 3.6578\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.7099 - mae: 3.8159 - val_loss: 27.2466 - val_mae: 3.7307\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.4942 - mae: 3.8838 - val_loss: 26.9675 - val_mae: 3.6843\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.4541 - mae: 3.8517 - val_loss: 27.1981 - val_mae: 3.7415\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.5196 - mae: 3.9799 - val_loss: 26.8397 - val_mae: 3.6683\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.6139 - mae: 3.7929 - val_loss: 26.7772 - val_mae: 3.6600\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.5454 - mae: 3.9229 - val_loss: 27.3932 - val_mae: 3.7983\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.4723 - mae: 3.8848 - val_loss: 26.9520 - val_mae: 3.6996\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.3205 - mae: 3.8786 - val_loss: 27.1089 - val_mae: 3.7433\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.3160 - mae: 3.8972 - val_loss: 27.2198 - val_mae: 3.7678\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.3586 - mae: 3.9407 - val_loss: 26.7517 - val_mae: 3.6746\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.2573 - mae: 3.8939 - val_loss: 26.7596 - val_mae: 3.6908\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.2609 - mae: 3.8970 - val_loss: 27.0978 - val_mae: 3.7650\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0878 - mae: 3.8348 - val_loss: 26.5706 - val_mae: 3.6504\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.2433 - mae: 3.8809 - val_loss: 26.5935 - val_mae: 3.6663\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.2736 - mae: 3.8209 - val_loss: 27.0171 - val_mae: 3.7524\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0727 - mae: 3.8433 - val_loss: 26.8733 - val_mae: 3.7308\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.1421 - mae: 3.9187 - val_loss: 26.6403 - val_mae: 3.7026\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.1179 - mae: 3.9583 - val_loss: 26.6529 - val_mae: 3.7065\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 32.0898 - mae: 3.8205 - val_loss: 26.8653 - val_mae: 3.7422\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9760 - mae: 3.9586 - val_loss: 27.1049 - val_mae: 3.8001\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9545 - mae: 3.9163 - val_loss: 26.4274 - val_mae: 3.6818\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.0036 - mae: 3.7798 - val_loss: 26.4607 - val_mae: 3.6789\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9165 - mae: 3.9189 - val_loss: 26.6567 - val_mae: 3.7270\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.9724 - mae: 3.8294 - val_loss: 26.5005 - val_mae: 3.6936\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.8902 - mae: 3.8824 - val_loss: 26.6120 - val_mae: 3.7132\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7650 - mae: 3.8418 - val_loss: 26.5496 - val_mae: 3.7018\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.7639 - mae: 3.8843 - val_loss: 26.6096 - val_mae: 3.7232\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7782 - mae: 3.8458 - val_loss: 26.4344 - val_mae: 3.6961\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7405 - mae: 3.8350 - val_loss: 26.6522 - val_mae: 3.7331\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.7170 - mae: 3.8601 - val_loss: 26.4260 - val_mae: 3.7030\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.6092 - mae: 3.8812 - val_loss: 26.6474 - val_mae: 3.7426\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 31.6855 - mae: 3.8719 - val_loss: 26.7217 - val_mae: 3.7638\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 34.9985 - mae: 3.6536\n",
      "Mean Absolute Error on Test Data: 3.6536216735839844\n",
      "7/7 [==============================] - 0s 818us/step\n",
      "R-squared: 0.057303410544555455\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 109.4934 - mae: 8.0418 - val_loss: 85.9665 - val_mae: 7.3064\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 80.9792 - mae: 6.1385 - val_loss: 54.0039 - val_mae: 5.1990\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 52.5706 - mae: 4.2792 - val_loss: 32.7145 - val_mae: 4.0696\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.3262 - mae: 4.2413 - val_loss: 31.8275 - val_mae: 4.2860\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1925 - mae: 4.2131 - val_loss: 31.8465 - val_mae: 4.1533\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.0450 - mae: 4.1632 - val_loss: 31.7451 - val_mae: 4.1707\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9347 - mae: 4.1831 - val_loss: 31.7929 - val_mae: 4.1375\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.8495 - mae: 4.1921 - val_loss: 31.7116 - val_mae: 4.1520\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.8020 - mae: 4.1968 - val_loss: 31.7370 - val_mae: 4.1409\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.7352 - mae: 4.2294 - val_loss: 31.6024 - val_mae: 4.1668\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.8397 - mae: 4.1199 - val_loss: 31.7416 - val_mae: 4.1231\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5537 - mae: 4.2041 - val_loss: 31.5019 - val_mae: 4.2127\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7871 - mae: 4.1660 - val_loss: 31.5013 - val_mae: 4.1733\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.4437 - mae: 4.1984 - val_loss: 31.4506 - val_mae: 4.1703\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3559 - mae: 4.1712 - val_loss: 31.5826 - val_mae: 4.1260\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3683 - mae: 4.1935 - val_loss: 31.3674 - val_mae: 4.1895\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3038 - mae: 4.1867 - val_loss: 31.4840 - val_mae: 4.1366\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2430 - mae: 4.1639 - val_loss: 31.3568 - val_mae: 4.1645\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2516 - mae: 4.1290 - val_loss: 31.3859 - val_mae: 4.1439\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1476 - mae: 4.2325 - val_loss: 31.3197 - val_mae: 4.1658\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1408 - mae: 4.1637 - val_loss: 31.3750 - val_mae: 4.1430\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0480 - mae: 4.1548 - val_loss: 31.2511 - val_mae: 4.1852\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2168 - mae: 4.3049 - val_loss: 31.3959 - val_mae: 4.1256\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.0640 - mae: 4.0995 - val_loss: 31.2547 - val_mae: 4.1452\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.0251 - mae: 4.2316 - val_loss: 31.4025 - val_mae: 4.1196\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.8896 - mae: 4.1225 - val_loss: 31.2719 - val_mae: 4.1419\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9652 - mae: 4.1397 - val_loss: 31.2286 - val_mae: 4.1430\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9488 - mae: 4.2782 - val_loss: 31.1412 - val_mae: 4.1917\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9806 - mae: 4.0968 - val_loss: 31.3762 - val_mae: 4.1159\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9654 - mae: 4.2607 - val_loss: 31.1822 - val_mae: 4.1662\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.7257 - mae: 4.1365 - val_loss: 31.3058 - val_mae: 4.1126\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6596 - mae: 4.1797 - val_loss: 31.1618 - val_mae: 4.1445\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6721 - mae: 4.2091 - val_loss: 31.2076 - val_mae: 4.1425\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.6814 - mae: 4.1158 - val_loss: 31.1400 - val_mae: 4.1303\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5934 - mae: 4.1834 - val_loss: 31.1656 - val_mae: 4.1263\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5730 - mae: 4.1054 - val_loss: 31.0684 - val_mae: 4.1547\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4115 - mae: 4.1855 - val_loss: 31.0462 - val_mae: 4.1481\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4899 - mae: 4.1203 - val_loss: 30.9520 - val_mae: 4.1676\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3932 - mae: 4.1955 - val_loss: 31.1897 - val_mae: 4.1113\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3967 - mae: 4.1467 - val_loss: 31.0278 - val_mae: 4.1300\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4043 - mae: 4.1445 - val_loss: 30.8918 - val_mae: 4.1714\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2859 - mae: 4.1451 - val_loss: 30.9606 - val_mae: 4.1338\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3309 - mae: 4.1255 - val_loss: 30.8347 - val_mae: 4.1787\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1650 - mae: 4.2001 - val_loss: 31.0670 - val_mae: 4.1065\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2843 - mae: 4.0893 - val_loss: 30.9408 - val_mae: 4.1205\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2252 - mae: 4.1530 - val_loss: 30.9136 - val_mae: 4.1173\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1912 - mae: 4.1818 - val_loss: 30.9528 - val_mae: 4.1092\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1631 - mae: 4.1522 - val_loss: 30.9756 - val_mae: 4.1124\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1266 - mae: 4.1751 - val_loss: 30.9414 - val_mae: 4.1068\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1081 - mae: 4.1641 - val_loss: 30.9321 - val_mae: 4.1208\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 34.1388 - mae: 4.2184\n",
      "Mean Absolute Error on Test Data: 4.218357563018799\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.051906572060708034\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 127.7506 - mae: 8.9899 - val_loss: 83.1520 - val_mae: 7.1997\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 103.4936 - mae: 7.5829 - val_loss: 56.6406 - val_mae: 5.2766\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 67.3708 - mae: 5.4516 - val_loss: 32.4650 - val_mae: 3.9227\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.1060 - mae: 4.7667 - val_loss: 34.7808 - val_mae: 4.5745\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.9390 - mae: 4.8043 - val_loss: 33.0075 - val_mae: 4.3330\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.7800 - mae: 4.6714 - val_loss: 33.1958 - val_mae: 4.3625\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.4030 - mae: 4.7522 - val_loss: 33.1761 - val_mae: 4.3586\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.2571 - mae: 4.6880 - val_loss: 32.9235 - val_mae: 4.3189\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.0249 - mae: 4.6513 - val_loss: 33.0471 - val_mae: 4.3324\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.8504 - mae: 4.7256 - val_loss: 32.9117 - val_mae: 4.3037\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.6687 - mae: 4.6438 - val_loss: 32.7746 - val_mae: 4.2770\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.7168 - mae: 4.7349 - val_loss: 32.8804 - val_mae: 4.2838\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.3966 - mae: 4.6374 - val_loss: 33.1344 - val_mae: 4.3104\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.3812 - mae: 4.6636 - val_loss: 33.1219 - val_mae: 4.3019\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.2659 - mae: 4.6257 - val_loss: 33.0092 - val_mae: 4.2787\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1780 - mae: 4.6824 - val_loss: 33.2261 - val_mae: 4.3021\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1361 - mae: 4.6778 - val_loss: 33.4979 - val_mae: 4.3376\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1516 - mae: 4.6330 - val_loss: 33.3953 - val_mae: 4.3177\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9992 - mae: 4.6865 - val_loss: 33.2020 - val_mae: 4.2798\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 44.1318 - mae: 4.6174 - val_loss: 33.7957 - val_mae: 4.3612\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.0123 - mae: 4.6998 - val_loss: 33.6613 - val_mae: 4.3399\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9330 - mae: 4.6546 - val_loss: 33.4231 - val_mae: 4.2995\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.8890 - mae: 4.6150 - val_loss: 33.0235 - val_mae: 4.2455\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.9060 - mae: 4.7418 - val_loss: 33.2877 - val_mae: 4.2878\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7377 - mae: 4.6027 - val_loss: 33.1607 - val_mae: 4.2633\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.7385 - mae: 4.6217 - val_loss: 33.4592 - val_mae: 4.3090\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6598 - mae: 4.6375 - val_loss: 33.1194 - val_mae: 4.2564\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7008 - mae: 4.6449 - val_loss: 33.4341 - val_mae: 4.3003\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6063 - mae: 4.6022 - val_loss: 33.0746 - val_mae: 4.2467\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5342 - mae: 4.6646 - val_loss: 33.3580 - val_mae: 4.2896\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.6979 - mae: 4.5798 - val_loss: 33.0274 - val_mae: 4.2367\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5564 - mae: 4.7175 - val_loss: 33.5988 - val_mae: 4.3231\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3616 - mae: 4.6047 - val_loss: 32.6940 - val_mae: 4.1824\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3483 - mae: 4.5987 - val_loss: 33.2225 - val_mae: 4.2642\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3293 - mae: 4.6621 - val_loss: 33.4289 - val_mae: 4.2973\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3277 - mae: 4.6184 - val_loss: 33.1288 - val_mae: 4.2507\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3805 - mae: 4.6432 - val_loss: 33.1675 - val_mae: 4.2578\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3502 - mae: 4.6680 - val_loss: 33.2554 - val_mae: 4.2610\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3303 - mae: 4.5926 - val_loss: 33.0007 - val_mae: 4.2314\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.3203 - mae: 4.6585 - val_loss: 32.9758 - val_mae: 4.2244\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1911 - mae: 4.5780 - val_loss: 32.9855 - val_mae: 4.2232\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1589 - mae: 4.6328 - val_loss: 32.9545 - val_mae: 4.2159\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1665 - mae: 4.6524 - val_loss: 32.9708 - val_mae: 4.2173\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.2915 - mae: 4.7020 - val_loss: 32.9411 - val_mae: 4.2070\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.1519 - mae: 4.5452 - val_loss: 32.9862 - val_mae: 4.2156\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0736 - mae: 4.6468 - val_loss: 33.9325 - val_mae: 4.3511\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.5097 - mae: 4.6141 - val_loss: 33.7665 - val_mae: 4.3326\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 43.0556 - mae: 4.6087 - val_loss: 33.0521 - val_mae: 4.2217\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9713 - mae: 4.6511 - val_loss: 33.4625 - val_mae: 4.2893\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.9122 - mae: 4.5982 - val_loss: 32.9714 - val_mae: 4.2148\n",
      "8/8 [==============================] - 0s 993us/step - loss: 37.1102 - mae: 4.2687\n",
      "Mean Absolute Error on Test Data: 4.268736839294434\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.08549440889324733\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 0s 9ms/step - loss: 2.9136 - mae: 1.1971 - val_loss: 2.6697 - val_mae: 0.9028\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.7142 - mae: 0.8593 - val_loss: 2.2565 - val_mae: 1.0497\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.6020 - mae: 0.9166 - val_loss: 2.2687 - val_mae: 1.0680\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5840 - mae: 0.9019 - val_loss: 2.2535 - val_mae: 1.0364\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5687 - mae: 0.8807 - val_loss: 2.2568 - val_mae: 1.0284\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5586 - mae: 0.8843 - val_loss: 2.2552 - val_mae: 1.0403\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5489 - mae: 0.8832 - val_loss: 2.2524 - val_mae: 1.0302\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5418 - mae: 0.8741 - val_loss: 2.2474 - val_mae: 1.0301\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5410 - mae: 0.8915 - val_loss: 2.2475 - val_mae: 1.0463\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5262 - mae: 0.8754 - val_loss: 2.2457 - val_mae: 1.0224\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5265 - mae: 0.8707 - val_loss: 2.2398 - val_mae: 1.0319\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5196 - mae: 0.8796 - val_loss: 2.2384 - val_mae: 1.0342\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5330 - mae: 0.8521 - val_loss: 2.2399 - val_mae: 1.0190\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5119 - mae: 0.8804 - val_loss: 2.2382 - val_mae: 1.0489\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5067 - mae: 0.8732 - val_loss: 2.2326 - val_mae: 1.0296\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4969 - mae: 0.8657 - val_loss: 2.2256 - val_mae: 1.0282\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5000 - mae: 0.8758 - val_loss: 2.2195 - val_mae: 1.0331\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.5212 - mae: 0.8467 - val_loss: 2.2134 - val_mae: 1.0234\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4954 - mae: 0.8829 - val_loss: 2.2215 - val_mae: 1.0533\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4759 - mae: 0.8639 - val_loss: 2.2299 - val_mae: 1.0126\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4910 - mae: 0.8585 - val_loss: 2.2221 - val_mae: 1.0368\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4851 - mae: 0.8564 - val_loss: 2.2227 - val_mae: 1.0371\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4770 - mae: 0.8706 - val_loss: 2.2161 - val_mae: 1.0365\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4716 - mae: 0.8597 - val_loss: 2.2106 - val_mae: 1.0281\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4729 - mae: 0.8500 - val_loss: 2.2064 - val_mae: 1.0249\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4690 - mae: 0.8564 - val_loss: 2.2081 - val_mae: 1.0321\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4642 - mae: 0.8720 - val_loss: 2.2062 - val_mae: 1.0423\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4618 - mae: 0.8499 - val_loss: 2.2148 - val_mae: 1.0252\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4601 - mae: 0.8647 - val_loss: 2.2105 - val_mae: 1.0281\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4624 - mae: 0.8442 - val_loss: 2.2043 - val_mae: 1.0273\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4535 - mae: 0.8632 - val_loss: 2.2005 - val_mae: 1.0387\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4497 - mae: 0.8585 - val_loss: 2.2154 - val_mae: 1.0324\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4403 - mae: 0.8582 - val_loss: 2.2049 - val_mae: 1.0300\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4388 - mae: 0.8479 - val_loss: 2.2014 - val_mae: 1.0232\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4402 - mae: 0.8570 - val_loss: 2.2108 - val_mae: 1.0139\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4366 - mae: 0.8430 - val_loss: 2.1890 - val_mae: 1.0224\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4362 - mae: 0.8666 - val_loss: 2.2016 - val_mae: 1.0315\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4336 - mae: 0.8348 - val_loss: 2.2140 - val_mae: 1.0183\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4180 - mae: 0.8468 - val_loss: 2.2042 - val_mae: 1.0430\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4597 - mae: 0.8926 - val_loss: 2.2043 - val_mae: 1.0304\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4834 - mae: 0.8231 - val_loss: 2.2140 - val_mae: 1.0144\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4098 - mae: 0.8536 - val_loss: 2.2163 - val_mae: 1.0597\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4228 - mae: 0.8680 - val_loss: 2.2148 - val_mae: 1.0261\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4255 - mae: 0.8428 - val_loss: 2.2080 - val_mae: 1.0418\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4152 - mae: 0.8562 - val_loss: 2.2284 - val_mae: 1.0375\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4101 - mae: 0.8392 - val_loss: 2.2183 - val_mae: 1.0356\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4061 - mae: 0.8535 - val_loss: 2.2080 - val_mae: 1.0440\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4283 - mae: 0.8470 - val_loss: 2.2060 - val_mae: 1.0169\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4028 - mae: 0.8582 - val_loss: 2.2172 - val_mae: 1.0538\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 1.4021 - mae: 0.8469 - val_loss: 2.2106 - val_mae: 1.0305\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.0353 - mae: 0.8100\n",
      "Mean Absolute Error on Test Data: 0.8099567890167236\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.09205300119559556\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 88.7331 - mae: 7.7326 - val_loss: 104.1395 - val_mae: 7.6445\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 65.2121 - mae: 6.1260 - val_loss: 72.1332 - val_mae: 5.5249\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.0514 - mae: 4.0791 - val_loss: 45.8715 - val_mae: 4.2326\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.5130 - mae: 3.8329 - val_loss: 43.9870 - val_mae: 4.4029\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9975 - mae: 3.7317 - val_loss: 44.5838 - val_mae: 4.2600\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.8845 - mae: 3.7158 - val_loss: 44.2190 - val_mae: 4.2691\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7151 - mae: 3.6943 - val_loss: 44.2646 - val_mae: 4.2473\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5866 - mae: 3.7099 - val_loss: 43.9465 - val_mae: 4.2761\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.6223 - mae: 3.6633 - val_loss: 44.2029 - val_mae: 4.2215\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.6214 - mae: 3.7549 - val_loss: 43.8834 - val_mae: 4.2413\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.3658 - mae: 3.6935 - val_loss: 43.9663 - val_mae: 4.2209\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.4521 - mae: 3.6521 - val_loss: 43.7805 - val_mae: 4.2387\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.3180 - mae: 3.7198 - val_loss: 43.7207 - val_mae: 4.2263\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.2544 - mae: 3.6369 - val_loss: 43.9830 - val_mae: 4.1858\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1694 - mae: 3.6544 - val_loss: 43.5696 - val_mae: 4.2213\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1607 - mae: 3.6800 - val_loss: 43.5721 - val_mae: 4.2155\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1462 - mae: 3.6546 - val_loss: 43.5233 - val_mae: 4.2037\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0943 - mae: 3.6962 - val_loss: 43.8484 - val_mae: 4.1667\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0533 - mae: 3.6341 - val_loss: 43.7743 - val_mae: 4.1739\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0214 - mae: 3.6559 - val_loss: 43.5288 - val_mae: 4.1935\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0116 - mae: 3.6683 - val_loss: 43.6749 - val_mae: 4.1723\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0074 - mae: 3.6437 - val_loss: 43.3709 - val_mae: 4.2216\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9897 - mae: 3.6764 - val_loss: 43.6317 - val_mae: 4.1829\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9637 - mae: 3.6384 - val_loss: 43.4093 - val_mae: 4.1944\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9307 - mae: 3.6910 - val_loss: 43.8037 - val_mae: 4.1504\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9619 - mae: 3.6047 - val_loss: 43.4138 - val_mae: 4.2030\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9985 - mae: 3.6802 - val_loss: 43.9980 - val_mae: 4.1378\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8902 - mae: 3.6272 - val_loss: 43.3127 - val_mae: 4.2151\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9007 - mae: 3.6534 - val_loss: 43.7064 - val_mae: 4.1528\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8479 - mae: 3.6294 - val_loss: 43.3041 - val_mae: 4.2011\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8694 - mae: 3.6438 - val_loss: 43.4250 - val_mae: 4.1806\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8435 - mae: 3.6920 - val_loss: 43.7856 - val_mae: 4.1550\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8713 - mae: 3.6227 - val_loss: 43.7909 - val_mae: 4.1487\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7601 - mae: 3.6505 - val_loss: 43.4705 - val_mae: 4.1853\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7598 - mae: 3.6211 - val_loss: 43.5429 - val_mae: 4.1811\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8060 - mae: 3.6561 - val_loss: 43.9170 - val_mae: 4.1318\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7381 - mae: 3.6092 - val_loss: 43.6102 - val_mae: 4.1677\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7329 - mae: 3.6762 - val_loss: 43.9040 - val_mae: 4.1612\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8452 - mae: 3.6003 - val_loss: 43.2991 - val_mae: 4.2480\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7080 - mae: 3.6596 - val_loss: 43.7008 - val_mae: 4.1634\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.6333 - mae: 3.6256 - val_loss: 43.8047 - val_mae: 4.1550\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7388 - mae: 3.6570 - val_loss: 43.9326 - val_mae: 4.1391\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5939 - mae: 3.5924 - val_loss: 43.5441 - val_mae: 4.1910\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5981 - mae: 3.6635 - val_loss: 43.5726 - val_mae: 4.1865\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5848 - mae: 3.6430 - val_loss: 43.5037 - val_mae: 4.2054\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.6364 - mae: 3.5909 - val_loss: 43.4047 - val_mae: 4.2073\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5363 - mae: 3.6462 - val_loss: 43.6966 - val_mae: 4.1566\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5378 - mae: 3.6354 - val_loss: 43.8798 - val_mae: 4.1499\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5730 - mae: 3.6186 - val_loss: 43.8467 - val_mae: 4.1489\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5327 - mae: 3.6238 - val_loss: 43.5450 - val_mae: 4.1700\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 47.9528 - mae: 4.3676\n",
      "Mean Absolute Error on Test Data: 4.367578983306885\n",
      "8/8 [==============================] - 0s 902us/step\n",
      "R-squared: 0.06184781390751093\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 84.3404 - mae: 7.3341 - val_loss: 71.8310 - val_mae: 6.5874\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 66.2526 - mae: 6.0640 - val_loss: 49.8402 - val_mae: 4.9310\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8175 - mae: 4.2189 - val_loss: 28.5004 - val_mae: 3.6154\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 29.3062 - mae: 3.7601 - val_loss: 27.0105 - val_mae: 3.9223\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.6514 - mae: 3.7968 - val_loss: 26.3821 - val_mae: 3.7713\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.4040 - mae: 3.6982 - val_loss: 26.1838 - val_mae: 3.7480\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.1904 - mae: 3.7149 - val_loss: 26.1119 - val_mae: 3.7920\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.0577 - mae: 3.7186 - val_loss: 25.8839 - val_mae: 3.7337\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.9534 - mae: 3.7177 - val_loss: 25.7844 - val_mae: 3.7437\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7849 - mae: 3.6804 - val_loss: 25.7332 - val_mae: 3.7776\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.7074 - mae: 3.7194 - val_loss: 25.5543 - val_mae: 3.7461\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5001 - mae: 3.6856 - val_loss: 25.4574 - val_mae: 3.7360\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.5691 - mae: 3.6287 - val_loss: 25.4298 - val_mae: 3.7554\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.4479 - mae: 3.7379 - val_loss: 25.3513 - val_mae: 3.7514\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.2825 - mae: 3.6986 - val_loss: 25.2734 - val_mae: 3.7348\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.2099 - mae: 3.6298 - val_loss: 25.2000 - val_mae: 3.7286\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1180 - mae: 3.6631 - val_loss: 25.2056 - val_mae: 3.7476\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0817 - mae: 3.6544 - val_loss: 25.1449 - val_mae: 3.7485\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.0490 - mae: 3.6713 - val_loss: 25.1014 - val_mae: 3.7367\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.9766 - mae: 3.6353 - val_loss: 25.0911 - val_mae: 3.7557\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8986 - mae: 3.6753 - val_loss: 25.1601 - val_mae: 3.7951\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8030 - mae: 3.6366 - val_loss: 25.0130 - val_mae: 3.7286\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8956 - mae: 3.6326 - val_loss: 25.0491 - val_mae: 3.7694\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.8222 - mae: 3.6794 - val_loss: 24.9651 - val_mae: 3.7225\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7893 - mae: 3.5896 - val_loss: 25.0113 - val_mae: 3.7783\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7981 - mae: 3.6478 - val_loss: 24.9595 - val_mae: 3.7327\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7560 - mae: 3.6837 - val_loss: 25.0116 - val_mae: 3.7916\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.7217 - mae: 3.6550 - val_loss: 24.9615 - val_mae: 3.7107\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.6533 - mae: 3.6769 - val_loss: 24.9608 - val_mae: 3.7778\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5415 - mae: 3.6533 - val_loss: 24.9357 - val_mae: 3.7585\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.6577 - mae: 3.6724 - val_loss: 24.8753 - val_mae: 3.7366\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5878 - mae: 3.6529 - val_loss: 24.8512 - val_mae: 3.7270\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.5354 - mae: 3.6204 - val_loss: 24.7959 - val_mae: 3.7284\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.4833 - mae: 3.5981 - val_loss: 24.9266 - val_mae: 3.7982\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3646 - mae: 3.6615 - val_loss: 24.7876 - val_mae: 3.7568\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3258 - mae: 3.6016 - val_loss: 24.7930 - val_mae: 3.7657\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.3982 - mae: 3.6862 - val_loss: 24.7431 - val_mae: 3.7441\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.2912 - mae: 3.6241 - val_loss: 24.7386 - val_mae: 3.7568\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.2122 - mae: 3.6073 - val_loss: 24.7527 - val_mae: 3.7691\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.2272 - mae: 3.6109 - val_loss: 24.7550 - val_mae: 3.7755\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.2548 - mae: 3.5920 - val_loss: 24.7515 - val_mae: 3.7638\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.1689 - mae: 3.6542 - val_loss: 24.7285 - val_mae: 3.7643\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1272 - mae: 3.6060 - val_loss: 24.7210 - val_mae: 3.7749\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.0908 - mae: 3.6220 - val_loss: 24.6866 - val_mae: 3.7642\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.0961 - mae: 3.5661 - val_loss: 24.6832 - val_mae: 3.7609\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.4748 - mae: 3.7021 - val_loss: 24.6280 - val_mae: 3.7533\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.0321 - mae: 3.6467 - val_loss: 24.6330 - val_mae: 3.7540\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.9500 - mae: 3.5954 - val_loss: 24.5974 - val_mae: 3.7518\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 26.0144 - mae: 3.6240 - val_loss: 24.6927 - val_mae: 3.7930\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 25.9186 - mae: 3.6480 - val_loss: 24.6030 - val_mae: 3.7099\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 28.4634 - mae: 3.8835\n",
      "Mean Absolute Error on Test Data: 3.8834779262542725\n",
      "7/7 [==============================] - 0s 924us/step\n",
      "R-squared: 0.023739797317215316\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 7.9092 - mae: 1.9899 - val_loss: 5.0557 - val_mae: 1.3896\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 4.4187 - mae: 1.3590 - val_loss: 3.6825 - val_mae: 1.3224\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.9871 - mae: 1.5052 - val_loss: 3.7513 - val_mae: 1.4089\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.9078 - mae: 1.4284 - val_loss: 3.6855 - val_mae: 1.3003\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.9048 - mae: 1.3914 - val_loss: 3.6867 - val_mae: 1.3350\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8878 - mae: 1.4457 - val_loss: 3.7092 - val_mae: 1.3630\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9007 - mae: 1.4130 - val_loss: 3.6958 - val_mae: 1.3302\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8860 - mae: 1.4470 - val_loss: 3.6969 - val_mae: 1.3387\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8644 - mae: 1.4094 - val_loss: 3.6965 - val_mae: 1.3224\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8649 - mae: 1.4239 - val_loss: 3.7046 - val_mae: 1.3444\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8626 - mae: 1.4205 - val_loss: 3.6979 - val_mae: 1.3384\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8470 - mae: 1.4177 - val_loss: 3.6924 - val_mae: 1.3356\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8479 - mae: 1.4190 - val_loss: 3.7049 - val_mae: 1.3468\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8412 - mae: 1.4122 - val_loss: 3.7003 - val_mae: 1.3370\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8421 - mae: 1.4347 - val_loss: 3.7035 - val_mae: 1.3427\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8375 - mae: 1.3993 - val_loss: 3.6952 - val_mae: 1.3349\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8358 - mae: 1.4363 - val_loss: 3.7028 - val_mae: 1.3477\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8263 - mae: 1.4029 - val_loss: 3.6975 - val_mae: 1.3333\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8312 - mae: 1.4270 - val_loss: 3.6987 - val_mae: 1.3386\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8036 - mae: 1.4103 - val_loss: 3.7127 - val_mae: 1.3640\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8204 - mae: 1.4520 - val_loss: 3.7147 - val_mae: 1.3557\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8003 - mae: 1.3971 - val_loss: 3.7042 - val_mae: 1.3342\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8177 - mae: 1.4239 - val_loss: 3.7008 - val_mae: 1.3399\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7986 - mae: 1.4272 - val_loss: 3.7005 - val_mae: 1.3445\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.8040 - mae: 1.4253 - val_loss: 3.7092 - val_mae: 1.3551\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7999 - mae: 1.3872 - val_loss: 3.7083 - val_mae: 1.3250\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7979 - mae: 1.4387 - val_loss: 3.7141 - val_mae: 1.3648\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7760 - mae: 1.4092 - val_loss: 3.6998 - val_mae: 1.3529\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7798 - mae: 1.4140 - val_loss: 3.7095 - val_mae: 1.3375\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7779 - mae: 1.4252 - val_loss: 3.7157 - val_mae: 1.3628\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7629 - mae: 1.4083 - val_loss: 3.7071 - val_mae: 1.3543\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7612 - mae: 1.4238 - val_loss: 3.7080 - val_mae: 1.3530\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7616 - mae: 1.4115 - val_loss: 3.7190 - val_mae: 1.3559\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7583 - mae: 1.3894 - val_loss: 3.7104 - val_mae: 1.3697\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7521 - mae: 1.4175 - val_loss: 3.7037 - val_mae: 1.3593\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7487 - mae: 1.4256 - val_loss: 3.7288 - val_mae: 1.3690\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7448 - mae: 1.3955 - val_loss: 3.7204 - val_mae: 1.3533\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7426 - mae: 1.4235 - val_loss: 3.7153 - val_mae: 1.3666\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7329 - mae: 1.4050 - val_loss: 3.7190 - val_mae: 1.3590\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7369 - mae: 1.4178 - val_loss: 3.7219 - val_mae: 1.3733\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7269 - mae: 1.4130 - val_loss: 3.7210 - val_mae: 1.3441\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7505 - mae: 1.4036 - val_loss: 3.7330 - val_mae: 1.3862\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7353 - mae: 1.4046 - val_loss: 3.7281 - val_mae: 1.3582\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7408 - mae: 1.4179 - val_loss: 3.7070 - val_mae: 1.3339\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7179 - mae: 1.4027 - val_loss: 3.7237 - val_mae: 1.3722\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7139 - mae: 1.4131 - val_loss: 3.7451 - val_mae: 1.3673\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7382 - mae: 1.3882 - val_loss: 3.7567 - val_mae: 1.4010\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.7194 - mae: 1.4173 - val_loss: 3.7361 - val_mae: 1.3703\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7163 - mae: 1.4005 - val_loss: 3.7446 - val_mae: 1.3951\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 3.6926 - mae: 1.4327 - val_loss: 3.7286 - val_mae: 1.3265\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.7307 - mae: 1.3469\n",
      "Mean Absolute Error on Test Data: 1.3468949794769287\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0925150924914021\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.1011 - mae: 1.3857 - val_loss: 2.1438 - val_mae: 1.1464\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.8441 - mae: 0.9310 - val_loss: 1.2608 - val_mae: 0.7500\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.3158 - mae: 0.7554 - val_loss: 1.0467 - val_mae: 0.8328\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2549 - mae: 0.8346 - val_loss: 1.0308 - val_mae: 0.8396\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.2193 - mae: 0.8036 - val_loss: 0.9902 - val_mae: 0.7947\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1956 - mae: 0.7776 - val_loss: 0.9723 - val_mae: 0.7917\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1818 - mae: 0.7757 - val_loss: 0.9560 - val_mae: 0.7902\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1734 - mae: 0.7867 - val_loss: 0.9364 - val_mae: 0.7987\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1608 - mae: 0.7803 - val_loss: 0.9194 - val_mae: 0.7779\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1504 - mae: 0.7644 - val_loss: 0.9067 - val_mae: 0.7770\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.1488 - mae: 0.7823 - val_loss: 0.8887 - val_mae: 0.7866\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1350 - mae: 0.7656 - val_loss: 0.8708 - val_mae: 0.7616\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1309 - mae: 0.7592 - val_loss: 0.8618 - val_mae: 0.7755\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1314 - mae: 0.7884 - val_loss: 0.8583 - val_mae: 0.7792\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1233 - mae: 0.7622 - val_loss: 0.8445 - val_mae: 0.7439\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1188 - mae: 0.7633 - val_loss: 0.8367 - val_mae: 0.7728\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1029 - mae: 0.7615 - val_loss: 0.8212 - val_mae: 0.7367\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1058 - mae: 0.7515 - val_loss: 0.8097 - val_mae: 0.7562\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1023 - mae: 0.7650 - val_loss: 0.8012 - val_mae: 0.7469\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0965 - mae: 0.7661 - val_loss: 0.8069 - val_mae: 0.7632\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.1034 - mae: 0.7804 - val_loss: 0.8018 - val_mae: 0.7614\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0970 - mae: 0.7771 - val_loss: 0.7913 - val_mae: 0.7479\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0883 - mae: 0.7481 - val_loss: 0.7895 - val_mae: 0.7364\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0858 - mae: 0.7421 - val_loss: 0.7900 - val_mae: 0.7367\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0803 - mae: 0.7513 - val_loss: 0.7920 - val_mae: 0.7500\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.0843 - mae: 0.7542 - val_loss: 0.7852 - val_mae: 0.7336\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0801 - mae: 0.7372 - val_loss: 0.7875 - val_mae: 0.7430\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0968 - mae: 0.7782 - val_loss: 0.8066 - val_mae: 0.7704\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0989 - mae: 0.7350 - val_loss: 0.8003 - val_mae: 0.7227\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0818 - mae: 0.7577 - val_loss: 0.8031 - val_mae: 0.7703\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0842 - mae: 0.7520 - val_loss: 0.7830 - val_mae: 0.7355\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0678 - mae: 0.7464 - val_loss: 0.7828 - val_mae: 0.7550\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.0684 - mae: 0.7526 - val_loss: 0.7726 - val_mae: 0.7417\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0673 - mae: 0.7498 - val_loss: 0.7684 - val_mae: 0.7384\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0667 - mae: 0.7382 - val_loss: 0.7665 - val_mae: 0.7392\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0651 - mae: 0.7536 - val_loss: 0.7764 - val_mae: 0.7568\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0622 - mae: 0.7438 - val_loss: 0.7648 - val_mae: 0.7376\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0657 - mae: 0.7551 - val_loss: 0.7669 - val_mae: 0.7492\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0596 - mae: 0.7436 - val_loss: 0.7731 - val_mae: 0.7444\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0581 - mae: 0.7456 - val_loss: 0.7744 - val_mae: 0.7514\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0569 - mae: 0.7371 - val_loss: 0.7678 - val_mae: 0.7352\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0593 - mae: 0.7410 - val_loss: 0.7634 - val_mae: 0.7411\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0593 - mae: 0.7249 - val_loss: 0.7753 - val_mae: 0.7366\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0529 - mae: 0.7368 - val_loss: 0.7750 - val_mae: 0.7510\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0526 - mae: 0.7523 - val_loss: 0.7738 - val_mae: 0.7555\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0535 - mae: 0.7360 - val_loss: 0.7693 - val_mae: 0.7422\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0505 - mae: 0.7477 - val_loss: 0.7643 - val_mae: 0.7532\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0607 - mae: 0.7354 - val_loss: 0.7612 - val_mae: 0.7436\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0634 - mae: 0.7698 - val_loss: 0.7772 - val_mae: 0.7655\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 1.0368 - mae: 0.7410 - val_loss: 0.7657 - val_mae: 0.7287\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7157 - mae: 0.7993\n",
      "Mean Absolute Error on Test Data: 0.7993057370185852\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "R-squared: -0.10817101492050174\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 21.1144 - mae: 3.1860 - val_loss: 21.4655 - val_mae: 2.7169\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 13.0473 - mae: 2.3108 - val_loss: 15.8339 - val_mae: 2.4868\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.4157 - mae: 2.4428 - val_loss: 15.7544 - val_mae: 2.5002\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.2316 - mae: 2.3132 - val_loss: 15.7493 - val_mae: 2.4479\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.1676 - mae: 2.3000 - val_loss: 15.6305 - val_mae: 2.4691\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.1289 - mae: 2.3366 - val_loss: 15.5897 - val_mae: 2.4646\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1585 - mae: 2.3187 - val_loss: 15.5123 - val_mae: 2.4829\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0762 - mae: 2.3117 - val_loss: 15.5023 - val_mae: 2.4489\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0803 - mae: 2.3476 - val_loss: 15.4372 - val_mae: 2.4585\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0727 - mae: 2.3211 - val_loss: 15.4901 - val_mae: 2.4249\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0512 - mae: 2.3049 - val_loss: 15.4001 - val_mae: 2.4467\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0781 - mae: 2.3566 - val_loss: 15.3521 - val_mae: 2.4588\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0052 - mae: 2.3472 - val_loss: 15.3859 - val_mae: 2.4296\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.1179 - mae: 2.2641 - val_loss: 15.2987 - val_mae: 2.4635\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9616 - mae: 2.3092 - val_loss: 15.3270 - val_mae: 2.4435\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9680 - mae: 2.3190 - val_loss: 15.3459 - val_mae: 2.4251\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9688 - mae: 2.2693 - val_loss: 15.2670 - val_mae: 2.4509\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9653 - mae: 2.3413 - val_loss: 15.2270 - val_mae: 2.4652\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9245 - mae: 2.3002 - val_loss: 15.3081 - val_mae: 2.4171\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9697 - mae: 2.2955 - val_loss: 15.2646 - val_mae: 2.4263\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9044 - mae: 2.2857 - val_loss: 15.2273 - val_mae: 2.4369\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9248 - mae: 2.2809 - val_loss: 15.1863 - val_mae: 2.4469\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8927 - mae: 2.3262 - val_loss: 15.1695 - val_mae: 2.4514\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9333 - mae: 2.3508 - val_loss: 15.2105 - val_mae: 2.4175\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8829 - mae: 2.2718 - val_loss: 15.1070 - val_mae: 2.4479\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9147 - mae: 2.3265 - val_loss: 15.1264 - val_mae: 2.4453\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8796 - mae: 2.3174 - val_loss: 15.2837 - val_mae: 2.4081\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9485 - mae: 2.2946 - val_loss: 15.1090 - val_mae: 2.4374\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8493 - mae: 2.3091 - val_loss: 15.2050 - val_mae: 2.4175\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8453 - mae: 2.3041 - val_loss: 15.0913 - val_mae: 2.4548\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8376 - mae: 2.2876 - val_loss: 15.0910 - val_mae: 2.4394\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8453 - mae: 2.3126 - val_loss: 15.1572 - val_mae: 2.4238\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8320 - mae: 2.2908 - val_loss: 15.1256 - val_mae: 2.4350\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8405 - mae: 2.3038 - val_loss: 15.0968 - val_mae: 2.4524\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8660 - mae: 2.2848 - val_loss: 15.0634 - val_mae: 2.4540\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8429 - mae: 2.2824 - val_loss: 15.0611 - val_mae: 2.4567\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.9075 - mae: 2.3458 - val_loss: 15.0905 - val_mae: 2.4516\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.9569 - mae: 2.3761 - val_loss: 15.1046 - val_mae: 2.4463\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 11.0238 - mae: 2.3739 - val_loss: 15.1189 - val_mae: 2.4246\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8213 - mae: 2.2635 - val_loss: 15.0135 - val_mae: 2.4659\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8834 - mae: 2.3368 - val_loss: 15.1439 - val_mae: 2.4220\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8742 - mae: 2.3484 - val_loss: 15.0816 - val_mae: 2.4429\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7753 - mae: 2.3031 - val_loss: 15.0141 - val_mae: 2.4611\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7729 - mae: 2.3248 - val_loss: 15.0655 - val_mae: 2.4556\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7350 - mae: 2.3062 - val_loss: 15.1751 - val_mae: 2.4211\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.8156 - mae: 2.2519 - val_loss: 15.0521 - val_mae: 2.4731\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7599 - mae: 2.3318 - val_loss: 15.1015 - val_mae: 2.4295\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7961 - mae: 2.2652 - val_loss: 15.0705 - val_mae: 2.4371\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7702 - mae: 2.3037 - val_loss: 15.1582 - val_mae: 2.4184\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 10.7842 - mae: 2.3166 - val_loss: 15.1098 - val_mae: 2.4434\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.8420 - mae: 2.3139\n",
      "Mean Absolute Error on Test Data: 2.3139138221740723\n",
      "7/7 [==============================] - 0s 994us/step\n",
      "R-squared: 0.07581693078994034\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 10.9639 - mae: 2.1499 - val_loss: 4.5786 - val_mae: 1.5101\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 7.4453 - mae: 1.5918 - val_loss: 3.0784 - val_mae: 1.4081\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.5211 - mae: 1.6706 - val_loss: 3.2449 - val_mae: 1.4871\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.4227 - mae: 1.6087 - val_loss: 3.1175 - val_mae: 1.4295\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.3869 - mae: 1.6075 - val_loss: 3.1444 - val_mae: 1.4412\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3794 - mae: 1.5900 - val_loss: 3.1210 - val_mae: 1.4220\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3381 - mae: 1.6039 - val_loss: 3.2220 - val_mae: 1.4662\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.3444 - mae: 1.6269 - val_loss: 3.1628 - val_mae: 1.4337\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.3176 - mae: 1.5863 - val_loss: 3.2133 - val_mae: 1.4542\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.3055 - mae: 1.6070 - val_loss: 3.2074 - val_mae: 1.4477\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2978 - mae: 1.5959 - val_loss: 3.2317 - val_mae: 1.4552\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.3015 - mae: 1.5962 - val_loss: 3.2807 - val_mae: 1.4776\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2941 - mae: 1.5918 - val_loss: 3.2597 - val_mae: 1.4676\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2863 - mae: 1.6015 - val_loss: 3.2435 - val_mae: 1.4575\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2943 - mae: 1.6247 - val_loss: 3.2438 - val_mae: 1.4579\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2876 - mae: 1.5569 - val_loss: 3.1962 - val_mae: 1.4337\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2662 - mae: 1.5850 - val_loss: 3.2606 - val_mae: 1.4612\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2478 - mae: 1.6134 - val_loss: 3.3478 - val_mae: 1.4985\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2587 - mae: 1.5926 - val_loss: 3.2313 - val_mae: 1.4497\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2427 - mae: 1.5947 - val_loss: 3.3289 - val_mae: 1.4914\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2554 - mae: 1.5845 - val_loss: 3.2940 - val_mae: 1.4762\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2351 - mae: 1.5938 - val_loss: 3.2657 - val_mae: 1.4630\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2302 - mae: 1.5721 - val_loss: 3.2909 - val_mae: 1.4783\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2337 - mae: 1.6134 - val_loss: 3.3189 - val_mae: 1.4863\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2107 - mae: 1.5851 - val_loss: 3.2940 - val_mae: 1.4742\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2076 - mae: 1.5868 - val_loss: 3.2671 - val_mae: 1.4580\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1991 - mae: 1.5754 - val_loss: 3.3026 - val_mae: 1.4782\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1908 - mae: 1.5870 - val_loss: 3.3278 - val_mae: 1.4920\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2242 - mae: 1.5786 - val_loss: 3.2341 - val_mae: 1.4450\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1964 - mae: 1.5942 - val_loss: 3.3207 - val_mae: 1.4869\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1809 - mae: 1.5866 - val_loss: 3.3406 - val_mae: 1.4916\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2108 - mae: 1.6235 - val_loss: 3.2385 - val_mae: 1.4395\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.2030 - mae: 1.5474 - val_loss: 3.3242 - val_mae: 1.4854\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1881 - mae: 1.6045 - val_loss: 3.2726 - val_mae: 1.4641\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1665 - mae: 1.5724 - val_loss: 3.2611 - val_mae: 1.4536\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1935 - mae: 1.5919 - val_loss: 3.2547 - val_mae: 1.4455\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1534 - mae: 1.5523 - val_loss: 3.3078 - val_mae: 1.4844\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1772 - mae: 1.5941 - val_loss: 3.4371 - val_mae: 1.5353\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1673 - mae: 1.5831 - val_loss: 3.2238 - val_mae: 1.4191\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1446 - mae: 1.5809 - val_loss: 3.3578 - val_mae: 1.5010\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1347 - mae: 1.5680 - val_loss: 3.3279 - val_mae: 1.4850\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1439 - mae: 1.5733 - val_loss: 3.2733 - val_mae: 1.4567\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1322 - mae: 1.6001 - val_loss: 3.3383 - val_mae: 1.4906\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1248 - mae: 1.5574 - val_loss: 3.3051 - val_mae: 1.4747\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.1084 - mae: 1.5825 - val_loss: 3.3843 - val_mae: 1.5090\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.0980 - mae: 1.5861 - val_loss: 3.2792 - val_mae: 1.4502\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 2ms/step - loss: 6.0957 - mae: 1.5484 - val_loss: 3.3215 - val_mae: 1.4848\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1428 - mae: 1.5707 - val_loss: 3.2632 - val_mae: 1.4513\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0515 - mae: 1.5709 - val_loss: 3.3926 - val_mae: 1.5075\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0635 - mae: 1.5925 - val_loss: 3.3005 - val_mae: 1.4665\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 6.1121 - mae: 1.7004\n",
      "Mean Absolute Error on Test Data: 1.700369954109192\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12583206190907592\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 4.6061 - mae: 1.5030 - val_loss: 1.5508 - val_mae: 0.8609\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.7613 - mae: 1.0992 - val_loss: 1.4415 - val_mae: 0.9878\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.5565 - mae: 1.1447 - val_loss: 1.3594 - val_mae: 0.9437\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.5130 - mae: 1.0918 - val_loss: 1.3413 - val_mae: 0.9335\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4882 - mae: 1.0957 - val_loss: 1.3695 - val_mae: 0.9444\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4596 - mae: 1.0871 - val_loss: 1.4032 - val_mae: 0.9564\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.4369 - mae: 1.0841 - val_loss: 1.4832 - val_mae: 0.9930\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.4159 - mae: 1.1108 - val_loss: 1.5051 - val_mae: 0.9996\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3985 - mae: 1.1015 - val_loss: 1.5039 - val_mae: 0.9929\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.4041 - mae: 1.0769 - val_loss: 1.4816 - val_mae: 0.9824\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3758 - mae: 1.0620 - val_loss: 1.5067 - val_mae: 0.9930\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3763 - mae: 1.0910 - val_loss: 1.4857 - val_mae: 0.9868\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3512 - mae: 1.0720 - val_loss: 1.4574 - val_mae: 0.9706\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3484 - mae: 1.0707 - val_loss: 1.4982 - val_mae: 0.9907\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3389 - mae: 1.0514 - val_loss: 1.4790 - val_mae: 0.9797\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3396 - mae: 1.0834 - val_loss: 1.5188 - val_mae: 0.9982\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3337 - mae: 1.0493 - val_loss: 1.4829 - val_mae: 0.9792\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3326 - mae: 1.0436 - val_loss: 1.5394 - val_mae: 1.0037\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3404 - mae: 1.0971 - val_loss: 1.4867 - val_mae: 0.9792\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3161 - mae: 1.0467 - val_loss: 1.5063 - val_mae: 0.9924\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3120 - mae: 1.0600 - val_loss: 1.5127 - val_mae: 0.9901\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3140 - mae: 1.0618 - val_loss: 1.4652 - val_mae: 0.9651\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3109 - mae: 1.0556 - val_loss: 1.5733 - val_mae: 1.0179\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3139 - mae: 1.0560 - val_loss: 1.6000 - val_mae: 1.0332\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3335 - mae: 1.0993 - val_loss: 1.4745 - val_mae: 0.9617\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3055 - mae: 1.0473 - val_loss: 1.5587 - val_mae: 1.0078\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3091 - mae: 1.0609 - val_loss: 1.6238 - val_mae: 1.0365\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3058 - mae: 1.0841 - val_loss: 1.5361 - val_mae: 0.9991\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3078 - mae: 1.0740 - val_loss: 1.5665 - val_mae: 1.0114\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2900 - mae: 1.0606 - val_loss: 1.6249 - val_mae: 1.0367\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2939 - mae: 1.0492 - val_loss: 1.5998 - val_mae: 1.0254\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.3094 - mae: 1.0748 - val_loss: 1.5482 - val_mae: 1.0084\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3044 - mae: 1.0611 - val_loss: 1.5270 - val_mae: 1.0031\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2959 - mae: 1.0404 - val_loss: 1.5454 - val_mae: 1.0011\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2759 - mae: 1.0543 - val_loss: 1.5601 - val_mae: 1.0159\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2789 - mae: 1.0580 - val_loss: 1.4988 - val_mae: 0.9825\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2843 - mae: 1.0488 - val_loss: 1.4918 - val_mae: 0.9869\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2828 - mae: 1.0337 - val_loss: 1.5478 - val_mae: 1.0074\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2873 - mae: 1.0654 - val_loss: 1.6354 - val_mae: 1.0475\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2786 - mae: 1.0407 - val_loss: 1.5117 - val_mae: 0.9918\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2755 - mae: 1.0569 - val_loss: 1.5466 - val_mae: 1.0055\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2865 - mae: 1.0443 - val_loss: 1.4597 - val_mae: 0.9672\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2869 - mae: 1.0297 - val_loss: 1.5451 - val_mae: 1.0114\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2677 - mae: 1.0578 - val_loss: 1.5045 - val_mae: 0.9861\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2707 - mae: 1.0419 - val_loss: 1.5027 - val_mae: 0.9860\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2731 - mae: 1.0562 - val_loss: 1.5763 - val_mae: 1.0178\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2804 - mae: 1.0578 - val_loss: 1.5357 - val_mae: 0.9925\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2622 - mae: 1.0369 - val_loss: 1.5351 - val_mae: 1.0029\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2613 - mae: 1.0595 - val_loss: 1.4997 - val_mae: 0.9819\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 2.2640 - mae: 1.0331 - val_loss: 1.6126 - val_mae: 1.0388\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.2279 - mae: 1.2712\n",
      "Mean Absolute Error on Test Data: 1.2712225914001465\n",
      "6/6 [==============================] - 0s 896us/step\n",
      "R-squared: 0.042026401484360654\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 18.0222 - mae: 3.0362 - val_loss: 23.0429 - val_mae: 3.0101\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.1215 - mae: 2.2068 - val_loss: 16.3004 - val_mae: 2.4185\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.7930 - mae: 1.9971 - val_loss: 14.4568 - val_mae: 2.4509\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6604 - mae: 2.0497 - val_loss: 14.5913 - val_mae: 2.3996\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6039 - mae: 1.9756 - val_loss: 14.4898 - val_mae: 2.3883\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5767 - mae: 2.0110 - val_loss: 14.3651 - val_mae: 2.3868\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5464 - mae: 1.9985 - val_loss: 14.3271 - val_mae: 2.3766\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5578 - mae: 1.9718 - val_loss: 14.2839 - val_mae: 2.3671\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5344 - mae: 2.0022 - val_loss: 14.2534 - val_mae: 2.3637\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5126 - mae: 2.0009 - val_loss: 14.1865 - val_mae: 2.3607\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5062 - mae: 1.9872 - val_loss: 14.1101 - val_mae: 2.3602\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4898 - mae: 1.9933 - val_loss: 14.0473 - val_mae: 2.3565\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5184 - mae: 2.0242 - val_loss: 14.2429 - val_mae: 2.3348\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4855 - mae: 1.9776 - val_loss: 14.0247 - val_mae: 2.3460\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4737 - mae: 1.9824 - val_loss: 13.9737 - val_mae: 2.3478\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4458 - mae: 1.9906 - val_loss: 14.0176 - val_mae: 2.3390\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4561 - mae: 1.9694 - val_loss: 14.0709 - val_mae: 2.3321\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4887 - mae: 2.0331 - val_loss: 14.0127 - val_mae: 2.3301\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4422 - mae: 1.9521 - val_loss: 14.0127 - val_mae: 2.3266\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4690 - mae: 1.9974 - val_loss: 14.0136 - val_mae: 2.3233\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4232 - mae: 1.9855 - val_loss: 13.9471 - val_mae: 2.3261\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4037 - mae: 1.9909 - val_loss: 13.9042 - val_mae: 2.3240\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3936 - mae: 1.9802 - val_loss: 13.8513 - val_mae: 2.3268\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4664 - mae: 2.0168 - val_loss: 13.9514 - val_mae: 2.3171\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4425 - mae: 1.9481 - val_loss: 13.7990 - val_mae: 2.3313\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.4164 - mae: 2.0280 - val_loss: 13.8733 - val_mae: 2.3205\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4364 - mae: 1.9494 - val_loss: 13.8091 - val_mae: 2.3227\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3852 - mae: 2.0049 - val_loss: 13.7983 - val_mae: 2.3229\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3886 - mae: 2.0017 - val_loss: 13.8274 - val_mae: 2.3149\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4018 - mae: 1.9943 - val_loss: 13.7668 - val_mae: 2.3224\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.5034 - mae: 1.9575 - val_loss: 13.7124 - val_mae: 2.3303\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4018 - mae: 2.0292 - val_loss: 13.7333 - val_mae: 2.3219\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3930 - mae: 1.9639 - val_loss: 13.6724 - val_mae: 2.3236\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3570 - mae: 2.0167 - val_loss: 13.7386 - val_mae: 2.3154\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3657 - mae: 1.9565 - val_loss: 13.7284 - val_mae: 2.3148\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3951 - mae: 2.0203 - val_loss: 13.7691 - val_mae: 2.3083\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3685 - mae: 1.9607 - val_loss: 13.6764 - val_mae: 2.3233\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3803 - mae: 1.9915 - val_loss: 13.7735 - val_mae: 2.3054\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3353 - mae: 1.9509 - val_loss: 13.5701 - val_mae: 2.3291\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4534 - mae: 2.0027 - val_loss: 13.7303 - val_mae: 2.3103\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3418 - mae: 2.0099 - val_loss: 13.5877 - val_mae: 2.3228\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3692 - mae: 1.9650 - val_loss: 13.6565 - val_mae: 2.3137\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.4226 - mae: 2.0632 - val_loss: 13.7440 - val_mae: 2.3017\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4220 - mae: 1.9454 - val_loss: 13.6266 - val_mae: 2.3145\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3147 - mae: 1.9658 - val_loss: 13.5385 - val_mae: 2.3304\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3397 - mae: 2.0041 - val_loss: 13.6237 - val_mae: 2.3135\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.2961 - mae: 2.0016 - val_loss: 13.5156 - val_mae: 2.3211\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3267 - mae: 1.9610 - val_loss: 13.6007 - val_mae: 2.3101\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2816 - mae: 1.9817 - val_loss: 13.4930 - val_mae: 2.3263\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.2870 - mae: 2.0017 - val_loss: 13.5717 - val_mae: 2.3139\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.5825 - mae: 1.7583\n",
      "Mean Absolute Error on Test Data: 1.7582504749298096\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.10205110459687372\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 116.7714 - mae: 8.8352 - val_loss: 120.7562 - val_mae: 8.7332\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 96.8640 - mae: 7.6452 - val_loss: 92.5762 - val_mae: 7.0976\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 62.7570 - mae: 5.5086 - val_loss: 53.1279 - val_mae: 4.7234\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 38.3754 - mae: 4.3613 - val_loss: 42.4216 - val_mae: 4.7329\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 37.3118 - mae: 4.5210 - val_loss: 42.6700 - val_mae: 4.4746\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.8770 - mae: 4.3838 - val_loss: 42.3726 - val_mae: 4.4936\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.8473 - mae: 4.3272 - val_loss: 42.3812 - val_mae: 4.4664\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.7840 - mae: 4.3908 - val_loss: 42.1140 - val_mae: 4.4833\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.7300 - mae: 4.3830 - val_loss: 41.8797 - val_mae: 4.4979\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7374 - mae: 4.3523 - val_loss: 42.1136 - val_mae: 4.4359\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.7373 - mae: 4.4326 - val_loss: 41.5328 - val_mae: 4.5273\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6469 - mae: 4.3664 - val_loss: 42.0003 - val_mae: 4.4126\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6469 - mae: 4.3559 - val_loss: 41.3906 - val_mae: 4.4985\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6068 - mae: 4.3563 - val_loss: 41.4496 - val_mae: 4.4639\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5548 - mae: 4.3870 - val_loss: 41.3462 - val_mae: 4.4649\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6235 - mae: 4.3206 - val_loss: 41.3769 - val_mae: 4.4524\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7120 - mae: 4.4257 - val_loss: 41.4555 - val_mae: 4.4279\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5559 - mae: 4.3747 - val_loss: 41.3513 - val_mae: 4.4235\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4994 - mae: 4.3395 - val_loss: 41.2293 - val_mae: 4.4443\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5245 - mae: 4.3836 - val_loss: 41.3032 - val_mae: 4.4207\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.6330 - mae: 4.3795 - val_loss: 41.2797 - val_mae: 4.4175\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4629 - mae: 4.3166 - val_loss: 41.1236 - val_mae: 4.4359\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5178 - mae: 4.3912 - val_loss: 40.7834 - val_mae: 4.4924\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4375 - mae: 4.3655 - val_loss: 41.1351 - val_mae: 4.4182\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4400 - mae: 4.3602 - val_loss: 41.0721 - val_mae: 4.4209\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4049 - mae: 4.3548 - val_loss: 40.9107 - val_mae: 4.4432\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4658 - mae: 4.3335 - val_loss: 40.8093 - val_mae: 4.4525\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4145 - mae: 4.3752 - val_loss: 40.7682 - val_mae: 4.4554\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4281 - mae: 4.3828 - val_loss: 41.1199 - val_mae: 4.4081\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4466 - mae: 4.3576 - val_loss: 41.0574 - val_mae: 4.4104\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3970 - mae: 4.3231 - val_loss: 40.7983 - val_mae: 4.4469\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5382 - mae: 4.3653 - val_loss: 40.8951 - val_mae: 4.4328\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4750 - mae: 4.3722 - val_loss: 40.7242 - val_mae: 4.4584\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4127 - mae: 4.3768 - val_loss: 40.7630 - val_mae: 4.4445\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.4527 - mae: 4.3545 - val_loss: 41.2326 - val_mae: 4.3820\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3231 - mae: 4.3270 - val_loss: 40.7344 - val_mae: 4.4516\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4070 - mae: 4.3642 - val_loss: 40.5923 - val_mae: 4.4851\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3915 - mae: 4.3806 - val_loss: 40.6931 - val_mae: 4.4435\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3778 - mae: 4.3451 - val_loss: 40.7093 - val_mae: 4.4480\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3407 - mae: 4.3606 - val_loss: 40.7079 - val_mae: 4.4317\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.2882 - mae: 4.3562 - val_loss: 40.9041 - val_mae: 4.4096\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3832 - mae: 4.3364 - val_loss: 40.6813 - val_mae: 4.4441\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3786 - mae: 4.4049 - val_loss: 40.9714 - val_mae: 4.4020\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3879 - mae: 4.3234 - val_loss: 41.1413 - val_mae: 4.3794\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.5427 - mae: 4.3819 - val_loss: 40.7560 - val_mae: 4.4252\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3186 - mae: 4.3164 - val_loss: 40.6807 - val_mae: 4.4268\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3106 - mae: 4.3741 - val_loss: 40.5876 - val_mae: 4.4445\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3507 - mae: 4.4147 - val_loss: 40.9321 - val_mae: 4.3955\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3738 - mae: 4.3129 - val_loss: 40.5202 - val_mae: 4.4551\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 36.3374 - mae: 4.3453 - val_loss: 40.8205 - val_mae: 4.4064\n",
      "8/8 [==============================] - 0s 857us/step - loss: 47.6437 - mae: 4.4629\n",
      "Mean Absolute Error on Test Data: 4.46293306350708\n",
      "8/8 [==============================] - 0s 802us/step\n",
      "R-squared: -0.0053457627952751\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 0s 6ms/step - loss: 17.1710 - mae: 3.0220 - val_loss: 14.7272 - val_mae: 2.6244\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.3622 - mae: 2.2125 - val_loss: 8.7339 - val_mae: 1.8349\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.2099 - mae: 1.9882 - val_loss: 7.6763 - val_mae: 1.8885\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.9378 - mae: 1.9966 - val_loss: 7.6974 - val_mae: 1.8200\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8371 - mae: 1.9508 - val_loss: 7.6506 - val_mae: 1.8281\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7887 - mae: 1.9681 - val_loss: 7.6580 - val_mae: 1.8251\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.7633 - mae: 1.9342 - val_loss: 7.6650 - val_mae: 1.8151\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6897 - mae: 1.9390 - val_loss: 7.6781 - val_mae: 1.8088\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6678 - mae: 1.9276 - val_loss: 7.6998 - val_mae: 1.8006\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6377 - mae: 1.9502 - val_loss: 7.6545 - val_mae: 1.8235\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6117 - mae: 1.9262 - val_loss: 7.6748 - val_mae: 1.8035\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6079 - mae: 1.9172 - val_loss: 7.6808 - val_mae: 1.7926\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5805 - mae: 1.9541 - val_loss: 7.6540 - val_mae: 1.8162\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5688 - mae: 1.9107 - val_loss: 7.6523 - val_mae: 1.8145\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5772 - mae: 1.9303 - val_loss: 7.7223 - val_mae: 1.7945\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5150 - mae: 1.9305 - val_loss: 7.6365 - val_mae: 1.8200\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5427 - mae: 1.9348 - val_loss: 7.7293 - val_mae: 1.7973\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4901 - mae: 1.9225 - val_loss: 7.6903 - val_mae: 1.8295\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4954 - mae: 1.9466 - val_loss: 7.6812 - val_mae: 1.8034\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4699 - mae: 1.9186 - val_loss: 7.6887 - val_mae: 1.8200\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4529 - mae: 1.9260 - val_loss: 7.7611 - val_mae: 1.8137\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4809 - mae: 1.9047 - val_loss: 7.7213 - val_mae: 1.8369\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4375 - mae: 1.9450 - val_loss: 7.8239 - val_mae: 1.8212\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4260 - mae: 1.9092 - val_loss: 7.8287 - val_mae: 1.8175\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4350 - mae: 1.9055 - val_loss: 7.7386 - val_mae: 1.8251\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4116 - mae: 1.9427 - val_loss: 7.8250 - val_mae: 1.8283\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4212 - mae: 1.9207 - val_loss: 7.8200 - val_mae: 1.8377\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4405 - mae: 1.9053 - val_loss: 7.8195 - val_mae: 1.8400\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4290 - mae: 1.9221 - val_loss: 7.8311 - val_mae: 1.8099\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3904 - mae: 1.9285 - val_loss: 7.9330 - val_mae: 1.8254\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4295 - mae: 1.9238 - val_loss: 8.0176 - val_mae: 1.8251\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3773 - mae: 1.9257 - val_loss: 7.8089 - val_mae: 1.8244\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3767 - mae: 1.8958 - val_loss: 7.9383 - val_mae: 1.8324\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3671 - mae: 1.9330 - val_loss: 7.9375 - val_mae: 1.8227\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3297 - mae: 1.9157 - val_loss: 7.8903 - val_mae: 1.8441\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4103 - mae: 1.9309 - val_loss: 7.9697 - val_mae: 1.8349\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3488 - mae: 1.9274 - val_loss: 8.0261 - val_mae: 1.8297\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3326 - mae: 1.9040 - val_loss: 7.9394 - val_mae: 1.8283\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3184 - mae: 1.9107 - val_loss: 7.9604 - val_mae: 1.8606\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3272 - mae: 1.9464 - val_loss: 8.1243 - val_mae: 1.8329\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3133 - mae: 1.8945 - val_loss: 7.9038 - val_mae: 1.8270\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3855 - mae: 1.9350 - val_loss: 8.1117 - val_mae: 1.8418\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3397 - mae: 1.9239 - val_loss: 7.8885 - val_mae: 1.8281\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3237 - mae: 1.9183 - val_loss: 8.0280 - val_mae: 1.8395\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3190 - mae: 1.9126 - val_loss: 8.0763 - val_mae: 1.8275\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2976 - mae: 1.9291 - val_loss: 8.0988 - val_mae: 1.8446\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2923 - mae: 1.9254 - val_loss: 8.0729 - val_mae: 1.8316\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2852 - mae: 1.8958 - val_loss: 8.0742 - val_mae: 1.8563\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2665 - mae: 1.9176 - val_loss: 8.0739 - val_mae: 1.8366\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.2645 - mae: 1.8953 - val_loss: 8.1426 - val_mae: 1.8595\n",
      "7/7 [==============================] - 0s 999us/step - loss: 6.1373 - mae: 1.8148\n",
      "Mean Absolute Error on Test Data: 1.814767599105835\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.12461278968392042\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 27.9603 - mae: 4.3576 - val_loss: 25.7470 - val_mae: 3.7359\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 15.6063 - mae: 2.8921 - val_loss: 14.2596 - val_mae: 2.6084\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 9.1082 - mae: 2.2395 - val_loss: 12.9464 - val_mae: 2.7203\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.9751 - mae: 2.2888 - val_loss: 12.8431 - val_mae: 2.6411\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.8465 - mae: 2.2233 - val_loss: 12.8257 - val_mae: 2.6175\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.8206 - mae: 2.2369 - val_loss: 12.7818 - val_mae: 2.6348\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.7708 - mae: 2.2242 - val_loss: 12.7850 - val_mae: 2.6152\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.7185 - mae: 2.2052 - val_loss: 12.7537 - val_mae: 2.6261\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.6884 - mae: 2.2284 - val_loss: 12.7582 - val_mae: 2.6115\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.6992 - mae: 2.1861 - val_loss: 12.7301 - val_mae: 2.6295\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6930 - mae: 2.2296 - val_loss: 12.7456 - val_mae: 2.6139\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.6174 - mae: 2.1989 - val_loss: 12.7221 - val_mae: 2.6207\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.6075 - mae: 2.2156 - val_loss: 12.7134 - val_mae: 2.6328\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5828 - mae: 2.2137 - val_loss: 12.7623 - val_mae: 2.5984\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.6223 - mae: 2.1818 - val_loss: 12.7202 - val_mae: 2.6206\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5715 - mae: 2.2132 - val_loss: 12.7030 - val_mae: 2.6384\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6524 - mae: 2.2592 - val_loss: 12.7317 - val_mae: 2.6129\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5599 - mae: 2.1999 - val_loss: 12.7644 - val_mae: 2.5994\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5864 - mae: 2.1801 - val_loss: 12.7225 - val_mae: 2.6243\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5390 - mae: 2.2075 - val_loss: 12.7280 - val_mae: 2.6164\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5675 - mae: 2.2072 - val_loss: 12.7739 - val_mae: 2.5959\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5003 - mae: 2.1864 - val_loss: 12.7288 - val_mae: 2.6136\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4997 - mae: 2.2113 - val_loss: 12.7273 - val_mae: 2.6148\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4848 - mae: 2.1865 - val_loss: 12.7388 - val_mae: 2.6072\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4581 - mae: 2.1894 - val_loss: 12.7347 - val_mae: 2.6382\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5059 - mae: 2.1933 - val_loss: 12.7324 - val_mae: 2.6136\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4708 - mae: 2.1983 - val_loss: 12.7358 - val_mae: 2.6136\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.5365 - mae: 2.2212 - val_loss: 12.7971 - val_mae: 2.5932\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4715 - mae: 2.1832 - val_loss: 12.7215 - val_mae: 2.6124\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4322 - mae: 2.1865 - val_loss: 12.7230 - val_mae: 2.6146\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4340 - mae: 2.1715 - val_loss: 12.7268 - val_mae: 2.6123\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4349 - mae: 2.2005 - val_loss: 12.7308 - val_mae: 2.6161\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4558 - mae: 2.2065 - val_loss: 12.7888 - val_mae: 2.5971\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4519 - mae: 2.1723 - val_loss: 12.7335 - val_mae: 2.6303\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4260 - mae: 2.1938 - val_loss: 12.7433 - val_mae: 2.6097\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4282 - mae: 2.1706 - val_loss: 12.7489 - val_mae: 2.6269\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4104 - mae: 2.1990 - val_loss: 12.7425 - val_mae: 2.6153\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3913 - mae: 2.1799 - val_loss: 12.7807 - val_mae: 2.6031\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4414 - mae: 2.1956 - val_loss: 12.7325 - val_mae: 2.6158\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4831 - mae: 2.1526 - val_loss: 12.7243 - val_mae: 2.6146\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4036 - mae: 2.2115 - val_loss: 12.7536 - val_mae: 2.6026\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3840 - mae: 2.1609 - val_loss: 12.7385 - val_mae: 2.6108\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3614 - mae: 2.1712 - val_loss: 12.7526 - val_mae: 2.6117\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4000 - mae: 2.2142 - val_loss: 12.7907 - val_mae: 2.6024\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.4047 - mae: 2.1514 - val_loss: 12.7580 - val_mae: 2.6248\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3767 - mae: 2.1939 - val_loss: 12.7529 - val_mae: 2.6010\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3848 - mae: 2.1778 - val_loss: 12.8198 - val_mae: 2.5951\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3755 - mae: 2.1663 - val_loss: 12.7623 - val_mae: 2.6320\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3310 - mae: 2.1860 - val_loss: 12.8028 - val_mae: 2.5971\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 8.3742 - mae: 2.1685 - val_loss: 12.7538 - val_mae: 2.6162\n",
      "7/7 [==============================] - 0s 828us/step - loss: 9.7511 - mae: 2.4307\n",
      "Mean Absolute Error on Test Data: 2.430692434310913\n",
      "7/7 [==============================] - 0s 998us/step\n",
      "R-squared: 0.005751553140917909\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 70.2438 - mae: 6.7720 - val_loss: 69.9031 - val_mae: 6.3298\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 47.9499 - mae: 5.0189 - val_loss: 43.3677 - val_mae: 4.2213\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 27.1837 - mae: 3.4638 - val_loss: 29.9408 - val_mae: 3.5848\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.9459 - mae: 3.5328 - val_loss: 29.9011 - val_mae: 3.5882\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.8084 - mae: 3.4560 - val_loss: 30.0655 - val_mae: 3.5427\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7250 - mae: 3.4314 - val_loss: 30.0506 - val_mae: 3.5419\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6853 - mae: 3.4286 - val_loss: 30.1187 - val_mae: 3.5330\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6698 - mae: 3.4435 - val_loss: 30.0620 - val_mae: 3.5336\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6438 - mae: 3.4082 - val_loss: 29.9331 - val_mae: 3.5578\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.6100 - mae: 3.4877 - val_loss: 30.0356 - val_mae: 3.5385\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5686 - mae: 3.4459 - val_loss: 30.2150 - val_mae: 3.5159\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5316 - mae: 3.3982 - val_loss: 30.0746 - val_mae: 3.5287\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4972 - mae: 3.4152 - val_loss: 29.9402 - val_mae: 3.5504\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4772 - mae: 3.4309 - val_loss: 30.0276 - val_mae: 3.5311\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4991 - mae: 3.4577 - val_loss: 30.0994 - val_mae: 3.5274\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4946 - mae: 3.4020 - val_loss: 29.9806 - val_mae: 3.5381\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4700 - mae: 3.4639 - val_loss: 30.0611 - val_mae: 3.5288\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3946 - mae: 3.3975 - val_loss: 30.1271 - val_mae: 3.5203\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3953 - mae: 3.4579 - val_loss: 29.9572 - val_mae: 3.5442\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4301 - mae: 3.3890 - val_loss: 30.0404 - val_mae: 3.5319\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4178 - mae: 3.4746 - val_loss: 30.0906 - val_mae: 3.5311\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3392 - mae: 3.4064 - val_loss: 30.1500 - val_mae: 3.5236\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4627 - mae: 3.4728 - val_loss: 30.1373 - val_mae: 3.5254\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3759 - mae: 3.3813 - val_loss: 30.1118 - val_mae: 3.5264\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3538 - mae: 3.4683 - val_loss: 29.9806 - val_mae: 3.5485\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3260 - mae: 3.3896 - val_loss: 30.1159 - val_mae: 3.5263\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2999 - mae: 3.4048 - val_loss: 29.9185 - val_mae: 3.5621\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3808 - mae: 3.4520 - val_loss: 30.1733 - val_mae: 3.5223\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3591 - mae: 3.4342 - val_loss: 30.1499 - val_mae: 3.5238\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2879 - mae: 3.4335 - val_loss: 30.2027 - val_mae: 3.5216\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3164 - mae: 3.3830 - val_loss: 29.9787 - val_mae: 3.5509\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.3079 - mae: 3.4550 - val_loss: 30.1600 - val_mae: 3.5247\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4563 - mae: 3.4853 - val_loss: 30.3423 - val_mae: 3.5124\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.4406 - mae: 3.4178 - val_loss: 29.9989 - val_mae: 3.5467\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2551 - mae: 3.3953 - val_loss: 30.0358 - val_mae: 3.5401\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1990 - mae: 3.4194 - val_loss: 29.9952 - val_mae: 3.5457\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2476 - mae: 3.4441 - val_loss: 30.0089 - val_mae: 3.5511\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3277 - mae: 3.3831 - val_loss: 29.9840 - val_mae: 3.5495\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2003 - mae: 3.4308 - val_loss: 29.9396 - val_mae: 3.5654\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2246 - mae: 3.4665 - val_loss: 30.3432 - val_mae: 3.5158\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2322 - mae: 3.3983 - val_loss: 29.9584 - val_mae: 3.5608\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2930 - mae: 3.4781 - val_loss: 30.2369 - val_mae: 3.5230\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2010 - mae: 3.3677 - val_loss: 30.0139 - val_mae: 3.5489\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2040 - mae: 3.4354 - val_loss: 30.0862 - val_mae: 3.5371\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1658 - mae: 3.4352 - val_loss: 29.9670 - val_mae: 3.5638\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1256 - mae: 3.4017 - val_loss: 30.1766 - val_mae: 3.5286\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1321 - mae: 3.4149 - val_loss: 29.9282 - val_mae: 3.5736\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1195 - mae: 3.4430 - val_loss: 30.2713 - val_mae: 3.5208\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2170 - mae: 3.4204 - val_loss: 30.0612 - val_mae: 3.5502\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.1143 - mae: 3.4227 - val_loss: 30.0725 - val_mae: 3.5465\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 27.6702 - mae: 3.4637\n",
      "Mean Absolute Error on Test Data: 3.4637279510498047\n",
      "8/8 [==============================] - 0s 854us/step\n",
      "R-squared: 0.05927209091852148\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 15.5687 - mae: 2.9448 - val_loss: 14.3253 - val_mae: 2.5881\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 10.2443 - mae: 2.1036 - val_loss: 8.9582 - val_mae: 1.8651\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 7.1168 - mae: 1.8520 - val_loss: 7.5007 - val_mae: 1.9407\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7857 - mae: 1.8919 - val_loss: 7.5227 - val_mae: 1.8777\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7263 - mae: 1.8494 - val_loss: 7.5211 - val_mae: 1.8698\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6987 - mae: 1.8568 - val_loss: 7.5017 - val_mae: 1.8735\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6910 - mae: 1.8535 - val_loss: 7.5293 - val_mae: 1.8534\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6697 - mae: 1.8232 - val_loss: 7.5159 - val_mae: 1.8538\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6610 - mae: 1.8633 - val_loss: 7.4872 - val_mae: 1.8664\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6195 - mae: 1.8324 - val_loss: 7.5325 - val_mae: 1.8483\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6128 - mae: 1.8331 - val_loss: 7.5141 - val_mae: 1.8537\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.6049 - mae: 1.8181 - val_loss: 7.4843 - val_mae: 1.8627\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5964 - mae: 1.8489 - val_loss: 7.5007 - val_mae: 1.8557\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5849 - mae: 1.8219 - val_loss: 7.5109 - val_mae: 1.8544\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5669 - mae: 1.8244 - val_loss: 7.4896 - val_mae: 1.8585\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5680 - mae: 1.8320 - val_loss: 7.4534 - val_mae: 1.8680\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5685 - mae: 1.8172 - val_loss: 7.4892 - val_mae: 1.8517\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5394 - mae: 1.8350 - val_loss: 7.4337 - val_mae: 1.8792\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5763 - mae: 1.8215 - val_loss: 7.5172 - val_mae: 1.8477\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.5370 - mae: 1.8440 - val_loss: 7.4559 - val_mae: 1.8745\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5107 - mae: 1.8199 - val_loss: 7.5278 - val_mae: 1.8527\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5115 - mae: 1.8208 - val_loss: 7.4513 - val_mae: 1.8697\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5042 - mae: 1.8226 - val_loss: 7.4949 - val_mae: 1.8598\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4978 - mae: 1.8048 - val_loss: 7.4897 - val_mae: 1.8579\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.5214 - mae: 1.8518 - val_loss: 7.4672 - val_mae: 1.8652\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4832 - mae: 1.8092 - val_loss: 7.4608 - val_mae: 1.8665\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4697 - mae: 1.8055 - val_loss: 7.4660 - val_mae: 1.8682\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4630 - mae: 1.8310 - val_loss: 7.4755 - val_mae: 1.8687\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4682 - mae: 1.8150 - val_loss: 7.4259 - val_mae: 1.8819\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4502 - mae: 1.8102 - val_loss: 7.5124 - val_mae: 1.8649\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4559 - mae: 1.8242 - val_loss: 7.5251 - val_mae: 1.8616\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4350 - mae: 1.8106 - val_loss: 7.4649 - val_mae: 1.8782\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4582 - mae: 1.8331 - val_loss: 7.4866 - val_mae: 1.8747\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4268 - mae: 1.7993 - val_loss: 7.5199 - val_mae: 1.8669\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4216 - mae: 1.8200 - val_loss: 7.5952 - val_mae: 1.8597\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4070 - mae: 1.7910 - val_loss: 7.5081 - val_mae: 1.8666\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4178 - mae: 1.8280 - val_loss: 7.5177 - val_mae: 1.8742\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4048 - mae: 1.8262 - val_loss: 7.5195 - val_mae: 1.8758\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3898 - mae: 1.7819 - val_loss: 7.5109 - val_mae: 1.8803\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.4389 - mae: 1.8267 - val_loss: 7.4900 - val_mae: 1.8793\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3555 - mae: 1.8239 - val_loss: 7.4955 - val_mae: 1.8834\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3598 - mae: 1.7929 - val_loss: 7.5010 - val_mae: 1.8902\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3642 - mae: 1.8306 - val_loss: 7.6018 - val_mae: 1.8689\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3498 - mae: 1.7951 - val_loss: 7.5415 - val_mae: 1.8859\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3869 - mae: 1.7810 - val_loss: 7.4749 - val_mae: 1.9086\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3830 - mae: 1.8367 - val_loss: 7.6290 - val_mae: 1.8734\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3332 - mae: 1.8032 - val_loss: 7.4942 - val_mae: 1.8934\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.3254 - mae: 1.7910 - val_loss: 7.5062 - val_mae: 1.8978\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.3364 - mae: 1.8023 - val_loss: 7.5341 - val_mae: 1.9079\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 6.2870 - mae: 1.8096 - val_loss: 7.5871 - val_mae: 1.8832\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 7.1021 - mae: 1.9239\n",
      "Mean Absolute Error on Test Data: 1.9238733053207397\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.00024449312733432205\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 32.9185 - mae: 4.0948 - val_loss: 16.9724 - val_mae: 3.0120\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 23.7252 - mae: 3.1315 - val_loss: 9.3142 - val_mae: 2.1402\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 16.8573 - mae: 2.6830 - val_loss: 8.5337 - val_mae: 2.3161\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.6337 - mae: 2.7455 - val_loss: 8.4019 - val_mae: 2.2771\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4292 - mae: 2.6942 - val_loss: 8.2380 - val_mae: 2.2177\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.3150 - mae: 2.6755 - val_loss: 8.3474 - val_mae: 2.2452\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.1688 - mae: 2.6387 - val_loss: 8.3417 - val_mae: 2.2369\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.0652 - mae: 2.6455 - val_loss: 8.4049 - val_mae: 2.2451\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 15.0765 - mae: 2.6980 - val_loss: 8.3516 - val_mae: 2.2190\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9690 - mae: 2.5985 - val_loss: 8.3934 - val_mae: 2.2258\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.9059 - mae: 2.5942 - val_loss: 8.4404 - val_mae: 2.2360\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8931 - mae: 2.6649 - val_loss: 8.5597 - val_mae: 2.2540\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8226 - mae: 2.5936 - val_loss: 8.3930 - val_mae: 2.2100\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.7955 - mae: 2.6070 - val_loss: 8.4889 - val_mae: 2.2317\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.8029 - mae: 2.5870 - val_loss: 8.4040 - val_mae: 2.2059\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7663 - mae: 2.6215 - val_loss: 8.5270 - val_mae: 2.2344\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.9085 - mae: 2.6534 - val_loss: 8.4906 - val_mae: 2.2172\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.9623 - mae: 2.5338 - val_loss: 8.5069 - val_mae: 2.2220\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7556 - mae: 2.5945 - val_loss: 8.5203 - val_mae: 2.2279\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7060 - mae: 2.6173 - val_loss: 8.6228 - val_mae: 2.2479\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7050 - mae: 2.5677 - val_loss: 8.4531 - val_mae: 2.2007\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7107 - mae: 2.5901 - val_loss: 8.4666 - val_mae: 2.2050\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6869 - mae: 2.5939 - val_loss: 8.5294 - val_mae: 2.2257\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7290 - mae: 2.5600 - val_loss: 8.5441 - val_mae: 2.2237\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6773 - mae: 2.5948 - val_loss: 8.4984 - val_mae: 2.2125\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6641 - mae: 2.5798 - val_loss: 8.6256 - val_mae: 2.2425\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6411 - mae: 2.5957 - val_loss: 8.4862 - val_mae: 2.2087\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6973 - mae: 2.5571 - val_loss: 8.6042 - val_mae: 2.2405\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6722 - mae: 2.6031 - val_loss: 8.5780 - val_mae: 2.2330\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6400 - mae: 2.5626 - val_loss: 8.4845 - val_mae: 2.2025\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6227 - mae: 2.5720 - val_loss: 8.6190 - val_mae: 2.2344\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.7170 - mae: 2.5769 - val_loss: 8.4394 - val_mae: 2.1945\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6268 - mae: 2.5729 - val_loss: 8.5593 - val_mae: 2.2330\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6414 - mae: 2.6108 - val_loss: 8.4961 - val_mae: 2.1965\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6244 - mae: 2.5679 - val_loss: 8.5292 - val_mae: 2.2072\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5946 - mae: 2.5676 - val_loss: 8.5587 - val_mae: 2.2152\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6146 - mae: 2.5851 - val_loss: 8.5080 - val_mae: 2.2109\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5978 - mae: 2.5661 - val_loss: 8.5740 - val_mae: 2.2256\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5884 - mae: 2.5726 - val_loss: 8.5011 - val_mae: 2.1986\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5817 - mae: 2.5668 - val_loss: 8.4619 - val_mae: 2.1973\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6323 - mae: 2.5892 - val_loss: 8.4375 - val_mae: 2.1807\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5877 - mae: 2.5410 - val_loss: 8.5438 - val_mae: 2.2133\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5641 - mae: 2.6054 - val_loss: 8.5721 - val_mae: 2.2292\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.6151 - mae: 2.5378 - val_loss: 8.5797 - val_mae: 2.2337\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5571 - mae: 2.5806 - val_loss: 8.4856 - val_mae: 2.2101\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5777 - mae: 2.5823 - val_loss: 8.6833 - val_mae: 2.2506\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5224 - mae: 2.5520 - val_loss: 8.4815 - val_mae: 2.1987\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5492 - mae: 2.5829 - val_loss: 8.5371 - val_mae: 2.2077\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 14.5652 - mae: 2.5636 - val_loss: 8.6514 - val_mae: 2.2470\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4845 - mae: 2.5662 - val_loss: 8.5781 - val_mae: 2.2155\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 16.3778 - mae: 2.8946\n",
      "Mean Absolute Error on Test Data: 2.8946423530578613\n",
      "7/7 [==============================] - 0s 746us/step\n",
      "R-squared: 0.03210270585163677\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 67.5555 - mae: 6.6524 - val_loss: 59.6675 - val_mae: 6.0635\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 48.8862 - mae: 5.1918 - val_loss: 38.2430 - val_mae: 4.3790\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 28.8516 - mae: 3.6747 - val_loss: 23.4829 - val_mae: 3.3581\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.7635 - mae: 3.5547 - val_loss: 23.1258 - val_mae: 3.4354\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6532 - mae: 3.5358 - val_loss: 22.9323 - val_mae: 3.3775\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.6432 - mae: 3.4889 - val_loss: 22.7578 - val_mae: 3.3691\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.4081 - mae: 3.5135 - val_loss: 22.6070 - val_mae: 3.3726\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3144 - mae: 3.5246 - val_loss: 22.5196 - val_mae: 3.3252\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.2166 - mae: 3.4627 - val_loss: 22.3591 - val_mae: 3.3297\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.1130 - mae: 3.5063 - val_loss: 22.2382 - val_mae: 3.3212\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 22.0008 - mae: 3.4698 - val_loss: 22.1659 - val_mae: 3.2947\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.9573 - mae: 3.4438 - val_loss: 22.0654 - val_mae: 3.3051\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8846 - mae: 3.4656 - val_loss: 21.9826 - val_mae: 3.3033\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.8312 - mae: 3.4762 - val_loss: 21.8863 - val_mae: 3.3035\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8210 - mae: 3.4552 - val_loss: 21.8220 - val_mae: 3.3005\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7442 - mae: 3.4464 - val_loss: 21.8203 - val_mae: 3.2695\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.7303 - mae: 3.4640 - val_loss: 21.7360 - val_mae: 3.2866\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7018 - mae: 3.4370 - val_loss: 21.7369 - val_mae: 3.2658\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6471 - mae: 3.4526 - val_loss: 21.6429 - val_mae: 3.2859\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6629 - mae: 3.4279 - val_loss: 21.6145 - val_mae: 3.2927\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6054 - mae: 3.4395 - val_loss: 21.6219 - val_mae: 3.2703\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6073 - mae: 3.4551 - val_loss: 21.6179 - val_mae: 3.2639\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5997 - mae: 3.4392 - val_loss: 21.5594 - val_mae: 3.2778\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5424 - mae: 3.4408 - val_loss: 21.6042 - val_mae: 3.2611\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5761 - mae: 3.4475 - val_loss: 21.5451 - val_mae: 3.2730\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5936 - mae: 3.4161 - val_loss: 21.5233 - val_mae: 3.2828\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5105 - mae: 3.4583 - val_loss: 21.5272 - val_mae: 3.2751\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.6042 - mae: 3.4388 - val_loss: 21.4981 - val_mae: 3.3002\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5028 - mae: 3.4258 - val_loss: 21.5185 - val_mae: 3.2766\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5819 - mae: 3.4108 - val_loss: 21.4930 - val_mae: 3.2969\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5468 - mae: 3.4571 - val_loss: 21.4864 - val_mae: 3.2931\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5639 - mae: 3.4577 - val_loss: 21.5954 - val_mae: 3.2490\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5115 - mae: 3.3917 - val_loss: 21.4921 - val_mae: 3.3022\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4897 - mae: 3.4644 - val_loss: 21.4893 - val_mae: 3.2805\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5233 - mae: 3.4718 - val_loss: 21.4869 - val_mae: 3.2768\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.5593 - mae: 3.4046 - val_loss: 21.4613 - val_mae: 3.2965\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4210 - mae: 3.4315 - val_loss: 21.4599 - val_mae: 3.2813\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4804 - mae: 3.4558 - val_loss: 21.5541 - val_mae: 3.2525\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4602 - mae: 3.4022 - val_loss: 21.4773 - val_mae: 3.2773\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4320 - mae: 3.4816 - val_loss: 21.4544 - val_mae: 3.2893\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3631 - mae: 3.4257 - val_loss: 21.5129 - val_mae: 3.2577\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.4144 - mae: 3.4256 - val_loss: 21.4519 - val_mae: 3.2920\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3510 - mae: 3.4322 - val_loss: 21.4808 - val_mae: 3.2658\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3752 - mae: 3.4183 - val_loss: 21.4652 - val_mae: 3.2738\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4112 - mae: 3.3972 - val_loss: 21.4536 - val_mae: 3.2774\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3868 - mae: 3.4217 - val_loss: 21.4356 - val_mae: 3.2987\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3652 - mae: 3.4435 - val_loss: 21.4351 - val_mae: 3.3076\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3549 - mae: 3.4345 - val_loss: 21.4146 - val_mae: 3.2821\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3486 - mae: 3.3993 - val_loss: 21.4224 - val_mae: 3.2753\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 21.3892 - mae: 3.4758 - val_loss: 21.4175 - val_mae: 3.2697\n",
      "8/8 [==============================] - 0s 812us/step - loss: 18.9297 - mae: 3.1958\n",
      "Mean Absolute Error on Test Data: 3.195772409439087\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0014689208282729904\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 135.4447 - mae: 9.6389 - val_loss: 119.9355 - val_mae: 9.2003\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 101.7596 - mae: 7.7400 - val_loss: 73.8966 - val_mae: 6.4379\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 56.4472 - mae: 4.9358 - val_loss: 36.0840 - val_mae: 4.3571\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.8506 - mae: 4.5475 - val_loss: 34.4147 - val_mae: 4.4219\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4640 - mae: 4.4943 - val_loss: 34.6735 - val_mae: 4.3433\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5422 - mae: 4.3697 - val_loss: 34.6507 - val_mae: 4.3445\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5160 - mae: 4.4711 - val_loss: 34.4461 - val_mae: 4.3419\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5942 - mae: 4.3665 - val_loss: 34.3393 - val_mae: 4.3395\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.5088 - mae: 4.5065 - val_loss: 34.2354 - val_mae: 4.3371\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4053 - mae: 4.4232 - val_loss: 34.2452 - val_mae: 4.3300\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3440 - mae: 4.4377 - val_loss: 34.3538 - val_mae: 4.3226\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.3176 - mae: 4.4115 - val_loss: 34.1582 - val_mae: 4.3271\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2874 - mae: 4.4354 - val_loss: 34.3814 - val_mae: 4.3146\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2389 - mae: 4.3733 - val_loss: 33.9898 - val_mae: 4.3368\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.4713 - mae: 4.5060 - val_loss: 34.4745 - val_mae: 4.3109\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2894 - mae: 4.3971 - val_loss: 34.2841 - val_mae: 4.3097\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2321 - mae: 4.4099 - val_loss: 34.0494 - val_mae: 4.3188\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1519 - mae: 4.4395 - val_loss: 34.2090 - val_mae: 4.3091\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2127 - mae: 4.3516 - val_loss: 33.9023 - val_mae: 4.3246\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1330 - mae: 4.4625 - val_loss: 33.7504 - val_mae: 4.3328\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2784 - mae: 4.4228 - val_loss: 33.7876 - val_mae: 4.3254\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0917 - mae: 4.4228 - val_loss: 33.8387 - val_mae: 4.3166\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1383 - mae: 4.3966 - val_loss: 33.8236 - val_mae: 4.3094\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.1804 - mae: 4.5032 - val_loss: 34.0439 - val_mae: 4.3007\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.2639 - mae: 4.3387 - val_loss: 33.6739 - val_mae: 4.3167\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0510 - mae: 4.4376 - val_loss: 33.9447 - val_mae: 4.2993\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0126 - mae: 4.3976 - val_loss: 33.8823 - val_mae: 4.2991\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9323 - mae: 4.4436 - val_loss: 33.6591 - val_mae: 4.3020\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 42.0727 - mae: 4.4013 - val_loss: 33.5393 - val_mae: 4.3178\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8278 - mae: 4.4019 - val_loss: 33.8414 - val_mae: 4.2920\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8274 - mae: 4.4116 - val_loss: 33.7085 - val_mae: 4.2924\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9451 - mae: 4.3787 - val_loss: 33.8995 - val_mae: 4.2877\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7489 - mae: 4.4308 - val_loss: 33.4780 - val_mae: 4.3032\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.8738 - mae: 4.3676 - val_loss: 33.6805 - val_mae: 4.2913\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7134 - mae: 4.4298 - val_loss: 33.6929 - val_mae: 4.2897\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7256 - mae: 4.4380 - val_loss: 33.6629 - val_mae: 4.2856\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7566 - mae: 4.3706 - val_loss: 33.5673 - val_mae: 4.2922\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7524 - mae: 4.3857 - val_loss: 34.0236 - val_mae: 4.2830\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.7981 - mae: 4.4184 - val_loss: 33.4241 - val_mae: 4.2940\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.5334 - mae: 4.3857 - val_loss: 33.9415 - val_mae: 4.2808\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6664 - mae: 4.3732 - val_loss: 33.4040 - val_mae: 4.2967\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5396 - mae: 4.3890 - val_loss: 33.4672 - val_mae: 4.2887\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4954 - mae: 4.4343 - val_loss: 33.5423 - val_mae: 4.2819\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.5969 - mae: 4.3681 - val_loss: 33.4243 - val_mae: 4.2877\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4185 - mae: 4.3931 - val_loss: 33.7076 - val_mae: 4.2807\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4508 - mae: 4.3779 - val_loss: 33.4145 - val_mae: 4.2891\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5805 - mae: 4.3754 - val_loss: 33.2687 - val_mae: 4.3002\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.4454 - mae: 4.3866 - val_loss: 33.6687 - val_mae: 4.2786\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.3792 - mae: 4.4082 - val_loss: 33.3434 - val_mae: 4.2826\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 41.6831 - mae: 4.3385 - val_loss: 33.2100 - val_mae: 4.2904\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 37.2227 - mae: 4.2150\n",
      "Mean Absolute Error on Test Data: 4.215010166168213\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.049475068738777606\n",
      "1/1 [==============================] - 0s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one    (runtime: 6m 40s (auto), 2m 19s (no GPU), 5m59s (nothing))\n",
    "\n",
    "# gpu deactivate\n",
    "tf.config.set_visible_devices([],'GPU') # 2m 19s\n",
    "\n",
    "# gpu ayarlanmasi\n",
    "#tf.config.set_soft_device_placement(True) # otomatik dogru cihaz kullanimi\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "#tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[1], 'GPU') # 2. gpu ayari\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "#tf.config.experimental.set_visible_devices(physical_devices[1], 'GPU')\n",
    "\n",
    "df = merged_all\n",
    "df_test = dict_test_merged\n",
    "features = ['Bildirimli_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik_max','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "output_var = df\n",
    "target = 'Bildirimsiz_sum'\n",
    "# ilceler = []\n",
    "\n",
    "# NN 3\n",
    "# ilceler = ['izmir-konak','izmir-kinik']\n",
    "all_submissions = []\n",
    "for ilce in ilceler:\n",
    "    df = merged_all[ilce]\n",
    "    df_test = dict_test_merged[ilce]\n",
    "    output_var = df['Bildirimsiz_sum']\n",
    "\n",
    "    # ilcelerin numerizasyonu\n",
    "    columns_tonumerate = ['Bayram_Flag']\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "    # test csv dosyasi numerizasyon\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_transform = scaler.fit_transform(df[features])\n",
    "    feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "    feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "    feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    feature_test = scaler2.fit_transform(df_test[features])\n",
    "    feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)\n",
    "\n",
    "\n",
    "    #with strategy.scope(): # strategy ayarlamasi\n",
    "    X = feature_transform\n",
    "    y = output_var\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Mean Absolute Error on Test Data:\", mae)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "    print(\"R-squared:\", r2)\n",
    "\n",
    "    predictions_new = model.predict(feature_test)\n",
    "    predictions_new = np.round(predictions_new).astype(int)\n",
    "    df_test['bildirimsiz_sum'] = predictions_new\n",
    "    df_test.to_csv('test_with_predictions.csv', index=False)\n",
    "\n",
    "    df_test.rename(columns={'Ilce': 'ilce'}, inplace=True)\n",
    "    df_test.rename(columns={'Tarih': 'tarih'}, inplace=True)\n",
    "    df_test.rename(columns={'Bildirimli_sum': 'bildirimli_sum'}, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    all_submissions.append(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bildirimsiz_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bildirimsiz_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m test \u001b[38;5;66;03m#test csv birlestirilmis hazir dosyasi\u001b[39;00m\n\u001b[0;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimli_sum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msicaklik vs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#kalan ozellikler de yazilmali\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m output_var \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbildirimsiz_sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimsiz_sum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m ilceler \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bildirimsiz_sum'"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "df = merged_all['izmir-konak'] #ml icin hazir csv dosyasi\n",
    "df_test = test #test csv birlestirilmis hazir dosyasi\n",
    "features = ['bildirimli_sum','sicaklik vs'] #kalan ozellikler de yazilmali\n",
    "output_var = df['bildirimsiz_sum']\n",
    "target = 'bildirimsiz_sum'\n",
    "ilceler = []\n",
    "\n",
    "dict = {}\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "ilceler = list(dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ilcelerin numerizasyonu -------- NEW olmasa da direkt sayilar olsa daha iyi olabilir\n",
    "columns_tonumerate = ['ilce','BAYRAM_FLAG','vs vs.']\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    encode = encoder.fit_transform(df[column])\n",
    "    df[column + '_NEW'] = encode #buraya _NEW eklemeli miyim tekrar bakmak gerek\n",
    "    df.drop(columns=[column], inplace=True)\n",
    "\n",
    "# csv dosyasi numerizasyon - sadece numarizasyon degil ayni zamanda weather ile birlestirme de yapilmali!!\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    encode = encoder.fit_transform(df_test[column])\n",
    "    df_test[column + '_NEW'] = encode\n",
    "    df_test.drop(columns=[column], inplace=True)\n",
    "\n",
    "#Scaling\n",
    "scaler = MinMaxScaler()\n",
    "feature_transform = scaler.fit_transform(df[features])\n",
    "feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "feature_transform.head()\n",
    "\n",
    "\n",
    "# test.csv numerizasyonu da gerekiyor ayni sekilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y test-train elde edimi\n",
    "x = df[features]\n",
    "y = output_var # = df[\"target_var\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "#Splitting to Training set and Test set --- burasi timeseries icin split\n",
    "timesplit = TimeSeriesSplit(n_splits=15)\n",
    "for train_index, test_index in timesplit.split(feature_transform):\n",
    "        X_tr, X_te = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_tr, y_te = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "#Process the data for LSTM\n",
    "trainX = np.array(X_tr)\n",
    "testX = np.array(X_te)\n",
    "X_tr = trainX.reshape(X_tr.shape[0], 1, X_tr.shape[1])\n",
    "X_te = testX.reshape(X_te.shape[0], 1, X_te.shape[1])\n",
    "\n",
    "#Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Model Training\n",
    "history=lstm.fit(X_tr, y_tr, epochs=100, batch_size=8, verbose=1, shuffle=False)\n",
    "\n",
    "#LSTM Prediction\n",
    "y_pr= lstm.predict(X_te)\n",
    "\n",
    "# Predicted vs True Adj Close Value – LSTM  --burasi copy paste\n",
    "plt.plot(y_te, label='True Value')\n",
    "plt.plot(y_pr, label='LSTM Value')\n",
    "plt.title(\"Prediction by LSTM\")\n",
    "plt.xlabel('Time Scale')\n",
    "plt.ylabel('Scaled USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# test_pred = lstm.predict(gercek test)\n",
    "# csv ye yazdir vs vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "X = df[features].values()\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "k=17\n",
    "neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_hat = neigh.predict(X_test)\n",
    "\n",
    "test_accuracy = neigh.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy with class weights:\", test_accuracy)\n",
    "print(\"egitim verisi dogrulugu \", metrics.accuracy_score(y_train,neigh.predict(X_train)))\n",
    "print(\"test verisi dogrulugu \", metrics.accuracy_score(y_test,y_hat))\n",
    "\n",
    "# test tahmin\n",
    "y_hat = neigh.predict(isteburayatestdosyasi)\n",
    "submission = pd.read_csv(\"sample_submission.csv\", low_memory=False)\n",
    "submission.iloc[:, 1] = y_hat\n",
    "# submission.to_csv(\"knnsubmission.csv\", index=False)\n",
    "\n",
    "# optimal k degeri\n",
    "\n",
    "# # Define the range of k values to try\n",
    "# k_values = range(1, 21)\n",
    "\n",
    "# # Perform cross-validation for each value of k\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "\n",
    "# # Find the optimal value of k with the highest cross-validation score\n",
    "# optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "# print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "# # Train the model with the optimal k value\n",
    "# neigh = KNeighborsClassifier(n_neighbors=optimal_k).fit(X_train, y_train)\n",
    "# test_accuracy = neigh.score(X_test, y_test)\n",
    "# print(\"Test accuracy with optimal k:\", test_accuracy)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "# alinan kaynakta goruntu isleme icin kullaniliyordu bazi uyusmazliklar olabilir\n",
    "# Cast the records into float values \n",
    "# x_train = x_train.astype('float32') \n",
    "# x_test = x_test.astype('float32') \n",
    "\n",
    "print(\"Feature matrix:\", x_train.shape) \n",
    "print(\"Target matrix:\", x_test.shape) \n",
    "print(\"Feature matrix:\", y_train.shape) \n",
    "print(\"Target matrix:\", y_test.shape)  \n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(x_train.shape)), \n",
    "    \n",
    "    # dense layer 1 \n",
    "    Dense(256, activation='sigmoid'),   \n",
    "    \n",
    "    # dense layer 2 \n",
    "    Dense(128, activation='sigmoid'),  \n",
    "    \n",
    "    # output layer \n",
    "    Dense(10, activation='sigmoid'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(x_train, y_train, epochs=10,  \n",
    "          batch_size=2000,  \n",
    "          validation_split=0.2)\n",
    "\n",
    "results = model.evaluate(x_test,  y_test, verbose = 0) \n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() specifies that the network is a linear stack of layers\n",
    "\n",
    "model.add() adds the hidden layer.\n",
    "\n",
    "Dense means that neurons between layers are fully connected\n",
    "\n",
    "input_dim defines the number of features in the training dataset\n",
    "\n",
    "activation defines the activation function\n",
    "\n",
    "loss selects the cost function\n",
    "\n",
    "optimizer selects the learning algorithm\n",
    "\n",
    "metrics selects the performance metrics to be saved for further analysis\n",
    "\n",
    "model.fit() initialize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN2\n",
    "\n",
    "X = df[features] #features\n",
    "y = df['target_var'] #expected values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy', 'mean_squared_error'])\n",
    "\n",
    "history = model.fit(X, y, epochs=3000, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X).round()\n",
    "num_correct_predictions = (y_pred == y).sum()\n",
    "accuracy = (num_correct_predictions / y.shape[0]) * 100\n",
    "print('Multi-layer perceptron accuracy: %.2f%%' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
