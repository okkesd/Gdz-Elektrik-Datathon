{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\\n\\n    Yapilmasi gerekenler:\\n        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\\n        - Farkli grafikler cikartmaya calis.\\n        - ML.\\n        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\\n        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\\n\\n    Sorunlar:\\n        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\\n        ortalamasi az olur. Burada farkli bir yontem bul.\\n        Gunluk maks min alinabilir\\n\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1-) Notlar\n",
    "\"\"\"\n",
    "    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\n",
    "\n",
    "    Yapilmasi gerekenler:\n",
    "        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\n",
    "        - Farkli grafikler cikartmaya calis.\n",
    "        - ML.\n",
    "        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\n",
    "        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\n",
    "\n",
    "    Sorunlar:\n",
    "        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\n",
    "        ortalamasi az olur. Burada farkli bir yontem bul.\n",
    "        Gunluk maks min alinabilir\n",
    "\"\"\"\n",
    "# 1-) read and preproccess train.csv\n",
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "# 3-) read and preprocess weather.csv\n",
    "# 4-) read and preprocess holidays.csv\n",
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi\n",
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği\n",
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli ve gerekli degil)\n",
    "# 10-) test icin birlestirme islemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-) Import required moduls and libraries\n",
    "\n",
    "# bildirimisiz_sum tahmin edilecek\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import os\n",
    "from unidecode import unidecode # to convert Turkish characters to English\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sm\n",
    "import statsmodels.api as sa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tarih         ilce  bildirimsiz_sum  bildirimli_sum\n",
      "19236 2021-01-01  izmir-konak                9               0\n",
      "19237 2021-01-02  izmir-konak               20               0\n",
      "19238 2021-01-03  izmir-konak                7               1\n",
      "19239 2021-01-04  izmir-konak               16               1\n",
      "19240 2021-01-05  izmir-konak                3               0\n",
      "...          ...          ...              ...             ...\n",
      "20355 2024-01-27  izmir-konak               12               3\n",
      "20356 2024-01-28  izmir-konak               13               1\n",
      "20357 2024-01-29  izmir-konak               22               0\n",
      "20358 2024-01-30  izmir-konak               28               1\n",
      "20359 2024-01-31  izmir-konak               16               0\n",
      "\n",
      "[1124 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-) read and preproccess train.csv\n",
    "train = pd.read_csv(\"./train.csv\", low_memory=False) # 46.944 satir, 4 kolon\n",
    "\n",
    "#print(train[\"tarih\"]) # 1.098 farkli tarih var, 47 farkli ilce var\n",
    "\n",
    "tarihler = []\n",
    "for i in train[\"tarih\"]:\n",
    "    tarihler.append(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "train[\"tarih\"] = tarihler\n",
    "\n",
    "# print(train.dtypes)\n",
    "\n",
    "dict :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak o ilcenin verisi (1096 gun) df olarak tutar\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "print(dict[\"izmir-konak\"])\n",
    "ilceler = (list(dict.keys()))\n",
    "#print(dict.keys()) # keys olarak her ilceyi, values olarak o ilcelerin bulundugu satirlari icerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'izmir-aliaga': 1106, 'izmir-balcova': 698, 'izmir-bayindir': 1105, 'izmir-bayrakli': 1086, 'izmir-bergama': 1120, 'izmir-beydag': 673, 'izmir-bornova': 1124, 'izmir-buca': 1115, 'izmir-cesme': 1125, 'izmir-cigli': 1071, 'izmir-dikili': 1119, 'izmir-foca': 1086, 'izmir-gaziemir': 920, 'izmir-guzelbahce': 856, 'izmir-karabaglar': 1100, 'izmir-karaburun': 1089, 'izmir-karsiyaka': 1085, 'izmir-kemalpasa': 1118, 'izmir-kinik': 914, 'izmir-kiraz': 1097, 'izmir-konak': 1124, 'izmir-menderes': 1125, 'izmir-menemen': 1119, 'izmir-narlidere': 783, 'izmir-odemis': 1124, 'izmir-seferihisar': 1111, 'izmir-selcuk': 872, 'izmir-tire': 1107, 'izmir-torbali': 1124, 'izmir-urla': 1122, 'manisa-ahmetli': 622, 'manisa-akhisar': 1126, 'manisa-alasehir': 1119, 'manisa-demirci': 938, 'manisa-golmarmara': 566, 'manisa-gordes': 1059, 'manisa-kirkagac': 950, 'manisa-koprubasi': 805, 'manisa-kula': 1039, 'manisa-salihli': 1126, 'manisa-sarigol': 1027, 'manisa-saruhanli': 1105, 'manisa-sehzadeler': 1123, 'manisa-selendi': 993, 'manisa-soma': 1086, 'manisa-turgutlu': 1121, 'manisa-yunusemre': 1125}\n"
     ]
    }
   ],
   "source": [
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "\"\"\"\n",
    "for label in dict.keys(): # her ilce icin bildirimsiz ve bildirimli olarak grafiklerini cikart\n",
    "    print(dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.bar(dict[label][\"tarih\"],dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.title(label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(dict[\"izmir-konak\"][\"tarih\"],dict[\"izmir-konak\"][\"bildirimsiz_sum\"])\n",
    "plt.title(label)\n",
    "plt.margins(0.01)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# ilce tarih sayilarini al hepsinde esit veri yok\n",
    "ilce_tarih_sayilari = {}\n",
    "for name in dict.keys():\n",
    "    ilce_tarih_sayilari[name] = len(list(dict[name][\"tarih\"].to_dict().values()))\n",
    "\n",
    "print(ilce_tarih_sayilari)\n",
    "for name in dict.keys():\n",
    "    dict[name].set_index(\"tarih\", inplace=True)\n",
    "\n",
    "# train.set_index(\"tarih\", inplace=True) # train'in tarih kolonunu indexe cevir\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'lat', 'lon', 't_2m:C', 'effective_cloud_cover:p',\n",
      "       'global_rad:W', 'relative_humidity_2m:p', 'wind_dir_10m:d',\n",
      "       'wind_speed_10m:ms', 'prob_precip_1h:p', 't_apparent:C', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3-) read and preprocess weather.csv\n",
    "\n",
    "weather = pd.read_csv(\"./weather.csv\", low_memory=False)\n",
    "print(weather.columns) # onemli kolonlar: date, t_apparent:C (hissedilen sicaklik), wind_dir_10m:d (ruzgar yonu),\n",
    "# wind_speed_10m:ms (ruzgar hizi), prob_precip_1h:p (yagis), ilce\n",
    "\n",
    "# ilceleri ayir\n",
    "ilce_weather = {} # keys olarak ilceleri, values olarak o ilcelerin saatlik (1165 gun) hava durumlarini tutar\n",
    "for label, group in weather.groupby(\"name\"):\n",
    "    ilce_weather[label.lower()] = group\n",
    "\n",
    "# tarihleri tarih formatina cevir\n",
    "#print(ilce_weather[\"izmir-konak\"].dtypes)\n",
    "for name in ilce_weather.keys():\n",
    "\n",
    "    tarihler = [] # duzenli tarihleri burada tut\n",
    "    for date in ilce_weather[name][\"date\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ilce_weather[name][\"date\"] = tarihler # duzenli tarihleri date kolonuna ata\n",
    "    ilce_weather[name].set_index(\"date\", inplace=True) # tarihleri indexe cevir\n",
    "    ilce_weather[name][\"tarih\"] = ilce_weather[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "ilce_weather_day = {} # ilce hava durumu verilerini gunluk olarak tut (ortalama ile)\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_day[name] = ilce_weather[name].resample(\"D\").mean(numeric_only=True)# index'teki tarihleri gune cevir\n",
    "    ilce_weather_day[name][\"tarih\"] = ilce_weather_day[name].index\n",
    "\n",
    "#print(ilce_weather[\"izmir-konak\"][\"tarih\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lon', 't_2m:C', 'effective_cloud_cover:p', 'global_rad:W',\n",
      "       'relative_humidity_2m:p', 'wind_dir_10m:d', 'wind_speed_10m:ms',\n",
      "       'prob_precip_1h:p', 't_apparent:C', 'tarih'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(ilce_weather_day[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "lat                               float64\n",
      "lon                               float64\n",
      "t_2m:C                            float64\n",
      "effective_cloud_cover:p           float64\n",
      "global_rad:W                      float64\n",
      "relative_humidity_2m:p            float64\n",
      "wind_dir_10m:d                    float64\n",
      "wind_speed_10m:ms                 float64\n",
      "prob_precip_1h:p                  float64\n",
      "t_apparent:C                      float64\n",
      "name                               object\n",
      "tarih                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3.1-) her ilcenin hava durumunda her gununu ayri ayri df lere koyup dict te tut (runtime: 6m 35s)\n",
    "\n",
    "ilce_weather_detailed = {} \n",
    "# {izmir-konak: {2021-01-01 : df , 2021-01-02 : df ,...} , manisa-akhisar: {2021-01-01 : df , 2021-01-02 : df ,...} }\n",
    "\n",
    "\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_detailed[name] = {}\n",
    "    for label,group in ilce_weather[name].groupby(\"date\"):\n",
    "\n",
    "        gun = label.strftime('%Y-%m-%d')\n",
    "        if gun in ilce_weather_detailed[name]:\n",
    "            ilce_weather_detailed[name][gun] = pd.concat([ilce_weather_detailed[name][gun], group], ignore_index=True)\n",
    "        else:\n",
    "            ilce_weather_detailed[name][gun] = group.copy()\n",
    "\n",
    "\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].keys())[0]))\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].values())[0]).dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat      lon  Sicaklik_max  Sicaklik_min  Bulutluluk_max  \\\n",
      "2021-01-01  38.4177  27.1283          15.3          11.9            90.0   \n",
      "2021-01-02  38.4177  27.1283          17.4          11.0            57.5   \n",
      "2021-01-03  38.4177  27.1283          15.3          11.2            99.8   \n",
      "2021-01-04  38.4177  27.1283          17.7          10.5            97.4   \n",
      "2021-01-05  38.4177  27.1283          16.7          11.2            99.7   \n",
      "\n",
      "            Bulutluluk_min  Guneslilik_max  Guneslilik_min  Bagil_nem_max  \\\n",
      "2021-01-01            28.2           275.4             0.0           93.5   \n",
      "2021-01-02            10.4           374.0             0.0           90.9   \n",
      "2021-01-03            12.4           151.9             0.0           84.6   \n",
      "2021-01-04             9.2           357.0             0.0           85.6   \n",
      "2021-01-05             5.4           362.3             0.0          100.0   \n",
      "\n",
      "            Bagil_nem_min  ...         Ilce      Tarih   Sicaklik  Bulutluluk  \\\n",
      "2021-01-01           82.3  ...  izmir-konak 2021-01-01  13.095833   59.033333   \n",
      "2021-01-02           64.9  ...  izmir-konak 2021-01-02  13.379167   29.912500   \n",
      "2021-01-03           72.9  ...  izmir-konak 2021-01-03  12.587500   69.916667   \n",
      "2021-01-04           55.8  ...  izmir-konak 2021-01-04  13.783333   45.604167   \n",
      "2021-01-05           59.6  ...  izmir-konak 2021-01-05  13.895833   35.670833   \n",
      "\n",
      "            Guneslilik  Bagil_nem  Ruzgar_yonu  Ruzgar_hizi      Yagis  \\\n",
      "2021-01-01   65.212500  87.962500   137.558333     3.129167   1.137500   \n",
      "2021-01-02   91.225000  80.720833   134.820833     2.158333   1.000000   \n",
      "2021-01-03   34.962500  79.725000   142.316667     2.300000   2.520833   \n",
      "2021-01-04   79.400000  71.362500   138.641667     3.979167   1.000000   \n",
      "2021-01-05   92.666667  82.308333   161.516667     2.591667  12.279167   \n",
      "\n",
      "           Hissedilen_sicaklik  \n",
      "2021-01-01           13.891667  \n",
      "2021-01-02           14.250000  \n",
      "2021-01-03           12.937500  \n",
      "2021-01-04           13.787500  \n",
      "2021-01-05           14.850000  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3.2-) 3.1'de ayrilan ilce gunlerini simdi her gun icin degerlerin min max'ini bulup ilce df'lerini tekrar olustur\n",
    "\n",
    "# runtime: 1m 4s\n",
    "weather_last = {} # key olarak ilceleri, value olarak da o ilcelerin hava durumu degerlerini min-max ile tutar\n",
    "for name in ilce_weather_detailed.keys():\n",
    "    weather_last[name] = pd.DataFrame()\n",
    "\n",
    "    for date, day_df in ilce_weather_detailed[name].items():\n",
    "        \n",
    "        # max min leri al\n",
    "        max_values = day_df.max()\n",
    "        min_values = day_df.min()\n",
    "\n",
    "        # satır oluştur\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Tarih\": [datetime.strptime(date, \"%Y-%m-%d\")],\n",
    "            \"lat\": [day_df[\"lat\"].iloc[0]],\n",
    "            \"lon\": [day_df[\"lon\"].iloc[0]],\n",
    "            \"Sicaklik_max\": [max_values[\"t_2m:C\"]],\n",
    "            \"Sicaklik_min\": [min_values[\"t_2m:C\"]],\n",
    "            \"Bulutluluk_max\": [max_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Bulutluluk_min\": [min_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Guneslilik_max\": [max_values.get(\"global_rad:W\", None)],  \n",
    "            \"Guneslilik_min\": [min_values.get(\"global_rad:W\", None)],  \n",
    "            \"Bagil_nem_max\": [max_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Bagil_nem_min\": [min_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Ruzgar_yonu_max\": [max_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_yonu_min\": [min_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_hizi_max\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Ruzgar_hizi_min\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Yagis_max\": [max_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Yagis_min\": [min_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Hissedilen_sicaklik_max\": [max_values.get(\"t_apparent:C\", None)],\n",
    "            \"Hissedilen_sicaklik_min\": [min_values.get(\"t_apparent:C\", None)],\n",
    "            \"Ilce\": [day_df[\"name\"].iloc[0].lower()]  # Ilce ekle\n",
    "        })\n",
    "\n",
    "        # her gunu o ilcenin df ine ekle\n",
    "        weather_last[name] = pd.concat([weather_last[name], new_row], ignore_index=True)\n",
    "\n",
    "  \n",
    "new_column_names = {\n",
    "    \"lat\" : \"lat\", \"lot\" : \"lot\", \"Sicaklik_max\" : \"Sicaklik_max\", \"Sicaklik_min\" : \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\" : \"Bulutluluk_max\", \"Bulutluluk_min\" : \"Bulutluluk_min\", \"Guneslilik_max\" : \"Guneslilik_max\",\n",
    "    \"Guneslilik_min\" : \"Guneslilik_min\", \"Bagil_nem_max\" : \"Bagil_nem_max\", \"Bagil_nem_min\" : \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\" : \"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\" : \"Ruzgar_yonu_min\", \"Ruzgar_hizi_max\" : \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi_min\" : \"Ruzgar_hizi_min\", \"Yagis_max\" : \"Yagis_max\", \"Yagis_min\" : \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\" : \"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\" : \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\" : \"Ilce\", \"t_2m:C\" : \"Sicaklik\", \"effective_cloud_cover:p\" : \"Bulutluluk\", \"global_rad:W\" : \"Guneslilik\",\n",
    "    \"relative_humidity_2m:p\" : \"Bagil_nem\", \"wind_dir_10m:d\" : \"Ruzgar_yonu\", \"wind_speed_10m:ms\" : \"Ruzgar_hizi\",\n",
    "    \"prob_precip_1h:p\" : \"Yagis\", \"t_apparent:C\" : \"Hissedilen_sicaklik\", \"Tarih\" : \"Tarih\"\n",
    "}\n",
    "\n",
    "for name in weather_last.keys():\n",
    "    weather_last[name].set_index(\"Tarih\", inplace=True) # tarih kolonunu indexe ata\n",
    "    weather_last[name][\"Tarih\"] = weather_last[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "    weather_last[name] = pd.concat([weather_last[name], ilce_weather_day[name][[\"t_2m:C\",\"effective_cloud_cover:p\",\n",
    "    \"global_rad:W\", \"relative_humidity_2m:p\",\"wind_dir_10m:d\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\",\n",
    "    \"t_apparent:C\"]]], axis=1) # mean leri ekle\n",
    "    \n",
    "    weather_last[name] = weather_last[name].rename(columns=new_column_names) # kolonlari tekrar isimlendir\n",
    "    \n",
    "\n",
    "print(weather_last[\"izmir-konak\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['lat', 'lon', 'Sicaklik_max', 'Sicaklik_min', 'Bulutluluk_max',\n",
      "       'Bulutluluk_min', 'Guneslilik_max', 'Guneslilik_min', 'Bagil_nem_max',\n",
      "       'Bagil_nem_min', 'Ruzgar_yonu_max', 'Ruzgar_yonu_min',\n",
      "       'Ruzgar_hizi_max', 'Ruzgar_hizi_min', 'Yagis_max', 'Yagis_min',\n",
      "       'Hissedilen_sicaklik_max', 'Hissedilen_sicaklik_min', 'Ilce', 'Tarih',\n",
      "       'Sicaklik', 'Bulutluluk', 'Guneslilik', 'Bagil_nem', 'Ruzgar_yonu',\n",
      "       'Ruzgar_hizi', 'Yagis', 'Hissedilen_sicaklik'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(weather_last[\"izmir-konak\"].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tatil Adı'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4-) read and preprocess holidays.csv\n",
    "\n",
    "holiday = pd.read_csv(\"./holidays.csv\", low_memory=False)\n",
    "\n",
    "# print(holiday.head())\n",
    "\n",
    "holiday[\"tarih\"] = holiday['Yıl'].astype(str) + '-' + holiday['Ay'].astype(str) + '-' + holiday['Gün'].astype(str)\n",
    "holiday['tarih'] = pd.to_datetime(holiday['tarih'], format='%Y-%m-%d')\n",
    "holiday.set_index(\"tarih\", inplace=True)\n",
    "holiday = holiday.drop(columns=[\"Yıl\", \"Ay\", \"Gün\"])\n",
    "\n",
    "print(holiday.columns) # index olarak tarihi (YY-AA-GG), Bayram_Flag olarak da bayram ismini tutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tarih         ilce  bildirimsiz_sum  bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "                 Tatil Adı  \n",
      "tarih                       \n",
      "2021-01-01  New Year's Day  \n",
      "2021-01-02             NaN  \n",
      "2021-01-03             NaN  \n",
      "2021-01-04             NaN  \n",
      "2021-01-05             NaN  \n",
      "...                    ...  \n",
      "2024-01-27             NaN  \n",
      "2024-01-28             NaN  \n",
      "2024-01-29             NaN  \n",
      "2024-01-30             NaN  \n",
      "2024-01-31             NaN  \n",
      "\n",
      "[1124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "\n",
    "def merge_holiday(df1, df2=holiday):\n",
    "    merged_df = pd.merge(df1, df2[\"Tatil Adı\"], left_index=True, right_index=True, how=\"left\")\n",
    "    #df1[\"Bayramlar\"] = df2[\"Bayram_Flag\"]\n",
    "    return merged_df\n",
    "\n",
    "dict_holiday = {}\n",
    "for name in dict.keys():\n",
    "    dict_holiday[name] = merge_holiday(dict[name],holiday)\n",
    "    dict_holiday[name]['tarih'] = dict_holiday[name].index\n",
    "    dict_holiday[name] = dict_holiday[name].reindex(columns=[\"tarih\", \"ilce\", \"bildirimsiz_sum\", \"bildirimli_sum\", \"Tatil Adı\"])\n",
    "    \n",
    "print(dict_holiday[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                                                     \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500    1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000    2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833    3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000    4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167    5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Xesth\\AppData\\Local\\Temp\\ipykernel_18164\\3102189797.py:35: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n"
     ]
    }
   ],
   "source": [
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "\n",
    "def merge_weather(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\n",
    "    \"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "\"\"\" ekstra eklenebilecek kolonlar : (bunlari ustteki diger kolonarin arkasina ekleyebilirsin, ayni sekilde alttaki isimlendirmeye de eklemeyi unutma)\n",
    "\"Bulutluluk_max\", \"Bulutluluk_min\", \"Guneslilik_max\", \"Guneslilik_min\",\"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\",\"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\"\n",
    ",\"Bulutluluk\",\"Guneslilik\",\"Ruzgar_yonu\",\"Hissedilen_sicaklik\"\n",
    "\"\"\"\n",
    "merged_all = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all[name] = merge_weather(dict_holiday[name], weather_last[name])\n",
    "\n",
    "    merged_all[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "    \n",
    "    merged_all[name]['Gün'] = range(1, len(merged_all[name]) + 1)\n",
    "\n",
    "print(merged_all[\"izmir-konak\"].head())\n",
    "\n",
    "all_in_one = pd.concat(merged_all.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "#print(\"\\nall_in_one: \\n\\n\",all_in_one.dtypes)\n",
    "\n",
    "merged_all_week = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_week[name] = merged_all[name].resample(\"W\").sum(numeric_only=True)\n",
    "#print(merged_all_week[\"izmir-konak\"])\n",
    "\n",
    "merged_all_month = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n",
    "#print(merged_all_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi (runtime: 19s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimli_siz\"):\n",
    "    os.makedirs(\"./graphs/bildirimli_siz\")\n",
    "    print(\"bildirimli_siz klasoru olustu\")\n",
    "\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "#     plt.figure(figsize=(17,8))\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "#     plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\n",
    "#     plt.xticks(rotation=90)\n",
    "#     plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "#     plt.xlabel(\"Tarih\", fontsize=13)\n",
    "#     plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "#     plt.margins(0.01)\n",
    "#     plt.legend()\n",
    "#     plt.grid()\n",
    "#     plt.subplots_adjust(bottom=0.15)\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\\n\\nanaliz = pd.concat([\\n    ayristirma2.observed,\\n    ayristirma2.trend,\\n    ayristirma2.seasonal,\\n    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\\n], axis=1)\\nanaliz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\\n\\nindeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\\n\\n\\nX = sa.add_constant(indeks)\\nmodel = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\\nsonuc = model.fit()\\nrsquared_value = sonuc.rsquared\\ny = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\\n\\nyeni_satirlar = pd.DataFrame(index=y)\\nanaliz = pd.concat([analiz, yeni_satirlar])\\n\\n# not: bu degerleri ayarla\\nmev = [\\n    1.038656,\\n    0.973940,\\n    0.987404,\\n    1.038656\\n]\\n\\nnan_indices = analiz.index[analiz[\\'Mevsimsellik\\'].isna()]\\nfor i, index in enumerate(nan_indices):\\n    if i < len(mev):\\n        analiz.at[index, \\'Mevsimsellik\\'] = mev[i]\\n#print(analiz[\"Mevsimsellik\"])\\n\\ngirdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\\nregmodel = sonuc.predict(sa.add_constant(girdi))\\n\\nanaliz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\\n\\n\\nprint(analiz.head())\\n\\nplt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\')\\n#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\', horizontalalignment=\\'left\\')\\nplt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\\n#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\\nplt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği (runtime: 18s)\n",
    "\n",
    "if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimsiz_detailed\"):\n",
    "    os.makedirs(\"./graphs/bildirimsiz_detailed\")\n",
    "    print(\"bildirimsiz_detailed klasoru olustu\")\n",
    "\n",
    "# for name in merged_all_week.keys():\n",
    "\n",
    "    # plt.figure(figsize=(17,8))\n",
    "    # plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    # plt.title(\"{} - Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    # plt.xticks(rotation=90)\n",
    "    # plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    # plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    # window_size = 3  # Hareketli ortalama penceresi\n",
    "    # merged_all_week[name]['Moving_Average'] = merged_all_week[name][\"Bildirimsiz_sum\"].rolling(window=window_size, center=True).mean()\n",
    "    # #plt.plot(merged_all_week[name]['Moving_Average'], label=\"MHO\", color=\"black\")\n",
    "\n",
    "    # ortalama = merged_all_week[name][\"Bildirimsiz_sum\"].mean()\n",
    "    # plt.axhline(y=ortalama, color='orange', linestyle='--', label='Ortalama %{:.1f}'.format(ortalama),linewidth=2.2)\n",
    "    # alpha = 0.2  # Yumuşatma parametresi \n",
    "    # # formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "    # merged_all_week[name]['EWMA'] = merged_all_week[name][\"Bildirimsiz_sum\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    # plt.plot(merged_all_week[name]['EWMA'], label=\"EWMA\", color=\"red\", lw=2.9)\n",
    "\n",
    "\n",
    "    # plt.margins(0.01)\n",
    "    # plt.legend()\n",
    "    # plt.grid()\n",
    "    # plt.subplots_adjust(bottom=0.15)\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(\"./graphs/bildirimsiz_detailed/{}.png\".format(name))\n",
    "    #plt.show()\n",
    "\n",
    "\"\"\"\n",
    "ayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\n",
    "\n",
    "analiz = pd.concat([\n",
    "    ayristirma2.observed,\n",
    "    ayristirma2.trend,\n",
    "    ayristirma2.seasonal,\n",
    "    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\n",
    "], axis=1)\n",
    "analiz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\n",
    "\n",
    "indeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\n",
    "\n",
    "\n",
    "X = sa.add_constant(indeks)\n",
    "model = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\n",
    "sonuc = model.fit()\n",
    "rsquared_value = sonuc.rsquared\n",
    "y = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\n",
    "\n",
    "yeni_satirlar = pd.DataFrame(index=y)\n",
    "analiz = pd.concat([analiz, yeni_satirlar])\n",
    "\n",
    "# not: bu degerleri ayarla\n",
    "mev = [\n",
    "    1.038656,\n",
    "    0.973940,\n",
    "    0.987404,\n",
    "    1.038656\n",
    "]\n",
    "\n",
    "nan_indices = analiz.index[analiz['Mevsimsellik'].isna()]\n",
    "for i, index in enumerate(nan_indices):\n",
    "    if i < len(mev):\n",
    "        analiz.at[index, 'Mevsimsellik'] = mev[i]\n",
    "#print(analiz[\"Mevsimsellik\"])\n",
    "\n",
    "girdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\n",
    "regmodel = sonuc.predict(sa.add_constant(girdi))\n",
    "\n",
    "analiz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\n",
    "\n",
    "\n",
    "print(analiz.head())\n",
    "\n",
    "plt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top')\n",
    "#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "plt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\n",
    "#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\n",
    "plt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.0: 30.0, 6.0: 26.285714285714285, 7.0: 34.170212765957444, 8.2: 31.0, 8.5: 36.0, 8.6: 34.0, 8.9: 37.0, 10.3: 26.0, 11.6: 26.0, 12.7: 35.0, 16.6: 37.0, 18.2: 44.0, 18.9: 28.0, 20.7: 35.0, 22.1: 26.0, 23.7: 33.0, 27.7: 25.0, 28.8: 52.5, 29.0: 32.0, 41.3: 35.0, 44.0: 69.0, 44.3: 27.0, 52.6: 22.0, 54.3: 32.0, 54.5: 13.0, 55.0: 20.0, 56.5: 18.0, 57.2: 43.0, 58.5: 37.0, 60.0: 44.0, 60.1: 28.0, 61.6: 29.0, 67.1: 32.0, 69.3: 45.0, 73.1: 22.0, 73.7: 32.0, 77.0: 61.0, 80.7: 41.0, 85.5: 23.0, 88.3: 38.0, 88.4: 37.0, 91.3: 49.0, 98.6: 72.0, 99.9: 41.0, 100.8: 54.0, 101.0: 31.0, 101.3: 27.0, 102.4: 28.0, 102.9: 18.0, 104.2: 40.0, 105.8: 53.0, 106.6: 30.0, 107.8: 20.0, 108.7: 35.0, 120.2: 40.0, 120.4: 28.0, 122.3: 31.0, 125.6: 54.0, 127.6: 39.0, 139.2: 53.0, 141.70000000000002: 35.0, 143.8: 68.0, 154.6: 27.0, 157.2: 41.0, 157.60000000000002: 31.0, 159.3: 47.0, 162.0: 58.0, 164.8: 32.0, 174.4: 29.0, 175.5: 37.0, 175.7: 33.0, 178.0: 27.0, 178.5: 53.0, 178.6: 60.0, 179.1: 53.0, 180.0: 30.0, 180.6: 28.0, 184.0: 43.0, 186.3: 68.0, 190.4: 35.0, 191.2: 84.0, 191.5: 33.0, 191.6: 32.0, 197.8: 66.0, 202.89999999999998: 35.0, 209.8: 45.0, 212.5: 40.0, 215.6: 59.0, 216.9: 37.0, 219.0: 30.0, 223.6: 25.0, 233.6: 35.0, 237.3: 64.0, 241.2: 35.0, 244.1: 39.0, 246.5: 62.0, 252.1: 52.0, 263.8: 64.0, 272.5: 27.0, 280.1: 60.0, 300.6: 35.0, 308.1: 40.0, 323.4: 76.0, 327.6: 65.0, 335.0: 35.0, 352.9: 85.0, 393.6: 48.0, 413.7: 40.0}\n",
      "     Ort. Yagis  Ort. Kesinti\n",
      "0           5.0     30.000000\n",
      "1           6.0     26.285714\n",
      "2           7.0     34.170213\n",
      "3           8.2     31.000000\n",
      "4           8.5     36.000000\n",
      "..          ...           ...\n",
      "103       327.6     65.000000\n",
      "104       335.0     35.000000\n",
      "105       352.9     85.000000\n",
      "106       393.6     48.000000\n",
      "107       413.7     40.000000\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli, gerekli degil)\n",
    "\n",
    "yagis_dict = {}\n",
    "for label,group in merged_all_week[\"izmir-aliaga\"].groupby(\"Yagis_max\"):\n",
    "    yagis_dict[label] = group\n",
    "\n",
    "yagis_dict_toplamlari = {}\n",
    "for deger in yagis_dict.keys():\n",
    "    yagis_dict_toplamlari[deger] = yagis_dict[deger][\"Bildirimsiz_sum\"].mean()\n",
    "print(yagis_dict_toplamlari)\n",
    "\n",
    "hesaplamalar = pd.DataFrame(list(yagis_dict_toplamlari.items()), columns=['Ort. Yagis', 'Ort. Kesinti'])\n",
    "print(hesaplamalar)\n",
    "window_size = 3  # Hareketli ortalama penceresi\n",
    "hesaplamalar['Moving_Average'] = hesaplamalar[\"Ort. Kesinti\"].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "alpha = 0.3  # Yumuşatma parametresi \n",
    "# formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "hesaplamalar['EWMA'] = hesaplamalar[\"Ort. Kesinti\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(14,6))\n",
    "# #plt.bar(merged_all_week[\"izmir-aliaga\"][\"Yagis\"],merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], width=0.05, label=\"Bildirimsiz\")\n",
    "# plt.scatter(yagis_dict_toplamlari.keys(), yagis_dict_toplamlari.values(), label=\"Ort. Kesinti\")\n",
    "# #plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['Moving_Average'], label=\"MHO\", color=\"red\")\n",
    "# plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['EWMA'], label=\"EWMA\", color=\"red\")\n",
    "# plt.margins(0.01)\n",
    "# plt.title(\"Izmir_Aliaga Yagis - Bildirimsiz (Haftalik)\")\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.xlabel(\"Ortalama Yagis Miktari\")\n",
    "# plt.ylabel(\"Ortalama Elektrik Kesintisi\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tarih         ilce  bildirimli_sum\n",
      "18   2024-02-01  izmir-konak               4\n",
      "65   2024-02-02  izmir-konak               1\n",
      "112  2024-02-03  izmir-konak               1\n",
      "159  2024-02-04  izmir-konak               0\n",
      "206  2024-02-05  izmir-konak               2\n",
      "Index(['Tarih', 'Ilce', 'Bildirimli_sum'], dtype='object')\n",
      "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
      "tarih                                                                          \n",
      "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
      "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
      "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
      "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
      "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
      "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
      "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
      "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
      "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
      "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
      "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
      "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
      "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
      "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
      "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
      "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
      "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
      "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
      "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
      "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
      "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
      "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
      "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
      "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
      "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
      "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
      "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
      "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
      "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
      "\n",
      "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
      "tarih                                                                     \n",
      "2024-02-01           4.6           89.5           50.8              2.9   \n",
      "2024-02-02           3.7           92.1           48.5              3.3   \n",
      "2024-02-03           5.6           88.7           49.0              4.7   \n",
      "2024-02-04           5.9           86.1           56.0              1.6   \n",
      "2024-02-05           7.1           95.7           59.0              2.7   \n",
      "2024-02-06           9.3           95.3           60.8              3.0   \n",
      "2024-02-07          12.0           95.7           67.3              6.9   \n",
      "2024-02-08          12.7           89.8           62.3              6.5   \n",
      "2024-02-09          10.8           94.2           60.1              3.5   \n",
      "2024-02-10          11.0           93.2           66.3              6.2   \n",
      "2024-02-11          13.8           82.0           54.5              8.0   \n",
      "2024-02-12          12.1           90.0           65.2              7.9   \n",
      "2024-02-13           8.9           96.1           52.5              3.9   \n",
      "2024-02-14           8.0           99.9           47.6              5.6   \n",
      "2024-02-15           8.2           80.6           59.9              5.3   \n",
      "2024-02-16           7.0           84.0           53.4              4.2   \n",
      "2024-02-17           7.4           87.0           53.2              4.0   \n",
      "2024-02-18           6.8           87.2           53.2              4.0   \n",
      "2024-02-19           6.3           84.7           49.8              3.7   \n",
      "2024-02-20           5.1           89.9           52.6              2.9   \n",
      "2024-02-21           6.1           88.8           44.0              3.4   \n",
      "2024-02-22           8.0           75.4           40.9              2.5   \n",
      "2024-02-23           8.6           95.3           62.4              4.3   \n",
      "2024-02-24           9.7           97.4           55.6              2.8   \n",
      "2024-02-25          10.6           90.8           48.1              3.1   \n",
      "2024-02-26          10.9           84.1           47.7              5.4   \n",
      "2024-02-27          11.8           87.6           50.0              2.6   \n",
      "2024-02-28          10.7           91.7           44.1              2.4   \n",
      "2024-02-29           9.9           92.8           41.2              2.6   \n",
      "\n",
      "            Ruzgar_hizi_min  Yagis_max  Yagis_min   Sicaklik  Bagil_nem  \\\n",
      "tarih                                                                     \n",
      "2024-02-01              2.9        1.0        1.0   7.416667  76.075000   \n",
      "2024-02-02              3.3        1.0        1.0   7.537500  76.600000   \n",
      "2024-02-03              4.7        1.0        1.0   8.354167  70.429167   \n",
      "2024-02-04              1.6        8.7        1.0   9.300000  72.108333   \n",
      "2024-02-05              2.7        1.0        1.0  11.412500  81.879167   \n",
      "2024-02-06              3.0        1.0        1.0  13.341667  82.562500   \n",
      "2024-02-07              6.9        4.6        1.0  14.958333  84.025000   \n",
      "2024-02-08              6.5        1.0        1.0  14.925000  80.550000   \n",
      "2024-02-09              3.5       57.1        1.0  13.379167  83.950000   \n",
      "2024-02-10              6.2       14.1        1.0  14.108333  79.545833   \n",
      "2024-02-11              8.0       94.2        1.0  15.645833  70.191667   \n",
      "2024-02-12              7.9       95.0        1.0  14.679167  78.145833   \n",
      "2024-02-13              3.9       31.7        1.0  11.904167  82.216667   \n",
      "2024-02-14              5.6        1.0        1.0  11.454167  76.887500   \n",
      "2024-02-15              5.3       68.7        1.0   9.841667  72.545833   \n",
      "2024-02-16              4.2        1.0        1.0   9.875000  72.495833   \n",
      "2024-02-17              4.0        1.0        1.0  10.062500  73.662500   \n",
      "2024-02-18              4.0        1.0        1.0  10.137500  73.129167   \n",
      "2024-02-19              3.7        1.0        1.0   9.700000  70.825000   \n",
      "2024-02-20              2.9        1.0        1.0   9.154167  75.720833   \n",
      "2024-02-21              3.4        8.3        1.0   9.941667  69.854167   \n",
      "2024-02-22              2.5        1.0        1.0  11.979167  60.583333   \n",
      "2024-02-23              4.3        1.0        1.0  12.425000  79.912500   \n",
      "2024-02-24              2.8        1.0        1.0  13.795833  81.883333   \n",
      "2024-02-25              3.1       25.7        1.0  14.058333  71.991667   \n",
      "2024-02-26              5.4       31.1        1.0  14.300000  67.937500   \n",
      "2024-02-27              2.6        1.0        1.0  14.954167  75.770833   \n",
      "2024-02-28              2.4        1.0        1.0  15.362500  73.537500   \n",
      "2024-02-29              2.6       85.1        1.0  15.991667  68.441667   \n",
      "\n",
      "            Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                    \n",
      "2024-02-01     2.054167   1.000000    1  \n",
      "2024-02-02     1.691667   1.000000    2  \n",
      "2024-02-03     2.483333   1.000000    3  \n",
      "2024-02-04     1.083333   1.616667    4  \n",
      "2024-02-05     2.004167   1.000000    5  \n",
      "2024-02-06     2.337500   1.000000    6  \n",
      "2024-02-07     4.529167   1.245833    7  \n",
      "2024-02-08     4.483333   1.000000    8  \n",
      "2024-02-09     2.491667   5.058333    9  \n",
      "2024-02-10     4.133333   2.512500   10  \n",
      "2024-02-11     6.191667  15.691667   11  \n",
      "2024-02-12     6.387500  37.950000   12  \n",
      "2024-02-13     2.108333   3.087500   13  \n",
      "2024-02-14     2.820833   1.000000   14  \n",
      "2024-02-15     4.150000   7.166667   15  \n",
      "2024-02-16     2.641667   1.000000   16  \n",
      "2024-02-17     2.487500   1.000000   17  \n",
      "2024-02-18     2.637500   1.000000   18  \n",
      "2024-02-19     2.166667   1.000000   19  \n",
      "2024-02-20     1.504167   1.000000   20  \n",
      "2024-02-21     1.695833   1.562500   21  \n",
      "2024-02-22     1.137500   1.000000   22  \n",
      "2024-02-23     2.583333   1.000000   23  \n",
      "2024-02-24     2.112500   1.000000   24  \n",
      "2024-02-25     2.170833   2.216667   25  \n",
      "2024-02-26     2.712500   5.658333   26  \n",
      "2024-02-27     1.112500   1.000000   27  \n",
      "2024-02-28     1.566667   1.000000   28  \n",
      "2024-02-29     1.633333  14.195833   29  \n"
     ]
    }
   ],
   "source": [
    "# 10-) test icin birlestirme islemleri\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\", low_memory=False) # 47 ilce icin 28 gunluk veriler var. (tarih, ilce, bildirimli_sum)\n",
    "#print(test)\n",
    "\n",
    "dict_test :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak ilcelerin 4 ocak - 31 ocak arasi verilerini df olarak tutar\n",
    "for label, group in test.groupby(\"ilce\"):\n",
    "    dict_test[label] = group\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].head())\n",
    "\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    tarihler = [] # duzgun tarihleri tutacak\n",
    "    for date in dict_test[name][\"tarih\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d\")) \n",
    "\n",
    "    dict_test[name][\"tarih\"] = tarihler # duzeltilmis tarihleri ata\n",
    "\n",
    "    dict_test[name].set_index(\"tarih\", inplace=True) # tarih kolonunu index'e ata\n",
    "    dict_test[name][\"Tarih\"] = dict_test[name].index # tarih kolonunu yeniden olustur\n",
    "    dict_test[name] = dict_test[name].iloc[:, [2, 0, 1]] # kolon siralarini duzenle\n",
    "    dict_test[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\"] # kolon isimlerini duzenle\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].columns)\n",
    "\n",
    "\n",
    "dict_test_merged = {} # birlestirilenleri tutacak dict\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    gecici = merge_holiday(dict_test[name], holiday) # test'e holiday ekle\n",
    "    dict_test_merged[name] = merge_weather(gecici, weather_last[name]) # sonra weather'i ekle\n",
    "\n",
    "    dict_test_merged[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "\n",
    "    dict_test_merged[name]['Gün'] = range(1, len(dict_test_merged[name]) + 1) # gun kolonu ekle (1-28 arasi oluyor)\n",
    "\n",
    "print(dict_test_merged[\"izmir-konak\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKINE OGRENMESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "...                    ...           ...           ...            ...   \n",
      "2024-01-27             NaN          12.7           4.6           89.1   \n",
      "2024-01-28             NaN          10.8           4.9           91.6   \n",
      "2024-01-29             NaN           8.9           3.9           83.9   \n",
      "2024-01-30             NaN           9.0           4.4           76.3   \n",
      "2024-01-31             NaN          11.4           4.5           77.6   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "...                   ...              ...              ...        ...   \n",
      "2024-01-27           45.5              2.1              2.1        1.0   \n",
      "2024-01-28           43.5              4.8              4.8        1.0   \n",
      "2024-01-29           52.2              6.9              6.9        1.0   \n",
      "2024-01-30           50.2              6.5              6.5       53.5   \n",
      "2024-01-31           47.7              5.6              5.6        1.0   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis   Gün  \n",
      "tarih                                                                      \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500     1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000     2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833     3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000     4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167     5  \n",
      "...               ...        ...        ...          ...        ...   ...  \n",
      "2024-01-27        1.0   8.379167  72.575000     1.100000   1.000000  1120  \n",
      "2024-01-28        1.0   7.587500  70.383333     2.925000   1.000000  1121  \n",
      "2024-01-29        1.0   5.970833  69.762500     4.650000   1.000000  1122  \n",
      "2024-01-30        1.0   6.475000  64.350000     4.837500   6.612500  1123  \n",
      "2024-01-31        1.0   7.191667  65.329167     3.695833   1.000000  1124  \n",
      "\n",
      "[1124 rows x 18 columns]\n",
      "['izmir-aliaga', 'izmir-balcova', 'izmir-bayindir', 'izmir-bayrakli', 'izmir-bergama', 'izmir-beydag', 'izmir-bornova', 'izmir-buca', 'izmir-cesme', 'izmir-cigli', 'izmir-dikili', 'izmir-foca', 'izmir-gaziemir', 'izmir-guzelbahce', 'izmir-karabaglar', 'izmir-karaburun', 'izmir-karsiyaka', 'izmir-kemalpasa', 'izmir-kinik', 'izmir-kiraz', 'izmir-konak', 'izmir-menderes', 'izmir-menemen', 'izmir-narlidere', 'izmir-odemis', 'izmir-seferihisar', 'izmir-selcuk', 'izmir-tire', 'izmir-torbali', 'izmir-urla', 'manisa-ahmetli', 'manisa-akhisar', 'manisa-alasehir', 'manisa-demirci', 'manisa-golmarmara', 'manisa-gordes', 'manisa-kirkagac', 'manisa-koprubasi', 'manisa-kula', 'manisa-salihli', 'manisa-sarigol', 'manisa-saruhanli', 'manisa-sehzadeler', 'manisa-selendi', 'manisa-soma', 'manisa-turgutlu', 'manisa-yunusemre']\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all['izmir-konak']\n",
    "df_test = dict_test_merged[\"izmir-konak\"]\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis']\n",
    "output_var = df['Bildirimsiz_sum']\n",
    "target = 'Bildirimsiz_sum'\n",
    "ilceler = []\n",
    "\n",
    "dict = {}\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "ilceler = list(dict.keys())\n",
    "\n",
    "print(df)\n",
    "print(ilceler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ilcelerin numerizasyonu\n",
    "columns_tonumerate = ['Bayram_Flag']\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "# test csv dosyasi numerizasyon\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "# indexi gun yapmak gerek!!!!!\n",
    "# gunu scale etmemek gerek!!!\n",
    "# bayrami da scale etmesek olur!\n",
    "# #Scaling\n",
    "scaler = MinMaxScaler()\n",
    "feature_transform = scaler.fit_transform(df[features])\n",
    "feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler2 = MinMaxScaler()\n",
    "feature_test = scaler2.fit_transform(df_test[features])\n",
    "feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bildirimli_sum</th>\n",
       "      <th>Sicaklik</th>\n",
       "      <th>Bayram_Flag</th>\n",
       "      <th>Bagil_nem</th>\n",
       "      <th>Ruzgar_hizi</th>\n",
       "      <th>Yagis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tarih</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-02-01</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.660860</td>\n",
       "      <td>0.183032</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-02</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.014091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.683256</td>\n",
       "      <td>0.114690</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-03</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.109329</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.420014</td>\n",
       "      <td>0.263943</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-04</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.219631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-05</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.465986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908461</td>\n",
       "      <td>0.173606</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-06</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.690962</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.937611</td>\n",
       "      <td>0.236449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-07</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.879495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.649647</td>\n",
       "      <td>0.006653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-08</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.875607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.851760</td>\n",
       "      <td>0.641005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-09</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.695335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996801</td>\n",
       "      <td>0.265515</td>\n",
       "      <td>0.109833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-10</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.780369</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.808923</td>\n",
       "      <td>0.575020</td>\n",
       "      <td>0.040934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-11</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.959670</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.409883</td>\n",
       "      <td>0.963079</td>\n",
       "      <td>0.397609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-12</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.846939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-13</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.523324</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.922858</td>\n",
       "      <td>0.193244</td>\n",
       "      <td>0.056495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-14</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.470845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.695521</td>\n",
       "      <td>0.327573</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-15</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.282799</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510309</td>\n",
       "      <td>0.578162</td>\n",
       "      <td>0.166892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-16</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.286686</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508176</td>\n",
       "      <td>0.293794</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-17</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.308552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.557945</td>\n",
       "      <td>0.264729</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-18</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.317298</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535194</td>\n",
       "      <td>0.293009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-19</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.266278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436900</td>\n",
       "      <td>0.204242</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-20</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.202624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645752</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-21</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.294461</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395485</td>\n",
       "      <td>0.115475</td>\n",
       "      <td>0.015223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-22</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.532070</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010212</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-23</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.584062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.824565</td>\n",
       "      <td>0.282797</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.743926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908638</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.774538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486669</td>\n",
       "      <td>0.205027</td>\n",
       "      <td>0.032927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-26</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.802721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.313722</td>\n",
       "      <td>0.307148</td>\n",
       "      <td>0.126071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-27</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.879009</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.647885</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-28</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.926628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552613</td>\n",
       "      <td>0.091123</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-02-29</th>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.335229</td>\n",
       "      <td>0.103692</td>\n",
       "      <td>0.357127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bildirimli_sum  Sicaklik  Bayram_Flag  Bagil_nem  Ruzgar_hizi  \\\n",
       "tarih                                                                       \n",
       "2024-02-01            1.00  0.000000          0.0   0.660860     0.183032   \n",
       "2024-02-02            0.25  0.014091          0.0   0.683256     0.114690   \n",
       "2024-02-03            0.25  0.109329          0.0   0.420014     0.263943   \n",
       "2024-02-04            0.00  0.219631          0.0   0.491646     0.000000   \n",
       "2024-02-05            0.50  0.465986          0.0   0.908461     0.173606   \n",
       "2024-02-06            0.25  0.690962          0.0   0.937611     0.236449   \n",
       "2024-02-07            0.00  0.879495          0.0   1.000000     0.649647   \n",
       "2024-02-08            0.75  0.875607          0.0   0.851760     0.641005   \n",
       "2024-02-09            0.25  0.695335          0.0   0.996801     0.265515   \n",
       "2024-02-10            1.00  0.780369          0.0   0.808923     0.575020   \n",
       "2024-02-11            0.00  0.959670          0.0   0.409883     0.963079   \n",
       "2024-02-12            0.00  0.846939          0.0   0.749200     1.000000   \n",
       "2024-02-13            0.00  0.523324          0.0   0.922858     0.193244   \n",
       "2024-02-14            0.00  0.470845          0.0   0.695521     0.327573   \n",
       "2024-02-15            0.00  0.282799          0.0   0.510309     0.578162   \n",
       "2024-02-16            0.00  0.286686          0.0   0.508176     0.293794   \n",
       "2024-02-17            1.00  0.308552          0.0   0.557945     0.264729   \n",
       "2024-02-18            0.00  0.317298          0.0   0.535194     0.293009   \n",
       "2024-02-19            0.25  0.266278          0.0   0.436900     0.204242   \n",
       "2024-02-20            0.00  0.202624          0.0   0.645752     0.079340   \n",
       "2024-02-21            0.00  0.294461          0.0   0.395485     0.115475   \n",
       "2024-02-22            0.50  0.532070          0.0   0.000000     0.010212   \n",
       "2024-02-23            0.75  0.584062          0.0   0.824565     0.282797   \n",
       "2024-02-24            0.25  0.743926          0.0   0.908638     0.194030   \n",
       "2024-02-25            0.25  0.774538          0.0   0.486669     0.205027   \n",
       "2024-02-26            0.00  0.802721          0.0   0.313722     0.307148   \n",
       "2024-02-27            0.75  0.879009          0.0   0.647885     0.005499   \n",
       "2024-02-28            0.00  0.926628          0.0   0.552613     0.091123   \n",
       "2024-02-29            0.00  1.000000          0.0   0.335229     0.103692   \n",
       "\n",
       "               Yagis  \n",
       "tarih                 \n",
       "2024-02-01  0.000000  \n",
       "2024-02-02  0.000000  \n",
       "2024-02-03  0.000000  \n",
       "2024-02-04  0.016689  \n",
       "2024-02-05  0.000000  \n",
       "2024-02-06  0.000000  \n",
       "2024-02-07  0.006653  \n",
       "2024-02-08  0.000000  \n",
       "2024-02-09  0.109833  \n",
       "2024-02-10  0.040934  \n",
       "2024-02-11  0.397609  \n",
       "2024-02-12  1.000000  \n",
       "2024-02-13  0.056495  \n",
       "2024-02-14  0.000000  \n",
       "2024-02-15  0.166892  \n",
       "2024-02-16  0.000000  \n",
       "2024-02-17  0.000000  \n",
       "2024-02-18  0.000000  \n",
       "2024-02-19  0.000000  \n",
       "2024-02-20  0.000000  \n",
       "2024-02-21  0.015223  \n",
       "2024-02-22  0.000000  \n",
       "2024-02-23  0.000000  \n",
       "2024-02-24  0.000000  \n",
       "2024-02-25  0.032927  \n",
       "2024-02-26  0.126071  \n",
       "2024-02-27  0.000000  \n",
       "2024-02-28  0.000000  \n",
       "2024-02-29  0.357127  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test\n",
    "# output_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y test-train elde edimi\n",
    "x = df[features]\n",
    "y = output_var # = df[\"target_var\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "#Splitting to Training set and Test set --- burasi timeseries icin split\n",
    "timesplit = TimeSeriesSplit(n_splits=15)\n",
    "for train_index, test_index in timesplit.split(feature_transform):\n",
    "        X_tr, X_te = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_tr, y_te = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 1s 939us/step - loss: 127.0392\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 96.3895\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 60.2745\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 50.6725\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 49.4799\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.9965\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.6104\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 48.2669\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.9528\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.6649\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.4001\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 47.1566\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.9323\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.7262\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.5360\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.3609\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.2002\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 46.0515\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.9147\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.7888\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.6730\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.5656\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.4659\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.3728\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.2857\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.2042\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.1257\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 45.0512\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9802\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.9133\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.8501\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7895\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.7317\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6772\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.6244\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5744\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.5262\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4798\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.4356\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3932\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3523\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.3126\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2749\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2385\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.2026\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1676\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1337\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.1004\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0685\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0365\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 44.0062\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.9759\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.9456\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.9162\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.8873\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.8591\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.8315\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.8035\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.7751\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.7467\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.7189\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.6918\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.6638\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.6381\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.6120\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.5864\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.5619\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.5372\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.5134\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.4885\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.4647\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.4403\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.4166\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.3932\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.3705\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.3463\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.3225\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.2989\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.2747\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.2516\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.2275\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.2048\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.1821\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.1596\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.1373\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.1151\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.0929\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.0711\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.0491\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.0279\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 43.0061\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.9845\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.9631\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.9412\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.9206\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.8991\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.8768\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.8558\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.8345\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 42.8136\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "#Process the data for LSTM\n",
    "trainX = np.array(X_tr)\n",
    "testX = np.array(X_te)\n",
    "X_tr = trainX.reshape(X_tr.shape[0], 1, X_tr.shape[1])\n",
    "X_te = testX.reshape(X_te.shape[0], 1, X_te.shape[1])\n",
    "\n",
    "#Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Model Training\n",
    "history=lstm.fit(X_tr, y_tr, epochs=100, batch_size=8, verbose=1, shuffle=False)\n",
    "\n",
    "#LSTM Prediction\n",
    "y_pr= lstm.predict(X_te)\n",
    "\n",
    "# Predicted vs True Adj Close Value – LSTM  --burasi copy paste\n",
    "plt.plot(y_te, label='True Value')\n",
    "plt.plot(y_pr, label='LSTM Value')\n",
    "plt.title(\"Prediction by LSTM\")\n",
    "plt.xlabel('Time Scale')\n",
    "plt.ylabel('Scaled USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# test_pred = lstm.predict(gercek test)\n",
    "# csv ye yazdir vs vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy with class weights: 0.06222222222222222\n",
      "egitim verisi dogrulugu  1.0\n",
      "test verisi dogrulugu  0.06222222222222222\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "k=8\n",
    "neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_hat = neigh.predict(X_test)\n",
    "\n",
    "test_accuracy = neigh.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy with class weights:\", test_accuracy)\n",
    "print(\"egitim verisi dogrulugu \", metrics.accuracy_score(y_train,neigh.predict(X_train)))\n",
    "print(\"test verisi dogrulugu \", metrics.accuracy_score(y_test,y_hat))\n",
    "\n",
    "# test tahmin --tahmini yazmada sikinti cikiyor tek bir ilceden tahmin yapinca iste\n",
    "y_hat = neigh.predict(df_test[features])\n",
    "submission = pd.read_csv(\"sample_submission.csv\", low_memory=False)\n",
    "y_hat = np.round(y_hat).astype(int)\n",
    "df_test['bildirimsiz_sum'] = y_hat\n",
    "df_test.to_csv('knnprediction.csv', index=False)\n",
    "\n",
    "# optimal k degeri\n",
    "\n",
    "# # Define the range of k values to try\n",
    "# k_values = range(1, 21)\n",
    "\n",
    "# # Perform cross-validation for each value of k\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "\n",
    "# # Find the optimal value of k with the highest cross-validation score\n",
    "# optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "# print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "# # Train the model with the optimal k value\n",
    "# neigh = KNeighborsClassifier(n_neighbors=optimal_k).fit(X_train, y_train)\n",
    "# test_accuracy = neigh.score(X_test, y_test)\n",
    "# print(\"Test accuracy with optimal k:\", test_accuracy)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() specifies that the network is a linear stack of layers\n",
    "\n",
    "model.add() adds the hidden layer.\n",
    "\n",
    "Dense means that neurons between layers are fully connected\n",
    "\n",
    "input_dim defines the number of features in the training dataset\n",
    "\n",
    "activation defines the activation function\n",
    "\n",
    "loss selects the cost function\n",
    "\n",
    "optimizer selects the learning algorithm\n",
    "\n",
    "metrics selects the performance metrics to be saved for further analysis\n",
    "\n",
    "model.fit() initialize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 34.2015 - mae: 4.2425 - val_loss: 19.7350 - val_mae: 2.8160\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 17.8870 - mae: 2.9912 - val_loss: 15.1012 - val_mae: 2.8456\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.1024 - mae: 2.7945 - val_loss: 14.6719 - val_mae: 2.6733\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.0980 - mae: 2.8305 - val_loss: 14.4529 - val_mae: 2.7527\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.0914 - mae: 2.8906 - val_loss: 14.3026 - val_mae: 2.6698\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5533 - mae: 2.7112 - val_loss: 14.1855 - val_mae: 2.7122\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.7180 - mae: 2.7828 - val_loss: 14.1394 - val_mae: 2.6941\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5851 - mae: 2.7553 - val_loss: 14.0999 - val_mae: 2.7077\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.7212 - mae: 2.8566 - val_loss: 14.0193 - val_mae: 2.6879\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.6966 - mae: 2.7708 - val_loss: 13.9677 - val_mae: 2.6854\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.4794 - mae: 2.7327 - val_loss: 13.9282 - val_mae: 2.6741\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.5778 - mae: 2.7670 - val_loss: 13.8619 - val_mae: 2.6824\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3193 - mae: 2.7489 - val_loss: 13.8204 - val_mae: 2.6570\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.1659 - mae: 2.7596 - val_loss: 13.8163 - val_mae: 2.6412\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3999 - mae: 2.7172 - val_loss: 13.7025 - val_mae: 2.6860\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3902 - mae: 2.7748 - val_loss: 13.6598 - val_mae: 2.6462\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.3195 - mae: 2.7553 - val_loss: 13.5972 - val_mae: 2.6331\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0957 - mae: 2.7437 - val_loss: 13.5777 - val_mae: 2.6137\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.2567 - mae: 2.7212 - val_loss: 13.5363 - val_mae: 2.6102\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0829 - mae: 2.7053 - val_loss: 13.4259 - val_mae: 2.6052\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.0516 - mae: 2.7295 - val_loss: 13.3175 - val_mae: 2.6179\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.9219 - mae: 2.6967 - val_loss: 13.2308 - val_mae: 2.6105\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.7951 - mae: 2.6901 - val_loss: 13.1350 - val_mae: 2.6702\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.6287 - mae: 2.7268 - val_loss: 13.1171 - val_mae: 2.5882\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.8041 - mae: 2.7127 - val_loss: 13.2260 - val_mae: 2.5550\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.6922 - mae: 2.7116 - val_loss: 12.9506 - val_mae: 2.6200\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.7628 - mae: 2.7129 - val_loss: 12.8836 - val_mae: 2.5770\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.6181 - mae: 2.6587 - val_loss: 12.8443 - val_mae: 2.5865\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5955 - mae: 2.7027 - val_loss: 12.8647 - val_mae: 2.5521\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.7202 - mae: 2.6884 - val_loss: 12.6598 - val_mae: 2.5672\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.6332 - mae: 2.6760 - val_loss: 12.6282 - val_mae: 2.5746\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2569 - mae: 2.7037 - val_loss: 12.6516 - val_mae: 2.5428\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4108 - mae: 2.6592 - val_loss: 12.5624 - val_mae: 2.6151\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3384 - mae: 2.7187 - val_loss: 12.5152 - val_mae: 2.5346\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2133 - mae: 2.6801 - val_loss: 12.4700 - val_mae: 2.5357\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3645 - mae: 2.6356 - val_loss: 12.4080 - val_mae: 2.5849\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3877 - mae: 2.6931 - val_loss: 12.3925 - val_mae: 2.5514\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0835 - mae: 2.6606 - val_loss: 12.4331 - val_mae: 2.5287\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3367 - mae: 2.7114 - val_loss: 12.4350 - val_mae: 2.5153\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2223 - mae: 2.6567 - val_loss: 12.2775 - val_mae: 2.5308\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0974 - mae: 2.6352 - val_loss: 12.1908 - val_mae: 2.5506\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0991 - mae: 2.6704 - val_loss: 12.2239 - val_mae: 2.5207\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2514 - mae: 2.6579 - val_loss: 12.3253 - val_mae: 2.6226\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.2583 - mae: 2.8853 - val_loss: 13.3491 - val_mae: 2.5604\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.6802 - mae: 2.7124 - val_loss: 12.1413 - val_mae: 2.5126\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0729 - mae: 2.6478 - val_loss: 12.2930 - val_mae: 2.4944\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1258 - mae: 2.6625 - val_loss: 12.4502 - val_mae: 2.4876\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3051 - mae: 2.6869 - val_loss: 12.3612 - val_mae: 2.4855\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8832 - mae: 2.6319 - val_loss: 12.0671 - val_mae: 2.5615\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1138 - mae: 2.7036 - val_loss: 12.3472 - val_mae: 2.4931\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12.3185 - mae: 2.6350\n",
      "Mean Absolute Error on Test Data: 2.634953022003174\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.022888452749230792\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 17ms/step - loss: 3.6662 - mae: 1.3814 - val_loss: 1.4232 - val_mae: 0.8207\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.9390 - mae: 1.0421 - val_loss: 1.5494 - val_mae: 1.0751\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.9587 - mae: 1.0925 - val_loss: 1.2793 - val_mae: 0.9036\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8783 - mae: 1.0019 - val_loss: 1.2525 - val_mae: 0.8807\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8996 - mae: 1.0414 - val_loss: 1.2986 - val_mae: 0.9273\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.8739 - mae: 1.0212 - val_loss: 1.2390 - val_mae: 0.8733\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8824 - mae: 1.0385 - val_loss: 1.3032 - val_mae: 0.9374\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8612 - mae: 1.0169 - val_loss: 1.2427 - val_mae: 0.8867\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8916 - mae: 1.0347 - val_loss: 1.2491 - val_mae: 0.8976\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8505 - mae: 1.0095 - val_loss: 1.2418 - val_mae: 0.8917\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8360 - mae: 1.0287 - val_loss: 1.2475 - val_mae: 0.8996\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7902 - mae: 1.0029 - val_loss: 1.2434 - val_mae: 0.8944\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8694 - mae: 1.0276 - val_loss: 1.2212 - val_mae: 0.8716\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8076 - mae: 1.0102 - val_loss: 1.2623 - val_mae: 0.9193\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8268 - mae: 1.0177 - val_loss: 1.2269 - val_mae: 0.8887\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8305 - mae: 1.0282 - val_loss: 1.2381 - val_mae: 0.9035\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8053 - mae: 1.0111 - val_loss: 1.2317 - val_mae: 0.8970\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7993 - mae: 0.9986 - val_loss: 1.2734 - val_mae: 0.9339\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7990 - mae: 1.0204 - val_loss: 1.2174 - val_mae: 0.8787\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7628 - mae: 0.9886 - val_loss: 1.2425 - val_mae: 0.9100\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 1.7890 - mae: 1.0054 - val_loss: 1.2183 - val_mae: 0.8857\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8187 - mae: 1.0197 - val_loss: 1.2413 - val_mae: 0.9111\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7926 - mae: 1.0039 - val_loss: 1.2316 - val_mae: 0.9011\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8088 - mae: 1.0237 - val_loss: 1.2208 - val_mae: 0.8841\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7718 - mae: 1.0252 - val_loss: 1.2637 - val_mae: 0.9277\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7484 - mae: 0.9858 - val_loss: 1.2151 - val_mae: 0.8854\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8096 - mae: 1.0331 - val_loss: 1.2276 - val_mae: 0.8970\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.8042 - mae: 0.9894 - val_loss: 1.2340 - val_mae: 0.9030\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7423 - mae: 1.0284 - val_loss: 1.2586 - val_mae: 0.9252\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7698 - mae: 0.9913 - val_loss: 1.2227 - val_mae: 0.8938\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7643 - mae: 1.0092 - val_loss: 1.2214 - val_mae: 0.8911\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7489 - mae: 0.9921 - val_loss: 1.2471 - val_mae: 0.9151\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7629 - mae: 1.0048 - val_loss: 1.2709 - val_mae: 0.9335\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7633 - mae: 1.0133 - val_loss: 1.2191 - val_mae: 0.8825\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7463 - mae: 1.0146 - val_loss: 1.2300 - val_mae: 0.8904\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7468 - mae: 0.9820 - val_loss: 1.2536 - val_mae: 0.9144\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7820 - mae: 1.0357 - val_loss: 1.2222 - val_mae: 0.8827\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7032 - mae: 0.9768 - val_loss: 1.2651 - val_mae: 0.9235\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7116 - mae: 1.0107 - val_loss: 1.2386 - val_mae: 0.8963\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7526 - mae: 0.9771 - val_loss: 1.2765 - val_mae: 0.9336\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7832 - mae: 1.0507 - val_loss: 1.2162 - val_mae: 0.8740\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7242 - mae: 0.9590 - val_loss: 1.2404 - val_mae: 0.9022\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 1.7249 - mae: 0.9998 - val_loss: 1.2619 - val_mae: 0.9182\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7261 - mae: 0.9970 - val_loss: 1.2439 - val_mae: 0.9033\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7325 - mae: 1.0090 - val_loss: 1.2358 - val_mae: 0.8934\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7311 - mae: 1.0005 - val_loss: 1.2584 - val_mae: 0.9172\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7078 - mae: 0.9759 - val_loss: 1.2721 - val_mae: 0.9168\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7243 - mae: 1.0100 - val_loss: 1.2567 - val_mae: 0.9031\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.6996 - mae: 0.9929 - val_loss: 1.3058 - val_mae: 0.9484\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 1.7116 - mae: 0.9819 - val_loss: 1.2496 - val_mae: 0.9008\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.4284 - mae: 0.9315\n",
      "Mean Absolute Error on Test Data: 0.9314990639686584\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08259022736736032\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 28.9898 - mae: 3.8342 - val_loss: 18.8909 - val_mae: 3.0461\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.2798 - mae: 2.8216 - val_loss: 14.6940 - val_mae: 2.9762\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.7700 - mae: 2.7850 - val_loss: 14.6114 - val_mae: 2.8591\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4212 - mae: 2.7619 - val_loss: 14.3467 - val_mae: 2.8831\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3254 - mae: 2.7577 - val_loss: 14.4209 - val_mae: 2.8421\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3118 - mae: 2.7150 - val_loss: 14.3268 - val_mae: 2.8460\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2366 - mae: 2.7309 - val_loss: 14.2892 - val_mae: 2.8473\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2561 - mae: 2.7115 - val_loss: 14.3436 - val_mae: 2.8308\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2236 - mae: 2.7277 - val_loss: 14.2700 - val_mae: 2.8467\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4021 - mae: 2.7671 - val_loss: 14.3576 - val_mae: 2.8333\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5222 - mae: 2.7599 - val_loss: 14.2470 - val_mae: 2.8631\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2977 - mae: 2.7219 - val_loss: 14.2451 - val_mae: 2.8621\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5899 - mae: 2.8629 - val_loss: 14.5584 - val_mae: 2.8203\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3196 - mae: 2.6546 - val_loss: 14.3044 - val_mae: 2.8560\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4449 - mae: 2.7953 - val_loss: 14.6106 - val_mae: 2.8226\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2404 - mae: 2.7066 - val_loss: 14.3454 - val_mae: 2.8584\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0042 - mae: 2.7474 - val_loss: 14.4151 - val_mae: 2.8431\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2811 - mae: 2.7088 - val_loss: 14.3735 - val_mae: 2.8543\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2864 - mae: 2.7980 - val_loss: 14.8942 - val_mae: 2.8142\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3451 - mae: 2.6970 - val_loss: 14.3634 - val_mae: 2.9021\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1798 - mae: 2.7443 - val_loss: 14.5105 - val_mae: 2.8423\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1686 - mae: 2.7662 - val_loss: 14.4597 - val_mae: 2.8579\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2740 - mae: 2.7822 - val_loss: 14.6925 - val_mae: 2.8337\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 14.0357 - mae: 2.7219 - val_loss: 14.4852 - val_mae: 2.8713\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9546 - mae: 2.7137 - val_loss: 14.7023 - val_mae: 2.8388\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9495 - mae: 2.7141 - val_loss: 14.5363 - val_mae: 2.8709\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8220 - mae: 2.7213 - val_loss: 14.6790 - val_mae: 2.8428\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8177 - mae: 2.7304 - val_loss: 14.7623 - val_mae: 2.8369\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8093 - mae: 2.7059 - val_loss: 14.5350 - val_mae: 2.8799\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9568 - mae: 2.7413 - val_loss: 14.6196 - val_mae: 2.8598\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9734 - mae: 2.7879 - val_loss: 14.7779 - val_mae: 2.8582\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7605 - mae: 2.7091 - val_loss: 14.6674 - val_mae: 2.8923\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8037 - mae: 2.6835 - val_loss: 14.7060 - val_mae: 2.8916\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8858 - mae: 2.7521 - val_loss: 15.1695 - val_mae: 2.8395\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7317 - mae: 2.6891 - val_loss: 14.7494 - val_mae: 2.8925\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7776 - mae: 2.7218 - val_loss: 14.8742 - val_mae: 2.8545\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6773 - mae: 2.7263 - val_loss: 14.9181 - val_mae: 2.8604\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9529 - mae: 2.7817 - val_loss: 15.0919 - val_mae: 2.8576\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9169 - mae: 2.7299 - val_loss: 14.9748 - val_mae: 2.8773\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9907 - mae: 2.6356 - val_loss: 14.8916 - val_mae: 2.9709\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4758 - mae: 2.9093 - val_loss: 15.8655 - val_mae: 2.8589\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0742 - mae: 2.7214 - val_loss: 14.9312 - val_mae: 2.8806\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6819 - mae: 2.6352 - val_loss: 14.9229 - val_mae: 2.8897\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6242 - mae: 2.7401 - val_loss: 15.0326 - val_mae: 2.8726\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.4744 - mae: 2.6936 - val_loss: 15.0991 - val_mae: 2.8683\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6413 - mae: 2.6621 - val_loss: 14.9620 - val_mae: 2.8980\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7798 - mae: 2.7636 - val_loss: 15.2564 - val_mae: 2.8598\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6664 - mae: 2.6413 - val_loss: 15.0076 - val_mae: 2.8927\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.4749 - mae: 2.7289 - val_loss: 15.1823 - val_mae: 2.8691\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.5445 - mae: 2.6565 - val_loss: 15.0325 - val_mae: 2.9143\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 13.9630 - mae: 2.5799\n",
      "Mean Absolute Error on Test Data: 2.5798590183258057\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.006972610061001028\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 29.7387 - mae: 3.6078 - val_loss: 36.5097 - val_mae: 2.9660\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 18.4078 - mae: 2.6445 - val_loss: 31.5729 - val_mae: 3.0596\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3332 - mae: 2.7281 - val_loss: 31.5681 - val_mae: 2.7580\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.6465 - mae: 2.5571 - val_loss: 30.9325 - val_mae: 2.8499\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3957 - mae: 2.6479 - val_loss: 30.6886 - val_mae: 2.8166\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3961 - mae: 2.6224 - val_loss: 30.7004 - val_mae: 2.7796\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.5107 - mae: 2.6576 - val_loss: 30.4339 - val_mae: 2.7689\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3794 - mae: 2.5239 - val_loss: 30.0486 - val_mae: 2.8397\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1950 - mae: 2.6660 - val_loss: 30.2149 - val_mae: 2.7487\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.2062 - mae: 2.6042 - val_loss: 29.8116 - val_mae: 2.7724\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.1412 - mae: 2.5286 - val_loss: 29.5342 - val_mae: 2.7860\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8574 - mae: 2.5804 - val_loss: 29.3203 - val_mae: 2.7686\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9581 - mae: 2.6084 - val_loss: 29.0760 - val_mae: 2.7763\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0093 - mae: 2.5738 - val_loss: 29.1184 - val_mae: 2.7167\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9007 - mae: 2.5048 - val_loss: 28.6449 - val_mae: 2.7811\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5297 - mae: 2.5576 - val_loss: 28.4039 - val_mae: 2.7493\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4522 - mae: 2.5839 - val_loss: 28.4474 - val_mae: 2.7225\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3837 - mae: 2.4962 - val_loss: 28.0737 - val_mae: 2.7745\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3662 - mae: 2.5987 - val_loss: 28.0985 - val_mae: 2.7056\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3560 - mae: 2.4817 - val_loss: 27.5590 - val_mae: 2.7424\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1709 - mae: 2.5327 - val_loss: 27.7993 - val_mae: 2.6998\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.3732 - mae: 2.5198 - val_loss: 27.3507 - val_mae: 2.7724\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2557 - mae: 2.4863 - val_loss: 27.1923 - val_mae: 2.7445\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0524 - mae: 2.5501 - val_loss: 27.8867 - val_mae: 2.6657\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0940 - mae: 2.5031 - val_loss: 27.1500 - val_mae: 2.7050\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0262 - mae: 2.4849 - val_loss: 26.7617 - val_mae: 2.7464\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2312 - mae: 2.5236 - val_loss: 27.0452 - val_mae: 2.6893\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9485 - mae: 2.4710 - val_loss: 26.7525 - val_mae: 2.7027\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1130 - mae: 2.5046 - val_loss: 26.6824 - val_mae: 2.7055\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9390 - mae: 2.4802 - val_loss: 26.5437 - val_mae: 2.7364\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0750 - mae: 2.4996 - val_loss: 26.8180 - val_mae: 2.6741\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0583 - mae: 2.5028 - val_loss: 26.7048 - val_mae: 2.7103\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5700 - mae: 2.4683 - val_loss: 26.4210 - val_mae: 2.7386\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8176 - mae: 2.4552 - val_loss: 26.3604 - val_mae: 2.8031\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0031 - mae: 2.4926 - val_loss: 26.5246 - val_mae: 2.6910\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6131 - mae: 2.4688 - val_loss: 26.3687 - val_mae: 2.7244\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5391 - mae: 2.4788 - val_loss: 27.0867 - val_mae: 2.6578\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7423 - mae: 2.4043 - val_loss: 26.0157 - val_mae: 2.7589\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4503 - mae: 2.5086 - val_loss: 26.6159 - val_mae: 2.6725\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6636 - mae: 2.4528 - val_loss: 26.2811 - val_mae: 2.7516\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4765 - mae: 2.4793 - val_loss: 26.1809 - val_mae: 2.7126\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3135 - mae: 2.4804 - val_loss: 26.5984 - val_mae: 2.6597\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4529 - mae: 2.4429 - val_loss: 26.1193 - val_mae: 2.7255\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6623 - mae: 2.4791 - val_loss: 26.1034 - val_mae: 2.7003\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5433 - mae: 2.4687 - val_loss: 26.0996 - val_mae: 2.7073\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4485 - mae: 2.4438 - val_loss: 26.1907 - val_mae: 2.7470\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3464 - mae: 2.4629 - val_loss: 26.1614 - val_mae: 2.7039\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2817 - mae: 2.4548 - val_loss: 26.2169 - val_mae: 2.7336\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0604 - mae: 2.4150 - val_loss: 25.9478 - val_mae: 2.7296\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0681 - mae: 2.4758 - val_loss: 26.1309 - val_mae: 2.6850\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 15.9653 - mae: 2.7482\n",
      "Mean Absolute Error on Test Data: 2.748178243637085\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.09112595945813806\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 59.6258 - mae: 6.0042 - val_loss: 44.6230 - val_mae: 4.7960\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.8124 - mae: 3.6588 - val_loss: 22.9395 - val_mae: 3.5773\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6419 - mae: 3.3624 - val_loss: 22.3163 - val_mae: 3.2903\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.2920 - mae: 3.2505 - val_loss: 21.9345 - val_mae: 3.3815\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.4201 - mae: 3.2906 - val_loss: 21.8419 - val_mae: 3.3196\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1527 - mae: 3.3109 - val_loss: 21.8427 - val_mae: 3.2833\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.4836 - mae: 3.2984 - val_loss: 21.9253 - val_mae: 3.2598\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7582 - mae: 3.3001 - val_loss: 21.5363 - val_mae: 3.3227\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3633 - mae: 3.2793 - val_loss: 21.9371 - val_mae: 3.2415\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3298 - mae: 3.2744 - val_loss: 21.4355 - val_mae: 3.3311\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.4212 - mae: 3.3043 - val_loss: 21.4360 - val_mae: 3.3017\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0667 - mae: 3.2708 - val_loss: 21.4031 - val_mae: 3.2935\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8393 - mae: 3.2400 - val_loss: 21.3386 - val_mae: 3.2767\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8047 - mae: 3.2401 - val_loss: 21.2553 - val_mae: 3.2948\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8860 - mae: 3.2604 - val_loss: 21.1872 - val_mae: 3.2946\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6294 - mae: 3.2398 - val_loss: 21.1862 - val_mae: 3.2746\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0852 - mae: 3.2782 - val_loss: 21.0528 - val_mae: 3.3394\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.0507 - mae: 3.2952 - val_loss: 21.4271 - val_mae: 3.2331\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8124 - mae: 3.2473 - val_loss: 20.9031 - val_mae: 3.3285\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6055 - mae: 3.2315 - val_loss: 21.2118 - val_mae: 3.2341\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9352 - mae: 3.2650 - val_loss: 20.8902 - val_mae: 3.2578\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7692 - mae: 3.2347 - val_loss: 20.8017 - val_mae: 3.3672\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4382 - mae: 3.2522 - val_loss: 20.7522 - val_mae: 3.2613\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3359 - mae: 3.2215 - val_loss: 20.7034 - val_mae: 3.2576\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3030 - mae: 3.2126 - val_loss: 20.5555 - val_mae: 3.2803\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4152 - mae: 3.2136 - val_loss: 20.8257 - val_mae: 3.2260\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2797 - mae: 3.2223 - val_loss: 20.7476 - val_mae: 3.2290\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5005 - mae: 3.1989 - val_loss: 20.5810 - val_mae: 3.3781\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1271 - mae: 3.2326 - val_loss: 20.6289 - val_mae: 3.2328\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1965 - mae: 3.2321 - val_loss: 20.7199 - val_mae: 3.2142\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1446 - mae: 3.1883 - val_loss: 20.3343 - val_mae: 3.2842\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1171 - mae: 3.2110 - val_loss: 20.3182 - val_mae: 3.3292\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7977 - mae: 3.2218 - val_loss: 20.7464 - val_mae: 3.2059\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1624 - mae: 3.2545 - val_loss: 20.6756 - val_mae: 3.2012\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 20.6672 - mae: 3.1755 - val_loss: 20.2908 - val_mae: 3.2172\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6538 - mae: 3.2052 - val_loss: 20.2961 - val_mae: 3.2252\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5958 - mae: 3.1716 - val_loss: 20.2030 - val_mae: 3.2379\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6985 - mae: 3.2192 - val_loss: 20.0799 - val_mae: 3.2543\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.2500 - mae: 3.2184 - val_loss: 20.5836 - val_mae: 3.2026\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5360 - mae: 3.1791 - val_loss: 20.0890 - val_mae: 3.2564\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7812 - mae: 3.1894 - val_loss: 20.0899 - val_mae: 3.2371\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.4833 - mae: 3.2193 - val_loss: 20.2632 - val_mae: 3.2028\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8750 - mae: 3.2086 - val_loss: 20.0346 - val_mae: 3.2303\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.4172 - mae: 3.1958 - val_loss: 19.9639 - val_mae: 3.2563\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.3432 - mae: 3.2130 - val_loss: 19.9537 - val_mae: 3.2696\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.2497 - mae: 3.1813 - val_loss: 20.0329 - val_mae: 3.2200\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4512 - mae: 3.2000 - val_loss: 19.9756 - val_mae: 3.2361\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5298 - mae: 3.2066 - val_loss: 20.0956 - val_mae: 3.1967\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5019 - mae: 3.1866 - val_loss: 19.9878 - val_mae: 3.2206\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.3425 - mae: 3.2092 - val_loss: 20.3057 - val_mae: 3.1892\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 26.4754 - mae: 3.4871\n",
      "Mean Absolute Error on Test Data: 3.487083673477173\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0739759702243763\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 16ms/step - loss: 7.9518 - mae: 1.8234 - val_loss: 2.5055 - val_mae: 0.9806\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.9435 - mae: 1.3193 - val_loss: 1.9234 - val_mae: 1.1687\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.6091 - mae: 1.4771 - val_loss: 1.8984 - val_mae: 1.1547\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5436 - mae: 1.3616 - val_loss: 1.7396 - val_mae: 1.0251\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4968 - mae: 1.3288 - val_loss: 1.7925 - val_mae: 1.0886\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.5184 - mae: 1.3896 - val_loss: 1.7953 - val_mae: 1.0944\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4149 - mae: 1.3684 - val_loss: 1.8179 - val_mae: 1.1090\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4357 - mae: 1.3954 - val_loss: 1.8247 - val_mae: 1.1154\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4318 - mae: 1.4046 - val_loss: 1.8216 - val_mae: 1.1140\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3532 - mae: 1.3366 - val_loss: 1.7765 - val_mae: 1.0850\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4380 - mae: 1.3806 - val_loss: 1.7986 - val_mae: 1.1045\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3963 - mae: 1.3905 - val_loss: 1.8121 - val_mae: 1.1123\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3277 - mae: 1.3379 - val_loss: 1.7437 - val_mae: 1.0619\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.4527 - mae: 1.3362 - val_loss: 1.8445 - val_mae: 1.1369\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2827 - mae: 1.3499 - val_loss: 1.8152 - val_mae: 1.1195\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2381 - mae: 1.3507 - val_loss: 1.8688 - val_mae: 1.1520\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3333 - mae: 1.3846 - val_loss: 1.8179 - val_mae: 1.1192\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3571 - mae: 1.3286 - val_loss: 1.8040 - val_mae: 1.1121\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3070 - mae: 1.3838 - val_loss: 1.8367 - val_mae: 1.1314\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3364 - mae: 1.3363 - val_loss: 1.7941 - val_mae: 1.1075\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2080 - mae: 1.3613 - val_loss: 1.8524 - val_mae: 1.1474\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2531 - mae: 1.3680 - val_loss: 1.7651 - val_mae: 1.0789\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2557 - mae: 1.3684 - val_loss: 1.8215 - val_mae: 1.1205\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1706 - mae: 1.3443 - val_loss: 1.8254 - val_mae: 1.1201\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 4.2244 - mae: 1.3115 - val_loss: 1.8363 - val_mae: 1.1246\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.3283 - mae: 1.4370 - val_loss: 1.8630 - val_mae: 1.1463\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.3259 - mae: 1.3056 - val_loss: 1.8047 - val_mae: 1.1093\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.3217 - mae: 1.3859 - val_loss: 1.9040 - val_mae: 1.1692\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2320 - mae: 1.3294 - val_loss: 1.7920 - val_mae: 1.0975\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2022 - mae: 1.3238 - val_loss: 1.9069 - val_mae: 1.1658\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1714 - mae: 1.3422 - val_loss: 1.8345 - val_mae: 1.1226\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1246 - mae: 1.3219 - val_loss: 1.8878 - val_mae: 1.1542\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.2280 - mae: 1.3284 - val_loss: 1.9076 - val_mae: 1.1708\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2408 - mae: 1.3732 - val_loss: 1.8731 - val_mae: 1.1457\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1664 - mae: 1.3032 - val_loss: 1.8440 - val_mae: 1.1271\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2348 - mae: 1.4089 - val_loss: 1.9410 - val_mae: 1.1769\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2243 - mae: 1.3811 - val_loss: 1.8230 - val_mae: 1.1116\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2041 - mae: 1.3003 - val_loss: 1.8590 - val_mae: 1.1330\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1696 - mae: 1.3575 - val_loss: 1.8970 - val_mae: 1.1545\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2330 - mae: 1.3437 - val_loss: 1.8395 - val_mae: 1.1161\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1053 - mae: 1.3082 - val_loss: 1.8497 - val_mae: 1.1284\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.2011 - mae: 1.3933 - val_loss: 1.9613 - val_mae: 1.1808\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1611 - mae: 1.3288 - val_loss: 1.8331 - val_mae: 1.1083\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1400 - mae: 1.3161 - val_loss: 1.8895 - val_mae: 1.1445\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1774 - mae: 1.3866 - val_loss: 2.0492 - val_mae: 1.2213\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1445 - mae: 1.3302 - val_loss: 1.8699 - val_mae: 1.1301\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.0789 - mae: 1.3630 - val_loss: 1.8974 - val_mae: 1.1433\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.1565 - mae: 1.3430 - val_loss: 1.8477 - val_mae: 1.1148\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 4.0950 - mae: 1.3222 - val_loss: 1.9394 - val_mae: 1.1637\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.1022 - mae: 1.3449 - val_loss: 1.9828 - val_mae: 1.1826\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 5.4554 - mae: 1.4514\n",
      "Mean Absolute Error on Test Data: 1.4513576030731201\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.18748567601657296\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 104.1677 - mae: 7.7586 - val_loss: 69.5879 - val_mae: 5.7048\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.8826 - mae: 4.6123 - val_loss: 42.9701 - val_mae: 4.6844\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.1713 - mae: 4.5708 - val_loss: 42.0830 - val_mae: 4.4749\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0417 - mae: 4.3014 - val_loss: 41.8587 - val_mae: 4.4819\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2164 - mae: 4.5005 - val_loss: 41.8842 - val_mae: 4.4330\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4992 - mae: 4.3744 - val_loss: 41.5708 - val_mae: 4.4579\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8946 - mae: 4.2876 - val_loss: 41.3743 - val_mae: 4.4645\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0697 - mae: 4.3441 - val_loss: 41.2430 - val_mae: 4.4580\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9410 - mae: 4.3416 - val_loss: 41.1639 - val_mae: 4.4473\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8216 - mae: 4.2964 - val_loss: 41.0488 - val_mae: 4.4497\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9914 - mae: 4.4104 - val_loss: 41.0905 - val_mae: 4.4083\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2172 - mae: 4.2280 - val_loss: 41.0506 - val_mae: 4.5382\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4135 - mae: 4.3956 - val_loss: 41.4959 - val_mae: 4.3639\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3853 - mae: 4.1738 - val_loss: 40.7596 - val_mae: 4.5123\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1820 - mae: 4.3032 - val_loss: 40.5655 - val_mae: 4.4186\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4534 - mae: 4.3853 - val_loss: 40.5664 - val_mae: 4.3858\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2873 - mae: 4.2918 - val_loss: 40.4590 - val_mae: 4.3837\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7211 - mae: 4.3079 - val_loss: 40.4557 - val_mae: 4.4606\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0478 - mae: 4.2553 - val_loss: 40.3492 - val_mae: 4.3888\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5672 - mae: 4.2557 - val_loss: 40.6817 - val_mae: 4.5292\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0930 - mae: 4.2628 - val_loss: 40.4030 - val_mae: 4.3476\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.4179 - mae: 4.3491 - val_loss: 40.9735 - val_mae: 4.2972\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.9545 - mae: 4.2130 - val_loss: 40.1103 - val_mae: 4.3736\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.9677 - mae: 4.2176 - val_loss: 40.2529 - val_mae: 4.3353\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.2831 - mae: 4.2354 - val_loss: 40.5082 - val_mae: 4.2915\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.7916 - mae: 4.2233 - val_loss: 40.2360 - val_mae: 4.4293\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.6445 - mae: 4.3791 - val_loss: 40.3198 - val_mae: 4.2992\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.4444 - mae: 4.1694 - val_loss: 40.1755 - val_mae: 4.3964\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.1029 - mae: 4.2626 - val_loss: 40.0490 - val_mae: 4.3400\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.4186 - mae: 4.1601 - val_loss: 40.1372 - val_mae: 4.3971\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.7023 - mae: 4.2501 - val_loss: 40.4930 - val_mae: 4.2697\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 38.9055 - mae: 4.2095 - val_loss: 40.0375 - val_mae: 4.3559\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.5296 - mae: 4.1959 - val_loss: 40.0460 - val_mae: 4.3606\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.7888 - mae: 4.3466 - val_loss: 40.3824 - val_mae: 4.2812\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.6463 - mae: 4.1457 - val_loss: 40.1369 - val_mae: 4.3309\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.5640 - mae: 4.2417 - val_loss: 40.3479 - val_mae: 4.2776\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.0495 - mae: 4.3394 - val_loss: 40.2314 - val_mae: 4.3017\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.7120 - mae: 4.1345 - val_loss: 40.5431 - val_mae: 4.4689\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.3051 - mae: 4.2737 - val_loss: 40.1058 - val_mae: 4.2949\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.5642 - mae: 4.1761 - val_loss: 39.9817 - val_mae: 4.3430\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.7297 - mae: 4.2207 - val_loss: 39.9691 - val_mae: 4.3598\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.7875 - mae: 4.1848 - val_loss: 40.0326 - val_mae: 4.3485\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.1862 - mae: 4.2626 - val_loss: 39.9837 - val_mae: 4.3197\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.0680 - mae: 4.1382 - val_loss: 40.0134 - val_mae: 4.3257\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 39.4142 - mae: 4.2626 - val_loss: 40.2775 - val_mae: 4.2819\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.6483 - mae: 4.1784 - val_loss: 40.0041 - val_mae: 4.3269\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.1569 - mae: 4.1755 - val_loss: 39.9482 - val_mae: 4.3295\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.9824 - mae: 4.2011 - val_loss: 40.1469 - val_mae: 4.3059\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.8807 - mae: 4.1578 - val_loss: 40.1287 - val_mae: 4.3470\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.1687 - mae: 4.1520 - val_loss: 40.1227 - val_mae: 4.3553\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 34.9253 - mae: 4.3620\n",
      "Mean Absolute Error on Test Data: 4.3620076179504395\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.008761664389632773\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 60.2901 - mae: 5.9122 - val_loss: 39.7812 - val_mae: 4.2916\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.8348 - mae: 3.5319 - val_loss: 23.9474 - val_mae: 3.6470\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.3227 - mae: 3.4798 - val_loss: 23.9676 - val_mae: 3.4224\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1827 - mae: 3.4118 - val_loss: 23.5300 - val_mae: 3.4421\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1110 - mae: 3.3134 - val_loss: 23.7444 - val_mae: 3.4140\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.4616 - mae: 3.3621 - val_loss: 23.3726 - val_mae: 3.4593\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.2655 - mae: 3.3670 - val_loss: 23.9886 - val_mae: 3.3782\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.6126 - mae: 3.3272 - val_loss: 23.3772 - val_mae: 3.4100\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.2548 - mae: 3.3118 - val_loss: 23.1901 - val_mae: 3.4776\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.4238 - mae: 3.4136 - val_loss: 24.3334 - val_mae: 3.3615\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1123 - mae: 3.2393 - val_loss: 23.1691 - val_mae: 3.4369\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1826 - mae: 3.3617 - val_loss: 23.8395 - val_mae: 3.3569\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0630 - mae: 3.2746 - val_loss: 23.2350 - val_mae: 3.4470\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.2595 - mae: 3.3492 - val_loss: 23.5085 - val_mae: 3.3826\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8446 - mae: 3.2284 - val_loss: 23.1432 - val_mae: 3.4208\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.1523 - mae: 3.3349 - val_loss: 23.5596 - val_mae: 3.3595\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8166 - mae: 3.2820 - val_loss: 23.4938 - val_mae: 3.3705\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6642 - mae: 3.2850 - val_loss: 23.2931 - val_mae: 3.3977\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.5379 - mae: 3.2186 - val_loss: 23.2472 - val_mae: 3.4829\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4579 - mae: 3.3780 - val_loss: 24.5501 - val_mae: 3.3420\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2649 - mae: 3.2038 - val_loss: 23.1287 - val_mae: 3.5299\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6829 - mae: 3.3495 - val_loss: 23.9285 - val_mae: 3.3530\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9112 - mae: 3.3028 - val_loss: 23.4675 - val_mae: 3.3697\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8920 - mae: 3.2635 - val_loss: 23.3879 - val_mae: 3.4800\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8758 - mae: 3.2798 - val_loss: 23.5745 - val_mae: 3.3788\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7230 - mae: 3.1529 - val_loss: 23.2173 - val_mae: 3.5009\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6995 - mae: 3.3627 - val_loss: 23.9174 - val_mae: 3.3767\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6184 - mae: 3.2930 - val_loss: 23.4685 - val_mae: 3.3887\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0629 - mae: 3.2902 - val_loss: 23.5279 - val_mae: 3.4473\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8008 - mae: 3.2173 - val_loss: 23.4280 - val_mae: 3.4875\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5556 - mae: 3.2508 - val_loss: 23.6505 - val_mae: 3.4072\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6567 - mae: 3.2351 - val_loss: 23.5157 - val_mae: 3.5167\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7009 - mae: 3.2896 - val_loss: 24.0367 - val_mae: 3.3946\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6278 - mae: 3.2719 - val_loss: 24.2106 - val_mae: 3.4302\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6059 - mae: 3.2307 - val_loss: 23.7590 - val_mae: 3.4649\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5589 - mae: 3.2148 - val_loss: 23.7218 - val_mae: 3.4546\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5794 - mae: 3.2448 - val_loss: 23.8120 - val_mae: 3.4405\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9729 - mae: 3.2303 - val_loss: 24.0274 - val_mae: 3.4844\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 23.2472 - mae: 3.2438 - val_loss: 24.4791 - val_mae: 3.4041\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.3086 - mae: 3.1994 - val_loss: 23.8469 - val_mae: 3.4530\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3179 - mae: 3.1945 - val_loss: 23.9408 - val_mae: 3.4800\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2877 - mae: 3.2866 - val_loss: 23.7724 - val_mae: 3.5322\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9626 - mae: 3.2412 - val_loss: 24.1935 - val_mae: 3.4373\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6511 - mae: 3.1547 - val_loss: 23.8376 - val_mae: 3.5598\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1004 - mae: 3.2284 - val_loss: 23.7547 - val_mae: 3.5265\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.0350 - mae: 3.2807 - val_loss: 24.0382 - val_mae: 3.4887\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5227 - mae: 3.2337 - val_loss: 23.9399 - val_mae: 3.5540\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1230 - mae: 3.1958 - val_loss: 23.7571 - val_mae: 3.5028\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7035 - mae: 3.2870 - val_loss: 24.7811 - val_mae: 3.4446\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2350 - mae: 3.2513 - val_loss: 24.8732 - val_mae: 3.5143\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.0706 - mae: 3.2166\n",
      "Mean Absolute Error on Test Data: 3.2166152000427246\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.024118589037594673\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 197.6220 - mae: 12.1414 - val_loss: 149.8805 - val_mae: 10.1048\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 125.6312 - mae: 8.8657 - val_loss: 68.3526 - val_mae: 6.0133\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 58.1767 - mae: 5.6795 - val_loss: 57.2441 - val_mae: 5.9956\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.3773 - mae: 5.4518 - val_loss: 52.3219 - val_mae: 5.5454\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.4141 - mae: 5.3739 - val_loss: 52.6781 - val_mae: 5.5902\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.6043 - mae: 5.3054 - val_loss: 52.0318 - val_mae: 5.5298\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3298 - mae: 5.3899 - val_loss: 52.0324 - val_mae: 5.5367\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.9091 - mae: 5.3450 - val_loss: 51.6686 - val_mae: 5.5022\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.2713 - mae: 5.3464 - val_loss: 51.4253 - val_mae: 5.4909\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3412 - mae: 5.3930 - val_loss: 51.6588 - val_mae: 5.5278\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.2002 - mae: 5.3882 - val_loss: 50.5830 - val_mae: 5.4054\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.8147 - mae: 5.2979 - val_loss: 51.5771 - val_mae: 5.5336\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.5523 - mae: 5.2780 - val_loss: 50.3995 - val_mae: 5.4141\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8696 - mae: 5.2486 - val_loss: 50.9995 - val_mae: 5.5008\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.4410 - mae: 5.3262 - val_loss: 49.9627 - val_mae: 5.3807\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8437 - mae: 5.3168 - val_loss: 50.6922 - val_mae: 5.4828\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5978 - mae: 5.3316 - val_loss: 49.3615 - val_mae: 5.3233\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.0741 - mae: 5.2851 - val_loss: 49.3157 - val_mae: 5.3404\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.4820 - mae: 5.2830 - val_loss: 49.1950 - val_mae: 5.3419\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.1987 - mae: 5.2273 - val_loss: 49.4806 - val_mae: 5.4019\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.7135 - mae: 5.2160 - val_loss: 49.1273 - val_mae: 5.3767\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.4776 - mae: 5.2382 - val_loss: 48.9330 - val_mae: 5.3590\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.7193 - mae: 5.2416 - val_loss: 48.4731 - val_mae: 5.3203\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.0617 - mae: 5.2024 - val_loss: 48.8082 - val_mae: 5.3735\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.5918 - mae: 5.2242 - val_loss: 48.9496 - val_mae: 5.3987\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.4551 - mae: 5.2503 - val_loss: 47.3323 - val_mae: 5.2290\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.2023 - mae: 5.1935 - val_loss: 47.4789 - val_mae: 5.2659\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.0028 - mae: 5.1456 - val_loss: 47.9029 - val_mae: 5.3331\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.0246 - mae: 5.2723 - val_loss: 47.0476 - val_mae: 5.2516\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.5581 - mae: 5.1871 - val_loss: 46.9138 - val_mae: 5.2479\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.8261 - mae: 5.1439 - val_loss: 46.9462 - val_mae: 5.2676\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.8221 - mae: 5.1618 - val_loss: 46.4208 - val_mae: 5.2302\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.4627 - mae: 5.1751 - val_loss: 45.9461 - val_mae: 5.1464\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.9421 - mae: 5.1682 - val_loss: 46.1769 - val_mae: 5.2078\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 47.4526 - mae: 5.1744 - val_loss: 46.2990 - val_mae: 5.2499\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.4238 - mae: 5.1644 - val_loss: 45.8933 - val_mae: 5.2203\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.2018 - mae: 5.1602 - val_loss: 45.9137 - val_mae: 5.2188\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.3435 - mae: 5.1849 - val_loss: 45.2809 - val_mae: 5.1663\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.7231 - mae: 5.1357 - val_loss: 45.1113 - val_mae: 5.1145\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 47.0343 - mae: 5.1796 - val_loss: 45.2987 - val_mae: 5.1863\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.7312 - mae: 5.0745 - val_loss: 45.6577 - val_mae: 5.2433\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.0480 - mae: 5.1457 - val_loss: 45.4828 - val_mae: 5.2261\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.3436 - mae: 5.1678 - val_loss: 44.5437 - val_mae: 5.0893\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 45.9855 - mae: 5.0997 - val_loss: 44.7717 - val_mae: 5.1719\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.0352 - mae: 5.1407 - val_loss: 44.1932 - val_mae: 5.0876\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.2095 - mae: 5.1673 - val_loss: 44.3068 - val_mae: 5.1359\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.7250 - mae: 5.0553 - val_loss: 44.5813 - val_mae: 5.1789\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.9233 - mae: 5.1667 - val_loss: 43.5687 - val_mae: 5.0105\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.4017 - mae: 5.1332 - val_loss: 43.7487 - val_mae: 5.0706\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.3293 - mae: 5.1098 - val_loss: 44.6014 - val_mae: 5.1860\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 40.9075 - mae: 5.0572\n",
      "Mean Absolute Error on Test Data: 5.057236194610596\n",
      "8/8 [==============================] - 0s 930us/step\n",
      "R-squared: 0.04031996032410112\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 25.3292 - mae: 3.8776 - val_loss: 14.3813 - val_mae: 2.7601\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 12.8469 - mae: 2.5192 - val_loss: 9.1245 - val_mae: 2.4662\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.5331 - mae: 2.4965 - val_loss: 8.5626 - val_mae: 2.3325\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1975 - mae: 2.4032 - val_loss: 8.6323 - val_mae: 2.3604\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.0511 - mae: 2.3552 - val_loss: 8.5367 - val_mae: 2.3318\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1084 - mae: 2.3475 - val_loss: 8.6172 - val_mae: 2.3553\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1323 - mae: 2.3862 - val_loss: 8.7236 - val_mae: 2.3759\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1936 - mae: 2.3451 - val_loss: 8.5372 - val_mae: 2.3319\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0800 - mae: 2.3749 - val_loss: 8.6522 - val_mae: 2.3622\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1288 - mae: 2.3544 - val_loss: 8.5381 - val_mae: 2.3316\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0354 - mae: 2.3564 - val_loss: 8.5944 - val_mae: 2.3462\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.1006 - mae: 2.3656 - val_loss: 8.6793 - val_mae: 2.3704\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0516 - mae: 2.3589 - val_loss: 8.3947 - val_mae: 2.2861\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0779 - mae: 2.3690 - val_loss: 8.4628 - val_mae: 2.3077\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8357 - mae: 2.3396 - val_loss: 8.5438 - val_mae: 2.3360\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9066 - mae: 2.3668 - val_loss: 8.4240 - val_mae: 2.2969\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.0994 - mae: 2.3339 - val_loss: 8.6431 - val_mae: 2.3667\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8995 - mae: 2.3542 - val_loss: 8.3744 - val_mae: 2.2805\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9272 - mae: 2.3134 - val_loss: 8.7496 - val_mae: 2.3895\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7776 - mae: 2.3422 - val_loss: 8.3737 - val_mae: 2.2725\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8402 - mae: 2.3001 - val_loss: 8.5937 - val_mae: 2.3547\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.9481 - mae: 2.3703 - val_loss: 8.4430 - val_mae: 2.2975\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8367 - mae: 2.3016 - val_loss: 8.5968 - val_mae: 2.3497\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.8772 - mae: 2.3628 - val_loss: 8.5487 - val_mae: 2.3325\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7984 - mae: 2.3220 - val_loss: 8.4075 - val_mae: 2.2802\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8177 - mae: 2.3212 - val_loss: 8.6441 - val_mae: 2.3511\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7720 - mae: 2.3420 - val_loss: 8.3472 - val_mae: 2.2668\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7823 - mae: 2.3014 - val_loss: 8.5423 - val_mae: 2.3309\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7049 - mae: 2.3124 - val_loss: 8.4676 - val_mae: 2.3086\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6929 - mae: 2.3253 - val_loss: 8.5113 - val_mae: 2.3182\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5665 - mae: 2.2958 - val_loss: 8.5001 - val_mae: 2.3195\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7437 - mae: 2.3136 - val_loss: 8.4917 - val_mae: 2.2951\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7464 - mae: 2.2942 - val_loss: 8.7594 - val_mae: 2.3810\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8036 - mae: 2.3265 - val_loss: 8.5883 - val_mae: 2.3271\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.8515 - mae: 2.3435 - val_loss: 8.3982 - val_mae: 2.2645\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7616 - mae: 2.3086 - val_loss: 8.7279 - val_mae: 2.3709\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6077 - mae: 2.3048 - val_loss: 8.4161 - val_mae: 2.2706\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6003 - mae: 2.2873 - val_loss: 8.5579 - val_mae: 2.3115\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5876 - mae: 2.3099 - val_loss: 8.6605 - val_mae: 2.3327\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.4807 - mae: 2.2792 - val_loss: 8.5665 - val_mae: 2.3121\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 9.7108 - mae: 2.2784 - val_loss: 8.9401 - val_mae: 2.4178\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6272 - mae: 2.3253 - val_loss: 8.3152 - val_mae: 2.2369\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7294 - mae: 2.3567 - val_loss: 8.4029 - val_mae: 2.2511\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.7042 - mae: 2.3014 - val_loss: 8.6395 - val_mae: 2.3382\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6904 - mae: 2.2898 - val_loss: 9.0081 - val_mae: 2.4136\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6819 - mae: 2.3195 - val_loss: 8.7317 - val_mae: 2.3496\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6023 - mae: 2.2906 - val_loss: 8.3923 - val_mae: 2.2487\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.6397 - mae: 2.3009 - val_loss: 8.6417 - val_mae: 2.3327\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.4894 - mae: 2.2923 - val_loss: 8.4284 - val_mae: 2.2606\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 9.5509 - mae: 2.2852 - val_loss: 8.4535 - val_mae: 2.2843\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.0150 - mae: 2.2594\n",
      "Mean Absolute Error on Test Data: 2.2594070434570312\n",
      "7/7 [==============================] - 0s 919us/step\n",
      "R-squared: 0.031123625625421147\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 37.1318 - mae: 4.9678 - val_loss: 28.2152 - val_mae: 3.9097\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.7965 - mae: 2.9695 - val_loss: 13.8974 - val_mae: 2.7609\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.6314 - mae: 2.7482 - val_loss: 13.9463 - val_mae: 2.6922\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.6003 - mae: 2.6369 - val_loss: 13.8549 - val_mae: 2.6875\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2201 - mae: 2.6412 - val_loss: 13.6846 - val_mae: 2.6977\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.3212 - mae: 2.6640 - val_loss: 13.7472 - val_mae: 2.6713\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0944 - mae: 2.5996 - val_loss: 13.5449 - val_mae: 2.6924\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2011 - mae: 2.6080 - val_loss: 13.5140 - val_mae: 2.6903\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2175 - mae: 2.6392 - val_loss: 13.6025 - val_mae: 2.6603\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2111 - mae: 2.6087 - val_loss: 13.4119 - val_mae: 2.6867\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.2837 - mae: 2.6340 - val_loss: 13.3721 - val_mae: 2.6832\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0880 - mae: 2.6213 - val_loss: 13.4543 - val_mae: 2.6533\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0475 - mae: 2.6468 - val_loss: 13.5965 - val_mae: 2.6459\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.1491 - mae: 2.6023 - val_loss: 13.2691 - val_mae: 2.6955\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7031 - mae: 2.6174 - val_loss: 13.3410 - val_mae: 2.6499\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8880 - mae: 2.6003 - val_loss: 13.1897 - val_mae: 2.6838\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 12.1043 - mae: 2.6371 - val_loss: 13.2401 - val_mae: 2.6550\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0309 - mae: 2.6316 - val_loss: 13.2787 - val_mae: 2.6549\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.1480 - mae: 2.6492 - val_loss: 13.3951 - val_mae: 2.6370\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.1263 - mae: 2.6029 - val_loss: 13.2078 - val_mae: 2.6803\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9903 - mae: 2.6041 - val_loss: 13.2835 - val_mae: 2.6421\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 12.0103 - mae: 2.6267 - val_loss: 13.2592 - val_mae: 2.6453\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9400 - mae: 2.6095 - val_loss: 13.1605 - val_mae: 2.6658\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9100 - mae: 2.6283 - val_loss: 13.4776 - val_mae: 2.6288\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8366 - mae: 2.6046 - val_loss: 13.3048 - val_mae: 2.6361\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 11.7458 - mae: 2.5557 - val_loss: 13.2143 - val_mae: 2.6459\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 11.7831 - mae: 2.6159 - val_loss: 13.1810 - val_mae: 2.6428\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9800 - mae: 2.6194 - val_loss: 13.1904 - val_mae: 2.6448\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9094 - mae: 2.6019 - val_loss: 13.0475 - val_mae: 2.6663\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8282 - mae: 2.6195 - val_loss: 13.3475 - val_mae: 2.6234\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6718 - mae: 2.5951 - val_loss: 13.0643 - val_mae: 2.6690\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7603 - mae: 2.6228 - val_loss: 13.2626 - val_mae: 2.6247\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8433 - mae: 2.6059 - val_loss: 13.1342 - val_mae: 2.6302\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7313 - mae: 2.5975 - val_loss: 13.0193 - val_mae: 2.6730\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6101 - mae: 2.5814 - val_loss: 13.1456 - val_mae: 2.6311\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.9749 - mae: 2.6159 - val_loss: 13.0154 - val_mae: 2.6452\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7828 - mae: 2.6036 - val_loss: 13.0262 - val_mae: 2.6527\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.5583 - mae: 2.5853 - val_loss: 13.1210 - val_mae: 2.6262\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8859 - mae: 2.6256 - val_loss: 13.1254 - val_mae: 2.6242\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 11.7365 - mae: 2.6024 - val_loss: 13.1050 - val_mae: 2.6335\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.5878 - mae: 2.5911 - val_loss: 13.0175 - val_mae: 2.6379\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.8242 - mae: 2.6127 - val_loss: 13.0060 - val_mae: 2.6342\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7141 - mae: 2.6090 - val_loss: 13.1610 - val_mae: 2.6184\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6136 - mae: 2.5954 - val_loss: 12.9418 - val_mae: 2.6371\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.6301 - mae: 2.6112 - val_loss: 13.5282 - val_mae: 2.6142\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7699 - mae: 2.5809 - val_loss: 12.9190 - val_mae: 2.6534\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7726 - mae: 2.6096 - val_loss: 13.1044 - val_mae: 2.6176\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7242 - mae: 2.5847 - val_loss: 13.0238 - val_mae: 2.6340\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.7031 - mae: 2.5957 - val_loss: 12.9643 - val_mae: 2.6611\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 11.5544 - mae: 2.5850 - val_loss: 12.9414 - val_mae: 2.6323\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 17.8610 - mae: 2.9735\n",
      "Mean Absolute Error on Test Data: 2.9735429286956787\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12073884164536086\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 16.9347 - mae: 3.1087 - val_loss: 10.8700 - val_mae: 2.4160\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.4570 - mae: 2.2222 - val_loss: 9.9526 - val_mae: 2.4040\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1613 - mae: 2.1221 - val_loss: 9.9794 - val_mae: 2.3877\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.2171 - mae: 2.1441 - val_loss: 9.9724 - val_mae: 2.3828\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0688 - mae: 2.1291 - val_loss: 9.9135 - val_mae: 2.3921\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1417 - mae: 2.1417 - val_loss: 9.8866 - val_mae: 2.3829\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0611 - mae: 2.1328 - val_loss: 9.9751 - val_mae: 2.3681\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1283 - mae: 2.1240 - val_loss: 9.8853 - val_mae: 2.3756\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0588 - mae: 2.1186 - val_loss: 10.0126 - val_mae: 2.3627\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9490 - mae: 2.1354 - val_loss: 9.8409 - val_mae: 2.3899\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1659 - mae: 2.1419 - val_loss: 9.8611 - val_mae: 2.3734\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1004 - mae: 2.1445 - val_loss: 9.8600 - val_mae: 2.3736\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9099 - mae: 2.1283 - val_loss: 9.8821 - val_mae: 2.3637\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8234 - mae: 2.1112 - val_loss: 9.8324 - val_mae: 2.3659\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9228 - mae: 2.1054 - val_loss: 9.8267 - val_mae: 2.3593\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9609 - mae: 2.0993 - val_loss: 9.8388 - val_mae: 2.4160\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9741 - mae: 2.1325 - val_loss: 10.1137 - val_mae: 2.3441\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0958 - mae: 2.1444 - val_loss: 9.8131 - val_mae: 2.3656\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9855 - mae: 2.1122 - val_loss: 9.9192 - val_mae: 2.3496\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.1774 - mae: 2.1301 - val_loss: 10.0629 - val_mae: 2.3456\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0504 - mae: 2.1198 - val_loss: 9.8012 - val_mae: 2.3740\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0820 - mae: 2.1269 - val_loss: 9.8280 - val_mae: 2.3678\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9849 - mae: 2.1376 - val_loss: 9.8163 - val_mae: 2.3664\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9593 - mae: 2.1162 - val_loss: 9.7963 - val_mae: 2.3773\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8885 - mae: 2.1090 - val_loss: 9.9947 - val_mae: 2.3413\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8469 - mae: 2.1221 - val_loss: 9.8886 - val_mae: 2.3465\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9778 - mae: 2.1144 - val_loss: 9.9378 - val_mae: 2.3400\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8971 - mae: 2.1073 - val_loss: 9.8499 - val_mae: 2.3493\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9087 - mae: 2.0933 - val_loss: 9.7967 - val_mae: 2.3609\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0228 - mae: 2.1464 - val_loss: 9.9476 - val_mae: 2.3484\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 8.0273 - mae: 2.1220 - val_loss: 9.8686 - val_mae: 2.3496\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8989 - mae: 2.1072 - val_loss: 9.8534 - val_mae: 2.3500\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9074 - mae: 2.0959 - val_loss: 9.9520 - val_mae: 2.3415\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8852 - mae: 2.0962 - val_loss: 9.7771 - val_mae: 2.3980\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7763 - mae: 2.0924 - val_loss: 9.8985 - val_mae: 2.3391\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7942 - mae: 2.1035 - val_loss: 9.7972 - val_mae: 2.3666\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9886 - mae: 2.1325 - val_loss: 9.8077 - val_mae: 2.3664\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8816 - mae: 2.1038 - val_loss: 9.7533 - val_mae: 2.3847\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8851 - mae: 2.1119 - val_loss: 9.9777 - val_mae: 2.3353\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8651 - mae: 2.1186 - val_loss: 9.9197 - val_mae: 2.3462\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8776 - mae: 2.1088 - val_loss: 9.7917 - val_mae: 2.3643\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.6943 - mae: 2.0927 - val_loss: 9.8100 - val_mae: 2.3526\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9375 - mae: 2.1077 - val_loss: 9.8196 - val_mae: 2.3965\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8439 - mae: 2.0982 - val_loss: 9.8764 - val_mae: 2.3497\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8035 - mae: 2.1071 - val_loss: 10.0317 - val_mae: 2.3437\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.9013 - mae: 2.1229 - val_loss: 9.8596 - val_mae: 2.3497\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8414 - mae: 2.0864 - val_loss: 9.8362 - val_mae: 2.3683\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7697 - mae: 2.1123 - val_loss: 9.8298 - val_mae: 2.3453\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.7140 - mae: 2.0721 - val_loss: 10.0126 - val_mae: 2.3393\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 7.8552 - mae: 2.1167 - val_loss: 10.1751 - val_mae: 2.3447\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 10.6107 - mae: 2.2864\n",
      "Mean Absolute Error on Test Data: 2.2864158153533936\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.026840755639001346\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 13ms/step - loss: 6.2874 - mae: 1.6748 - val_loss: 4.4119 - val_mae: 1.5311\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.6527 - mae: 1.6011 - val_loss: 4.4120 - val_mae: 1.5326\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.4867 - mae: 1.4994 - val_loss: 4.4091 - val_mae: 1.5303\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4824 - mae: 1.4974 - val_loss: 4.3937 - val_mae: 1.5327\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4724 - mae: 1.4989 - val_loss: 4.3688 - val_mae: 1.5379\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4713 - mae: 1.5239 - val_loss: 4.4383 - val_mae: 1.5224\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4137 - mae: 1.4761 - val_loss: 4.3590 - val_mae: 1.5348\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4765 - mae: 1.5163 - val_loss: 4.3965 - val_mae: 1.5261\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.5091 - mae: 1.4905 - val_loss: 4.3708 - val_mae: 1.5286\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4691 - mae: 1.4875 - val_loss: 4.3241 - val_mae: 1.5472\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4836 - mae: 1.5440 - val_loss: 4.4845 - val_mae: 1.5097\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4218 - mae: 1.4829 - val_loss: 4.3367 - val_mae: 1.5279\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4442 - mae: 1.5474 - val_loss: 4.4809 - val_mae: 1.5085\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.4391 - mae: 1.4580 - val_loss: 4.2998 - val_mae: 1.5381\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.5027 - mae: 1.5414 - val_loss: 4.5640 - val_mae: 1.5006\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4155 - mae: 1.4567 - val_loss: 4.2696 - val_mae: 1.5434\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3642 - mae: 1.4994 - val_loss: 4.3632 - val_mae: 1.5162\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4241 - mae: 1.4941 - val_loss: 4.3248 - val_mae: 1.5205\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3171 - mae: 1.4868 - val_loss: 4.3320 - val_mae: 1.5150\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3408 - mae: 1.4912 - val_loss: 4.2573 - val_mae: 1.5341\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3760 - mae: 1.5072 - val_loss: 4.3593 - val_mae: 1.5085\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2850 - mae: 1.4709 - val_loss: 4.3492 - val_mae: 1.5066\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.4015 - mae: 1.4860 - val_loss: 4.3186 - val_mae: 1.5087\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2876 - mae: 1.4905 - val_loss: 4.2942 - val_mae: 1.5145\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3379 - mae: 1.4803 - val_loss: 4.2966 - val_mae: 1.5122\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2805 - mae: 1.4953 - val_loss: 4.3613 - val_mae: 1.5027\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3797 - mae: 1.4723 - val_loss: 4.2799 - val_mae: 1.5218\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3766 - mae: 1.4745 - val_loss: 4.2794 - val_mae: 1.5184\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2941 - mae: 1.5039 - val_loss: 4.3747 - val_mae: 1.5011\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 4.3865 - mae: 1.4698 - val_loss: 4.2570 - val_mae: 1.5257\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2543 - mae: 1.4867 - val_loss: 4.4362 - val_mae: 1.4954\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3503 - mae: 1.4728 - val_loss: 4.2629 - val_mae: 1.5191\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3661 - mae: 1.4983 - val_loss: 4.2435 - val_mae: 1.5335\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3145 - mae: 1.4625 - val_loss: 4.2707 - val_mae: 1.5182\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3623 - mae: 1.5537 - val_loss: 4.4939 - val_mae: 1.4975\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3295 - mae: 1.4801 - val_loss: 4.2928 - val_mae: 1.5166\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2382 - mae: 1.4505 - val_loss: 4.2704 - val_mae: 1.5181\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2529 - mae: 1.4782 - val_loss: 4.3084 - val_mae: 1.5108\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2078 - mae: 1.4688 - val_loss: 4.3167 - val_mae: 1.5103\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2504 - mae: 1.5217 - val_loss: 4.5344 - val_mae: 1.4949\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2113 - mae: 1.4469 - val_loss: 4.3170 - val_mae: 1.5077\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.1786 - mae: 1.4811 - val_loss: 4.4121 - val_mae: 1.5011\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.3736 - mae: 1.4348 - val_loss: 4.2849 - val_mae: 1.5331\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2075 - mae: 1.4951 - val_loss: 4.4004 - val_mae: 1.5085\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 4.2178 - mae: 1.4832 - val_loss: 4.4015 - val_mae: 1.5039\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2172 - mae: 1.4525 - val_loss: 4.2936 - val_mae: 1.5246\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2279 - mae: 1.5025 - val_loss: 4.5951 - val_mae: 1.5057\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2353 - mae: 1.4617 - val_loss: 4.2850 - val_mae: 1.5223\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2017 - mae: 1.4718 - val_loss: 4.4449 - val_mae: 1.4998\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.2453 - mae: 1.4641 - val_loss: 4.3460 - val_mae: 1.5148\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.2348 - mae: 1.3990\n",
      "Mean Absolute Error on Test Data: 1.3990219831466675\n",
      "6/6 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.03901862289945601\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 3s 146ms/step - loss: 5.5578 - mae: 1.5945 - val_loss: 3.4978 - val_mae: 1.3253\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.8276 - mae: 1.4736 - val_loss: 3.4336 - val_mae: 1.3676\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.8083 - mae: 1.4043 - val_loss: 3.4575 - val_mae: 1.3126\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.6915 - mae: 1.4002 - val_loss: 3.4205 - val_mae: 1.3290\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.7458 - mae: 1.3944 - val_loss: 3.4499 - val_mae: 1.3024\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6622 - mae: 1.3844 - val_loss: 3.3988 - val_mae: 1.3383\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6439 - mae: 1.4001 - val_loss: 3.4722 - val_mae: 1.2833\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6619 - mae: 1.3652 - val_loss: 3.3846 - val_mae: 1.3596\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.6392 - mae: 1.4139 - val_loss: 3.3980 - val_mae: 1.3050\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5801 - mae: 1.3768 - val_loss: 3.4085 - val_mae: 1.2920\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5861 - mae: 1.3442 - val_loss: 3.3957 - val_mae: 1.3026\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5376 - mae: 1.3804 - val_loss: 3.3855 - val_mae: 1.3119\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5893 - mae: 1.3883 - val_loss: 3.3863 - val_mae: 1.3094\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5738 - mae: 1.3567 - val_loss: 3.3906 - val_mae: 1.3095\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5628 - mae: 1.3862 - val_loss: 3.3798 - val_mae: 1.3034\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5599 - mae: 1.3485 - val_loss: 3.3626 - val_mae: 1.3141\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4959 - mae: 1.3362 - val_loss: 3.3850 - val_mae: 1.3019\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4684 - mae: 1.3501 - val_loss: 3.3841 - val_mae: 1.3132\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5085 - mae: 1.3587 - val_loss: 3.4243 - val_mae: 1.2863\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3.5478 - mae: 1.3799 - val_loss: 3.3875 - val_mae: 1.3026\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5375 - mae: 1.3486 - val_loss: 3.3863 - val_mae: 1.3026\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5639 - mae: 1.4119 - val_loss: 3.4271 - val_mae: 1.2735\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5224 - mae: 1.3615 - val_loss: 3.3523 - val_mae: 1.3203\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4994 - mae: 1.3507 - val_loss: 3.3585 - val_mae: 1.3277\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4785 - mae: 1.3471 - val_loss: 3.3721 - val_mae: 1.3155\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5183 - mae: 1.3852 - val_loss: 3.4419 - val_mae: 1.2701\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.4578 - mae: 1.3374 - val_loss: 3.3820 - val_mae: 1.3574\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.5633 - mae: 1.3948 - val_loss: 3.3868 - val_mae: 1.3062\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4754 - mae: 1.3250 - val_loss: 3.3607 - val_mae: 1.3308\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4315 - mae: 1.3625 - val_loss: 3.3821 - val_mae: 1.2969\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3730 - mae: 1.3219 - val_loss: 3.3605 - val_mae: 1.3278\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3.4277 - mae: 1.3472 - val_loss: 3.4103 - val_mae: 1.2760\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 3.4313 - mae: 1.3281 - val_loss: 3.3621 - val_mae: 1.3434\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4734 - mae: 1.3661 - val_loss: 3.3846 - val_mae: 1.2717\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4303 - mae: 1.3891 - val_loss: 3.3876 - val_mae: 1.2670\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4481 - mae: 1.3355 - val_loss: 3.3286 - val_mae: 1.3152\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.4248 - mae: 1.3527 - val_loss: 3.3589 - val_mae: 1.2885\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 3.4346 - mae: 1.3140 - val_loss: 3.3247 - val_mae: 1.3440\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.4271 - mae: 1.3632 - val_loss: 3.3383 - val_mae: 1.2949\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 3.3809 - mae: 1.3593 - val_loss: 3.3665 - val_mae: 1.2914\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3961 - mae: 1.3221 - val_loss: 3.3132 - val_mae: 1.3126\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3202 - mae: 1.3541 - val_loss: 3.3463 - val_mae: 1.2845\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3543 - mae: 1.3142 - val_loss: 3.3660 - val_mae: 1.3741\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 6ms/step - loss: 3.4258 - mae: 1.3981 - val_loss: 3.4587 - val_mae: 1.2516\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3289 - mae: 1.3178 - val_loss: 3.3068 - val_mae: 1.3456\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3541 - mae: 1.3461 - val_loss: 3.2948 - val_mae: 1.3115\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4326 - mae: 1.3901 - val_loss: 3.4812 - val_mae: 1.2528\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.4006 - mae: 1.3093 - val_loss: 3.3024 - val_mae: 1.3143\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3057 - mae: 1.3268 - val_loss: 3.2905 - val_mae: 1.3318\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 3.3269 - mae: 1.3649 - val_loss: 3.4486 - val_mae: 1.2534\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.3587 - mae: 1.4722\n",
      "Mean Absolute Error on Test Data: 1.4721693992614746\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.006577780457294624\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 35.2953 - mae: 4.3948 - val_loss: 36.2535 - val_mae: 3.4799\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.0239 - mae: 3.2176 - val_loss: 31.1722 - val_mae: 3.6786\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.8230 - mae: 3.1765 - val_loss: 30.0049 - val_mae: 3.4174\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.7531 - mae: 3.1277 - val_loss: 29.6056 - val_mae: 3.3612\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.3916 - mae: 3.0913 - val_loss: 28.9989 - val_mae: 3.3909\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 17.0581 - mae: 3.0511 - val_loss: 28.6551 - val_mae: 3.3427\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.9434 - mae: 3.0844 - val_loss: 28.4926 - val_mae: 3.3638\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8938 - mae: 3.0371 - val_loss: 28.4778 - val_mae: 3.3631\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7568 - mae: 3.0488 - val_loss: 28.2431 - val_mae: 3.3365\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.7436 - mae: 3.0577 - val_loss: 28.2010 - val_mae: 3.3014\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.4486 - mae: 3.0296 - val_loss: 28.3985 - val_mae: 3.2933\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.6445 - mae: 3.0043 - val_loss: 28.1540 - val_mae: 3.3554\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3997 - mae: 2.9942 - val_loss: 27.9893 - val_mae: 3.3278\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.5592 - mae: 3.0027 - val_loss: 27.9816 - val_mae: 3.3988\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3038 - mae: 3.0506 - val_loss: 28.1889 - val_mae: 3.2624\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1860 - mae: 2.9963 - val_loss: 28.2059 - val_mae: 3.3124\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.2560 - mae: 2.9803 - val_loss: 28.1505 - val_mae: 3.3199\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 16.2718 - mae: 2.9608 - val_loss: 28.1264 - val_mae: 3.3576\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9850 - mae: 2.9794 - val_loss: 28.4855 - val_mae: 3.2355\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3949 - mae: 2.9820 - val_loss: 28.0066 - val_mae: 3.3066\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0926 - mae: 2.9554 - val_loss: 28.2199 - val_mae: 3.3381\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0275 - mae: 3.0007 - val_loss: 28.2305 - val_mae: 3.3073\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9895 - mae: 2.9738 - val_loss: 27.9579 - val_mae: 3.3217\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9809 - mae: 2.9683 - val_loss: 28.2114 - val_mae: 3.3744\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3617 - mae: 2.9659 - val_loss: 28.2132 - val_mae: 3.4028\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1288 - mae: 2.9829 - val_loss: 28.7010 - val_mae: 3.2199\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0851 - mae: 2.9861 - val_loss: 28.7352 - val_mae: 3.2504\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3069 - mae: 2.9870 - val_loss: 28.1367 - val_mae: 3.3779\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.1923 - mae: 2.9707 - val_loss: 28.2091 - val_mae: 3.3446\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0297 - mae: 3.0127 - val_loss: 28.6630 - val_mae: 3.2142\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8051 - mae: 2.9333 - val_loss: 28.1157 - val_mae: 3.3383\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9126 - mae: 2.9613 - val_loss: 28.1257 - val_mae: 3.3584\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.9005 - mae: 2.9477 - val_loss: 28.1871 - val_mae: 3.3127\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8380 - mae: 3.0060 - val_loss: 28.2418 - val_mae: 3.2991\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7955 - mae: 2.9244 - val_loss: 28.2509 - val_mae: 3.3548\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.6320 - mae: 2.9434 - val_loss: 28.2443 - val_mae: 3.2782\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6149 - mae: 2.9352 - val_loss: 28.4027 - val_mae: 3.3013\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7440 - mae: 2.9716 - val_loss: 28.5016 - val_mae: 3.2956\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7569 - mae: 2.8984 - val_loss: 28.3530 - val_mae: 3.4234\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.3133 - mae: 3.0159 - val_loss: 28.4452 - val_mae: 3.3057\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7129 - mae: 2.9730 - val_loss: 28.7325 - val_mae: 3.2365\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7806 - mae: 2.9333 - val_loss: 28.1581 - val_mae: 3.3705\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6385 - mae: 2.9462 - val_loss: 28.4591 - val_mae: 3.3021\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6778 - mae: 2.9713 - val_loss: 28.5619 - val_mae: 3.2416\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.7383 - mae: 2.9403 - val_loss: 28.3510 - val_mae: 3.3070\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5833 - mae: 2.9144 - val_loss: 28.2366 - val_mae: 3.2892\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6944 - mae: 2.9374 - val_loss: 28.1555 - val_mae: 3.3269\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3145 - mae: 2.9293 - val_loss: 28.3029 - val_mae: 3.2969\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2257 - mae: 2.9195 - val_loss: 28.3022 - val_mae: 3.2806\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.4492 - mae: 2.9368 - val_loss: 28.6482 - val_mae: 3.2248\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.7392 - mae: 3.2277\n",
      "Mean Absolute Error on Test Data: 3.2277374267578125\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0905689424779802\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 40.8907 - mae: 4.1994 - val_loss: 26.9336 - val_mae: 3.1873\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 26.2622 - mae: 3.0755 - val_loss: 19.5377 - val_mae: 3.1870\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 23.3177 - mae: 3.2268 - val_loss: 18.6084 - val_mae: 2.9055\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.9838 - mae: 2.9770 - val_loss: 18.1394 - val_mae: 2.9436\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.5522 - mae: 3.0712 - val_loss: 17.8499 - val_mae: 2.9696\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.4170 - mae: 2.9844 - val_loss: 17.4374 - val_mae: 2.9130\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 22.2113 - mae: 3.0674 - val_loss: 17.0787 - val_mae: 2.8297\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 22.0857 - mae: 2.9068 - val_loss: 16.8383 - val_mae: 2.8631\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.7440 - mae: 3.0516 - val_loss: 16.5422 - val_mae: 2.7612\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.7943 - mae: 2.9271 - val_loss: 16.4471 - val_mae: 2.8462\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.6188 - mae: 2.9929 - val_loss: 16.2504 - val_mae: 2.8355\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.7213 - mae: 2.9689 - val_loss: 16.2489 - val_mae: 2.8648\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.8151 - mae: 2.9358 - val_loss: 16.0545 - val_mae: 2.7762\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.6804 - mae: 3.0362 - val_loss: 15.9191 - val_mae: 2.7461\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.5588 - mae: 2.8904 - val_loss: 15.8935 - val_mae: 2.7959\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3830 - mae: 2.9128 - val_loss: 16.0283 - val_mae: 2.8914\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2137 - mae: 2.9630 - val_loss: 15.8921 - val_mae: 2.8520\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2344 - mae: 2.9600 - val_loss: 15.7078 - val_mae: 2.7106\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.1808 - mae: 2.8878 - val_loss: 15.6647 - val_mae: 2.7601\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3682 - mae: 2.9291 - val_loss: 15.6464 - val_mae: 2.8029\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2102 - mae: 2.9619 - val_loss: 15.6057 - val_mae: 2.7193\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9125 - mae: 2.8714 - val_loss: 15.6288 - val_mae: 2.8288\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0888 - mae: 2.9725 - val_loss: 15.5121 - val_mae: 2.7203\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.3821 - mae: 2.8948 - val_loss: 15.5897 - val_mae: 2.8470\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2742 - mae: 2.9385 - val_loss: 15.4391 - val_mae: 2.7716\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.0061 - mae: 2.8747 - val_loss: 15.4831 - val_mae: 2.7944\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 21.2573 - mae: 2.9651 - val_loss: 15.3226 - val_mae: 2.7101\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7024 - mae: 2.9212 - val_loss: 15.3651 - val_mae: 2.7571\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6874 - mae: 2.8435 - val_loss: 15.5670 - val_mae: 2.8886\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 21.0117 - mae: 2.9153 - val_loss: 15.3554 - val_mae: 2.7531\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9607 - mae: 2.9492 - val_loss: 15.3337 - val_mae: 2.7660\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9602 - mae: 2.8307 - val_loss: 15.6258 - val_mae: 2.9068\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8742 - mae: 2.9535 - val_loss: 15.2655 - val_mae: 2.7382\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.9479 - mae: 2.8567 - val_loss: 15.6567 - val_mae: 2.9180\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8675 - mae: 2.9609 - val_loss: 15.2436 - val_mae: 2.7449\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6390 - mae: 2.8613 - val_loss: 15.3370 - val_mae: 2.7956\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6392 - mae: 2.9534 - val_loss: 15.2163 - val_mae: 2.6907\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.8427 - mae: 2.8622 - val_loss: 15.1867 - val_mae: 2.7319\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.6673 - mae: 2.9178 - val_loss: 15.3009 - val_mae: 2.7313\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.7223 - mae: 2.8384 - val_loss: 15.6151 - val_mae: 2.8998\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5698 - mae: 2.9271 - val_loss: 15.4090 - val_mae: 2.8081\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4901 - mae: 2.9035 - val_loss: 15.3215 - val_mae: 2.7140\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5410 - mae: 2.8536 - val_loss: 15.7309 - val_mae: 2.9377\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4646 - mae: 2.8846 - val_loss: 15.2656 - val_mae: 2.7269\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.3953 - mae: 2.8725 - val_loss: 15.3384 - val_mae: 2.7559\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5436 - mae: 2.9192 - val_loss: 15.3100 - val_mae: 2.6754\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.4729 - mae: 2.8616 - val_loss: 15.3340 - val_mae: 2.7945\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.2182 - mae: 2.8370 - val_loss: 15.2746 - val_mae: 2.7917\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 20.0477 - mae: 2.8378 - val_loss: 15.3182 - val_mae: 2.6918\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 20.5005 - mae: 2.8932 - val_loss: 15.2736 - val_mae: 2.6834\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 21.1686 - mae: 3.1558\n",
      "Mean Absolute Error on Test Data: 3.15582013130188\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04170844086082848\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 12ms/step - loss: 37.7746 - mae: 4.7756 - val_loss: 50.5422 - val_mae: 4.6328\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 19.6437 - mae: 3.1381 - val_loss: 30.1656 - val_mae: 3.4650\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.5036 - mae: 2.9929 - val_loss: 30.9195 - val_mae: 3.3703\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2131 - mae: 2.9146 - val_loss: 30.2958 - val_mae: 3.3999\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.0996 - mae: 2.8861 - val_loss: 30.2674 - val_mae: 3.3854\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1898 - mae: 2.9287 - val_loss: 30.2505 - val_mae: 3.3703\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1054 - mae: 2.8526 - val_loss: 30.0958 - val_mae: 3.3690\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7856 - mae: 2.8991 - val_loss: 30.0718 - val_mae: 3.3679\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6348 - mae: 2.8276 - val_loss: 29.9638 - val_mae: 3.3733\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7411 - mae: 2.8409 - val_loss: 29.6935 - val_mae: 3.3833\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6859 - mae: 2.8703 - val_loss: 30.1599 - val_mae: 3.3350\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6527 - mae: 2.8194 - val_loss: 29.9328 - val_mae: 3.3510\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3688 - mae: 2.8542 - val_loss: 30.2421 - val_mae: 3.3269\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6427 - mae: 2.8472 - val_loss: 29.7362 - val_mae: 3.3411\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3129 - mae: 2.8168 - val_loss: 29.9948 - val_mae: 3.3160\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3891 - mae: 2.7901 - val_loss: 29.4585 - val_mae: 3.3727\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3541 - mae: 2.8487 - val_loss: 30.3242 - val_mae: 3.2994\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2197 - mae: 2.8249 - val_loss: 29.2861 - val_mae: 3.3506\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.3086 - mae: 2.8140 - val_loss: 29.5502 - val_mae: 3.3092\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0882 - mae: 2.7700 - val_loss: 29.5690 - val_mae: 3.3210\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0058 - mae: 2.7748 - val_loss: 29.3950 - val_mae: 3.3276\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1957 - mae: 2.7838 - val_loss: 29.6406 - val_mae: 3.3268\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2288 - mae: 2.8035 - val_loss: 29.2274 - val_mae: 3.3680\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9444 - mae: 2.7754 - val_loss: 29.8661 - val_mae: 3.2960\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0439 - mae: 2.8024 - val_loss: 29.8967 - val_mae: 3.3046\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0114 - mae: 2.7590 - val_loss: 29.4003 - val_mae: 3.3373\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0592 - mae: 2.7821 - val_loss: 29.4139 - val_mae: 3.3354\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0411 - mae: 2.7583 - val_loss: 29.4997 - val_mae: 3.3290\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.9504 - mae: 2.7429 - val_loss: 28.7557 - val_mae: 3.4514\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8041 - mae: 2.8326 - val_loss: 30.2745 - val_mae: 3.2892\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.1533 - mae: 2.7879 - val_loss: 29.7680 - val_mae: 3.3041\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7370 - mae: 2.7356 - val_loss: 29.5459 - val_mae: 3.3300\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8305 - mae: 2.7514 - val_loss: 29.5785 - val_mae: 3.3443\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.0162 - mae: 2.7752 - val_loss: 30.0804 - val_mae: 3.3046\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6850 - mae: 2.7824 - val_loss: 29.7088 - val_mae: 3.3178\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8937 - mae: 2.7445 - val_loss: 29.6490 - val_mae: 3.3269\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6929 - mae: 2.7504 - val_loss: 29.6959 - val_mae: 3.3224\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.4847 - mae: 2.7237 - val_loss: 29.0000 - val_mae: 3.3615\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8973 - mae: 2.7664 - val_loss: 28.7054 - val_mae: 3.4335\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8043 - mae: 2.8082 - val_loss: 31.7330 - val_mae: 3.3182\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6786 - mae: 2.7360 - val_loss: 28.6888 - val_mae: 3.4127\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8175 - mae: 2.7242 - val_loss: 28.9452 - val_mae: 3.3783\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 13.5433 - mae: 2.7652 - val_loss: 29.5356 - val_mae: 3.3182\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8121 - mae: 2.7425 - val_loss: 29.2071 - val_mae: 3.3409\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.7559 - mae: 2.7393 - val_loss: 29.3788 - val_mae: 3.3299\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6862 - mae: 2.7273 - val_loss: 29.2791 - val_mae: 3.3974\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5652 - mae: 2.7504 - val_loss: 29.4309 - val_mae: 3.3391\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.5710 - mae: 2.7411 - val_loss: 29.2771 - val_mae: 3.3673\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.6708 - mae: 2.7778 - val_loss: 30.3972 - val_mae: 3.3189\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 13.8235 - mae: 2.7407 - val_loss: 28.6702 - val_mae: 3.4530\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 12.1868 - mae: 2.8334\n",
      "Mean Absolute Error on Test Data: 2.8333568572998047\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.021064540029812617\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 64.2879 - mae: 6.1513 - val_loss: 35.5346 - val_mae: 4.2436\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.6348 - mae: 3.8298 - val_loss: 20.3648 - val_mae: 3.5384\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.0866 - mae: 3.6048 - val_loss: 18.9617 - val_mae: 3.1651\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.8387 - mae: 3.4881 - val_loss: 19.0001 - val_mae: 3.1568\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.6297 - mae: 3.5590 - val_loss: 19.0762 - val_mae: 3.1570\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.7529 - mae: 3.5415 - val_loss: 19.1214 - val_mae: 3.1450\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4140 - mae: 3.4917 - val_loss: 19.2072 - val_mae: 3.1865\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.4039 - mae: 3.4804 - val_loss: 19.2955 - val_mae: 3.2257\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1677 - mae: 3.4531 - val_loss: 19.2322 - val_mae: 3.1851\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.2591 - mae: 3.4757 - val_loss: 19.0566 - val_mae: 3.0989\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0908 - mae: 3.4884 - val_loss: 19.2570 - val_mae: 3.2055\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 24.0639 - mae: 3.4571 - val_loss: 19.1892 - val_mae: 3.1619\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1144 - mae: 3.4814 - val_loss: 19.2383 - val_mae: 3.1629\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.5312 - mae: 3.4416 - val_loss: 19.2733 - val_mae: 3.2035\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0496 - mae: 3.4968 - val_loss: 19.5302 - val_mae: 3.2474\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9704 - mae: 3.4157 - val_loss: 19.1595 - val_mae: 3.1216\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1809 - mae: 3.6143 - val_loss: 19.2935 - val_mae: 3.0896\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.3045 - mae: 3.4474 - val_loss: 19.5736 - val_mae: 3.2457\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.1998 - mae: 3.4350 - val_loss: 19.4558 - val_mae: 3.2451\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9405 - mae: 3.4646 - val_loss: 19.1888 - val_mae: 3.1310\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0039 - mae: 3.4312 - val_loss: 19.7036 - val_mae: 3.2818\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.8720 - mae: 3.5103 - val_loss: 19.2976 - val_mae: 3.0765\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0380 - mae: 3.4277 - val_loss: 19.8236 - val_mae: 3.3197\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8251 - mae: 3.5066 - val_loss: 19.5040 - val_mae: 3.2060\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 24.0472 - mae: 3.4721 - val_loss: 19.3592 - val_mae: 3.1363\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6374 - mae: 3.4237 - val_loss: 19.5005 - val_mae: 3.2164\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9559 - mae: 3.5029 - val_loss: 19.3182 - val_mae: 3.1108\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7538 - mae: 3.4962 - val_loss: 19.4020 - val_mae: 3.1392\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6108 - mae: 3.3920 - val_loss: 19.7525 - val_mae: 3.2767\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.9065 - mae: 3.5351 - val_loss: 19.4401 - val_mae: 3.1068\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6785 - mae: 3.4295 - val_loss: 19.3740 - val_mae: 3.1751\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6101 - mae: 3.4168 - val_loss: 19.6711 - val_mae: 3.2431\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8801 - mae: 3.5151 - val_loss: 19.3890 - val_mae: 3.1057\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6546 - mae: 3.4228 - val_loss: 19.3507 - val_mae: 3.1700\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8664 - mae: 3.5067 - val_loss: 19.5459 - val_mae: 3.1499\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7139 - mae: 3.4856 - val_loss: 19.7084 - val_mae: 3.1343\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5063 - mae: 3.4089 - val_loss: 19.7689 - val_mae: 3.2492\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3482 - mae: 3.4389 - val_loss: 19.6034 - val_mae: 3.2284\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1982 - mae: 3.4768 - val_loss: 19.5407 - val_mae: 3.0940\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.8033 - mae: 3.4485 - val_loss: 20.0152 - val_mae: 3.3479\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4661 - mae: 3.3736 - val_loss: 19.9575 - val_mae: 3.2967\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4936 - mae: 3.4988 - val_loss: 19.5867 - val_mae: 3.1522\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3293 - mae: 3.4252 - val_loss: 19.7121 - val_mae: 3.2421\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 23.2066 - mae: 3.4092 - val_loss: 19.6334 - val_mae: 3.2190\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4309 - mae: 3.4920 - val_loss: 19.8241 - val_mae: 3.2172\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5548 - mae: 3.4467 - val_loss: 19.4522 - val_mae: 3.1883\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1715 - mae: 3.4181 - val_loss: 19.7156 - val_mae: 3.2231\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1972 - mae: 3.4384 - val_loss: 19.6511 - val_mae: 3.1868\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2436 - mae: 3.4666 - val_loss: 19.6179 - val_mae: 3.1723\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6459 - mae: 3.4132 - val_loss: 19.6221 - val_mae: 3.1926\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 18.2829 - mae: 3.3126\n",
      "Mean Absolute Error on Test Data: 3.3126022815704346\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08503276804582094\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 13ms/step - loss: 5.5132 - mae: 1.6851 - val_loss: 3.0013 - val_mae: 1.2336\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.2493 - mae: 1.4009 - val_loss: 2.9397 - val_mae: 1.3181\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0350 - mae: 1.2990 - val_loss: 2.8757 - val_mae: 1.2571\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0612 - mae: 1.3041 - val_loss: 2.8654 - val_mae: 1.2728\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0584 - mae: 1.3204 - val_loss: 2.8620 - val_mae: 1.2752\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0169 - mae: 1.2978 - val_loss: 2.8509 - val_mae: 1.2681\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9790 - mae: 1.2912 - val_loss: 2.8421 - val_mae: 1.2570\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.0587 - mae: 1.3082 - val_loss: 2.8381 - val_mae: 1.2649\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9828 - mae: 1.2930 - val_loss: 2.8334 - val_mae: 1.2637\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9378 - mae: 1.3001 - val_loss: 2.8358 - val_mae: 1.2678\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9801 - mae: 1.2914 - val_loss: 2.8129 - val_mae: 1.2451\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 3.0279 - mae: 1.3202 - val_loss: 2.8059 - val_mae: 1.2359\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9780 - mae: 1.2947 - val_loss: 2.7971 - val_mae: 1.2308\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8988 - mae: 1.2764 - val_loss: 2.8150 - val_mae: 1.2655\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9512 - mae: 1.2859 - val_loss: 2.7931 - val_mae: 1.2306\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9036 - mae: 1.2934 - val_loss: 2.8018 - val_mae: 1.2546\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9341 - mae: 1.2882 - val_loss: 2.7836 - val_mae: 1.2312\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9286 - mae: 1.2736 - val_loss: 2.8091 - val_mae: 1.2660\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9827 - mae: 1.3117 - val_loss: 2.7844 - val_mae: 1.2419\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9468 - mae: 1.2851 - val_loss: 2.8041 - val_mae: 1.2606\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9497 - mae: 1.2914 - val_loss: 2.7686 - val_mae: 1.2272\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8956 - mae: 1.2679 - val_loss: 2.7785 - val_mae: 1.2452\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.8742 - mae: 1.2941 - val_loss: 2.7661 - val_mae: 1.2145\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9046 - mae: 1.2531 - val_loss: 2.7795 - val_mae: 1.2536\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9006 - mae: 1.2822 - val_loss: 2.7564 - val_mae: 1.2203\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8083 - mae: 1.2592 - val_loss: 2.7580 - val_mae: 1.2339\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.8493 - mae: 1.2561 - val_loss: 2.8526 - val_mae: 1.2993\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.9201 - mae: 1.3123 - val_loss: 2.7732 - val_mae: 1.2040\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8380 - mae: 1.2613 - val_loss: 2.7806 - val_mae: 1.2606\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.9043 - mae: 1.2726 - val_loss: 2.7872 - val_mae: 1.2602\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8621 - mae: 1.2775 - val_loss: 2.7442 - val_mae: 1.2172\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7985 - mae: 1.2474 - val_loss: 2.7635 - val_mae: 1.2430\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 2.8478 - mae: 1.2579 - val_loss: 2.7581 - val_mae: 1.2391\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8543 - mae: 1.2751 - val_loss: 2.7507 - val_mae: 1.2215\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8419 - mae: 1.2778 - val_loss: 2.7528 - val_mae: 1.2111\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8468 - mae: 1.2705 - val_loss: 2.7626 - val_mae: 1.2404\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8604 - mae: 1.2620 - val_loss: 2.8225 - val_mae: 1.2830\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8331 - mae: 1.2811 - val_loss: 2.7748 - val_mae: 1.2012\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7708 - mae: 1.2553 - val_loss: 2.7716 - val_mae: 1.2455\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8151 - mae: 1.2389 - val_loss: 2.7952 - val_mae: 1.2555\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7777 - mae: 1.2573 - val_loss: 2.7633 - val_mae: 1.2247\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7817 - mae: 1.2550 - val_loss: 2.7780 - val_mae: 1.2480\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8325 - mae: 1.2678 - val_loss: 2.7768 - val_mae: 1.2456\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 5ms/step - loss: 2.7800 - mae: 1.2573 - val_loss: 2.7489 - val_mae: 1.2296\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7722 - mae: 1.2488 - val_loss: 2.8092 - val_mae: 1.2781\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7166 - mae: 1.2602 - val_loss: 2.7479 - val_mae: 1.2103\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.8229 - mae: 1.2675 - val_loss: 2.7651 - val_mae: 1.2425\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7575 - mae: 1.2686 - val_loss: 2.7449 - val_mae: 1.2101\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7030 - mae: 1.2453 - val_loss: 2.7758 - val_mae: 1.2547\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 2.7669 - mae: 1.2503 - val_loss: 2.7855 - val_mae: 1.2579\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 2.5315 - mae: 1.2336\n",
      "Mean Absolute Error on Test Data: 1.2336108684539795\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.06396061736348413\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 58.5324 - mae: 4.8033 - val_loss: 49.5027 - val_mae: 4.3235\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 39.6000 - mae: 3.6563 - val_loss: 34.4186 - val_mae: 4.0795\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 35.1421 - mae: 4.0732 - val_loss: 33.9991 - val_mae: 3.9791\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 34.5622 - mae: 3.7213 - val_loss: 33.6832 - val_mae: 3.9199\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.8800 - mae: 3.8227 - val_loss: 33.1214 - val_mae: 3.8979\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.4688 - mae: 3.7901 - val_loss: 32.5929 - val_mae: 3.8948\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.2527 - mae: 3.8464 - val_loss: 32.7399 - val_mae: 3.7871\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 33.1245 - mae: 3.6873 - val_loss: 32.0716 - val_mae: 3.8092\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.7089 - mae: 3.6842 - val_loss: 31.8202 - val_mae: 3.7889\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 32.4033 - mae: 3.7419 - val_loss: 31.6502 - val_mae: 3.7399\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.3092 - mae: 3.7089 - val_loss: 31.2401 - val_mae: 3.7477\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 32.4643 - mae: 3.6587 - val_loss: 30.8324 - val_mae: 3.7965\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8927 - mae: 3.7478 - val_loss: 30.8860 - val_mae: 3.7389\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8945 - mae: 3.6216 - val_loss: 30.6777 - val_mae: 3.7350\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.7752 - mae: 3.6421 - val_loss: 30.3240 - val_mae: 3.7841\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8904 - mae: 3.7073 - val_loss: 30.2823 - val_mae: 3.7235\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.8702 - mae: 3.7777 - val_loss: 30.4509 - val_mae: 3.6752\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.7365 - mae: 3.5884 - val_loss: 29.9473 - val_mae: 3.7536\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 31.2620 - mae: 3.7409 - val_loss: 30.4485 - val_mae: 3.6563\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7440 - mae: 3.5893 - val_loss: 30.1693 - val_mae: 3.6737\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7985 - mae: 3.5834 - val_loss: 29.7693 - val_mae: 3.7112\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 31.1327 - mae: 3.6809 - val_loss: 30.1063 - val_mae: 3.6791\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.5032 - mae: 3.5138 - val_loss: 29.4902 - val_mae: 3.8051\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.7190 - mae: 3.6246 - val_loss: 29.7204 - val_mae: 3.6813\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6895 - mae: 3.6460 - val_loss: 29.7894 - val_mae: 3.6707\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.6710 - mae: 3.6183 - val_loss: 29.7433 - val_mae: 3.6753\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.5843 - mae: 3.6217 - val_loss: 29.6070 - val_mae: 3.6797\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.3629 - mae: 3.5529 - val_loss: 29.5726 - val_mae: 3.6654\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 30.6690 - mae: 3.5075 - val_loss: 29.3589 - val_mae: 3.7597\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.5045 - mae: 3.5870 - val_loss: 29.2193 - val_mae: 3.6897\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8463 - mae: 3.5972 - val_loss: 29.5104 - val_mae: 3.6617\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8101 - mae: 3.5654 - val_loss: 29.2306 - val_mae: 3.7386\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 30.1397 - mae: 3.5511 - val_loss: 29.3732 - val_mae: 3.8302\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8628 - mae: 3.5261 - val_loss: 29.2200 - val_mae: 3.7765\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.0949 - mae: 3.5079 - val_loss: 29.0071 - val_mae: 3.7706\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.7626 - mae: 3.6004 - val_loss: 29.3176 - val_mae: 3.6811\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.7925 - mae: 3.4425 - val_loss: 29.4756 - val_mae: 3.7959\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.8718 - mae: 3.6272 - val_loss: 29.2913 - val_mae: 3.7010\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.6494 - mae: 3.4938 - val_loss: 29.2644 - val_mae: 3.8956\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.0752 - mae: 3.4881 - val_loss: 29.2493 - val_mae: 3.7344\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.9941 - mae: 3.5503 - val_loss: 29.3117 - val_mae: 3.7785\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 29.3475 - mae: 3.5308 - val_loss: 29.4582 - val_mae: 3.7126\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.6411 - mae: 3.4621 - val_loss: 28.9390 - val_mae: 3.8167\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.3761 - mae: 3.5224 - val_loss: 29.3284 - val_mae: 3.7445\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.6469 - mae: 3.5926 - val_loss: 29.1971 - val_mae: 3.7660\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.9672 - mae: 3.4155 - val_loss: 29.3683 - val_mae: 3.9079\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 28.9852 - mae: 3.6679 - val_loss: 29.2830 - val_mae: 3.7126\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 29.1338 - mae: 3.4306 - val_loss: 29.2749 - val_mae: 3.9086\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.3731 - mae: 3.5105 - val_loss: 29.6658 - val_mae: 3.7456\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 29.4128 - mae: 3.5699 - val_loss: 29.5103 - val_mae: 3.7481\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 20.6071 - mae: 3.1508\n",
      "Mean Absolute Error on Test Data: 3.150815486907959\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.10016596968294389\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 146.9442 - mae: 8.4798 - val_loss: 67.5604 - val_mae: 5.9640\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 90.6376 - mae: 5.5492 - val_loss: 35.5381 - val_mae: 4.7239\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 74.1844 - mae: 5.6622 - val_loss: 34.9177 - val_mae: 4.6926\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 71.6587 - mae: 5.2223 - val_loss: 34.2147 - val_mae: 4.6251\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 69.8840 - mae: 5.2024 - val_loss: 34.3505 - val_mae: 4.6577\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 70.6656 - mae: 5.2184 - val_loss: 34.3126 - val_mae: 4.6436\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 68.6873 - mae: 5.2003 - val_loss: 33.2789 - val_mae: 4.4780\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 67.8429 - mae: 5.0987 - val_loss: 33.8569 - val_mae: 4.5560\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 67.1875 - mae: 5.1289 - val_loss: 33.4901 - val_mae: 4.5011\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.5580 - mae: 5.0039 - val_loss: 33.2939 - val_mae: 4.4763\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.5612 - mae: 5.0734 - val_loss: 33.6139 - val_mae: 4.5165\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 66.7257 - mae: 4.9163 - val_loss: 33.6263 - val_mae: 4.5109\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.2557 - mae: 5.0131 - val_loss: 33.6294 - val_mae: 4.5117\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.1794 - mae: 4.9549 - val_loss: 33.1064 - val_mae: 4.4396\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 65.5558 - mae: 4.9540 - val_loss: 33.6563 - val_mae: 4.5137\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.9934 - mae: 4.9862 - val_loss: 32.6928 - val_mae: 4.3636\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.6317 - mae: 4.9496 - val_loss: 32.9790 - val_mae: 4.4379\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.8799 - mae: 4.9102 - val_loss: 32.8611 - val_mae: 4.4090\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 64.7735 - mae: 4.9259 - val_loss: 32.6456 - val_mae: 4.4087\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.8514 - mae: 4.9413 - val_loss: 32.6088 - val_mae: 4.3807\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.6240 - mae: 4.9227 - val_loss: 33.1766 - val_mae: 4.4596\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.2719 - mae: 4.9219 - val_loss: 32.9072 - val_mae: 4.4339\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.6927 - mae: 4.8187 - val_loss: 32.8130 - val_mae: 4.4130\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.5825 - mae: 4.8831 - val_loss: 32.6504 - val_mae: 4.4121\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.0147 - mae: 4.8479 - val_loss: 32.0523 - val_mae: 4.3169\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.9309 - mae: 4.8297 - val_loss: 33.0673 - val_mae: 4.4571\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.9766 - mae: 4.8326 - val_loss: 32.5834 - val_mae: 4.3652\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.6324 - mae: 4.8710 - val_loss: 34.3434 - val_mae: 4.6030\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 63.3964 - mae: 4.8419 - val_loss: 32.7061 - val_mae: 4.3995\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.9165 - mae: 4.8363 - val_loss: 32.1740 - val_mae: 4.3324\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.1684 - mae: 4.7592 - val_loss: 33.2100 - val_mae: 4.4952\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 63.4670 - mae: 4.8706 - val_loss: 32.0438 - val_mae: 4.3014\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.4089 - mae: 4.7263 - val_loss: 32.4549 - val_mae: 4.3972\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.8950 - mae: 4.8898 - val_loss: 32.3539 - val_mae: 4.3262\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.3135 - mae: 4.8252 - val_loss: 32.8315 - val_mae: 4.4010\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.0729 - mae: 4.7689 - val_loss: 32.1672 - val_mae: 4.3276\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.1568 - mae: 4.8328 - val_loss: 32.4916 - val_mae: 4.3332\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.8915 - mae: 4.7972 - val_loss: 32.2733 - val_mae: 4.3513\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.3526 - mae: 4.7514 - val_loss: 32.8204 - val_mae: 4.4331\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.5856 - mae: 4.8409 - val_loss: 32.1503 - val_mae: 4.2691\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 62.3119 - mae: 4.7301 - val_loss: 34.0020 - val_mae: 4.5623\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.9801 - mae: 4.8257 - val_loss: 32.4940 - val_mae: 4.3956\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.4432 - mae: 4.7459 - val_loss: 33.1132 - val_mae: 4.4458\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.4086 - mae: 4.8183 - val_loss: 32.8673 - val_mae: 4.4442\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.5348 - mae: 4.7776 - val_loss: 32.3851 - val_mae: 4.3846\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.5416 - mae: 4.8003 - val_loss: 32.3122 - val_mae: 4.3435\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.0973 - mae: 4.7273 - val_loss: 32.3842 - val_mae: 4.3747\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.3697 - mae: 4.8320 - val_loss: 32.1319 - val_mae: 4.3137\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 60.8181 - mae: 4.7755 - val_loss: 33.3832 - val_mae: 4.4571\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 62.4175 - mae: 4.7125 - val_loss: 33.9038 - val_mae: 4.5641\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 58.7866 - mae: 5.3962\n",
      "Mean Absolute Error on Test Data: 5.396213531494141\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.18622152928133895\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 83.1985 - mae: 7.2969 - val_loss: 65.4045 - val_mae: 6.1832\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.6895 - mae: 4.5491 - val_loss: 28.0167 - val_mae: 3.9367\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.8365 - mae: 4.0910 - val_loss: 27.8696 - val_mae: 3.9248\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.6468 - mae: 3.8669 - val_loss: 27.7363 - val_mae: 3.9112\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.5241 - mae: 3.8703 - val_loss: 27.5562 - val_mae: 3.9144\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 28.0932 - mae: 3.8754 - val_loss: 27.5213 - val_mae: 3.8916\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.5877 - mae: 3.9196 - val_loss: 27.4970 - val_mae: 3.8727\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.2660 - mae: 3.8297 - val_loss: 27.2528 - val_mae: 3.8791\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0940 - mae: 3.8514 - val_loss: 27.0246 - val_mae: 3.8872\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.1900 - mae: 3.8323 - val_loss: 26.8950 - val_mae: 3.9121\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9980 - mae: 3.9002 - val_loss: 27.4307 - val_mae: 3.8453\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3488 - mae: 3.8813 - val_loss: 27.0794 - val_mae: 3.8511\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6781 - mae: 3.8223 - val_loss: 26.9040 - val_mae: 3.8594\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0854 - mae: 3.8968 - val_loss: 26.7807 - val_mae: 3.8596\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0342 - mae: 3.8484 - val_loss: 27.0719 - val_mae: 3.8295\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9700 - mae: 3.8318 - val_loss: 26.6574 - val_mae: 3.8536\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3556 - mae: 3.8950 - val_loss: 26.8568 - val_mae: 3.8277\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0165 - mae: 3.8719 - val_loss: 26.7292 - val_mae: 3.8396\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0807 - mae: 3.7906 - val_loss: 26.4839 - val_mae: 3.8681\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0684 - mae: 3.8622 - val_loss: 26.6121 - val_mae: 3.8262\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.2323 - mae: 3.9298 - val_loss: 27.6151 - val_mae: 3.8026\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9030 - mae: 3.8370 - val_loss: 26.4987 - val_mae: 3.8230\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0723 - mae: 3.8789 - val_loss: 26.8901 - val_mae: 3.7934\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.7232 - mae: 3.8217 - val_loss: 26.4061 - val_mae: 3.8254\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.8166 - mae: 3.8291 - val_loss: 26.8727 - val_mae: 3.7849\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9423 - mae: 3.8453 - val_loss: 26.2698 - val_mae: 3.8343\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6169 - mae: 3.8771 - val_loss: 27.0557 - val_mae: 3.7764\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9389 - mae: 3.8317 - val_loss: 26.2354 - val_mae: 3.8310\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4563 - mae: 3.7952 - val_loss: 26.7194 - val_mae: 3.7778\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5824 - mae: 3.8862 - val_loss: 26.7695 - val_mae: 3.7674\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4150 - mae: 3.8084 - val_loss: 26.5863 - val_mae: 3.7714\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5317 - mae: 3.8159 - val_loss: 26.1806 - val_mae: 3.8137\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4940 - mae: 3.8193 - val_loss: 26.4059 - val_mae: 3.7707\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2277 - mae: 3.8147 - val_loss: 26.2704 - val_mae: 3.7821\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4456 - mae: 3.7989 - val_loss: 26.1869 - val_mae: 3.7905\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4146 - mae: 3.8358 - val_loss: 26.9696 - val_mae: 3.7419\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9875 - mae: 3.7917 - val_loss: 26.0182 - val_mae: 3.8254\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5973 - mae: 3.8403 - val_loss: 26.0303 - val_mae: 3.8266\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2593 - mae: 3.8119 - val_loss: 26.3080 - val_mae: 3.7505\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9684 - mae: 3.8589 - val_loss: 26.3742 - val_mae: 3.7456\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0430 - mae: 3.7210 - val_loss: 25.9857 - val_mae: 3.8246\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1950 - mae: 3.8235 - val_loss: 26.2833 - val_mae: 3.7485\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 27.2193 - mae: 3.7931 - val_loss: 26.4428 - val_mae: 3.7412\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6672 - mae: 3.8140 - val_loss: 26.7579 - val_mae: 3.7251\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7592 - mae: 3.7624 - val_loss: 25.9782 - val_mae: 3.8181\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.3764 - mae: 3.8085 - val_loss: 26.5570 - val_mae: 3.7247\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8907 - mae: 3.8384 - val_loss: 26.3832 - val_mae: 3.7432\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2671 - mae: 3.8065 - val_loss: 26.2316 - val_mae: 3.7368\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7977 - mae: 3.7471 - val_loss: 25.9189 - val_mae: 3.8167\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7454 - mae: 3.8431 - val_loss: 26.5444 - val_mae: 3.7254\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 30.8530 - mae: 3.8735\n",
      "Mean Absolute Error on Test Data: 3.8735311031341553\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.06771671598103901\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 71.9968 - mae: 7.0453 - val_loss: 52.2123 - val_mae: 5.4886\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.7133 - mae: 4.4653 - val_loss: 22.2962 - val_mae: 3.2933\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.4925 - mae: 3.6273 - val_loss: 21.8432 - val_mae: 3.5061\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9241 - mae: 3.4739 - val_loss: 21.5536 - val_mae: 3.3719\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6915 - mae: 3.4896 - val_loss: 21.5167 - val_mae: 3.3946\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.6175 - mae: 3.4481 - val_loss: 21.5130 - val_mae: 3.4060\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5856 - mae: 3.5104 - val_loss: 21.4453 - val_mae: 3.3809\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.8554 - mae: 3.4506 - val_loss: 21.4339 - val_mae: 3.3751\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2518 - mae: 3.4143 - val_loss: 21.4175 - val_mae: 3.3789\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5806 - mae: 3.4623 - val_loss: 21.4272 - val_mae: 3.4076\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5626 - mae: 3.4951 - val_loss: 21.3074 - val_mae: 3.3007\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5201 - mae: 3.4212 - val_loss: 21.3403 - val_mae: 3.3998\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7513 - mae: 3.4792 - val_loss: 21.2417 - val_mae: 3.3446\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5781 - mae: 3.4721 - val_loss: 21.2601 - val_mae: 3.3728\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2481 - mae: 3.4668 - val_loss: 21.1917 - val_mae: 3.3265\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9236 - mae: 3.4790 - val_loss: 21.1768 - val_mae: 3.3128\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4785 - mae: 3.4410 - val_loss: 21.2612 - val_mae: 3.3859\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.5423 - mae: 3.4400 - val_loss: 21.1891 - val_mae: 3.3708\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2928 - mae: 3.4621 - val_loss: 21.2624 - val_mae: 3.2579\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9810 - mae: 3.3824 - val_loss: 21.3710 - val_mae: 3.4545\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2989 - mae: 3.4699 - val_loss: 21.1175 - val_mae: 3.2714\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4050 - mae: 3.4386 - val_loss: 21.2109 - val_mae: 3.4078\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2829 - mae: 3.4286 - val_loss: 21.0218 - val_mae: 3.3192\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8185 - mae: 3.4092 - val_loss: 21.0346 - val_mae: 3.3347\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8090 - mae: 3.4026 - val_loss: 20.9714 - val_mae: 3.3176\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2332 - mae: 3.4591 - val_loss: 20.8989 - val_mae: 3.3063\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5582 - mae: 3.3838 - val_loss: 22.1109 - val_mae: 3.6313\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2389 - mae: 3.4877 - val_loss: 21.0771 - val_mae: 3.2262\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4070 - mae: 3.4756 - val_loss: 21.0723 - val_mae: 3.3997\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8657 - mae: 3.4163 - val_loss: 20.9153 - val_mae: 3.3261\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8120 - mae: 3.3951 - val_loss: 20.8737 - val_mae: 3.2911\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6119 - mae: 3.4002 - val_loss: 20.8383 - val_mae: 3.3184\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5218 - mae: 3.3706 - val_loss: 20.9611 - val_mae: 3.3748\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0207 - mae: 3.4290 - val_loss: 20.9518 - val_mae: 3.3619\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.6464 - mae: 3.4220 - val_loss: 20.9585 - val_mae: 3.3709\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5507 - mae: 3.4101 - val_loss: 20.8268 - val_mae: 3.2541\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5920 - mae: 3.4096 - val_loss: 20.9322 - val_mae: 3.3754\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8242 - mae: 3.3853 - val_loss: 20.9673 - val_mae: 3.3774\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.4826 - mae: 3.3830 - val_loss: 20.7983 - val_mae: 3.3340\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1718 - mae: 3.4635 - val_loss: 20.7592 - val_mae: 3.2722\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6122 - mae: 3.4478 - val_loss: 20.7361 - val_mae: 3.2854\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.2962 - mae: 3.3746 - val_loss: 20.9419 - val_mae: 3.3985\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5265 - mae: 3.4425 - val_loss: 20.7097 - val_mae: 3.2507\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.4424 - mae: 3.3998 - val_loss: 20.7822 - val_mae: 3.3356\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6815 - mae: 3.4179 - val_loss: 20.6931 - val_mae: 3.2888\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.1203 - mae: 3.3602 - val_loss: 20.8180 - val_mae: 3.3604\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.1433 - mae: 3.3674 - val_loss: 20.6942 - val_mae: 3.2976\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.2249 - mae: 3.3814 - val_loss: 20.7673 - val_mae: 3.3084\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 19.9603 - mae: 3.3689 - val_loss: 20.8126 - val_mae: 3.3101\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.4776 - mae: 3.3797 - val_loss: 21.1491 - val_mae: 3.4308\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 25.9108 - mae: 3.4838\n",
      "Mean Absolute Error on Test Data: 3.4838297367095947\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.12306257165084844\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 6.4952 - mae: 1.5158 - val_loss: 4.3858 - val_mae: 1.2878\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.6894 - mae: 1.3249 - val_loss: 3.8804 - val_mae: 1.3568\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.5572 - mae: 1.2617 - val_loss: 3.9896 - val_mae: 1.2680\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4674 - mae: 1.2474 - val_loss: 3.8931 - val_mae: 1.2916\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4645 - mae: 1.2392 - val_loss: 3.9088 - val_mae: 1.2873\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4490 - mae: 1.2435 - val_loss: 3.9096 - val_mae: 1.2921\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4328 - mae: 1.2398 - val_loss: 3.9166 - val_mae: 1.2999\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4211 - mae: 1.2334 - val_loss: 3.9442 - val_mae: 1.2965\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4160 - mae: 1.2435 - val_loss: 3.9300 - val_mae: 1.3106\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4003 - mae: 1.2506 - val_loss: 3.9820 - val_mae: 1.2952\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4051 - mae: 1.2188 - val_loss: 3.9411 - val_mae: 1.3114\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4026 - mae: 1.2496 - val_loss: 3.9586 - val_mae: 1.3188\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3465 - mae: 1.2320 - val_loss: 4.0249 - val_mae: 1.3003\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3886 - mae: 1.2171 - val_loss: 3.9618 - val_mae: 1.3249\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.3899 - mae: 1.2429 - val_loss: 4.0036 - val_mae: 1.3075\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5049 - mae: 1.2671 - val_loss: 4.0411 - val_mae: 1.3093\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3730 - mae: 1.2100 - val_loss: 3.9645 - val_mae: 1.3343\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4177 - mae: 1.2798 - val_loss: 4.0070 - val_mae: 1.3085\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4359 - mae: 1.2245 - val_loss: 3.9949 - val_mae: 1.3297\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3226 - mae: 1.2252 - val_loss: 3.9844 - val_mae: 1.3162\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4003 - mae: 1.2457 - val_loss: 3.9943 - val_mae: 1.3296\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3253 - mae: 1.2372 - val_loss: 4.0208 - val_mae: 1.3103\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.3888 - mae: 1.2549 - val_loss: 4.0020 - val_mae: 1.3368\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4201 - mae: 1.2581 - val_loss: 4.0310 - val_mae: 1.3135\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3278 - mae: 1.2385 - val_loss: 4.0225 - val_mae: 1.3169\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3562 - mae: 1.2276 - val_loss: 4.0149 - val_mae: 1.3200\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2918 - mae: 1.2106 - val_loss: 4.0141 - val_mae: 1.3227\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4354 - mae: 1.2891 - val_loss: 4.0936 - val_mae: 1.2948\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3300 - mae: 1.2174 - val_loss: 4.0142 - val_mae: 1.3305\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4089 - mae: 1.2748 - val_loss: 4.0333 - val_mae: 1.3091\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.4387 - mae: 1.2375 - val_loss: 3.9970 - val_mae: 1.3338\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2699 - mae: 1.2407 - val_loss: 4.0335 - val_mae: 1.3212\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3842 - mae: 1.3037 - val_loss: 4.0951 - val_mae: 1.3158\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3968 - mae: 1.2051 - val_loss: 4.0249 - val_mae: 1.3354\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.4261 - mae: 1.2861 - val_loss: 4.0742 - val_mae: 1.3062\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3178 - mae: 1.2125 - val_loss: 4.0941 - val_mae: 1.2969\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3043 - mae: 1.2359 - val_loss: 4.0203 - val_mae: 1.3143\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2952 - mae: 1.2291 - val_loss: 4.0470 - val_mae: 1.3208\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2666 - mae: 1.2197 - val_loss: 4.0382 - val_mae: 1.3229\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2830 - mae: 1.2365 - val_loss: 4.0840 - val_mae: 1.3232\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3172 - mae: 1.2386 - val_loss: 4.1258 - val_mae: 1.3226\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2071 - mae: 1.2057 - val_loss: 4.0363 - val_mae: 1.3360\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2628 - mae: 1.2463 - val_loss: 4.1315 - val_mae: 1.3255\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2656 - mae: 1.2267 - val_loss: 4.0736 - val_mae: 1.3101\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.2338 - mae: 1.2318 - val_loss: 4.0859 - val_mae: 1.3160\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 4.2928 - mae: 1.2421 - val_loss: 4.0570 - val_mae: 1.3159\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3300 - mae: 1.2362 - val_loss: 4.0084 - val_mae: 1.3288\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.1842 - mae: 1.2024 - val_loss: 4.0274 - val_mae: 1.3207\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3187 - mae: 1.2755 - val_loss: 4.0436 - val_mae: 1.3363\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 4.3113 - mae: 1.2408 - val_loss: 4.0438 - val_mae: 1.3313\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 2.3583 - mae: 1.0940\n",
      "Mean Absolute Error on Test Data: 1.0940005779266357\n",
      "5/5 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.02818554788607308\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 138.0540 - mae: 9.1560 - val_loss: 111.2865 - val_mae: 7.6146\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 77.6229 - mae: 5.9257 - val_loss: 54.5529 - val_mae: 5.3282\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 53.3984 - mae: 5.3412 - val_loss: 53.7412 - val_mae: 5.5266\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.3014 - mae: 4.9813 - val_loss: 53.6883 - val_mae: 5.3921\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.5218 - mae: 5.0398 - val_loss: 53.3788 - val_mae: 5.4754\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.8469 - mae: 4.9761 - val_loss: 53.3635 - val_mae: 5.3777\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.8333 - mae: 4.9963 - val_loss: 53.2641 - val_mae: 5.5045\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 52.3811 - mae: 5.1066 - val_loss: 53.1722 - val_mae: 5.3804\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.1199 - mae: 4.9988 - val_loss: 52.9545 - val_mae: 5.4213\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.6846 - mae: 4.9968 - val_loss: 52.8868 - val_mae: 5.4379\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3744 - mae: 4.9956 - val_loss: 52.8008 - val_mae: 5.3985\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.3968 - mae: 4.9468 - val_loss: 52.7210 - val_mae: 5.3794\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.9008 - mae: 4.9305 - val_loss: 52.6726 - val_mae: 5.3912\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.4386 - mae: 5.0182 - val_loss: 52.6177 - val_mae: 5.3677\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.2729 - mae: 5.0060 - val_loss: 52.6613 - val_mae: 5.3234\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.8977 - mae: 4.9578 - val_loss: 52.5628 - val_mae: 5.3498\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9931 - mae: 4.9367 - val_loss: 52.4694 - val_mae: 5.3695\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.6504 - mae: 5.0367 - val_loss: 52.5430 - val_mae: 5.3148\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.1655 - mae: 4.9852 - val_loss: 52.5059 - val_mae: 5.4079\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.1815 - mae: 4.9378 - val_loss: 52.4649 - val_mae: 5.3925\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.5699 - mae: 4.9978 - val_loss: 52.3893 - val_mae: 5.3629\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.7826 - mae: 4.9156 - val_loss: 52.3669 - val_mae: 5.3694\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9452 - mae: 5.0248 - val_loss: 52.3668 - val_mae: 5.3100\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.3783 - mae: 4.9226 - val_loss: 52.3292 - val_mae: 5.3388\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.3211 - mae: 4.9067 - val_loss: 52.3204 - val_mae: 5.3756\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5500 - mae: 5.0095 - val_loss: 52.3713 - val_mae: 5.2859\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 50.2783 - mae: 4.9195 - val_loss: 52.3102 - val_mae: 5.4123\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9866 - mae: 5.0177 - val_loss: 52.2244 - val_mae: 5.2800\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 51.2813 - mae: 4.9788 - val_loss: 52.2662 - val_mae: 5.2675\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.4153 - mae: 4.8924 - val_loss: 52.1417 - val_mae: 5.3555\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.2815 - mae: 5.0283 - val_loss: 52.4604 - val_mae: 5.2135\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.5924 - mae: 4.9374 - val_loss: 52.1840 - val_mae: 5.3909\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.7877 - mae: 4.9634 - val_loss: 52.1103 - val_mae: 5.2821\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.2074 - mae: 4.9989 - val_loss: 52.1365 - val_mae: 5.2577\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 50.3531 - mae: 4.9364 - val_loss: 52.0984 - val_mae: 5.3701\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.0518 - mae: 4.9183 - val_loss: 52.0347 - val_mae: 5.2998\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.7715 - mae: 4.9470 - val_loss: 52.0221 - val_mae: 5.2795\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.5297 - mae: 4.9450 - val_loss: 52.0695 - val_mae: 5.3013\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9227 - mae: 4.9245 - val_loss: 52.1000 - val_mae: 5.2575\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.5348 - mae: 4.9336 - val_loss: 52.0455 - val_mae: 5.3308\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.0487 - mae: 4.9967 - val_loss: 52.2268 - val_mae: 5.2076\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.2181 - mae: 4.9164 - val_loss: 52.6435 - val_mae: 5.1531\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.3060 - mae: 4.8993 - val_loss: 51.9679 - val_mae: 5.2820\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8981 - mae: 4.9264 - val_loss: 51.9438 - val_mae: 5.2387\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 50.1625 - mae: 4.9978 - val_loss: 51.9129 - val_mae: 5.2563\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.8416 - mae: 4.8496 - val_loss: 52.0178 - val_mae: 5.3808\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.2038 - mae: 4.9413 - val_loss: 51.9987 - val_mae: 5.3621\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.3614 - mae: 4.9977 - val_loss: 52.4302 - val_mae: 5.1622\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 49.9988 - mae: 4.9332 - val_loss: 51.9891 - val_mae: 5.2325\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 48.8892 - mae: 4.8906 - val_loss: 51.9996 - val_mae: 5.2350\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 40.4120 - mae: 4.6042\n",
      "Mean Absolute Error on Test Data: 4.604184627532959\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.1243762398535666\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 33.9909 - mae: 4.4212 - val_loss: 16.5996 - val_mae: 2.8780\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 16.9877 - mae: 2.9720 - val_loss: 13.0457 - val_mae: 2.9185\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 15.2200 - mae: 2.9691 - val_loss: 11.9196 - val_mae: 2.7309\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.9758 - mae: 2.8452 - val_loss: 11.8405 - val_mae: 2.7271\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.9458 - mae: 2.8961 - val_loss: 11.7375 - val_mae: 2.7101\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 14.7678 - mae: 2.8474 - val_loss: 11.7158 - val_mae: 2.7108\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4933 - mae: 2.8420 - val_loss: 11.6381 - val_mae: 2.6973\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5741 - mae: 2.8497 - val_loss: 11.6082 - val_mae: 2.6904\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3674 - mae: 2.7929 - val_loss: 11.7156 - val_mae: 2.7196\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3728 - mae: 2.7772 - val_loss: 11.8597 - val_mae: 2.7461\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3824 - mae: 2.8617 - val_loss: 11.6030 - val_mae: 2.6860\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2821 - mae: 2.8071 - val_loss: 11.6702 - val_mae: 2.7098\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.5687 - mae: 2.8269 - val_loss: 11.8836 - val_mae: 2.7567\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.3928 - mae: 2.8510 - val_loss: 11.7705 - val_mae: 2.7329\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1969 - mae: 2.7770 - val_loss: 11.8617 - val_mae: 2.7489\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.2024 - mae: 2.8345 - val_loss: 11.8053 - val_mae: 2.7372\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1120 - mae: 2.8317 - val_loss: 11.7949 - val_mae: 2.7320\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0426 - mae: 2.7905 - val_loss: 11.7402 - val_mae: 2.7100\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1035 - mae: 2.8179 - val_loss: 11.7693 - val_mae: 2.7218\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.4818 - mae: 2.8279 - val_loss: 11.8269 - val_mae: 2.7324\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0871 - mae: 2.7813 - val_loss: 12.1120 - val_mae: 2.7913\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8892 - mae: 2.8123 - val_loss: 11.8249 - val_mae: 2.6859\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1228 - mae: 2.7841 - val_loss: 11.8780 - val_mae: 2.7312\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1528 - mae: 2.7949 - val_loss: 11.9966 - val_mae: 2.7653\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.0014 - mae: 2.8105 - val_loss: 11.9488 - val_mae: 2.7505\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8434 - mae: 2.7501 - val_loss: 12.0290 - val_mae: 2.7729\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7839 - mae: 2.7761 - val_loss: 11.9215 - val_mae: 2.7403\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7542 - mae: 2.7514 - val_loss: 11.9482 - val_mae: 2.7426\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.6916 - mae: 2.7262 - val_loss: 12.2326 - val_mae: 2.8082\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7629 - mae: 2.8389 - val_loss: 11.9085 - val_mae: 2.7159\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8980 - mae: 2.7927 - val_loss: 12.0748 - val_mae: 2.7692\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7793 - mae: 2.7377 - val_loss: 12.0858 - val_mae: 2.7611\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7135 - mae: 2.7593 - val_loss: 12.1352 - val_mae: 2.7723\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6451 - mae: 2.7242 - val_loss: 12.4635 - val_mae: 2.8446\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9215 - mae: 2.7909 - val_loss: 12.5618 - val_mae: 2.8581\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.8371 - mae: 2.8288 - val_loss: 12.0919 - val_mae: 2.7529\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7666 - mae: 2.7803 - val_loss: 12.3132 - val_mae: 2.8090\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6383 - mae: 2.7465 - val_loss: 12.3740 - val_mae: 2.8155\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6346 - mae: 2.7647 - val_loss: 12.1581 - val_mae: 2.7663\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7693 - mae: 2.7708 - val_loss: 12.4229 - val_mae: 2.8306\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 14.1321 - mae: 2.8297 - val_loss: 12.1050 - val_mae: 2.7251\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.7645 - mae: 2.7674 - val_loss: 12.4607 - val_mae: 2.8396\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.4613 - mae: 2.7179 - val_loss: 12.3151 - val_mae: 2.8047\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 13.6944 - mae: 2.7558 - val_loss: 12.4208 - val_mae: 2.8224\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.9659 - mae: 2.8368 - val_loss: 12.3213 - val_mae: 2.7531\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.4652 - mae: 2.7750 - val_loss: 12.2736 - val_mae: 2.7759\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.4899 - mae: 2.7159 - val_loss: 12.4546 - val_mae: 2.8356\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6178 - mae: 2.7741 - val_loss: 12.2186 - val_mae: 2.7675\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6082 - mae: 2.7367 - val_loss: 12.5750 - val_mae: 2.8568\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 13.6606 - mae: 2.7507 - val_loss: 12.3655 - val_mae: 2.8034\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 17.0690 - mae: 3.0974\n",
      "Mean Absolute Error on Test Data: 3.0973565578460693\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.0028945596182691213\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 13ms/step - loss: 7.9725 - mae: 1.7434 - val_loss: 4.4931 - val_mae: 1.3233\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.7710 - mae: 1.5640 - val_loss: 4.3507 - val_mae: 1.4918\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.5691 - mae: 1.4831 - val_loss: 4.2633 - val_mae: 1.3720\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3602 - mae: 1.4420 - val_loss: 4.2581 - val_mae: 1.3977\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.3981 - mae: 1.4886 - val_loss: 4.2610 - val_mae: 1.4088\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.3743 - mae: 1.4176 - val_loss: 4.2660 - val_mae: 1.3921\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2342 - mae: 1.4560 - val_loss: 4.2858 - val_mae: 1.4241\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2158 - mae: 1.4504 - val_loss: 4.2911 - val_mae: 1.3964\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1215 - mae: 1.3987 - val_loss: 4.3098 - val_mae: 1.4145\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2048 - mae: 1.4622 - val_loss: 4.3246 - val_mae: 1.3975\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1465 - mae: 1.4057 - val_loss: 4.3405 - val_mae: 1.3863\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2356 - mae: 1.4710 - val_loss: 4.3473 - val_mae: 1.3980\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.2399 - mae: 1.3933 - val_loss: 4.3665 - val_mae: 1.4291\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 5.1555 - mae: 1.4546 - val_loss: 4.3643 - val_mae: 1.4312\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1252 - mae: 1.4268 - val_loss: 4.3608 - val_mae: 1.4245\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0756 - mae: 1.4402 - val_loss: 4.3772 - val_mae: 1.4062\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1704 - mae: 1.4531 - val_loss: 4.3853 - val_mae: 1.4076\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1017 - mae: 1.4079 - val_loss: 4.4052 - val_mae: 1.4084\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0675 - mae: 1.4523 - val_loss: 4.4137 - val_mae: 1.4132\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0425 - mae: 1.3904 - val_loss: 4.4095 - val_mae: 1.4353\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1256 - mae: 1.4520 - val_loss: 4.4014 - val_mae: 1.4009\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0501 - mae: 1.4188 - val_loss: 4.4450 - val_mae: 1.4504\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0358 - mae: 1.4333 - val_loss: 4.4438 - val_mae: 1.4399\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0990 - mae: 1.4476 - val_loss: 4.4584 - val_mae: 1.3748\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0305 - mae: 1.4305 - val_loss: 4.4539 - val_mae: 1.4770\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9874 - mae: 1.3960 - val_loss: 4.4263 - val_mae: 1.4362\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9718 - mae: 1.4262 - val_loss: 4.4210 - val_mae: 1.4298\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.1375 - mae: 1.4297 - val_loss: 4.4227 - val_mae: 1.4405\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9715 - mae: 1.4074 - val_loss: 4.4225 - val_mae: 1.4281\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 5.0328 - mae: 1.4375 - val_loss: 4.4273 - val_mae: 1.4135\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9349 - mae: 1.4085 - val_loss: 4.4426 - val_mae: 1.4582\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 5.0070 - mae: 1.4456 - val_loss: 4.4202 - val_mae: 1.4098\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9782 - mae: 1.3987 - val_loss: 4.4325 - val_mae: 1.4614\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9009 - mae: 1.4203 - val_loss: 4.4175 - val_mae: 1.4307\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9834 - mae: 1.4118 - val_loss: 4.4138 - val_mae: 1.4444\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8947 - mae: 1.4354 - val_loss: 4.4489 - val_mae: 1.4301\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 4.8981 - mae: 1.4148 - val_loss: 4.4648 - val_mae: 1.4139\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.9480 - mae: 1.4082 - val_loss: 4.4581 - val_mae: 1.4678\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8762 - mae: 1.4108 - val_loss: 4.4280 - val_mae: 1.4487\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8555 - mae: 1.4257 - val_loss: 4.4780 - val_mae: 1.4644\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8703 - mae: 1.4179 - val_loss: 4.4354 - val_mae: 1.4397\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.7083 - mae: 1.3751 - val_loss: 4.4412 - val_mae: 1.4515\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8764 - mae: 1.3959 - val_loss: 4.4303 - val_mae: 1.4139\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.7855 - mae: 1.4069 - val_loss: 4.4382 - val_mae: 1.4509\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8466 - mae: 1.4283 - val_loss: 4.4793 - val_mae: 1.4117\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8596 - mae: 1.3484 - val_loss: 4.4927 - val_mae: 1.4928\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8456 - mae: 1.4283 - val_loss: 4.4754 - val_mae: 1.4557\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.7880 - mae: 1.4329 - val_loss: 4.5064 - val_mae: 1.4615\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8842 - mae: 1.4021 - val_loss: 4.4500 - val_mae: 1.4298\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 4.8063 - mae: 1.3759 - val_loss: 4.4690 - val_mae: 1.4632\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.8085 - mae: 1.4917\n",
      "Mean Absolute Error on Test Data: 1.4917482137680054\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.16302983504349022\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 68.0916 - mae: 5.7753 - val_loss: 36.8054 - val_mae: 3.6369\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.1016 - mae: 3.9172 - val_loss: 29.5783 - val_mae: 4.0527\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 34.2547 - mae: 4.2288 - val_loss: 27.5893 - val_mae: 3.7154\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.8257 - mae: 3.8897 - val_loss: 27.2474 - val_mae: 3.6712\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.0782 - mae: 3.8570 - val_loss: 27.3199 - val_mae: 3.6937\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 32.9319 - mae: 3.9497 - val_loss: 27.4220 - val_mae: 3.7275\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.8471 - mae: 4.0045 - val_loss: 27.4575 - val_mae: 3.7526\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.7666 - mae: 3.8319 - val_loss: 27.0529 - val_mae: 3.6725\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 33.5464 - mae: 4.1481 - val_loss: 26.8489 - val_mae: 3.6440\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.4887 - mae: 3.8117 - val_loss: 26.5803 - val_mae: 3.6028\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.2438 - mae: 3.9596 - val_loss: 27.0175 - val_mae: 3.6955\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 32.8073 - mae: 3.9345 - val_loss: 26.5951 - val_mae: 3.5958\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.1941 - mae: 3.8150 - val_loss: 27.2168 - val_mae: 3.7334\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.5465 - mae: 4.0079 - val_loss: 26.7219 - val_mae: 3.6494\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.3194 - mae: 3.8345 - val_loss: 26.5253 - val_mae: 3.6222\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.4909 - mae: 3.9282 - val_loss: 26.7661 - val_mae: 3.6687\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.9485 - mae: 3.9083 - val_loss: 26.7847 - val_mae: 3.6728\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.1828 - mae: 3.8498 - val_loss: 26.5690 - val_mae: 3.6467\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.4757 - mae: 4.0325 - val_loss: 26.1546 - val_mae: 3.5802\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.6945 - mae: 3.7500 - val_loss: 26.5735 - val_mae: 3.6500\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.3703 - mae: 3.9493 - val_loss: 26.2233 - val_mae: 3.5752\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.2060 - mae: 3.7659 - val_loss: 27.2027 - val_mae: 3.7758\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.6890 - mae: 3.9667 - val_loss: 26.4439 - val_mae: 3.6606\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.9492 - mae: 3.8087 - val_loss: 27.0309 - val_mae: 3.7640\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.6929 - mae: 4.0202 - val_loss: 26.1331 - val_mae: 3.5723\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.8436 - mae: 3.7905 - val_loss: 26.7169 - val_mae: 3.7041\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.8594 - mae: 3.8790 - val_loss: 26.6179 - val_mae: 3.6796\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.5532 - mae: 3.9195 - val_loss: 26.5962 - val_mae: 3.7024\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.4220 - mae: 3.8982 - val_loss: 26.4675 - val_mae: 3.6734\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.8031 - mae: 3.8452 - val_loss: 26.0379 - val_mae: 3.5798\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.6549 - mae: 3.8531 - val_loss: 26.3420 - val_mae: 3.6519\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.6887 - mae: 3.8715 - val_loss: 25.9095 - val_mae: 3.5793\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.4929 - mae: 3.8627 - val_loss: 26.3787 - val_mae: 3.6635\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 32.0771 - mae: 3.8799 - val_loss: 26.8324 - val_mae: 3.7565\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 31.5979 - mae: 3.8339 - val_loss: 26.9566 - val_mae: 3.7727\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.3638 - mae: 3.9120 - val_loss: 25.9994 - val_mae: 3.6158\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 30.8576 - mae: 3.7535 - val_loss: 27.5626 - val_mae: 3.8849\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.1708 - mae: 3.8939 - val_loss: 25.5587 - val_mae: 3.5076\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.5090 - mae: 3.8120 - val_loss: 26.2197 - val_mae: 3.6500\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.2734 - mae: 3.8811 - val_loss: 26.3026 - val_mae: 3.6953\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.0338 - mae: 3.9206 - val_loss: 25.8389 - val_mae: 3.5729\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 31.2136 - mae: 3.8708 - val_loss: 26.6050 - val_mae: 3.7288\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.1549 - mae: 3.8782 - val_loss: 26.3583 - val_mae: 3.6920\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.2665 - mae: 3.8907 - val_loss: 26.1774 - val_mae: 3.6538\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 30.6934 - mae: 3.8455 - val_loss: 26.5688 - val_mae: 3.7343\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.0462 - mae: 3.8315 - val_loss: 26.7413 - val_mae: 3.7444\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.0718 - mae: 3.8089 - val_loss: 26.9123 - val_mae: 3.8019\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 31.8524 - mae: 4.0184 - val_loss: 25.6526 - val_mae: 3.5565\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 30.3120 - mae: 3.7458 - val_loss: 27.2367 - val_mae: 3.8506\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 30.6582 - mae: 3.9214 - val_loss: 25.9978 - val_mae: 3.6126\n",
      "7/7 [==============================] - 0s 1000us/step - loss: 34.1360 - mae: 3.4744\n",
      "Mean Absolute Error on Test Data: 3.4743573665618896\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.08053648036175454\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 100.1889 - mae: 7.4107 - val_loss: 63.2360 - val_mae: 5.7958\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 53.4493 - mae: 4.4420 - val_loss: 31.1254 - val_mae: 4.3707\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.5190 - mae: 4.3589 - val_loss: 30.6644 - val_mae: 4.1109\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.5521 - mae: 4.1484 - val_loss: 30.7588 - val_mae: 4.0755\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2079 - mae: 4.1751 - val_loss: 30.9371 - val_mae: 4.0523\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4412 - mae: 4.1173 - val_loss: 30.6547 - val_mae: 4.1118\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.3498 - mae: 4.2526 - val_loss: 30.8994 - val_mae: 4.0562\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.0391 - mae: 4.1342 - val_loss: 30.6752 - val_mae: 4.0922\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4576 - mae: 4.2555 - val_loss: 30.9034 - val_mae: 4.0486\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.8917 - mae: 4.1471 - val_loss: 30.5526 - val_mae: 4.1251\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.2225 - mae: 4.2291 - val_loss: 30.6244 - val_mae: 4.0803\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7720 - mae: 4.1392 - val_loss: 30.7076 - val_mae: 4.0618\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.5234 - mae: 4.2815 - val_loss: 31.1289 - val_mae: 4.0248\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.2245 - mae: 4.0299 - val_loss: 30.4887 - val_mae: 4.1702\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4241 - mae: 4.2558 - val_loss: 30.8270 - val_mae: 4.0392\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1985 - mae: 4.1186 - val_loss: 30.3874 - val_mae: 4.1506\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6414 - mae: 4.1993 - val_loss: 30.8094 - val_mae: 4.0342\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3854 - mae: 4.1407 - val_loss: 30.4362 - val_mae: 4.2068\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.6935 - mae: 4.1767 - val_loss: 30.4496 - val_mae: 4.0673\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2650 - mae: 4.1679 - val_loss: 30.5079 - val_mae: 4.0673\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4857 - mae: 4.1360 - val_loss: 30.4152 - val_mae: 4.0926\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9676 - mae: 4.2209 - val_loss: 30.6898 - val_mae: 4.0346\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6938 - mae: 4.1291 - val_loss: 30.2760 - val_mae: 4.0977\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2721 - mae: 4.2355 - val_loss: 30.8555 - val_mae: 4.0210\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3018 - mae: 4.0895 - val_loss: 30.3435 - val_mae: 4.0863\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 41.9735 - mae: 4.1636 - val_loss: 30.4723 - val_mae: 4.0483\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4131 - mae: 4.2293 - val_loss: 30.7891 - val_mae: 4.0210\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6240 - mae: 4.0500 - val_loss: 30.0948 - val_mae: 4.0816\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8962 - mae: 4.2449 - val_loss: 30.5230 - val_mae: 4.0309\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.0368 - mae: 4.1417 - val_loss: 30.4062 - val_mae: 4.0406\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8262 - mae: 4.0842 - val_loss: 30.0854 - val_mae: 4.0923\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5735 - mae: 4.1211 - val_loss: 30.0443 - val_mae: 4.0866\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.7636 - mae: 4.1927 - val_loss: 30.2391 - val_mae: 4.0459\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1189 - mae: 4.1580 - val_loss: 30.5314 - val_mae: 4.0074\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9741 - mae: 4.0703 - val_loss: 29.8751 - val_mae: 4.0807\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1280 - mae: 4.2430 - val_loss: 30.0986 - val_mae: 4.0705\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5994 - mae: 4.1452 - val_loss: 29.9874 - val_mae: 4.0813\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7581 - mae: 4.0483 - val_loss: 29.8394 - val_mae: 4.1213\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.6235 - mae: 4.2107 - val_loss: 30.5144 - val_mae: 4.0108\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5687 - mae: 4.0405 - val_loss: 30.0886 - val_mae: 4.1910\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1554 - mae: 4.1807 - val_loss: 30.3599 - val_mae: 4.0060\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1171 - mae: 4.1185 - val_loss: 30.1752 - val_mae: 4.0157\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1239 - mae: 4.1258 - val_loss: 30.0837 - val_mae: 4.1310\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1176 - mae: 4.0749 - val_loss: 29.8310 - val_mae: 4.0509\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.8449 - mae: 4.1215 - val_loss: 30.0732 - val_mae: 4.0087\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0377 - mae: 4.0864 - val_loss: 29.8512 - val_mae: 4.0960\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.4455 - mae: 4.0975 - val_loss: 30.2946 - val_mae: 3.9853\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0875 - mae: 4.0754 - val_loss: 29.5994 - val_mae: 4.0641\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.2348 - mae: 4.1138 - val_loss: 29.6588 - val_mae: 4.0373\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.0249 - mae: 4.1197 - val_loss: 29.7270 - val_mae: 4.0963\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 34.7042 - mae: 4.2659\n",
      "Mean Absolute Error on Test Data: 4.265941143035889\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.036204254378031764\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 115.4364 - mae: 8.2924 - val_loss: 58.7096 - val_mae: 5.4258\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 61.2793 - mae: 5.2441 - val_loss: 35.0735 - val_mae: 4.6025\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 46.9188 - mae: 5.0842 - val_loss: 32.0042 - val_mae: 4.1701\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.3776 - mae: 4.5943 - val_loss: 33.3241 - val_mae: 4.3612\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.6734 - mae: 4.8046 - val_loss: 32.8981 - val_mae: 4.2777\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.9294 - mae: 4.7361 - val_loss: 33.4889 - val_mae: 4.3369\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 44.2061 - mae: 4.7169 - val_loss: 33.4015 - val_mae: 4.3133\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.8579 - mae: 4.6557 - val_loss: 33.2376 - val_mae: 4.2662\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.6004 - mae: 4.6647 - val_loss: 33.4532 - val_mae: 4.2845\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.0463 - mae: 4.6714 - val_loss: 34.4763 - val_mae: 4.4110\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7194 - mae: 4.6648 - val_loss: 32.9593 - val_mae: 4.2215\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 44.2886 - mae: 4.7216 - val_loss: 33.5158 - val_mae: 4.2996\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 43.9868 - mae: 4.6052 - val_loss: 33.1801 - val_mae: 4.2641\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.6575 - mae: 4.6512 - val_loss: 33.0269 - val_mae: 4.2426\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4500 - mae: 4.6550 - val_loss: 33.4358 - val_mae: 4.2894\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6799 - mae: 4.5417 - val_loss: 33.0638 - val_mae: 4.2556\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0343 - mae: 4.6703 - val_loss: 32.7912 - val_mae: 4.1945\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.4074 - mae: 4.6589 - val_loss: 33.3087 - val_mae: 4.2862\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.7579 - mae: 4.6487 - val_loss: 32.6185 - val_mae: 4.2070\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.9076 - mae: 4.6398 - val_loss: 32.6734 - val_mae: 4.2043\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5137 - mae: 4.5946 - val_loss: 33.2264 - val_mae: 4.2798\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.9066 - mae: 4.7106 - val_loss: 31.8529 - val_mae: 4.0107\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8863 - mae: 4.5906 - val_loss: 33.3754 - val_mae: 4.3198\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3429 - mae: 4.5528 - val_loss: 33.1895 - val_mae: 4.2804\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.8207 - mae: 4.6463 - val_loss: 32.0993 - val_mae: 4.1176\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 42.3819 - mae: 4.5320 - val_loss: 33.3554 - val_mae: 4.3334\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6539 - mae: 4.6911 - val_loss: 31.9388 - val_mae: 4.0511\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6281 - mae: 4.6303 - val_loss: 31.7555 - val_mae: 4.0258\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5667 - mae: 4.5526 - val_loss: 33.2258 - val_mae: 4.2986\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8861 - mae: 4.6415 - val_loss: 31.6741 - val_mae: 4.0238\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.5910 - mae: 4.5852 - val_loss: 31.8230 - val_mae: 4.1059\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 42.7623 - mae: 4.6600 - val_loss: 31.6282 - val_mae: 4.0451\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.9599 - mae: 4.5570 - val_loss: 31.8793 - val_mae: 4.0862\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9823 - mae: 4.6142 - val_loss: 33.2977 - val_mae: 4.2583\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9551 - mae: 4.5505 - val_loss: 33.7627 - val_mae: 4.3708\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3895 - mae: 4.6357 - val_loss: 31.4274 - val_mae: 3.9505\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3935 - mae: 4.5458 - val_loss: 33.4938 - val_mae: 4.2780\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5439 - mae: 4.5803 - val_loss: 31.2940 - val_mae: 3.9943\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5402 - mae: 4.6099 - val_loss: 31.8317 - val_mae: 4.0364\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1464 - mae: 4.4649 - val_loss: 32.1111 - val_mae: 4.1627\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.9035 - mae: 4.5301 - val_loss: 32.1836 - val_mae: 4.1365\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.5307 - mae: 4.5429 - val_loss: 32.3808 - val_mae: 4.1521\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.7916 - mae: 4.5633 - val_loss: 31.3459 - val_mae: 3.9664\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.6112 - mae: 4.4854 - val_loss: 32.8467 - val_mae: 4.2231\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5969 - mae: 4.5981 - val_loss: 31.3692 - val_mae: 3.9268\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 40.8055 - mae: 4.5366 - val_loss: 32.4603 - val_mae: 4.1717\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.9231 - mae: 4.5588 - val_loss: 31.4708 - val_mae: 4.0480\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.4648 - mae: 4.6070 - val_loss: 31.2032 - val_mae: 3.9194\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.8209 - mae: 4.4248 - val_loss: 33.4979 - val_mae: 4.3049\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1273 - mae: 4.6330 - val_loss: 30.9576 - val_mae: 3.8509\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 37.6636 - mae: 4.1169\n",
      "Mean Absolute Error on Test Data: 4.116906642913818\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.07185543987088672\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 1s 18ms/step - loss: 2.7880 - mae: 1.1240 - val_loss: 2.3231 - val_mae: 0.9901\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6518 - mae: 0.9247 - val_loss: 2.3288 - val_mae: 1.1135\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5722 - mae: 0.9229 - val_loss: 2.2720 - val_mae: 1.0373\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6171 - mae: 0.8704 - val_loss: 2.2929 - val_mae: 1.0041\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.6260 - mae: 0.8985 - val_loss: 2.2775 - val_mae: 1.0774\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.5782 - mae: 0.8820 - val_loss: 2.2572 - val_mae: 1.0252\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5379 - mae: 0.8544 - val_loss: 2.2556 - val_mae: 1.0512\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5608 - mae: 0.9011 - val_loss: 2.2603 - val_mae: 1.0464\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5465 - mae: 0.8794 - val_loss: 2.2644 - val_mae: 1.0263\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5399 - mae: 0.8709 - val_loss: 2.2511 - val_mae: 1.0397\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5409 - mae: 0.8817 - val_loss: 2.2484 - val_mae: 1.0331\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5194 - mae: 0.8690 - val_loss: 2.2520 - val_mae: 1.0388\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5410 - mae: 0.8817 - val_loss: 2.2506 - val_mae: 1.0326\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 7ms/step - loss: 1.5435 - mae: 0.8941 - val_loss: 2.2496 - val_mae: 1.0464\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5385 - mae: 0.8521 - val_loss: 2.2538 - val_mae: 1.0107\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5336 - mae: 0.8889 - val_loss: 2.2325 - val_mae: 1.0524\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4688 - mae: 0.8452 - val_loss: 2.2337 - val_mae: 1.0303\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5086 - mae: 0.8629 - val_loss: 2.2287 - val_mae: 1.0468\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.5135 - mae: 0.8890 - val_loss: 2.2282 - val_mae: 1.0263\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4856 - mae: 0.8468 - val_loss: 2.2469 - val_mae: 1.0371\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4772 - mae: 0.8609 - val_loss: 2.2317 - val_mae: 1.0455\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4935 - mae: 0.8861 - val_loss: 2.2273 - val_mae: 1.0189\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4731 - mae: 0.8443 - val_loss: 2.2201 - val_mae: 1.0274\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4897 - mae: 0.8839 - val_loss: 2.2276 - val_mae: 1.0308\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4968 - mae: 0.8742 - val_loss: 2.2299 - val_mae: 1.0400\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.4454 - mae: 0.8446 - val_loss: 2.2227 - val_mae: 1.0290\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4733 - mae: 0.8591 - val_loss: 2.2192 - val_mae: 1.0555\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4505 - mae: 0.8539 - val_loss: 2.2063 - val_mae: 1.0387\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4724 - mae: 0.8627 - val_loss: 2.2196 - val_mae: 1.0183\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4599 - mae: 0.8542 - val_loss: 2.2048 - val_mae: 1.0524\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4707 - mae: 0.8790 - val_loss: 2.2047 - val_mae: 1.0185\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4567 - mae: 0.8196 - val_loss: 2.1714 - val_mae: 1.0269\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.4280 - mae: 0.8492 - val_loss: 2.1885 - val_mae: 1.0208\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4395 - mae: 0.8444 - val_loss: 2.1893 - val_mae: 1.0345\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4375 - mae: 0.8603 - val_loss: 2.1880 - val_mae: 1.0210\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4014 - mae: 0.8565 - val_loss: 2.1806 - val_mae: 1.0295\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4242 - mae: 0.8324 - val_loss: 2.1987 - val_mae: 1.0271\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4366 - mae: 0.8552 - val_loss: 2.1876 - val_mae: 1.0318\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4042 - mae: 0.8478 - val_loss: 2.1980 - val_mae: 1.0297\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3902 - mae: 0.8399 - val_loss: 2.2003 - val_mae: 1.0278\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 1.3905 - mae: 0.8342 - val_loss: 2.1948 - val_mae: 1.0499\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4265 - mae: 0.8586 - val_loss: 2.2026 - val_mae: 1.0174\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4343 - mae: 0.8298 - val_loss: 2.1712 - val_mae: 1.0360\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4121 - mae: 0.8638 - val_loss: 2.1961 - val_mae: 1.0380\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4122 - mae: 0.8345 - val_loss: 2.1760 - val_mae: 1.0429\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.4076 - mae: 0.8516 - val_loss: 2.1891 - val_mae: 1.0255\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3775 - mae: 0.8234 - val_loss: 2.1745 - val_mae: 1.0308\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 6ms/step - loss: 1.3660 - mae: 0.8691 - val_loss: 2.2012 - val_mae: 1.0421\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3904 - mae: 0.8293 - val_loss: 2.1832 - val_mae: 1.0282\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 1.3964 - mae: 0.8512 - val_loss: 2.1810 - val_mae: 1.0509\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 1.1073 - mae: 0.8412\n",
      "Mean Absolute Error on Test Data: 0.8411663770675659\n",
      "4/4 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.028877032237880074\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 11ms/step - loss: 77.1393 - mae: 6.9730 - val_loss: 77.2142 - val_mae: 5.8624\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 34.5503 - mae: 4.0224 - val_loss: 44.3751 - val_mae: 4.5907\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.5589 - mae: 3.9361 - val_loss: 45.5037 - val_mae: 4.1560\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1471 - mae: 3.5954 - val_loss: 44.2376 - val_mae: 4.2506\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.9330 - mae: 3.7805 - val_loss: 44.8352 - val_mae: 4.1707\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4461 - mae: 3.6657 - val_loss: 43.8722 - val_mae: 4.2534\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6394 - mae: 3.7247 - val_loss: 44.1034 - val_mae: 4.2004\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.8611 - mae: 3.6989 - val_loss: 44.4221 - val_mae: 4.1404\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4636 - mae: 3.6786 - val_loss: 43.9470 - val_mae: 4.1726\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2380 - mae: 3.6612 - val_loss: 43.7953 - val_mae: 4.1765\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1836 - mae: 3.6695 - val_loss: 43.4318 - val_mae: 4.2231\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9982 - mae: 3.6831 - val_loss: 43.8659 - val_mae: 4.1525\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0631 - mae: 3.6467 - val_loss: 43.4182 - val_mae: 4.1790\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.6854 - mae: 3.6314 - val_loss: 44.1255 - val_mae: 4.1214\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8190 - mae: 3.6456 - val_loss: 43.4528 - val_mae: 4.1780\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5559 - mae: 3.6899 - val_loss: 43.6116 - val_mae: 4.1412\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0639 - mae: 3.6434 - val_loss: 43.3567 - val_mae: 4.1809\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0702 - mae: 3.6890 - val_loss: 44.5382 - val_mae: 4.0909\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7258 - mae: 3.5530 - val_loss: 43.0535 - val_mae: 4.2891\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2006 - mae: 3.7128 - val_loss: 43.7837 - val_mae: 4.1240\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8498 - mae: 3.6308 - val_loss: 43.0695 - val_mae: 4.2245\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0428 - mae: 3.6708 - val_loss: 44.2940 - val_mae: 4.0781\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.4010 - mae: 3.6757 - val_loss: 43.6329 - val_mae: 4.1348\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.5325 - mae: 3.6257 - val_loss: 43.3404 - val_mae: 4.1226\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8559 - mae: 3.6334 - val_loss: 43.3250 - val_mae: 4.1733\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2995 - mae: 3.6298 - val_loss: 43.4129 - val_mae: 4.1588\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.4690 - mae: 3.6661 - val_loss: 44.1275 - val_mae: 4.0848\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 26.3726 - mae: 3.5874 - val_loss: 43.1080 - val_mae: 4.1842\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.4557 - mae: 3.6504 - val_loss: 43.7284 - val_mae: 4.0947\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.7997 - mae: 3.5651 - val_loss: 43.3729 - val_mae: 4.1450\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0164 - mae: 3.6189 - val_loss: 43.3913 - val_mae: 4.1349\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0966 - mae: 3.5901 - val_loss: 43.3134 - val_mae: 4.1688\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0268 - mae: 3.6016 - val_loss: 44.0061 - val_mae: 4.1071\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 25.6725 - mae: 3.5689 - val_loss: 43.0627 - val_mae: 4.1687\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1347 - mae: 3.6742 - val_loss: 43.8944 - val_mae: 4.1244\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0525 - mae: 3.6208 - val_loss: 43.8622 - val_mae: 4.1115\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0832 - mae: 3.6007 - val_loss: 43.6419 - val_mae: 4.1421\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0931 - mae: 3.5984 - val_loss: 43.0815 - val_mae: 4.2243\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1905 - mae: 3.5718 - val_loss: 42.9324 - val_mae: 4.2365\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2099 - mae: 3.6857 - val_loss: 44.8635 - val_mae: 4.0894\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.2024 - mae: 3.6069 - val_loss: 43.5538 - val_mae: 4.1404\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.2610 - mae: 3.5207 - val_loss: 43.1938 - val_mae: 4.1643\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.7216 - mae: 3.6273 - val_loss: 44.3782 - val_mae: 4.1141\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.7822 - mae: 3.6027 - val_loss: 43.5235 - val_mae: 4.1473\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.3211 - mae: 3.5356 - val_loss: 43.0344 - val_mae: 4.2047\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.4746 - mae: 3.5811 - val_loss: 43.3429 - val_mae: 4.1705\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.8347 - mae: 3.5910 - val_loss: 43.1924 - val_mae: 4.1840\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.9391 - mae: 3.6083 - val_loss: 42.9547 - val_mae: 4.2385\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1103 - mae: 3.6464 - val_loss: 44.4392 - val_mae: 4.1236\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.6301 - mae: 3.5738 - val_loss: 43.4429 - val_mae: 4.1935\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 47.9504 - mae: 4.3714\n",
      "Mean Absolute Error on Test Data: 4.3713555335998535\n",
      "8/8 [==============================] - 0s 1000us/step\n",
      "R-squared: 0.0618945251868428\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 78.6574 - mae: 6.9373 - val_loss: 55.7131 - val_mae: 5.3985\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.4114 - mae: 4.2837 - val_loss: 26.4093 - val_mae: 3.7882\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.7320 - mae: 3.8546 - val_loss: 26.2501 - val_mae: 3.8380\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.4280 - mae: 3.7613 - val_loss: 25.9380 - val_mae: 3.7057\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.0643 - mae: 3.6534 - val_loss: 25.9066 - val_mae: 3.8384\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 28.3744 - mae: 3.7526 - val_loss: 25.6475 - val_mae: 3.7776\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 27.8489 - mae: 3.6933 - val_loss: 25.4867 - val_mae: 3.7305\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.6049 - mae: 3.7111 - val_loss: 25.4954 - val_mae: 3.8026\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.7732 - mae: 3.7208 - val_loss: 25.4444 - val_mae: 3.6580\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.3669 - mae: 3.7021 - val_loss: 25.2146 - val_mae: 3.7273\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.3743 - mae: 3.6654 - val_loss: 25.2095 - val_mae: 3.7721\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5213 - mae: 3.6475 - val_loss: 25.5411 - val_mae: 3.8928\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.4179 - mae: 3.7096 - val_loss: 25.1176 - val_mae: 3.7544\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0521 - mae: 3.6587 - val_loss: 25.0844 - val_mae: 3.6865\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5304 - mae: 3.6273 - val_loss: 25.7217 - val_mae: 3.9489\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0807 - mae: 3.7210 - val_loss: 24.9951 - val_mae: 3.7237\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.1769 - mae: 3.6766 - val_loss: 25.0023 - val_mae: 3.6894\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.2018 - mae: 3.6499 - val_loss: 24.9850 - val_mae: 3.7298\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.8119 - mae: 3.6730 - val_loss: 24.9226 - val_mae: 3.7230\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7905 - mae: 3.5777 - val_loss: 24.9979 - val_mae: 3.7931\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.5603 - mae: 3.7238 - val_loss: 24.9323 - val_mae: 3.7486\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 27.0606 - mae: 3.6376 - val_loss: 24.9474 - val_mae: 3.7726\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7440 - mae: 3.6091 - val_loss: 25.0276 - val_mae: 3.8159\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9695 - mae: 3.6820 - val_loss: 24.8208 - val_mae: 3.6979\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.9546 - mae: 3.6781 - val_loss: 24.7967 - val_mae: 3.7133\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.5654 - mae: 3.6222 - val_loss: 24.9923 - val_mae: 3.8271\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2009 - mae: 3.6186 - val_loss: 24.8140 - val_mae: 3.7694\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.7055 - mae: 3.6326 - val_loss: 24.8517 - val_mae: 3.7869\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2253 - mae: 3.6410 - val_loss: 24.7552 - val_mae: 3.7563\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2238 - mae: 3.6876 - val_loss: 24.7231 - val_mae: 3.6765\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2556 - mae: 3.6522 - val_loss: 24.7848 - val_mae: 3.7032\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1804 - mae: 3.6066 - val_loss: 24.8012 - val_mae: 3.7795\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 26.1529 - mae: 3.5966 - val_loss: 25.1442 - val_mae: 3.8805\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.3855 - mae: 3.6514 - val_loss: 24.6148 - val_mae: 3.7165\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.1636 - mae: 3.6526 - val_loss: 24.5700 - val_mae: 3.7095\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.8209 - mae: 3.5806 - val_loss: 24.8893 - val_mae: 3.8386\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2727 - mae: 3.6512 - val_loss: 24.5961 - val_mae: 3.7342\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2516 - mae: 3.6358 - val_loss: 24.5050 - val_mae: 3.7022\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.5688 - mae: 3.6190 - val_loss: 24.4986 - val_mae: 3.7321\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.4862 - mae: 3.5937 - val_loss: 24.4369 - val_mae: 3.6816\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.7762 - mae: 3.6023 - val_loss: 24.3546 - val_mae: 3.7286\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.8043 - mae: 3.5651 - val_loss: 24.6974 - val_mae: 3.8328\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.0310 - mae: 3.5836 - val_loss: 24.8143 - val_mae: 3.8553\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.5674 - mae: 3.6341 - val_loss: 24.3486 - val_mae: 3.6662\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.5467 - mae: 3.5590 - val_loss: 24.5593 - val_mae: 3.7945\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.7628 - mae: 3.5644 - val_loss: 24.3804 - val_mae: 3.7398\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.5566 - mae: 3.6197 - val_loss: 24.3378 - val_mae: 3.7194\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.6693 - mae: 3.5750 - val_loss: 24.3856 - val_mae: 3.7699\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.6652 - mae: 3.5436 - val_loss: 24.3003 - val_mae: 3.7428\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 25.3306 - mae: 3.6084 - val_loss: 24.2827 - val_mae: 3.7360\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 28.5846 - mae: 3.9448\n",
      "Mean Absolute Error on Test Data: 3.9448401927948\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.019581019810072653\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 13ms/step - loss: 6.4358 - mae: 1.6981 - val_loss: 3.7096 - val_mae: 1.2460\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 4.0396 - mae: 1.4905 - val_loss: 3.6801 - val_mae: 1.4022\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9338 - mae: 1.4326 - val_loss: 3.6258 - val_mae: 1.2973\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.9477 - mae: 1.4297 - val_loss: 3.6343 - val_mae: 1.3262\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 3.9448 - mae: 1.4174 - val_loss: 3.6465 - val_mae: 1.3552\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8786 - mae: 1.4303 - val_loss: 3.6496 - val_mae: 1.3029\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8310 - mae: 1.4154 - val_loss: 3.6614 - val_mae: 1.3496\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8534 - mae: 1.4381 - val_loss: 3.6824 - val_mae: 1.3479\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9362 - mae: 1.4283 - val_loss: 3.6584 - val_mae: 1.3309\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.9212 - mae: 1.4444 - val_loss: 3.6701 - val_mae: 1.3396\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 3.8309 - mae: 1.4002 - val_loss: 3.6627 - val_mae: 1.3372\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8639 - mae: 1.4539 - val_loss: 3.6838 - val_mae: 1.3167\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8178 - mae: 1.3680 - val_loss: 3.6940 - val_mae: 1.3737\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8332 - mae: 1.4551 - val_loss: 3.6997 - val_mae: 1.3178\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8822 - mae: 1.4319 - val_loss: 3.6712 - val_mae: 1.3319\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.8109 - mae: 1.3856 - val_loss: 3.6652 - val_mae: 1.3333\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7961 - mae: 1.4179 - val_loss: 3.6872 - val_mae: 1.3453\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7994 - mae: 1.4381 - val_loss: 3.6940 - val_mae: 1.3267\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7856 - mae: 1.4009 - val_loss: 3.7182 - val_mae: 1.3952\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7568 - mae: 1.4202 - val_loss: 3.7161 - val_mae: 1.3175\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7626 - mae: 1.4148 - val_loss: 3.7056 - val_mae: 1.3352\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7945 - mae: 1.4167 - val_loss: 3.7006 - val_mae: 1.3694\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7182 - mae: 1.3604 - val_loss: 3.7092 - val_mae: 1.3849\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7799 - mae: 1.4323 - val_loss: 3.7026 - val_mae: 1.3336\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7367 - mae: 1.3929 - val_loss: 3.7060 - val_mae: 1.3698\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7759 - mae: 1.4484 - val_loss: 3.7458 - val_mae: 1.3318\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7511 - mae: 1.3842 - val_loss: 3.7171 - val_mae: 1.3549\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7440 - mae: 1.4113 - val_loss: 3.7021 - val_mae: 1.3536\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6549 - mae: 1.4067 - val_loss: 3.7450 - val_mae: 1.3263\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7060 - mae: 1.3805 - val_loss: 3.8336 - val_mae: 1.4488\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6876 - mae: 1.4140 - val_loss: 3.7511 - val_mae: 1.3312\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6762 - mae: 1.3753 - val_loss: 3.7644 - val_mae: 1.4022\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7790 - mae: 1.4149 - val_loss: 3.7280 - val_mae: 1.3790\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6668 - mae: 1.4446 - val_loss: 3.8042 - val_mae: 1.3179\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.8840 - mae: 1.4011 - val_loss: 3.7979 - val_mae: 1.4371\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7385 - mae: 1.4078 - val_loss: 3.7630 - val_mae: 1.3617\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 3.7086 - mae: 1.4077 - val_loss: 3.7678 - val_mae: 1.3607\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6390 - mae: 1.3827 - val_loss: 3.7299 - val_mae: 1.3811\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6276 - mae: 1.3836 - val_loss: 3.7360 - val_mae: 1.3712\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6298 - mae: 1.3987 - val_loss: 3.7969 - val_mae: 1.3785\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6517 - mae: 1.3903 - val_loss: 3.8358 - val_mae: 1.4350\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6520 - mae: 1.3848 - val_loss: 3.7993 - val_mae: 1.3528\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6782 - mae: 1.3843 - val_loss: 3.8217 - val_mae: 1.4046\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7125 - mae: 1.3934 - val_loss: 3.7590 - val_mae: 1.3768\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6505 - mae: 1.4217 - val_loss: 3.8480 - val_mae: 1.3298\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6658 - mae: 1.3874 - val_loss: 3.8206 - val_mae: 1.3701\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.7030 - mae: 1.3987 - val_loss: 3.8056 - val_mae: 1.4095\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6073 - mae: 1.3992 - val_loss: 3.7864 - val_mae: 1.3787\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6583 - mae: 1.3710 - val_loss: 3.8132 - val_mae: 1.4101\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 3.6491 - mae: 1.3839 - val_loss: 3.7924 - val_mae: 1.4102\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.5580 - mae: 1.3790\n",
      "Mean Absolute Error on Test Data: 1.3789936304092407\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.13451971142496977\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 19ms/step - loss: 2.4936 - mae: 1.1531 - val_loss: 1.2339 - val_mae: 0.7638\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2762 - mae: 0.7716 - val_loss: 1.0780 - val_mae: 0.8804\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2665 - mae: 0.8331 - val_loss: 0.9654 - val_mae: 0.8048\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2325 - mae: 0.7782 - val_loss: 0.9442 - val_mae: 0.7754\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.2110 - mae: 0.7609 - val_loss: 0.9136 - val_mae: 0.7848\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.2118 - mae: 0.7997 - val_loss: 0.8940 - val_mae: 0.7853\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1814 - mae: 0.7606 - val_loss: 0.8890 - val_mae: 0.7563\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1693 - mae: 0.7530 - val_loss: 0.8673 - val_mae: 0.7809\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1511 - mae: 0.7697 - val_loss: 0.8530 - val_mae: 0.7597\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1752 - mae: 0.7516 - val_loss: 0.8491 - val_mae: 0.7460\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1703 - mae: 0.7795 - val_loss: 0.8439 - val_mae: 0.7741\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1581 - mae: 0.7601 - val_loss: 0.8436 - val_mae: 0.7618\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.1252 - mae: 0.7474 - val_loss: 0.8347 - val_mae: 0.7629\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1406 - mae: 0.7531 - val_loss: 0.8337 - val_mae: 0.7701\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0911 - mae: 0.7370 - val_loss: 0.8261 - val_mae: 0.7572\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1126 - mae: 0.7482 - val_loss: 0.8260 - val_mae: 0.7721\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1101 - mae: 0.7710 - val_loss: 0.8173 - val_mae: 0.7693\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1241 - mae: 0.7584 - val_loss: 0.7929 - val_mae: 0.7459\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1471 - mae: 0.7631 - val_loss: 0.7852 - val_mae: 0.7411\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0986 - mae: 0.7406 - val_loss: 0.7931 - val_mae: 0.7584\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.0926 - mae: 0.7618 - val_loss: 0.7869 - val_mae: 0.7454\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1173 - mae: 0.7242 - val_loss: 0.7862 - val_mae: 0.7489\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0840 - mae: 0.7696 - val_loss: 0.7748 - val_mae: 0.7432\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0709 - mae: 0.7236 - val_loss: 0.7709 - val_mae: 0.7385\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0643 - mae: 0.7442 - val_loss: 0.7723 - val_mae: 0.7449\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0988 - mae: 0.7602 - val_loss: 0.7667 - val_mae: 0.7338\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.1135 - mae: 0.7485 - val_loss: 0.7641 - val_mae: 0.7292\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.1236 - mae: 0.7254 - val_loss: 0.7747 - val_mae: 0.7591\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0856 - mae: 0.7769 - val_loss: 0.7829 - val_mae: 0.7550\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0723 - mae: 0.7510 - val_loss: 0.7799 - val_mae: 0.7326\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0846 - mae: 0.7244 - val_loss: 0.7841 - val_mae: 0.7453\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0949 - mae: 0.7360 - val_loss: 0.7706 - val_mae: 0.7445\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.0654 - mae: 0.7367 - val_loss: 0.7608 - val_mae: 0.7392\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0803 - mae: 0.7296 - val_loss: 0.7519 - val_mae: 0.7306\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0371 - mae: 0.7257 - val_loss: 0.7569 - val_mae: 0.7499\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0599 - mae: 0.7561 - val_loss: 0.7663 - val_mae: 0.7472\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0518 - mae: 0.7042 - val_loss: 0.7636 - val_mae: 0.7437\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 1.0834 - mae: 0.7482 - val_loss: 0.7555 - val_mae: 0.7455\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0566 - mae: 0.7177 - val_loss: 0.7506 - val_mae: 0.7410\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0988 - mae: 0.8059 - val_loss: 0.7608 - val_mae: 0.7453\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0925 - mae: 0.7020 - val_loss: 0.7669 - val_mae: 0.7373\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0989 - mae: 0.7688 - val_loss: 0.7593 - val_mae: 0.7435\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0650 - mae: 0.7227 - val_loss: 0.7542 - val_mae: 0.7361\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0669 - mae: 0.7328 - val_loss: 0.7564 - val_mae: 0.7400\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 1.0505 - mae: 0.7295 - val_loss: 0.7458 - val_mae: 0.7384\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 1.0472 - mae: 0.7243 - val_loss: 0.7431 - val_mae: 0.7425\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0572 - mae: 0.7355 - val_loss: 0.7552 - val_mae: 0.7524\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0723 - mae: 0.7402 - val_loss: 0.7508 - val_mae: 0.7425\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0622 - mae: 0.7491 - val_loss: 0.7366 - val_mae: 0.7325\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 1.0502 - mae: 0.7221 - val_loss: 0.7422 - val_mae: 0.7299\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 1.7165 - mae: 0.8287\n",
      "Mean Absolute Error on Test Data: 0.8286576271057129\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.10866978672004102\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 19.1202 - mae: 2.9153 - val_loss: 17.4056 - val_mae: 2.4225\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.6001 - mae: 2.3692 - val_loss: 15.6358 - val_mae: 2.5534\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.3419 - mae: 2.3794 - val_loss: 15.5639 - val_mae: 2.4079\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0746 - mae: 2.2899 - val_loss: 15.3880 - val_mae: 2.4544\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1552 - mae: 2.3460 - val_loss: 15.3614 - val_mae: 2.4301\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.1143 - mae: 2.3127 - val_loss: 15.2954 - val_mae: 2.4506\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1826 - mae: 2.3378 - val_loss: 15.3130 - val_mae: 2.4111\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0833 - mae: 2.3021 - val_loss: 15.2433 - val_mae: 2.4220\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0965 - mae: 2.3421 - val_loss: 15.3204 - val_mae: 2.3941\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1063 - mae: 2.3112 - val_loss: 15.1742 - val_mae: 2.4844\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0849 - mae: 2.3291 - val_loss: 15.3414 - val_mae: 2.3947\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1018 - mae: 2.3370 - val_loss: 15.1977 - val_mae: 2.4148\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 11.0819 - mae: 2.3344 - val_loss: 15.2507 - val_mae: 2.3973\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0866 - mae: 2.2814 - val_loss: 15.1642 - val_mae: 2.4330\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0098 - mae: 2.3098 - val_loss: 15.2052 - val_mae: 2.4042\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9831 - mae: 2.3424 - val_loss: 15.2260 - val_mae: 2.3937\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.1316 - mae: 2.2910 - val_loss: 15.0410 - val_mae: 2.4768\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0723 - mae: 2.3227 - val_loss: 15.2755 - val_mae: 2.3922\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0383 - mae: 2.2860 - val_loss: 15.0999 - val_mae: 2.4950\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.8582 - mae: 2.3277 - val_loss: 15.1702 - val_mae: 2.4113\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9577 - mae: 2.3148 - val_loss: 15.2163 - val_mae: 2.4597\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8804 - mae: 2.3354 - val_loss: 15.2433 - val_mae: 2.3869\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7266 - mae: 2.2493 - val_loss: 14.9899 - val_mae: 2.4536\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 11.0565 - mae: 2.3064 - val_loss: 15.0836 - val_mae: 2.4258\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8414 - mae: 2.3200 - val_loss: 14.9951 - val_mae: 2.4312\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7979 - mae: 2.3144 - val_loss: 15.2794 - val_mae: 2.3946\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 10.8312 - mae: 2.2587 - val_loss: 15.2114 - val_mae: 2.4283\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8103 - mae: 2.3025 - val_loss: 15.0264 - val_mae: 2.4276\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8839 - mae: 2.3112 - val_loss: 15.1427 - val_mae: 2.4000\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7347 - mae: 2.3158 - val_loss: 15.0662 - val_mae: 2.4607\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7477 - mae: 2.3168 - val_loss: 15.2418 - val_mae: 2.3871\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7577 - mae: 2.2569 - val_loss: 14.9138 - val_mae: 2.4724\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.6778 - mae: 2.3155 - val_loss: 15.2262 - val_mae: 2.3968\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7435 - mae: 2.2920 - val_loss: 15.0338 - val_mae: 2.4196\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9015 - mae: 2.2699 - val_loss: 14.9967 - val_mae: 2.4593\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7769 - mae: 2.2728 - val_loss: 15.1850 - val_mae: 2.3998\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8330 - mae: 2.3062 - val_loss: 15.0850 - val_mae: 2.4343\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7700 - mae: 2.2598 - val_loss: 14.9530 - val_mae: 2.4422\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 10.7882 - mae: 2.3210 - val_loss: 15.1549 - val_mae: 2.3938\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8407 - mae: 2.2902 - val_loss: 15.0945 - val_mae: 2.4095\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7906 - mae: 2.2696 - val_loss: 15.1027 - val_mae: 2.4040\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7522 - mae: 2.2977 - val_loss: 15.0505 - val_mae: 2.4128\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7155 - mae: 2.2434 - val_loss: 15.0022 - val_mae: 2.4553\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8390 - mae: 2.3781 - val_loss: 15.2102 - val_mae: 2.3917\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.9081 - mae: 2.2275 - val_loss: 15.0865 - val_mae: 2.4448\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7174 - mae: 2.2872 - val_loss: 15.0914 - val_mae: 2.4059\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7956 - mae: 2.3311 - val_loss: 15.3132 - val_mae: 2.3873\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7826 - mae: 2.2639 - val_loss: 15.0273 - val_mae: 2.4294\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.7095 - mae: 2.3178 - val_loss: 15.0829 - val_mae: 2.3947\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 10.8887 - mae: 2.3171 - val_loss: 15.3517 - val_mae: 2.3803\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.9413 - mae: 2.2398\n",
      "Mean Absolute Error on Test Data: 2.2398476600646973\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.06649286108958685\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "19/19 [==============================] - 1s 12ms/step - loss: 8.7000 - mae: 1.7677 - val_loss: 3.0610 - val_mae: 1.4201\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.7601 - mae: 1.7179 - val_loss: 3.0691 - val_mae: 1.4256\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5061 - mae: 1.5906 - val_loss: 3.0579 - val_mae: 1.4129\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.5437 - mae: 1.6233 - val_loss: 3.1146 - val_mae: 1.4393\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4031 - mae: 1.5934 - val_loss: 3.1050 - val_mae: 1.4261\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2844 - mae: 1.5869 - val_loss: 3.1453 - val_mae: 1.4399\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3762 - mae: 1.6375 - val_loss: 3.1275 - val_mae: 1.4075\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3578 - mae: 1.5804 - val_loss: 3.1401 - val_mae: 1.4076\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.4033 - mae: 1.5919 - val_loss: 3.2099 - val_mae: 1.4530\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.3038 - mae: 1.6194 - val_loss: 3.1320 - val_mae: 1.3748\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3576 - mae: 1.5518 - val_loss: 3.2644 - val_mae: 1.4721\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3556 - mae: 1.6176 - val_loss: 3.1728 - val_mae: 1.4205\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2628 - mae: 1.5877 - val_loss: 3.2149 - val_mae: 1.4417\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2593 - mae: 1.5959 - val_loss: 3.2559 - val_mae: 1.4538\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1601 - mae: 1.5744 - val_loss: 3.2056 - val_mae: 1.4361\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.2884 - mae: 1.5861 - val_loss: 3.2552 - val_mae: 1.4583\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2982 - mae: 1.5653 - val_loss: 3.1902 - val_mae: 1.4215\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3088 - mae: 1.5934 - val_loss: 3.3545 - val_mae: 1.4989\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2754 - mae: 1.6013 - val_loss: 3.2668 - val_mae: 1.4637\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1574 - mae: 1.6062 - val_loss: 3.2553 - val_mae: 1.4526\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2244 - mae: 1.5539 - val_loss: 3.3386 - val_mae: 1.4961\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2310 - mae: 1.6061 - val_loss: 3.2114 - val_mae: 1.4323\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.3134 - mae: 1.5954 - val_loss: 3.2601 - val_mae: 1.4640\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1934 - mae: 1.5622 - val_loss: 3.3070 - val_mae: 1.4805\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1013 - mae: 1.5745 - val_loss: 3.2546 - val_mae: 1.4501\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.2519 - mae: 1.5885 - val_loss: 3.2370 - val_mae: 1.4400\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1321 - mae: 1.5784 - val_loss: 3.2711 - val_mae: 1.4466\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1314 - mae: 1.5580 - val_loss: 3.2538 - val_mae: 1.4495\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1663 - mae: 1.5844 - val_loss: 3.2352 - val_mae: 1.4157\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1721 - mae: 1.5871 - val_loss: 3.2512 - val_mae: 1.4496\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0779 - mae: 1.5426 - val_loss: 3.3647 - val_mae: 1.4877\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9706 - mae: 1.5701 - val_loss: 3.3303 - val_mae: 1.4584\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9811 - mae: 1.5583 - val_loss: 3.3076 - val_mae: 1.4541\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0642 - mae: 1.5382 - val_loss: 3.2589 - val_mae: 1.4419\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1470 - mae: 1.6232 - val_loss: 3.2649 - val_mae: 1.4092\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1116 - mae: 1.5310 - val_loss: 3.5092 - val_mae: 1.5419\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 6.1586 - mae: 1.5944 - val_loss: 3.2608 - val_mae: 1.4275\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0916 - mae: 1.5588 - val_loss: 3.4544 - val_mae: 1.5188\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1471 - mae: 1.5767 - val_loss: 3.4999 - val_mae: 1.5355\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.1512 - mae: 1.5781 - val_loss: 3.2380 - val_mae: 1.4175\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0602 - mae: 1.5731 - val_loss: 3.2706 - val_mae: 1.4414\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9222 - mae: 1.5277 - val_loss: 3.3573 - val_mae: 1.4703\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0318 - mae: 1.5498 - val_loss: 3.4947 - val_mae: 1.5229\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 6.0347 - mae: 1.5718 - val_loss: 3.4383 - val_mae: 1.4983\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9483 - mae: 1.5527 - val_loss: 3.5221 - val_mae: 1.5370\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9402 - mae: 1.5505 - val_loss: 3.4817 - val_mae: 1.5209\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9092 - mae: 1.5801 - val_loss: 3.3558 - val_mae: 1.4293\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9704 - mae: 1.5610 - val_loss: 3.3508 - val_mae: 1.4475\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 0s 4ms/step - loss: 5.7856 - mae: 1.5184 - val_loss: 3.3962 - val_mae: 1.4745\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 0s 3ms/step - loss: 5.9643 - mae: 1.5554 - val_loss: 3.5699 - val_mae: 1.5346\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 6.1098 - mae: 1.7671\n",
      "Mean Absolute Error on Test Data: 1.7670818567276\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12616426118170632\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 14ms/step - loss: 4.2649 - mae: 1.3998 - val_loss: 1.3689 - val_mae: 0.9096\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.6164 - mae: 1.1476 - val_loss: 1.6238 - val_mae: 1.0593\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.5227 - mae: 1.1219 - val_loss: 1.4059 - val_mae: 0.9502\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4886 - mae: 1.0912 - val_loss: 1.4339 - val_mae: 0.9618\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4975 - mae: 1.1008 - val_loss: 1.4879 - val_mae: 0.9930\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.5041 - mae: 1.1012 - val_loss: 1.4626 - val_mae: 0.9777\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4314 - mae: 1.0739 - val_loss: 1.4599 - val_mae: 0.9738\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4245 - mae: 1.1128 - val_loss: 1.5257 - val_mae: 1.0093\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4239 - mae: 1.1018 - val_loss: 1.4044 - val_mae: 0.9380\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4716 - mae: 1.0908 - val_loss: 1.4763 - val_mae: 0.9836\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.4099 - mae: 1.0687 - val_loss: 1.4822 - val_mae: 0.9817\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3437 - mae: 1.0620 - val_loss: 1.5006 - val_mae: 0.9886\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3457 - mae: 1.0733 - val_loss: 1.5206 - val_mae: 1.0012\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3272 - mae: 1.1108 - val_loss: 1.4930 - val_mae: 0.9828\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3417 - mae: 1.0509 - val_loss: 1.6283 - val_mae: 1.0500\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3380 - mae: 1.0829 - val_loss: 1.4647 - val_mae: 0.9679\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3281 - mae: 1.0506 - val_loss: 1.6525 - val_mae: 1.0603\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3563 - mae: 1.1000 - val_loss: 1.4927 - val_mae: 0.9762\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3478 - mae: 1.0634 - val_loss: 1.5678 - val_mae: 1.0187\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2987 - mae: 1.0881 - val_loss: 1.5189 - val_mae: 0.9914\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2995 - mae: 1.0502 - val_loss: 1.6118 - val_mae: 1.0371\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3404 - mae: 1.1041 - val_loss: 1.5396 - val_mae: 0.9932\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2887 - mae: 1.0589 - val_loss: 1.7118 - val_mae: 1.0706\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3593 - mae: 1.0998 - val_loss: 1.4629 - val_mae: 0.9407\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3191 - mae: 1.0362 - val_loss: 1.7127 - val_mae: 1.0772\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3255 - mae: 1.0483 - val_loss: 1.5487 - val_mae: 1.0047\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2474 - mae: 1.0557 - val_loss: 1.5777 - val_mae: 1.0113\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3103 - mae: 1.0701 - val_loss: 1.4766 - val_mae: 0.9658\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2894 - mae: 1.0687 - val_loss: 1.5003 - val_mae: 0.9896\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2923 - mae: 1.0394 - val_loss: 1.5087 - val_mae: 0.9926\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 2.3034 - mae: 1.0432 - val_loss: 1.5268 - val_mae: 0.9921\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3109 - mae: 1.0745 - val_loss: 1.5787 - val_mae: 1.0069\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2911 - mae: 1.0402 - val_loss: 1.6247 - val_mae: 1.0429\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2569 - mae: 1.0673 - val_loss: 1.4707 - val_mae: 0.9625\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3168 - mae: 1.0398 - val_loss: 1.5890 - val_mae: 1.0244\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2686 - mae: 1.0757 - val_loss: 1.5374 - val_mae: 0.9933\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2468 - mae: 1.0337 - val_loss: 1.5692 - val_mae: 1.0123\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 6ms/step - loss: 2.2703 - mae: 1.0428 - val_loss: 1.5623 - val_mae: 1.0010\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3132 - mae: 1.0741 - val_loss: 1.4918 - val_mae: 0.9637\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2634 - mae: 1.0482 - val_loss: 1.6115 - val_mae: 1.0349\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2842 - mae: 1.0123 - val_loss: 1.5898 - val_mae: 1.0319\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2944 - mae: 1.0940 - val_loss: 1.5207 - val_mae: 0.9870\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2767 - mae: 1.0845 - val_loss: 1.5025 - val_mae: 0.9643\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 2.3677 - mae: 1.0336 - val_loss: 1.6556 - val_mae: 1.0594\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2550 - mae: 1.0670 - val_loss: 1.5209 - val_mae: 0.9902\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2678 - mae: 1.0803 - val_loss: 1.5887 - val_mae: 1.0090\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2691 - mae: 1.0070 - val_loss: 1.6892 - val_mae: 1.0645\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2422 - mae: 1.0674 - val_loss: 1.5259 - val_mae: 0.9809\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.2336 - mae: 1.0345 - val_loss: 1.6126 - val_mae: 1.0311\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 2.3132 - mae: 1.0913 - val_loss: 1.5106 - val_mae: 0.9811\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 3.2137 - mae: 1.2351\n",
      "Mean Absolute Error on Test Data: 1.2351163625717163\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04626508381071759\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 11ms/step - loss: 13.4525 - mae: 2.4356 - val_loss: 15.2492 - val_mae: 2.4157\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0988 - mae: 2.1511 - val_loss: 14.7122 - val_mae: 2.4024\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.7250 - mae: 1.9682 - val_loss: 14.4648 - val_mae: 2.3786\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6823 - mae: 2.0056 - val_loss: 14.3793 - val_mae: 2.3687\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6880 - mae: 2.0333 - val_loss: 14.3727 - val_mae: 2.3594\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.7116 - mae: 2.0178 - val_loss: 14.2394 - val_mae: 2.3538\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.6431 - mae: 1.9834 - val_loss: 14.2232 - val_mae: 2.3432\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5537 - mae: 2.0014 - val_loss: 14.1985 - val_mae: 2.3370\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4429 - mae: 1.9765 - val_loss: 14.0503 - val_mae: 2.3374\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5724 - mae: 2.0342 - val_loss: 13.9376 - val_mae: 2.3354\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4714 - mae: 2.0149 - val_loss: 14.1361 - val_mae: 2.3150\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4212 - mae: 1.9625 - val_loss: 13.7714 - val_mae: 2.3339\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3845 - mae: 2.0031 - val_loss: 14.0482 - val_mae: 2.3137\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.3909 - mae: 1.9649 - val_loss: 13.8601 - val_mae: 2.3241\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4618 - mae: 2.0060 - val_loss: 13.9865 - val_mae: 2.3195\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4218 - mae: 2.0069 - val_loss: 13.8329 - val_mae: 2.3223\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4266 - mae: 1.9567 - val_loss: 13.5700 - val_mae: 2.3508\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3856 - mae: 2.0173 - val_loss: 14.0658 - val_mae: 2.3004\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4824 - mae: 1.9984 - val_loss: 14.1564 - val_mae: 2.2976\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4792 - mae: 2.0018 - val_loss: 13.7696 - val_mae: 2.3188\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8.5086 - mae: 1.9951 - val_loss: 13.6844 - val_mae: 2.3270\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4120 - mae: 2.0010 - val_loss: 14.1192 - val_mae: 2.3001\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4009 - mae: 1.9921 - val_loss: 13.6180 - val_mae: 2.3395\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4480 - mae: 1.9850 - val_loss: 14.1036 - val_mae: 2.2923\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3491 - mae: 1.9679 - val_loss: 13.6857 - val_mae: 2.3210\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4101 - mae: 1.9833 - val_loss: 13.8838 - val_mae: 2.3010\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.2440 - mae: 2.0016 - val_loss: 13.7165 - val_mae: 2.3080\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3845 - mae: 1.9666 - val_loss: 13.6712 - val_mae: 2.3119\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3475 - mae: 2.0021 - val_loss: 13.8723 - val_mae: 2.2975\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3777 - mae: 1.9778 - val_loss: 13.5005 - val_mae: 2.3368\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.2960 - mae: 1.9620 - val_loss: 13.7524 - val_mae: 2.2980\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4358 - mae: 2.0370 - val_loss: 13.8236 - val_mae: 2.2916\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3320 - mae: 1.9894 - val_loss: 13.6304 - val_mae: 2.3201\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.2721 - mae: 1.9601 - val_loss: 13.5502 - val_mae: 2.3146\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3145 - mae: 1.9955 - val_loss: 13.8502 - val_mae: 2.2947\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1997 - mae: 1.9275 - val_loss: 13.4144 - val_mae: 2.3427\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1722 - mae: 1.9872 - val_loss: 13.7418 - val_mae: 2.2980\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3022 - mae: 1.9967 - val_loss: 13.6039 - val_mae: 2.3011\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1636 - mae: 1.9798 - val_loss: 14.0214 - val_mae: 2.2822\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 8.1672 - mae: 1.9675 - val_loss: 13.7756 - val_mae: 2.2934\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1964 - mae: 1.9895 - val_loss: 13.6439 - val_mae: 2.2885\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1078 - mae: 1.9383 - val_loss: 13.5835 - val_mae: 2.3086\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0002 - mae: 1.9674 - val_loss: 13.6004 - val_mae: 2.2910\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.1105 - mae: 1.9589 - val_loss: 13.6436 - val_mae: 2.2911\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 8.1799 - mae: 1.9804 - val_loss: 13.3974 - val_mae: 2.3109\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0249 - mae: 1.9734 - val_loss: 13.6827 - val_mae: 2.2875\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0837 - mae: 1.9456 - val_loss: 13.2085 - val_mae: 2.3269\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.0608 - mae: 2.0082 - val_loss: 13.9508 - val_mae: 2.2735\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9077 - mae: 1.9453 - val_loss: 13.2799 - val_mae: 2.3351\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9713 - mae: 1.9757 - val_loss: 13.2455 - val_mae: 2.3366\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.6978 - mae: 1.7980\n",
      "Mean Absolute Error on Test Data: 1.7980355024337769\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.08632096356687946\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 95.7512 - mae: 7.5795 - val_loss: 73.9244 - val_mae: 5.9495\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 45.5775 - mae: 4.7742 - val_loss: 41.6564 - val_mae: 4.7993\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 38.0973 - mae: 4.5356 - val_loss: 42.0716 - val_mae: 4.4234\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 37.1143 - mae: 4.3020 - val_loss: 41.2769 - val_mae: 4.4980\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.9383 - mae: 4.4094 - val_loss: 40.9797 - val_mae: 4.4981\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.4407 - mae: 4.3388 - val_loss: 40.7755 - val_mae: 4.4825\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.0455 - mae: 4.4902 - val_loss: 41.7935 - val_mae: 4.3496\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7274 - mae: 4.3036 - val_loss: 40.3190 - val_mae: 4.4921\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.5582 - mae: 4.5138 - val_loss: 42.3921 - val_mae: 4.3141\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.1993 - mae: 4.3231 - val_loss: 40.1976 - val_mae: 4.5287\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.9064 - mae: 4.3841 - val_loss: 40.5763 - val_mae: 4.4082\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8335 - mae: 4.3239 - val_loss: 40.4008 - val_mae: 4.4251\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7975 - mae: 4.3765 - val_loss: 40.0231 - val_mae: 4.4553\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3743 - mae: 4.3485 - val_loss: 40.8371 - val_mae: 4.3547\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.7955 - mae: 4.4087 - val_loss: 40.1126 - val_mae: 4.4263\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.9178 - mae: 4.3219 - val_loss: 39.9919 - val_mae: 4.4672\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.0628 - mae: 4.4035 - val_loss: 40.1110 - val_mae: 4.4428\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5854 - mae: 4.4248 - val_loss: 41.1188 - val_mae: 4.3173\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5206 - mae: 4.3081 - val_loss: 39.8830 - val_mae: 4.4515\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8889 - mae: 4.4095 - val_loss: 40.1368 - val_mae: 4.4107\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 37.0208 - mae: 4.3823 - val_loss: 40.9388 - val_mae: 4.3173\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3177 - mae: 4.3303 - val_loss: 39.9223 - val_mae: 4.4254\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4745 - mae: 4.2997 - val_loss: 40.0638 - val_mae: 4.4197\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.9587 - mae: 4.4039 - val_loss: 40.7680 - val_mae: 4.3424\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3731 - mae: 4.2852 - val_loss: 39.8633 - val_mae: 4.4534\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.8371 - mae: 4.4290 - val_loss: 40.0775 - val_mae: 4.3844\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3462 - mae: 4.3407 - val_loss: 39.9705 - val_mae: 4.4264\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.9036 - mae: 4.3330 - val_loss: 40.2945 - val_mae: 4.3727\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.2868 - mae: 4.2939 - val_loss: 39.8824 - val_mae: 4.4791\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3370 - mae: 4.3501 - val_loss: 40.2850 - val_mae: 4.3931\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.2660 - mae: 4.3463 - val_loss: 39.8326 - val_mae: 4.4771\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5631 - mae: 4.3598 - val_loss: 39.8583 - val_mae: 4.4232\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.1906 - mae: 4.3319 - val_loss: 40.2582 - val_mae: 4.3817\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4318 - mae: 4.3736 - val_loss: 40.0280 - val_mae: 4.4068\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4081 - mae: 4.3441 - val_loss: 40.5362 - val_mae: 4.3342\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3696 - mae: 4.3279 - val_loss: 40.0976 - val_mae: 4.3739\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.9135 - mae: 4.3646 - val_loss: 40.4354 - val_mae: 4.3598\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4029 - mae: 4.3941 - val_loss: 40.7400 - val_mae: 4.3216\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.4183 - mae: 4.3184 - val_loss: 39.8909 - val_mae: 4.4241\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.7038 - mae: 4.3618 - val_loss: 40.4727 - val_mae: 4.3337\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.3582 - mae: 4.3010 - val_loss: 39.8508 - val_mae: 4.4656\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.0855 - mae: 4.3721 - val_loss: 40.3668 - val_mae: 4.3506\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 36.4412 - mae: 4.3723 - val_loss: 40.6320 - val_mae: 4.3188\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.5309 - mae: 4.3068 - val_loss: 40.0502 - val_mae: 4.4404\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 36.0765 - mae: 4.3784 - val_loss: 40.3693 - val_mae: 4.3777\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.8605 - mae: 4.2799 - val_loss: 39.7759 - val_mae: 4.4689\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.5128 - mae: 4.3075 - val_loss: 40.7922 - val_mae: 4.3173\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.7913 - mae: 4.2972 - val_loss: 39.9303 - val_mae: 4.4179\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.9228 - mae: 4.3247 - val_loss: 40.9300 - val_mae: 4.3202\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 35.8325 - mae: 4.2964 - val_loss: 39.9664 - val_mae: 4.4311\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 47.6491 - mae: 4.4861\n",
      "Mean Absolute Error on Test Data: 4.486099720001221\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: -0.005460902476088769\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "21/21 [==============================] - 1s 11ms/step - loss: 14.0242 - mae: 2.5525 - val_loss: 9.5802 - val_mae: 1.8903\n",
      "Epoch 2/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4410 - mae: 2.0287 - val_loss: 7.8400 - val_mae: 1.9099\n",
      "Epoch 3/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.7209 - mae: 1.9621 - val_loss: 7.8938 - val_mae: 1.8081\n",
      "Epoch 4/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.8117 - mae: 1.9507 - val_loss: 7.8154 - val_mae: 1.8212\n",
      "Epoch 5/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.7118 - mae: 1.9498 - val_loss: 7.7919 - val_mae: 1.8117\n",
      "Epoch 6/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6478 - mae: 1.9424 - val_loss: 7.7610 - val_mae: 1.8405\n",
      "Epoch 7/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.6153 - mae: 1.9753 - val_loss: 7.9181 - val_mae: 1.7898\n",
      "Epoch 8/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6669 - mae: 1.8966 - val_loss: 7.7585 - val_mae: 1.8634\n",
      "Epoch 9/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6015 - mae: 1.9747 - val_loss: 7.8015 - val_mae: 1.8345\n",
      "Epoch 10/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5517 - mae: 1.9595 - val_loss: 7.8538 - val_mae: 1.8071\n",
      "Epoch 11/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5544 - mae: 1.9101 - val_loss: 7.8019 - val_mae: 1.8711\n",
      "Epoch 12/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5647 - mae: 1.9743 - val_loss: 7.9827 - val_mae: 1.7858\n",
      "Epoch 13/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.6603 - mae: 1.9135 - val_loss: 7.7651 - val_mae: 1.8192\n",
      "Epoch 14/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5059 - mae: 1.9201 - val_loss: 7.7456 - val_mae: 1.8595\n",
      "Epoch 15/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5151 - mae: 1.9303 - val_loss: 7.7800 - val_mae: 1.8135\n",
      "Epoch 16/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5112 - mae: 1.9288 - val_loss: 7.7642 - val_mae: 1.8496\n",
      "Epoch 17/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4939 - mae: 1.9556 - val_loss: 7.9037 - val_mae: 1.7939\n",
      "Epoch 18/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5014 - mae: 1.8907 - val_loss: 7.7247 - val_mae: 1.8600\n",
      "Epoch 19/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.5286 - mae: 1.9651 - val_loss: 8.0296 - val_mae: 1.7862\n",
      "Epoch 20/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.6004 - mae: 1.9370 - val_loss: 7.8270 - val_mae: 1.8128\n",
      "Epoch 21/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4591 - mae: 1.9318 - val_loss: 7.7853 - val_mae: 1.8127\n",
      "Epoch 22/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5059 - mae: 1.9448 - val_loss: 7.8199 - val_mae: 1.8078\n",
      "Epoch 23/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3481 - mae: 1.9010 - val_loss: 7.8734 - val_mae: 1.8309\n",
      "Epoch 24/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.4811 - mae: 1.9671 - val_loss: 8.0217 - val_mae: 1.7940\n",
      "Epoch 25/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.5187 - mae: 1.9199 - val_loss: 7.7762 - val_mae: 1.8357\n",
      "Epoch 26/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3260 - mae: 1.8881 - val_loss: 7.7915 - val_mae: 1.8303\n",
      "Epoch 27/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4118 - mae: 1.9547 - val_loss: 7.8172 - val_mae: 1.8145\n",
      "Epoch 28/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3244 - mae: 1.8983 - val_loss: 7.8531 - val_mae: 1.8685\n",
      "Epoch 29/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4056 - mae: 1.9120 - val_loss: 7.7622 - val_mae: 1.8169\n",
      "Epoch 30/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.4897 - mae: 1.9486 - val_loss: 7.8234 - val_mae: 1.8209\n",
      "Epoch 31/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3136 - mae: 1.9197 - val_loss: 7.8415 - val_mae: 1.8136\n",
      "Epoch 32/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.2956 - mae: 1.9244 - val_loss: 7.9448 - val_mae: 1.8313\n",
      "Epoch 33/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4287 - mae: 1.9412 - val_loss: 7.8599 - val_mae: 1.8260\n",
      "Epoch 34/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.4248 - mae: 1.9242 - val_loss: 7.8773 - val_mae: 1.8140\n",
      "Epoch 35/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3580 - mae: 1.9177 - val_loss: 7.8246 - val_mae: 1.8327\n",
      "Epoch 36/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3292 - mae: 1.9173 - val_loss: 7.9710 - val_mae: 1.8074\n",
      "Epoch 37/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3502 - mae: 1.8908 - val_loss: 7.7790 - val_mae: 1.8649\n",
      "Epoch 38/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3713 - mae: 1.9391 - val_loss: 7.9570 - val_mae: 1.7997\n",
      "Epoch 39/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.2864 - mae: 1.8996 - val_loss: 7.9042 - val_mae: 1.8331\n",
      "Epoch 40/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.2730 - mae: 1.9379 - val_loss: 7.9975 - val_mae: 1.8026\n",
      "Epoch 41/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3278 - mae: 1.8987 - val_loss: 7.8851 - val_mae: 1.8200\n",
      "Epoch 42/50\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 7.1541 - mae: 1.8967 - val_loss: 7.8951 - val_mae: 1.8105\n",
      "Epoch 43/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3933 - mae: 1.9117 - val_loss: 7.8084 - val_mae: 1.8453\n",
      "Epoch 44/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.2681 - mae: 1.9406 - val_loss: 7.9207 - val_mae: 1.8168\n",
      "Epoch 45/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.2596 - mae: 1.9160 - val_loss: 8.0066 - val_mae: 1.8140\n",
      "Epoch 46/50\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.2034 - mae: 1.8814 - val_loss: 7.9142 - val_mae: 1.8323\n",
      "Epoch 47/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.0220 - mae: 1.8621 - val_loss: 7.9290 - val_mae: 1.8824\n",
      "Epoch 48/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.3852 - mae: 1.9289 - val_loss: 8.1821 - val_mae: 1.8043\n",
      "Epoch 49/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.1528 - mae: 1.8985 - val_loss: 7.8661 - val_mae: 1.8634\n",
      "Epoch 50/50\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.1951 - mae: 1.9013 - val_loss: 7.9170 - val_mae: 1.8447\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.0114 - mae: 1.8004\n",
      "Mean Absolute Error on Test Data: 1.8004193305969238\n",
      "7/7 [==============================] - 0s 1000us/step\n",
      "R-squared: -0.10154553622330553\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 21.7450 - mae: 3.6739 - val_loss: 15.2767 - val_mae: 2.6917\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 9.1776 - mae: 2.2890 - val_loss: 12.7870 - val_mae: 2.7172\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.8867 - mae: 2.2509 - val_loss: 12.7872 - val_mae: 2.6028\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7347 - mae: 2.2179 - val_loss: 12.7081 - val_mae: 2.6347\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7540 - mae: 2.2491 - val_loss: 12.7127 - val_mae: 2.6364\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7531 - mae: 2.2277 - val_loss: 12.7201 - val_mae: 2.6375\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7706 - mae: 2.2677 - val_loss: 12.8785 - val_mae: 2.5939\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6580 - mae: 2.2074 - val_loss: 12.7316 - val_mae: 2.6440\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7435 - mae: 2.2532 - val_loss: 12.7871 - val_mae: 2.6119\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7270 - mae: 2.2383 - val_loss: 12.7814 - val_mae: 2.6191\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5605 - mae: 2.2058 - val_loss: 12.7740 - val_mae: 2.6202\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.7833 - mae: 2.2406 - val_loss: 12.7963 - val_mae: 2.6093\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6108 - mae: 2.2144 - val_loss: 12.7916 - val_mae: 2.6117\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4831 - mae: 2.2145 - val_loss: 12.7731 - val_mae: 2.6136\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.4773 - mae: 2.1698 - val_loss: 12.7920 - val_mae: 2.6769\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5229 - mae: 2.2576 - val_loss: 12.9413 - val_mae: 2.5848\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6649 - mae: 2.1772 - val_loss: 12.7854 - val_mae: 2.6905\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5465 - mae: 2.2576 - val_loss: 13.1943 - val_mae: 2.5718\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6018 - mae: 2.2228 - val_loss: 12.7238 - val_mae: 2.6278\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4223 - mae: 2.1847 - val_loss: 12.7369 - val_mae: 2.6362\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5497 - mae: 2.2146 - val_loss: 12.7775 - val_mae: 2.6087\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5185 - mae: 2.2157 - val_loss: 12.7553 - val_mae: 2.6205\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6876 - mae: 2.1986 - val_loss: 12.9040 - val_mae: 2.7216\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6122 - mae: 2.2539 - val_loss: 12.9845 - val_mae: 2.5822\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5431 - mae: 2.2230 - val_loss: 12.7801 - val_mae: 2.6381\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3913 - mae: 2.1858 - val_loss: 12.8060 - val_mae: 2.6054\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4079 - mae: 2.1832 - val_loss: 12.7655 - val_mae: 2.6147\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5118 - mae: 2.2148 - val_loss: 12.8083 - val_mae: 2.6006\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4959 - mae: 2.1923 - val_loss: 12.7496 - val_mae: 2.6407\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3105 - mae: 2.2080 - val_loss: 12.8364 - val_mae: 2.5939\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5509 - mae: 2.2246 - val_loss: 13.0827 - val_mae: 2.5743\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.5021 - mae: 2.1998 - val_loss: 12.7983 - val_mae: 2.6357\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2513 - mae: 2.1948 - val_loss: 12.9473 - val_mae: 2.5881\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.6013 - mae: 2.2234 - val_loss: 12.8920 - val_mae: 2.5963\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4278 - mae: 2.1912 - val_loss: 12.8215 - val_mae: 2.6125\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3263 - mae: 2.1809 - val_loss: 12.8138 - val_mae: 2.6220\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.3580 - mae: 2.1872 - val_loss: 12.8459 - val_mae: 2.6080\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.4912 - mae: 2.2123 - val_loss: 12.9473 - val_mae: 2.5891\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3044 - mae: 2.1739 - val_loss: 12.8194 - val_mae: 2.6564\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3474 - mae: 2.2139 - val_loss: 12.7754 - val_mae: 2.6365\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2026 - mae: 2.1795 - val_loss: 12.8522 - val_mae: 2.5990\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2079 - mae: 2.1524 - val_loss: 12.7433 - val_mae: 2.6336\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.3453 - mae: 2.2055 - val_loss: 12.7881 - val_mae: 2.6174\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3522 - mae: 2.1772 - val_loss: 12.8224 - val_mae: 2.6028\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2439 - mae: 2.1929 - val_loss: 12.8225 - val_mae: 2.5990\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3099 - mae: 2.1805 - val_loss: 12.8335 - val_mae: 2.6168\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2059 - mae: 2.1777 - val_loss: 12.8404 - val_mae: 2.6227\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.3092 - mae: 2.1801 - val_loss: 12.8625 - val_mae: 2.6058\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 8.2323 - mae: 2.1668 - val_loss: 12.8998 - val_mae: 2.5938\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 8.2933 - mae: 2.1630 - val_loss: 12.8509 - val_mae: 2.6161\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 9.8936 - mae: 2.4075\n",
      "Mean Absolute Error on Test Data: 2.4075376987457275\n",
      "7/7 [==============================] - 0s 919us/step\n",
      "R-squared: -0.008772592165898763\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 72.1522 - mae: 6.8603 - val_loss: 64.2549 - val_mae: 5.9124\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 37.4213 - mae: 4.1616 - val_loss: 30.4556 - val_mae: 3.4990\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6543 - mae: 3.5789 - val_loss: 29.6458 - val_mae: 3.5744\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5159 - mae: 3.4040 - val_loss: 29.8537 - val_mae: 3.5222\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6491 - mae: 3.4483 - val_loss: 29.6465 - val_mae: 3.5641\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5958 - mae: 3.4423 - val_loss: 29.8689 - val_mae: 3.5200\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.6112 - mae: 3.4299 - val_loss: 29.8272 - val_mae: 3.5211\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5447 - mae: 3.4312 - val_loss: 30.1642 - val_mae: 3.4966\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1290 - mae: 3.3484 - val_loss: 29.7121 - val_mae: 3.5391\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4857 - mae: 3.4452 - val_loss: 29.9065 - val_mae: 3.5078\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1021 - mae: 3.4534 - val_loss: 29.9139 - val_mae: 3.5085\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.7487 - mae: 3.3610 - val_loss: 29.7042 - val_mae: 3.5389\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4287 - mae: 3.4259 - val_loss: 29.8222 - val_mae: 3.5143\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2703 - mae: 3.4442 - val_loss: 30.0394 - val_mae: 3.4997\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.5694 - mae: 3.4252 - val_loss: 29.6913 - val_mae: 3.5516\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9954 - mae: 3.4439 - val_loss: 30.1453 - val_mae: 3.4923\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4955 - mae: 3.3765 - val_loss: 29.8036 - val_mae: 3.5270\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7901 - mae: 3.3964 - val_loss: 29.9523 - val_mae: 3.5057\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9060 - mae: 3.3905 - val_loss: 29.8435 - val_mae: 3.5224\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.0638 - mae: 3.4213 - val_loss: 29.9615 - val_mae: 3.5088\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2652 - mae: 3.4082 - val_loss: 29.7035 - val_mae: 3.5667\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4578 - mae: 3.4889 - val_loss: 30.0366 - val_mae: 3.5014\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9757 - mae: 3.3423 - val_loss: 29.8979 - val_mae: 3.5112\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3805 - mae: 3.4865 - val_loss: 30.3298 - val_mae: 3.4901\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.4218 - mae: 3.4058 - val_loss: 30.1162 - val_mae: 3.5026\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 22.9865 - mae: 3.3886 - val_loss: 29.7675 - val_mae: 3.5276\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.3205 - mae: 3.3854 - val_loss: 30.0085 - val_mae: 3.5034\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8761 - mae: 3.4244 - val_loss: 30.1817 - val_mae: 3.4987\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7068 - mae: 3.4226 - val_loss: 29.7774 - val_mae: 3.5448\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.9620 - mae: 3.3608 - val_loss: 29.7369 - val_mae: 3.5547\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5972 - mae: 3.4192 - val_loss: 30.0363 - val_mae: 3.5171\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.1115 - mae: 3.4087 - val_loss: 29.8365 - val_mae: 3.5293\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.2259 - mae: 3.4025 - val_loss: 29.8316 - val_mae: 3.5243\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1375 - mae: 3.3656 - val_loss: 29.7128 - val_mae: 3.5384\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.5313 - mae: 3.4080 - val_loss: 29.8077 - val_mae: 3.5332\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9415 - mae: 3.4106 - val_loss: 29.6774 - val_mae: 3.5465\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5813 - mae: 3.4061 - val_loss: 29.9016 - val_mae: 3.5198\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 23.0635 - mae: 3.3770 - val_loss: 29.7299 - val_mae: 3.5544\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.5164 - mae: 3.3979 - val_loss: 30.3341 - val_mae: 3.5122\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 22.6750 - mae: 3.4028 - val_loss: 29.7904 - val_mae: 3.5447\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.4785 - mae: 3.3340 - val_loss: 29.6818 - val_mae: 3.5571\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8945 - mae: 3.4344 - val_loss: 30.6077 - val_mae: 3.5108\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.4764 - mae: 3.4053 - val_loss: 30.0296 - val_mae: 3.5128\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6835 - mae: 3.3661 - val_loss: 29.5825 - val_mae: 3.5559\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3938 - mae: 3.3895 - val_loss: 29.5821 - val_mae: 3.5676\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.8944 - mae: 3.3937 - val_loss: 29.5859 - val_mae: 3.5407\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.6810 - mae: 3.4105 - val_loss: 30.2498 - val_mae: 3.5076\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.9072 - mae: 3.4194 - val_loss: 29.7021 - val_mae: 3.5278\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.3847 - mae: 3.3718 - val_loss: 29.9750 - val_mae: 3.5042\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7868 - mae: 3.4205 - val_loss: 29.8966 - val_mae: 3.5181\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 27.0601 - mae: 3.3415\n",
      "Mean Absolute Error on Test Data: 3.341501235961914\n",
      "8/8 [==============================] - 0s 857us/step\n",
      "R-squared: 0.08001430135399323\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 1s 12ms/step - loss: 12.0854 - mae: 2.4484 - val_loss: 8.3596 - val_mae: 1.8117\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.9205 - mae: 1.9163 - val_loss: 7.2864 - val_mae: 1.9061\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7693 - mae: 1.8503 - val_loss: 7.4508 - val_mae: 1.8045\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6291 - mae: 1.8213 - val_loss: 7.3261 - val_mae: 1.8478\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7185 - mae: 1.8595 - val_loss: 7.4131 - val_mae: 1.8181\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.7066 - mae: 1.8331 - val_loss: 7.4255 - val_mae: 1.8169\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6285 - mae: 1.8523 - val_loss: 7.3819 - val_mae: 1.8311\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.6650 - mae: 1.8308 - val_loss: 7.4683 - val_mae: 1.8179\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5805 - mae: 1.8566 - val_loss: 7.4728 - val_mae: 1.8125\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6034 - mae: 1.8285 - val_loss: 7.3821 - val_mae: 1.8533\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6347 - mae: 1.7951 - val_loss: 7.4153 - val_mae: 1.8262\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.6029 - mae: 1.8773 - val_loss: 7.5699 - val_mae: 1.8070\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5175 - mae: 1.8387 - val_loss: 7.4370 - val_mae: 1.8343\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.6093 - mae: 1.7987 - val_loss: 7.3841 - val_mae: 1.8493\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.5532 - mae: 1.8733 - val_loss: 7.4389 - val_mae: 1.8351\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4803 - mae: 1.8109 - val_loss: 7.4442 - val_mae: 1.8499\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4795 - mae: 1.7991 - val_loss: 7.3887 - val_mae: 1.8662\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4346 - mae: 1.8075 - val_loss: 7.4980 - val_mae: 1.8422\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4895 - mae: 1.8465 - val_loss: 7.6677 - val_mae: 1.8357\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.5830 - mae: 1.8354 - val_loss: 7.4589 - val_mae: 1.8778\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4834 - mae: 1.8261 - val_loss: 7.4863 - val_mae: 1.8755\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4006 - mae: 1.8100 - val_loss: 7.5394 - val_mae: 1.8644\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3087 - mae: 1.8131 - val_loss: 7.5566 - val_mae: 1.8592\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4276 - mae: 1.7910 - val_loss: 7.5270 - val_mae: 1.9615\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4728 - mae: 1.8474 - val_loss: 7.6608 - val_mae: 1.8400\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.4620 - mae: 1.8227 - val_loss: 7.6195 - val_mae: 1.8490\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3034 - mae: 1.8010 - val_loss: 7.5352 - val_mae: 1.8687\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2526 - mae: 1.8074 - val_loss: 7.6535 - val_mae: 1.8684\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2949 - mae: 1.7956 - val_loss: 7.4993 - val_mae: 1.8827\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 0s 5ms/step - loss: 6.3869 - mae: 1.7975 - val_loss: 7.5431 - val_mae: 1.9149\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2559 - mae: 1.8023 - val_loss: 7.6766 - val_mae: 1.8836\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2640 - mae: 1.7983 - val_loss: 7.5991 - val_mae: 1.9602\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2343 - mae: 1.7916 - val_loss: 7.6672 - val_mae: 1.8889\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1915 - mae: 1.7901 - val_loss: 7.6551 - val_mae: 1.8849\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.3092 - mae: 1.8052 - val_loss: 7.6375 - val_mae: 1.8974\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.0849 - mae: 1.7950 - val_loss: 7.7747 - val_mae: 1.8839\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1816 - mae: 1.7614 - val_loss: 7.5754 - val_mae: 1.9501\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.3338 - mae: 1.8095 - val_loss: 7.6694 - val_mae: 1.9045\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2116 - mae: 1.8090 - val_loss: 7.6384 - val_mae: 1.9133\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1839 - mae: 1.7794 - val_loss: 7.6448 - val_mae: 1.9105\n",
      "Epoch 41/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.1804 - mae: 1.7966 - val_loss: 7.6896 - val_mae: 1.9261\n",
      "Epoch 42/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2102 - mae: 1.7925 - val_loss: 7.7062 - val_mae: 1.9784\n",
      "Epoch 43/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2104 - mae: 1.8097 - val_loss: 7.8023 - val_mae: 1.9033\n",
      "Epoch 44/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1995 - mae: 1.7932 - val_loss: 7.7444 - val_mae: 1.9136\n",
      "Epoch 45/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1915 - mae: 1.7921 - val_loss: 7.6172 - val_mae: 1.9663\n",
      "Epoch 46/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1060 - mae: 1.7805 - val_loss: 7.7025 - val_mae: 1.9162\n",
      "Epoch 47/50\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 6.2310 - mae: 1.7933 - val_loss: 7.6520 - val_mae: 1.9318\n",
      "Epoch 48/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1130 - mae: 1.7804 - val_loss: 7.6556 - val_mae: 1.9447\n",
      "Epoch 49/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.1725 - mae: 1.8182 - val_loss: 7.7740 - val_mae: 1.8989\n",
      "Epoch 50/50\n",
      "20/20 [==============================] - 0s 3ms/step - loss: 6.2684 - mae: 1.7900 - val_loss: 7.7577 - val_mae: 1.9119\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 6.9619 - mae: 1.9147\n",
      "Mean Absolute Error on Test Data: 1.9146593809127808\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.019982240606221557\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "22/22 [==============================] - 1s 11ms/step - loss: 28.0679 - mae: 3.5341 - val_loss: 9.3110 - val_mae: 2.1286\n",
      "Epoch 2/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.8903 - mae: 2.7690 - val_loss: 8.8643 - val_mae: 2.4142\n",
      "Epoch 3/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 16.0987 - mae: 2.8112 - val_loss: 7.9676 - val_mae: 2.1742\n",
      "Epoch 4/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.8078 - mae: 2.6541 - val_loss: 8.2048 - val_mae: 2.2511\n",
      "Epoch 5/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.6232 - mae: 2.7473 - val_loss: 7.9830 - val_mae: 2.1460\n",
      "Epoch 6/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3923 - mae: 2.5952 - val_loss: 8.3396 - val_mae: 2.2672\n",
      "Epoch 7/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3866 - mae: 2.7269 - val_loss: 8.2363 - val_mae: 2.2221\n",
      "Epoch 8/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2380 - mae: 2.6167 - val_loss: 8.2791 - val_mae: 2.2268\n",
      "Epoch 9/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.0162 - mae: 2.6741 - val_loss: 8.4859 - val_mae: 2.2666\n",
      "Epoch 10/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1245 - mae: 2.6164 - val_loss: 8.3838 - val_mae: 2.2339\n",
      "Epoch 11/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.1313 - mae: 2.6040 - val_loss: 8.2600 - val_mae: 2.1967\n",
      "Epoch 12/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.3752 - mae: 2.7227 - val_loss: 8.2390 - val_mae: 2.1394\n",
      "Epoch 13/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.2123 - mae: 2.5518 - val_loss: 8.5564 - val_mae: 2.2794\n",
      "Epoch 14/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 15.2212 - mae: 2.6599 - val_loss: 8.1676 - val_mae: 2.1524\n",
      "Epoch 15/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9118 - mae: 2.6251 - val_loss: 8.5342 - val_mae: 2.2582\n",
      "Epoch 16/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 15.0002 - mae: 2.6154 - val_loss: 8.4022 - val_mae: 2.2175\n",
      "Epoch 17/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9975 - mae: 2.6360 - val_loss: 8.4127 - val_mae: 2.2299\n",
      "Epoch 18/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7372 - mae: 2.5750 - val_loss: 8.3879 - val_mae: 2.2128\n",
      "Epoch 19/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.7204 - mae: 2.5744 - val_loss: 8.2958 - val_mae: 2.1885\n",
      "Epoch 20/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8012 - mae: 2.6004 - val_loss: 8.6371 - val_mae: 2.2824\n",
      "Epoch 21/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8783 - mae: 2.5855 - val_loss: 8.3794 - val_mae: 2.2170\n",
      "Epoch 22/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9210 - mae: 2.6015 - val_loss: 8.3207 - val_mae: 2.1985\n",
      "Epoch 23/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8335 - mae: 2.6170 - val_loss: 8.2581 - val_mae: 2.1794\n",
      "Epoch 24/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8604 - mae: 2.5726 - val_loss: 8.4039 - val_mae: 2.2132\n",
      "Epoch 25/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8506 - mae: 2.6366 - val_loss: 8.4165 - val_mae: 2.2260\n",
      "Epoch 26/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7980 - mae: 2.5978 - val_loss: 8.2245 - val_mae: 2.1499\n",
      "Epoch 27/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7487 - mae: 2.6109 - val_loss: 8.2603 - val_mae: 2.1636\n",
      "Epoch 28/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.8302 - mae: 2.5688 - val_loss: 8.4522 - val_mae: 2.2358\n",
      "Epoch 29/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5397 - mae: 2.5513 - val_loss: 8.2677 - val_mae: 2.1907\n",
      "Epoch 30/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6320 - mae: 2.5941 - val_loss: 8.2813 - val_mae: 2.1871\n",
      "Epoch 31/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5764 - mae: 2.5628 - val_loss: 8.5552 - val_mae: 2.2590\n",
      "Epoch 32/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5590 - mae: 2.5814 - val_loss: 8.4687 - val_mae: 2.2408\n",
      "Epoch 33/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8209 - mae: 2.6110 - val_loss: 8.3016 - val_mae: 2.1809\n",
      "Epoch 34/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6371 - mae: 2.6158 - val_loss: 8.3164 - val_mae: 2.1472\n",
      "Epoch 35/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5091 - mae: 2.5203 - val_loss: 8.7485 - val_mae: 2.3093\n",
      "Epoch 36/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6592 - mae: 2.5653 - val_loss: 8.4195 - val_mae: 2.2260\n",
      "Epoch 37/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4642 - mae: 2.5746 - val_loss: 8.3168 - val_mae: 2.1708\n",
      "Epoch 38/50\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 14.4923 - mae: 2.5587 - val_loss: 8.4320 - val_mae: 2.2197\n",
      "Epoch 39/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.8450 - mae: 2.6181 - val_loss: 8.3028 - val_mae: 2.1299\n",
      "Epoch 40/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.9393 - mae: 2.6121 - val_loss: 8.6443 - val_mae: 2.2821\n",
      "Epoch 41/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.4612 - mae: 2.5310 - val_loss: 8.2996 - val_mae: 2.1987\n",
      "Epoch 42/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5308 - mae: 2.5938 - val_loss: 8.4221 - val_mae: 2.2061\n",
      "Epoch 43/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.7206 - mae: 2.5828 - val_loss: 8.4086 - val_mae: 2.2143\n",
      "Epoch 44/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3850 - mae: 2.5192 - val_loss: 8.3204 - val_mae: 2.1919\n",
      "Epoch 45/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3634 - mae: 2.5761 - val_loss: 8.5517 - val_mae: 2.2504\n",
      "Epoch 46/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.5910 - mae: 2.5640 - val_loss: 8.2830 - val_mae: 2.1708\n",
      "Epoch 47/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.2872 - mae: 2.5829 - val_loss: 8.1707 - val_mae: 2.1486\n",
      "Epoch 48/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3826 - mae: 2.5630 - val_loss: 8.2607 - val_mae: 2.1717\n",
      "Epoch 49/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.6766 - mae: 2.5285 - val_loss: 8.5788 - val_mae: 2.2620\n",
      "Epoch 50/50\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 14.3978 - mae: 2.5756 - val_loss: 8.4806 - val_mae: 2.2307\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 16.0223 - mae: 2.8525\n",
      "Mean Absolute Error on Test Data: 2.8524749279022217\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.05311387696082848\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 58.4690 - mae: 5.9438 - val_loss: 39.5731 - val_mae: 4.5248\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 26.2774 - mae: 3.5826 - val_loss: 22.6236 - val_mae: 3.5532\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.7205 - mae: 3.6711 - val_loss: 21.8213 - val_mae: 3.2414\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1705 - mae: 3.4260 - val_loss: 21.3936 - val_mae: 3.2986\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 22.1517 - mae: 3.5808 - val_loss: 21.3862 - val_mae: 3.2386\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.8033 - mae: 3.4374 - val_loss: 21.2889 - val_mae: 3.2540\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.8361 - mae: 3.4921 - val_loss: 21.2779 - val_mae: 3.2344\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9371 - mae: 3.5052 - val_loss: 21.4392 - val_mae: 3.2205\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7415 - mae: 3.4024 - val_loss: 21.2225 - val_mae: 3.3025\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.9365 - mae: 3.5625 - val_loss: 21.3246 - val_mae: 3.2311\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1681 - mae: 3.4101 - val_loss: 21.2245 - val_mae: 3.2458\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4433 - mae: 3.4392 - val_loss: 21.3415 - val_mae: 3.2257\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4127 - mae: 3.4580 - val_loss: 21.3682 - val_mae: 3.2333\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.5444 - mae: 3.3991 - val_loss: 21.2382 - val_mae: 3.2617\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4184 - mae: 3.4325 - val_loss: 21.2051 - val_mae: 3.2627\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.3333 - mae: 3.4348 - val_loss: 21.3016 - val_mae: 3.2449\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1551 - mae: 3.3912 - val_loss: 21.1863 - val_mae: 3.2935\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3431 - mae: 3.4327 - val_loss: 21.4654 - val_mae: 3.2381\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4706 - mae: 3.4240 - val_loss: 21.3708 - val_mae: 3.3546\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4291 - mae: 3.4554 - val_loss: 21.2854 - val_mae: 3.2905\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4900 - mae: 3.4389 - val_loss: 21.3092 - val_mae: 3.2624\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.4699 - mae: 3.4754 - val_loss: 21.3176 - val_mae: 3.2568\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.3515 - mae: 3.4080 - val_loss: 21.3409 - val_mae: 3.3565\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.7398 - mae: 3.4961 - val_loss: 21.8762 - val_mae: 3.2493\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0026 - mae: 3.4053 - val_loss: 21.2424 - val_mae: 3.3035\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0872 - mae: 3.3922 - val_loss: 21.2064 - val_mae: 3.3044\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1042 - mae: 3.4449 - val_loss: 21.2802 - val_mae: 3.2694\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 21.3564 - mae: 3.4564 - val_loss: 21.2968 - val_mae: 3.2737\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2162 - mae: 3.4130 - val_loss: 21.2839 - val_mae: 3.2745\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9509 - mae: 3.4076 - val_loss: 21.2608 - val_mae: 3.2908\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.1302 - mae: 3.4269 - val_loss: 21.3108 - val_mae: 3.2690\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0245 - mae: 3.3913 - val_loss: 21.3031 - val_mae: 3.3214\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 5ms/step - loss: 21.1615 - mae: 3.4206 - val_loss: 21.2667 - val_mae: 3.2931\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8752 - mae: 3.4210 - val_loss: 21.2906 - val_mae: 3.2832\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0095 - mae: 3.4065 - val_loss: 21.4255 - val_mae: 3.2668\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.2423 - mae: 3.3988 - val_loss: 21.3959 - val_mae: 3.3695\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0519 - mae: 3.3912 - val_loss: 21.3368 - val_mae: 3.3219\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.8314 - mae: 3.4353 - val_loss: 21.6056 - val_mae: 3.2659\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7732 - mae: 3.3725 - val_loss: 21.3562 - val_mae: 3.3438\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7685 - mae: 3.3822 - val_loss: 21.3465 - val_mae: 3.3147\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0679 - mae: 3.4174 - val_loss: 21.3636 - val_mae: 3.2954\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.9971 - mae: 3.4339 - val_loss: 21.6341 - val_mae: 3.2585\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 21.0983 - mae: 3.4236 - val_loss: 21.3197 - val_mae: 3.2951\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7760 - mae: 3.4379 - val_loss: 22.0249 - val_mae: 3.2788\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5197 - mae: 3.3649 - val_loss: 21.3777 - val_mae: 3.3507\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 20.8479 - mae: 3.4069 - val_loss: 21.5611 - val_mae: 3.2790\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.7217 - mae: 3.3845 - val_loss: 21.5298 - val_mae: 3.3187\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.6127 - mae: 3.3697 - val_loss: 21.4495 - val_mae: 3.3636\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.5309 - mae: 3.3641 - val_loss: 21.5391 - val_mae: 3.2860\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 20.4076 - mae: 3.3153 - val_loss: 21.4229 - val_mae: 3.3539\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 19.1886 - mae: 3.2367\n",
      "Mean Absolute Error on Test Data: 3.236724615097046\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.012189589697748415\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "23/23 [==============================] - 1s 10ms/step - loss: 133.7092 - mae: 9.5218 - val_loss: 108.1829 - val_mae: 8.5614\n",
      "Epoch 2/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 79.6281 - mae: 6.3305 - val_loss: 41.5449 - val_mae: 4.5383\n",
      "Epoch 3/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.9259 - mae: 4.6254 - val_loss: 33.9498 - val_mae: 4.4286\n",
      "Epoch 4/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.5179 - mae: 4.4881 - val_loss: 34.4886 - val_mae: 4.3236\n",
      "Epoch 5/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.5651 - mae: 4.4845 - val_loss: 33.9248 - val_mae: 4.3403\n",
      "Epoch 6/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.2230 - mae: 4.5000 - val_loss: 34.3658 - val_mae: 4.3072\n",
      "Epoch 7/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7753 - mae: 4.4236 - val_loss: 34.1575 - val_mae: 4.3099\n",
      "Epoch 8/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.4550 - mae: 4.4829 - val_loss: 33.9770 - val_mae: 4.3113\n",
      "Epoch 9/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1316 - mae: 4.4183 - val_loss: 33.9988 - val_mae: 4.3020\n",
      "Epoch 10/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4421 - mae: 4.4733 - val_loss: 33.9896 - val_mae: 4.2962\n",
      "Epoch 11/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.7301 - mae: 4.4995 - val_loss: 34.0249 - val_mae: 4.2892\n",
      "Epoch 12/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8949 - mae: 4.4142 - val_loss: 33.6599 - val_mae: 4.3008\n",
      "Epoch 13/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9967 - mae: 4.4965 - val_loss: 34.1758 - val_mae: 4.2826\n",
      "Epoch 14/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.6817 - mae: 4.5128 - val_loss: 34.1175 - val_mae: 4.2782\n",
      "Epoch 15/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9651 - mae: 4.3764 - val_loss: 33.4824 - val_mae: 4.3059\n",
      "Epoch 16/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9361 - mae: 4.4783 - val_loss: 33.8671 - val_mae: 4.2743\n",
      "Epoch 17/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.4117 - mae: 4.4380 - val_loss: 33.8006 - val_mae: 4.2708\n",
      "Epoch 18/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 43.0023 - mae: 4.5066 - val_loss: 34.0360 - val_mae: 4.2649\n",
      "Epoch 19/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8630 - mae: 4.4065 - val_loss: 33.6697 - val_mae: 4.2674\n",
      "Epoch 20/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 42.2781 - mae: 4.4819 - val_loss: 34.0636 - val_mae: 4.2621\n",
      "Epoch 21/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.2320 - mae: 4.3831 - val_loss: 33.5136 - val_mae: 4.2626\n",
      "Epoch 22/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1900 - mae: 4.4210 - val_loss: 33.4566 - val_mae: 4.2670\n",
      "Epoch 23/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.3120 - mae: 4.4636 - val_loss: 34.3249 - val_mae: 4.2588\n",
      "Epoch 24/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3826 - mae: 4.3768 - val_loss: 33.3242 - val_mae: 4.2665\n",
      "Epoch 25/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8266 - mae: 4.4310 - val_loss: 33.5262 - val_mae: 4.2563\n",
      "Epoch 26/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 42.1053 - mae: 4.4643 - val_loss: 33.4307 - val_mae: 4.2532\n",
      "Epoch 27/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8055 - mae: 4.4944 - val_loss: 33.8595 - val_mae: 4.2528\n",
      "Epoch 28/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.9223 - mae: 4.4713 - val_loss: 33.5735 - val_mae: 4.2443\n",
      "Epoch 29/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.2857 - mae: 4.3625 - val_loss: 32.9397 - val_mae: 4.2646\n",
      "Epoch 30/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5860 - mae: 4.4382 - val_loss: 33.2484 - val_mae: 4.2527\n",
      "Epoch 31/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6474 - mae: 4.4643 - val_loss: 33.1111 - val_mae: 4.2559\n",
      "Epoch 32/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8718 - mae: 4.4701 - val_loss: 33.8101 - val_mae: 4.2430\n",
      "Epoch 33/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.4157 - mae: 4.3923 - val_loss: 33.2110 - val_mae: 4.2451\n",
      "Epoch 34/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6346 - mae: 4.4663 - val_loss: 33.3896 - val_mae: 4.2374\n",
      "Epoch 35/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.4859 - mae: 4.3759 - val_loss: 33.8247 - val_mae: 4.2381\n",
      "Epoch 36/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3106 - mae: 4.3651 - val_loss: 33.2567 - val_mae: 4.2429\n",
      "Epoch 37/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3905 - mae: 4.4518 - val_loss: 33.3101 - val_mae: 4.2361\n",
      "Epoch 38/50\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 41.9042 - mae: 4.4628 - val_loss: 33.3955 - val_mae: 4.2342\n",
      "Epoch 39/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3170 - mae: 4.3764 - val_loss: 33.1401 - val_mae: 4.2277\n",
      "Epoch 40/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.3509 - mae: 4.4709 - val_loss: 33.8666 - val_mae: 4.2370\n",
      "Epoch 41/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.8401 - mae: 4.4471 - val_loss: 33.0421 - val_mae: 4.2305\n",
      "Epoch 42/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.9007 - mae: 4.3849 - val_loss: 33.2879 - val_mae: 4.2277\n",
      "Epoch 43/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.6480 - mae: 4.4070 - val_loss: 33.3140 - val_mae: 4.2297\n",
      "Epoch 44/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.1945 - mae: 4.4490 - val_loss: 33.2561 - val_mae: 4.2235\n",
      "Epoch 45/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.4466 - mae: 4.4092 - val_loss: 33.2717 - val_mae: 4.2238\n",
      "Epoch 46/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.4082 - mae: 4.3856 - val_loss: 32.6964 - val_mae: 4.2237\n",
      "Epoch 47/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7535 - mae: 4.4523 - val_loss: 33.0813 - val_mae: 4.2160\n",
      "Epoch 48/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.5137 - mae: 4.5144 - val_loss: 35.2326 - val_mae: 4.2536\n",
      "Epoch 49/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 41.7389 - mae: 4.3606 - val_loss: 32.7975 - val_mae: 4.2246\n",
      "Epoch 50/50\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 40.7171 - mae: 4.4516 - val_loss: 33.1918 - val_mae: 4.2108\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 36.4930 - mae: 4.0580\n",
      "Mean Absolute Error on Test Data: 4.057954788208008\n",
      "8/8 [==============================] - 0s 931us/step\n",
      "R-squared: 0.06810840224454573\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one\n",
    "df = merged_all\n",
    "df_test = dict_test_merged\n",
    "features = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis',\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "features_gun = ['Bildirimli_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis','Gün',\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik','Bagil_nem','Ruzgar_hizi','Yagis',\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik','Bayram_Flag','Bagil_nem','Ruzgar_hizi','Yagis',\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\"]\n",
    "output_var = df\n",
    "target = 'Bildirimsiz_sum'\n",
    "# ilceler = []\n",
    "\n",
    "# NN 3\n",
    "# ilceler = ['izmir-konak','izmir-kinik']\n",
    "all_submissions = []\n",
    "for ilce in ilceler:\n",
    "    df = merged_all[ilce]\n",
    "    df_test = dict_test_merged[ilce]\n",
    "    output_var = df['Bildirimsiz_sum']\n",
    "\n",
    "    # ilcelerin numerizasyonu\n",
    "    columns_tonumerate = ['Bayram_Flag']\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "    # test csv dosyasi numerizasyon\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_transform = scaler.fit_transform(df[features])\n",
    "    feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "    feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "    feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    feature_test = scaler2.fit_transform(df_test[features])\n",
    "    feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)\n",
    "\n",
    "\n",
    "\n",
    "    X = feature_transform\n",
    "    y = output_var\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=53)\n",
    "\n",
    "    # model = tf.keras.Sequential([\n",
    "    #     tf.keras.layers.Dense(64, activation='relu', input_shape=(len(features),)),\n",
    "    #     tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #     tf.keras.layers.Dense(1)\n",
    "    # ])\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(len(features),)),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Mean Absolute Error on Test Data:\", mae)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"R-squared:\", r2)\n",
    "\n",
    "    predictions_new = model.predict(feature_test)\n",
    "    predictions_new = np.round(predictions_new).astype(int)\n",
    "    df_test['bildirimsiz_sum'] = predictions_new\n",
    "    df_test.to_csv('test_with_predictions.csv', index=False)\n",
    "\n",
    "    df_test.rename(columns={'Ilce': 'ilce'}, inplace=True)\n",
    "    df_test.rename(columns={'Tarih': 'tarih'}, inplace=True)\n",
    "    df_test.rename(columns={'Bildirimli_sum': 'bildirimli_sum'}, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    all_submissions.append(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "birlesmisunited = all_submissions[0]\n",
    "for i in range(1,len(ilceler)): # 1 2 3 4   47\n",
    "    birlesmisunited = pd.concat([birlesmisunited,all_submissions[i]])\n",
    "\n",
    "birlesmisunited['tarih'] = pd.to_datetime(birlesmisunited['tarih'])\n",
    "birlesmisunited = birlesmisunited.sort_values(by='tarih')\n",
    "birlesmisunited = birlesmisunited.sort_values(by=['tarih', 'ilce'])\n",
    "\n",
    "birlesmisunited['unique_id'] = birlesmisunited['tarih'].astype(str) + '-' + birlesmisunited['ilce']\n",
    "birlesmisunited.drop(['tarih', 'ilce'], axis=1, inplace=True)\n",
    "birlesmisunited.insert(0, 'unique_id', birlesmisunited.pop('unique_id'))\n",
    "\n",
    "for column in birlesmisunited.columns:\n",
    "    if column == 'unique_id':\n",
    "        pass\n",
    "    elif column == 'bildirimsiz_sum':\n",
    "        pass\n",
    "    else:\n",
    "        birlesmisunited.drop(column, axis=1, inplace=True)\n",
    "\n",
    "birlesmisunited.to_csv('subbb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
