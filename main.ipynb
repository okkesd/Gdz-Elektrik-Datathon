{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\\n\\n    Yapilmasi gerekenler:\\n        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\\n        - Farkli grafikler cikartmaya calis.\\n        - ML.\\n        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\\n        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\\n\\n    Sorunlar:\\n        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\\n        ortalamasi az olur. Burada farkli bir yontem bul.\\n        Gunluk maks min alinabilir\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1-) Notlar\n",
    "\"\"\"\n",
    "    Her ilce icin esit sayida veri yok. Bu sayilar ilce_tarih_sayilari degiskeninde tutulu.\n",
    "\n",
    "    Yapilmasi gerekenler:\n",
    "        - Bu grafiklere trend tahmin gibi şeyler uygulamaya calis\n",
    "        - Farkli grafikler cikartmaya calis.\n",
    "        - ML.\n",
    "        - Hava kosullarindan iyi, orta, kotu, cok kotu gibi bir bilgi cikartmaya calis. Belki burada yapay zeka\n",
    "        kullanabilirsin. orda bir formül belirlemek lazim ona göre siniflandirilir.\n",
    "\n",
    "    Sorunlar:\n",
    "        - Weather'da degerler gunluk ortalama seklinde. 1 saat firtina olsa sonra tum gun yagmur yagmasa o gunun\n",
    "        ortalamasi az olur. Burada farkli bir yontem bul.\n",
    "        Gunluk maks min alinabilir\n",
    "\"\"\"\n",
    "# 1-) read and preproccess train.csv\n",
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "# 3-) read and preprocess weather.csv\n",
    "# 4-) read and preprocess holidays.csv\n",
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi\n",
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği\n",
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli ve gerekli degil)\n",
    "# 10-) test icin birlestirme islemleri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-) Import required moduls and libraries\n",
    "\n",
    "# bildirimisiz_sum tahmin edilecek\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import math\n",
    "import os\n",
    "from unidecode import unidecode # to convert Turkish characters to English\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose as sm\n",
    "import statsmodels.api as sa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Flatten \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Activation\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           tarih         ilce  bildirimsiz_sum  bildirimli_sum\n",
      "19236 2021-01-01  izmir-konak                9               0\n",
      "19237 2021-01-02  izmir-konak               20               0\n",
      "19238 2021-01-03  izmir-konak                7               1\n",
      "19239 2021-01-04  izmir-konak               16               1\n",
      "19240 2021-01-05  izmir-konak                3               0\n",
      "...          ...          ...              ...             ...\n",
      "20355 2024-01-27  izmir-konak               12               3\n",
      "20356 2024-01-28  izmir-konak               13               1\n",
      "20357 2024-01-29  izmir-konak               22               0\n",
      "20358 2024-01-30  izmir-konak               28               1\n",
      "20359 2024-01-31  izmir-konak               16               0\n",
      "\n",
      "[1124 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1-) read and preproccess train.csv\n",
    "train = pd.read_csv(\"./train.csv\", low_memory=False) # 46.944 satir, 4 kolon\n",
    "\n",
    "#print(train[\"tarih\"]) # 1.098 farkli tarih var, 47 farkli ilce var\n",
    "\n",
    "tarihler = []\n",
    "for i in train[\"tarih\"]:\n",
    "    tarihler.append(datetime.strptime(i, \"%Y-%m-%d\"))\n",
    "train[\"tarih\"] = tarihler\n",
    "\n",
    "# print(train.dtypes)\n",
    "\n",
    "dict :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak o ilcenin verisi (1096 gun) df olarak tutar\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "print(dict[\"izmir-konak\"])\n",
    "ilceler = (list(dict.keys()))\n",
    "#print(dict.keys()) # keys olarak her ilceyi, values olarak o ilcelerin bulundugu satirlari icerir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'izmir-aliaga': 1106, 'izmir-balcova': 698, 'izmir-bayindir': 1105, 'izmir-bayrakli': 1086, 'izmir-bergama': 1120, 'izmir-beydag': 673, 'izmir-bornova': 1124, 'izmir-buca': 1115, 'izmir-cesme': 1125, 'izmir-cigli': 1071, 'izmir-dikili': 1119, 'izmir-foca': 1086, 'izmir-gaziemir': 920, 'izmir-guzelbahce': 856, 'izmir-karabaglar': 1100, 'izmir-karaburun': 1089, 'izmir-karsiyaka': 1085, 'izmir-kemalpasa': 1118, 'izmir-kinik': 914, 'izmir-kiraz': 1097, 'izmir-konak': 1124, 'izmir-menderes': 1125, 'izmir-menemen': 1119, 'izmir-narlidere': 783, 'izmir-odemis': 1124, 'izmir-seferihisar': 1111, 'izmir-selcuk': 872, 'izmir-tire': 1107, 'izmir-torbali': 1124, 'izmir-urla': 1122, 'manisa-ahmetli': 622, 'manisa-akhisar': 1126, 'manisa-alasehir': 1119, 'manisa-demirci': 938, 'manisa-golmarmara': 566, 'manisa-gordes': 1059, 'manisa-kirkagac': 950, 'manisa-koprubasi': 805, 'manisa-kula': 1039, 'manisa-salihli': 1126, 'manisa-sarigol': 1027, 'manisa-saruhanli': 1105, 'manisa-sehzadeler': 1123, 'manisa-selendi': 993, 'manisa-soma': 1086, 'manisa-turgutlu': 1121, 'manisa-yunusemre': 1125}\n"
     ]
    }
   ],
   "source": [
    "# 2-) extract ilce and keep preprocessing train.csv\n",
    "\"\"\"\n",
    "for label in dict.keys(): # her ilce icin bildirimsiz ve bildirimli olarak grafiklerini cikart\n",
    "    print(dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.bar(dict[label][\"tarih\"],dict[label][\"bildirimsiz_sum\"])\n",
    "    plt.title(label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.bar(dict[\"izmir-konak\"][\"tarih\"],dict[\"izmir-konak\"][\"bildirimsiz_sum\"])\n",
    "plt.title(label)\n",
    "plt.margins(0.01)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "# ilce tarih sayilarini al hepsinde esit veri yok\n",
    "ilce_tarih_sayilari = {}\n",
    "for name in dict.keys():\n",
    "    ilce_tarih_sayilari[name] = len(list(dict[name][\"tarih\"].to_dict().values()))\n",
    "\n",
    "print(ilce_tarih_sayilari)\n",
    "for name in dict.keys():\n",
    "    dict[name].set_index(\"tarih\", inplace=True)\n",
    "\n",
    "# train.set_index(\"tarih\", inplace=True) # train'in tarih kolonunu indexe cevir\n",
    "# print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['date', 'lat', 'lon', 't_2m:C', 'effective_cloud_cover:p',\n",
      "       'global_rad:W', 'relative_humidity_2m:p', 'wind_dir_10m:d',\n",
      "       'wind_speed_10m:ms', 'prob_precip_1h:p', 't_apparent:C', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 3-) read and preprocess weather.csv  (runtime: 9s)\n",
    "\n",
    "weather = pd.read_csv(\"./weather.csv\", low_memory=False)\n",
    "print(weather.columns) # onemli kolonlar: date, t_apparent:C (hissedilen sicaklik), wind_dir_10m:d (ruzgar yonu),\n",
    "# wind_speed_10m:ms (ruzgar hizi), prob_precip_1h:p (yagis), ilce\n",
    "\n",
    "# ilceleri ayir\n",
    "ilce_weather = {} # keys olarak ilceleri, values olarak o ilcelerin saatlik (1165 gun) hava durumlarini tutar\n",
    "for label, group in weather.groupby(\"name\"):\n",
    "    ilce_weather[label.lower()] = group\n",
    "\n",
    "# tarihleri tarih formatina cevir\n",
    "#print(ilce_weather[\"izmir-konak\"].dtypes)\n",
    "for name in ilce_weather.keys():\n",
    "\n",
    "    tarihler = [] # duzenli tarihleri burada tut\n",
    "    for date in ilce_weather[name][\"date\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "    ilce_weather[name][\"date\"] = tarihler # duzenli tarihleri date kolonuna ata\n",
    "    ilce_weather[name].set_index(\"date\", inplace=True) # tarihleri indexe cevir\n",
    "    ilce_weather[name][\"tarih\"] = ilce_weather[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "ilce_weather_day = {} # ilce hava durumu verilerini gunluk olarak tut\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_day[name] = ilce_weather[name].resample(\"D\").mean(numeric_only=True)# index'teki tarihleri gune cevir\n",
    "    ilce_weather_day[name][\"tarih\"] = ilce_weather_day[name].index\n",
    "    \n",
    "#print(ilce_weather[\"izmir-konak\"][\"tarih\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-01\n",
      "lat                               float64\n",
      "lon                               float64\n",
      "t_2m:C                            float64\n",
      "effective_cloud_cover:p           float64\n",
      "global_rad:W                      float64\n",
      "relative_humidity_2m:p            float64\n",
      "wind_dir_10m:d                    float64\n",
      "wind_speed_10m:ms                 float64\n",
      "prob_precip_1h:p                  float64\n",
      "t_apparent:C                      float64\n",
      "name                               object\n",
      "tarih                      datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3.1-) her ilcenin hava durumunda her gununu ayri ayri df lere koyup dict te tut (runtime: 3m 45s)\n",
    "\n",
    "ilce_weather_detailed = {} \n",
    "# {izmir-konak: {2021-01-01 : df , 2021-01-02 : df ,...} , manisa-akhisar: {2021-01-01 : df , 2021-01-02 : df ,...} }\n",
    "\n",
    "\n",
    "for name in ilce_weather.keys():\n",
    "    ilce_weather_detailed[name] = {}\n",
    "    for label,group in ilce_weather[name].groupby(\"date\"):\n",
    "\n",
    "        gun = label.strftime('%Y-%m-%d')\n",
    "        if gun in ilce_weather_detailed[name]:\n",
    "            ilce_weather_detailed[name][gun] = pd.concat([ilce_weather_detailed[name][gun], group], ignore_index=True)\n",
    "        else:\n",
    "            ilce_weather_detailed[name][gun] = group.copy()\n",
    "\n",
    "\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].keys())[0]))\n",
    "print((list(ilce_weather_detailed[\"izmir-konak\"].values())[0]).dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                lat      lon  Sicaklik_max  Sicaklik_min  Bulutluluk_max  \\\n",
      "2021-01-01  38.4177  27.1283          15.3          11.9            90.0   \n",
      "2021-01-02  38.4177  27.1283          17.4          11.0            57.5   \n",
      "2021-01-03  38.4177  27.1283          15.3          11.2            99.8   \n",
      "2021-01-04  38.4177  27.1283          17.7          10.5            97.4   \n",
      "2021-01-05  38.4177  27.1283          16.7          11.2            99.7   \n",
      "\n",
      "            Bulutluluk_min  Guneslilik_max  Guneslilik_min  Bagil_nem_max  \\\n",
      "2021-01-01            28.2           275.4             0.0           93.5   \n",
      "2021-01-02            10.4           374.0             0.0           90.9   \n",
      "2021-01-03            12.4           151.9             0.0           84.6   \n",
      "2021-01-04             9.2           357.0             0.0           85.6   \n",
      "2021-01-05             5.4           362.3             0.0          100.0   \n",
      "\n",
      "            Bagil_nem_min  ...         Ilce      Tarih   Sicaklik  Bulutluluk  \\\n",
      "2021-01-01           82.3  ...  izmir-konak 2021-01-01  13.095833   59.033333   \n",
      "2021-01-02           64.9  ...  izmir-konak 2021-01-02  13.379167   29.912500   \n",
      "2021-01-03           72.9  ...  izmir-konak 2021-01-03  12.587500   69.916667   \n",
      "2021-01-04           55.8  ...  izmir-konak 2021-01-04  13.783333   45.604167   \n",
      "2021-01-05           59.6  ...  izmir-konak 2021-01-05  13.895833   35.670833   \n",
      "\n",
      "            Guneslilik  Bagil_nem  Ruzgar_yonu  Ruzgar_hizi      Yagis  \\\n",
      "2021-01-01   65.212500  87.962500   137.558333     3.129167   1.137500   \n",
      "2021-01-02   91.225000  80.720833   134.820833     2.158333   1.000000   \n",
      "2021-01-03   34.962500  79.725000   142.316667     2.300000   2.520833   \n",
      "2021-01-04   79.400000  71.362500   138.641667     3.979167   1.000000   \n",
      "2021-01-05   92.666667  82.308333   161.516667     2.591667  12.279167   \n",
      "\n",
      "           Hissedilen_sicaklik  \n",
      "2021-01-01           13.891667  \n",
      "2021-01-02           14.250000  \n",
      "2021-01-03           12.937500  \n",
      "2021-01-04           13.787500  \n",
      "2021-01-05           14.850000  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "# 3.2-) 3.1'de ayrilan ilce gunlerini simdi her gun icin degerlerin min max'ini bulup ilce df'lerini tekrar olustur\n",
    "\n",
    "# runtime: 1m 4s\n",
    "weather_last = {} # key olarak ilceleri, value olarak da o ilcelerin hava durumu degerlerini min-max ile tutar\n",
    "for name in ilce_weather_detailed.keys():\n",
    "    weather_last[name] = pd.DataFrame()\n",
    "\n",
    "    for date, day_df in ilce_weather_detailed[name].items():\n",
    "        \n",
    "        # max min leri al\n",
    "        max_values = day_df.max()\n",
    "        min_values = day_df.min()\n",
    "\n",
    "        # satır oluştur\n",
    "        new_row = pd.DataFrame({\n",
    "            \"Tarih\": [datetime.strptime(date, \"%Y-%m-%d\")],\n",
    "            \"lat\": [day_df[\"lat\"].iloc[0]],\n",
    "            \"lon\": [day_df[\"lon\"].iloc[0]],\n",
    "            \"Sicaklik_max\": [max_values[\"t_2m:C\"]],\n",
    "            \"Sicaklik_min\": [min_values[\"t_2m:C\"]],\n",
    "            \"Bulutluluk_max\": [max_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Bulutluluk_min\": [min_values.get(\"effective_cloud_cover:p\", None)],\n",
    "            \"Guneslilik_max\": [max_values.get(\"global_rad:W\", None)],  \n",
    "            \"Guneslilik_min\": [min_values.get(\"global_rad:W\", None)],  \n",
    "            \"Bagil_nem_max\": [max_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Bagil_nem_min\": [min_values.get(\"relative_humidity_2m:p\", None)],\n",
    "            \"Ruzgar_yonu_max\": [max_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_yonu_min\": [min_values.get(\"wind_dir_10m:d\", None)],\n",
    "            \"Ruzgar_hizi_max\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Ruzgar_hizi_min\": [max_values.get(\"wind_speed_10m:ms\", None)],\n",
    "            \"Yagis_max\": [max_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Yagis_min\": [min_values.get(\"prob_precip_1h:p\", None)],\n",
    "            \"Hissedilen_sicaklik_max\": [max_values.get(\"t_apparent:C\", None)],\n",
    "            \"Hissedilen_sicaklik_min\": [min_values.get(\"t_apparent:C\", None)],\n",
    "            \"Ilce\": [day_df[\"name\"].iloc[0].lower()]  # Ilce ekle\n",
    "        })\n",
    "\n",
    "        # her gunu o ilcenin df ine ekle\n",
    "        weather_last[name] = pd.concat([weather_last[name], new_row], ignore_index=True)\n",
    "\n",
    "  \n",
    "new_column_names = {\n",
    "    \"lat\" : \"lat\", \"lot\" : \"lot\", \"Sicaklik_max\" : \"Sicaklik_max\", \"Sicaklik_min\" : \"Sicaklik_min\",\n",
    "    \"Bulutluluk_max\" : \"Bulutluluk_max\", \"Bulutluluk_min\" : \"Bulutluluk_min\", \"Guneslilik_max\" : \"Guneslilik_max\",\n",
    "    \"Guneslilik_min\" : \"Guneslilik_min\", \"Bagil_nem_max\" : \"Bagil_nem_max\", \"Bagil_nem_min\" : \"Bagil_nem_min\",\n",
    "    \"Ruzgar_yonu_max\" : \"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\" : \"Ruzgar_yonu_min\", \"Ruzgar_hizi_max\" : \"Ruzgar_hizi_max\",\n",
    "    \"Ruzgar_hizi_min\" : \"Ruzgar_hizi_min\", \"Yagis_max\" : \"Yagis_max\", \"Yagis_min\" : \"Yagis_min\",\n",
    "    \"Hissedilen_sicaklik_max\" : \"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\" : \"Hissedilen_sicaklik_min\",\n",
    "    \"Ilce\" : \"Ilce\", \"t_2m:C\" : \"Sicaklik\", \"effective_cloud_cover:p\" : \"Bulutluluk\", \"global_rad:W\" : \"Guneslilik\",\n",
    "    \"relative_humidity_2m:p\" : \"Bagil_nem\", \"wind_dir_10m:d\" : \"Ruzgar_yonu\", \"wind_speed_10m:ms\" : \"Ruzgar_hizi\",\n",
    "    \"prob_precip_1h:p\" : \"Yagis\", \"t_apparent:C\" : \"Hissedilen_sicaklik\", \"Tarih\" : \"Tarih\"\n",
    "}\n",
    "\n",
    "for name in weather_last.keys():\n",
    "    weather_last[name].set_index(\"Tarih\", inplace=True) # tarih kolonunu indexe ata\n",
    "    weather_last[name][\"Tarih\"] = weather_last[name].index # tarih kolonunu tekrardan olustur\n",
    "\n",
    "    weather_last[name] = pd.concat([weather_last[name], ilce_weather_day[name][[\"t_2m:C\",\"effective_cloud_cover:p\",\n",
    "    \"global_rad:W\", \"relative_humidity_2m:p\",\"wind_dir_10m:d\",\"wind_speed_10m:ms\",\"prob_precip_1h:p\",\n",
    "    \"t_apparent:C\"]]], axis=1) # mean leri ekle\n",
    "    \n",
    "    weather_last[name] = weather_last[name].rename(columns=new_column_names) # kolonlari tekrar isimlendir\n",
    "    \n",
    "\n",
    "print(weather_last[\"izmir-konak\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3.3-) K-means ile gunleri ozetleme\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "for name in weather_last.keys():\n",
    "\n",
    "    x = x = weather_last[name].drop(columns=[\"Ilce\",\"Tarih\"]).iloc[:, 3:].values\n",
    "    #print(x)\n",
    "    \"\"\"\n",
    "    wcss_list= []\n",
    "\n",
    "    for i in range(1, 11):  \n",
    "\n",
    "        kmeans = KMeans(n_clusters=i, init='k-means++', random_state= 42)  \n",
    "\n",
    "        kmeans.fit(x)  \n",
    "\n",
    "        wcss_list.append(kmeans.inertia_) \n",
    "    \n",
    "    plt.plot(range(1, 11), wcss_list)  \n",
    "    plt.title('The Elobw Method Graph')  \n",
    "    plt.xlabel('Number of clusters(k)')  \n",
    "    plt.ylabel('wcss_list')  \n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # k = 3 or 7\n",
    "    #training the K-means model on a dataset  \n",
    "\n",
    "    kmeans = KMeans(n_clusters=4, init='k-means++', random_state= 42)  \n",
    "\n",
    "    y_predict= kmeans.fit_predict(x)\n",
    "\n",
    "    # kolona ekle\n",
    "    weather_last[name][\"Ozet\"] = y_predict\n",
    "    #print(weather_last[name].head())\n",
    "    #visulaizing the clusters  \n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.scatter(x[y_predict == 0, 0], x[y_predict == 0, 1], s = 100, c = 'blue', label = 'Cluster 1') #for first cluster  \n",
    "    plt.scatter(x[y_predict == 1, 0], x[y_predict == 1, 1], s = 100, c = 'green', label = 'Cluster 2') #for second cluster  \n",
    "    plt.scatter(x[y_predict== 2, 0], x[y_predict == 2, 1], s = 100, c = 'red', label = 'Cluster 3') #for third cluster  \n",
    "    plt.scatter(x[y_predict == 3, 0], x[y_predict == 3, 1], s = 100, c = 'cyan', label = 'Cluster 4') #for fourth cluster  \n",
    "    plt.scatter(x[y_predict == 4, 0], x[y_predict == 4, 1], s = 100, c = 'magenta', label = 'Cluster 5') #for fifth cluster  \n",
    "    #plt.scatter(x[y_predict == 5, 0], x[y_predict == 5, 1], s = 100, c = 'black', label = 'Cluster 6') #for fifth cluster  \n",
    "    #plt.scatter(x[y_predict == 6, 0], x[y_predict == 6, 1], s = 100, c = 'gray', label = 'Cluster 7') #for fifth cluster  \n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 300, c = 'yellow', label = 'Centroid')\n",
    "    plt.title('Clusters of customers')  \n",
    "    plt.xlabel('Annual Income (k$)')  \n",
    "    plt.ylabel('Spending Score (1-100)')  \n",
    "    plt.legend()  \n",
    "    plt.show()  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Tatil Adı'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 4-) read and preprocess holidays.csv\n",
    "\n",
    "holiday = pd.read_csv(\"./holidays.csv\", low_memory=False)\n",
    "\n",
    "# print(holiday.head())\n",
    "\n",
    "holiday[\"tarih\"] = holiday['Yıl'].astype(str) + '-' + holiday['Ay'].astype(str) + '-' + holiday['Gün'].astype(str)\n",
    "holiday['tarih'] = pd.to_datetime(holiday['tarih'], format='%Y-%m-%d')\n",
    "holiday.set_index(\"tarih\", inplace=True)\n",
    "holiday = holiday.drop(columns=[\"Yıl\", \"Ay\", \"Gün\"])\n",
    "\n",
    "print(holiday.columns) # index olarak tarihi (YY-AA-GG), Bayram_Flag olarak da bayram ismini tutar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                tarih         ilce  bildirimsiz_sum  bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "...               ...          ...              ...             ...   \n",
      "2024-01-27 2024-01-27  izmir-konak               12               3   \n",
      "2024-01-28 2024-01-28  izmir-konak               13               1   \n",
      "2024-01-29 2024-01-29  izmir-konak               22               0   \n",
      "2024-01-30 2024-01-30  izmir-konak               28               1   \n",
      "2024-01-31 2024-01-31  izmir-konak               16               0   \n",
      "\n",
      "                 Tatil Adı  \n",
      "tarih                       \n",
      "2021-01-01  New Year's Day  \n",
      "2021-01-02             NaN  \n",
      "2021-01-03             NaN  \n",
      "2021-01-04             NaN  \n",
      "2021-01-05             NaN  \n",
      "...                    ...  \n",
      "2024-01-27             NaN  \n",
      "2024-01-28             NaN  \n",
      "2024-01-29             NaN  \n",
      "2024-01-30             NaN  \n",
      "2024-01-31             NaN  \n",
      "\n",
      "[1124 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5-) merge the train data and holidays, return a new dict called dict_holiday\n",
    "\n",
    "def merge_holiday(df1, df2=holiday):\n",
    "    merged_df = pd.merge(df1, df2[\"Tatil Adı\"], left_index=True, right_index=True, how=\"left\")\n",
    "    #df1[\"Bayramlar\"] = df2[\"Bayram_Flag\"]\n",
    "    return merged_df\n",
    "\n",
    "dict_holiday = {}\n",
    "for name in dict.keys():\n",
    "    dict_holiday[name] = merge_holiday(dict[name],holiday)\n",
    "    dict_holiday[name]['tarih'] = dict_holiday[name].index\n",
    "    dict_holiday[name] = dict_holiday[name].reindex(columns=[\"tarih\", \"ilce\", \"bildirimsiz_sum\", \"bildirimli_sum\", \"Tatil Adı\"])\n",
    "    \n",
    "print(dict_holiday[\"izmir-konak\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Tarih         Ilce  Bildirimsiz_sum  Bildirimli_sum  \\\n",
      "tarih                                                                 \n",
      "2021-01-01 2021-01-01  izmir-konak                9               0   \n",
      "2021-01-02 2021-01-02  izmir-konak               20               0   \n",
      "2021-01-03 2021-01-03  izmir-konak                7               1   \n",
      "2021-01-04 2021-01-04  izmir-konak               16               1   \n",
      "2021-01-05 2021-01-05  izmir-konak                3               0   \n",
      "\n",
      "               Bayram_Flag  Sicaklik_max  Sicaklik_min  Bagil_nem_max  \\\n",
      "tarih                                                                   \n",
      "2021-01-01  New Year's Day          15.3          11.9           93.5   \n",
      "2021-01-02             NaN          17.4          11.0           90.9   \n",
      "2021-01-03             NaN          15.3          11.2           84.6   \n",
      "2021-01-04             NaN          17.7          10.5           85.6   \n",
      "2021-01-05             NaN          16.7          11.2          100.0   \n",
      "\n",
      "            Bagil_nem_min  Ruzgar_hizi_max  Ruzgar_hizi_min  Yagis_max  \\\n",
      "tarih                                                                    \n",
      "2021-01-01           82.3              4.0              4.0        4.3   \n",
      "2021-01-02           64.9              3.3              3.3        1.0   \n",
      "2021-01-03           72.9              3.3              3.3       27.9   \n",
      "2021-01-04           55.8              6.6              6.6        1.0   \n",
      "2021-01-05           59.6              5.9              5.9       94.4   \n",
      "\n",
      "            Yagis_min   Sicaklik  Bagil_nem  Ruzgar_hizi      Yagis  Ozet  Gün  \n",
      "tarih                                                                           \n",
      "2021-01-01        1.0  13.095833  87.962500     3.129167   1.137500     2    1  \n",
      "2021-01-02        1.0  13.379167  80.720833     2.158333   1.000000     2    2  \n",
      "2021-01-03        1.0  12.587500  79.725000     2.300000   2.520833     2    3  \n",
      "2021-01-04        1.0  13.783333  71.362500     3.979167   1.000000     2    4  \n",
      "2021-01-05        1.0  13.895833  82.308333     2.591667  12.279167     2    5  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okkes\\AppData\\Local\\Temp\\ipykernel_11600\\2656318892.py:35: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n"
     ]
    }
   ],
   "source": [
    "# 6-) merge the dict_holiday and weather, return merged_all which contains all of the required columns\n",
    "\n",
    "def merge_weather(df1, df2):\n",
    "    merged_df = pd.merge(df1, df2[[\"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\n",
    "    \"Bagil_nem_min\",\"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\n",
    "    \"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\",\"Ozet\"]], left_index=True, right_index=True, how=\"left\")\n",
    "    return merged_df\n",
    "\n",
    "\"\"\" ekstra eklenebilecek kolonlar : (bunlari ustteki diger kolonarin arkasina ekleyebilirsin, ayni sekilde alttaki isimlendirmeye de eklemeyi unutma)\n",
    "\"Bulutluluk_max\", \"Bulutluluk_min\", \"Guneslilik_max\", \"Guneslilik_min\",\"Ruzgar_yonu_max\", \"Ruzgar_yonu_min\",\"Hissedilen_sicaklik_max\", \"Hissedilen_sicaklik_min\"\n",
    ",\"Bulutluluk\",\"Guneslilik\",\"Ruzgar_yonu\",\"Hissedilen_sicaklik\"\n",
    "\"\"\"\n",
    "merged_all = {} # key olarak tum ilceler, values olarak kesintiler, bayramlar, hava durumu verilerini (1096 gun) tutan df'i tutar\n",
    "for name in dict_holiday.keys():\n",
    "    merged_all[name] = merge_weather(dict_holiday[name], weather_last[name])\n",
    "\n",
    "    merged_all[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimsiz_sum\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\",\"Ozet\"]\n",
    "    \n",
    "    merged_all[name]['Gün'] = range(1, len(merged_all[name]) + 1)\n",
    "\n",
    "print(merged_all[\"izmir-konak\"].head())\n",
    "\n",
    "all_in_one = pd.concat(merged_all.values(), ignore_index=True) # tum ilceleri birlestir\n",
    "#print(\"\\nall_in_one: \\n\\n\",all_in_one.dtypes)\n",
    "\n",
    "merged_all_week = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_week[name] = merged_all[name].resample(\"W\").sum(numeric_only=True)\n",
    "#print(merged_all_week[\"izmir-konak\"])\n",
    "\n",
    "merged_all_month = {}\n",
    "for name in merged_all.keys():\n",
    "    merged_all_month[name] = merged_all[name].resample(\"M\").sum(numeric_only=True)\n",
    "#print(merged_all_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if not os.path.exists(\"graphs\"):\\n    os.makedirs(\"graphs\")\\n    print(\"images klasörü olustu\")\\nif not os.path.exists(\"./graphs/bildirimli_siz\"):\\n    os.makedirs(\"./graphs/bildirimli_siz\")\\n    print(\"bildirimli_siz klasoru olustu\")\\n\\n\\nfor name in merged_all_week.keys():\\n    plt.figure(figsize=(17,8))\\n    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\\n    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\\n    plt.xticks(rotation=90)\\n    plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\\n    plt.xlabel(\"Tarih\", fontsize=13)\\n    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\\n\\n    plt.margins(0.01)\\n    plt.legend()\\n    plt.grid()\\n    plt.subplots_adjust(bottom=0.15)\\n    plt.tight_layout()\\n    #plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\\n    #plt.show()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7-) Her ilcenin Bildirimli+Bildirimsiz kesinti grafigi (runtime: 19s)\n",
    "\n",
    "\"\"\"if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimli_siz\"):\n",
    "    os.makedirs(\"./graphs/bildirimli_siz\")\n",
    "    print(\"bildirimli_siz klasoru olustu\")\n",
    "\n",
    "\n",
    "for name in merged_all_week.keys():\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimli_sum\"], label=\"Bildirimli\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"{} Bildirimli Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    plt.margins(0.01)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"./graphs/bildirimli_siz/{}.png\".format(name))\n",
    "    #plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\\n\\nanaliz = pd.concat([\\n    ayristirma2.observed,\\n    ayristirma2.trend,\\n    ayristirma2.seasonal,\\n    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\\n], axis=1)\\nanaliz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\\n\\nindeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\\n\\n\\nX = sa.add_constant(indeks)\\nmodel = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\\nsonuc = model.fit()\\nrsquared_value = sonuc.rsquared\\ny = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\\n\\nyeni_satirlar = pd.DataFrame(index=y)\\nanaliz = pd.concat([analiz, yeni_satirlar])\\n\\n# not: bu degerleri ayarla\\nmev = [\\n    1.038656,\\n    0.973940,\\n    0.987404,\\n    1.038656\\n]\\n\\nnan_indices = analiz.index[analiz[\\'Mevsimsellik\\'].isna()]\\nfor i, index in enumerate(nan_indices):\\n    if i < len(mev):\\n        analiz.at[index, \\'Mevsimsellik\\'] = mev[i]\\n#print(analiz[\"Mevsimsellik\"])\\n\\ngirdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\\nregmodel = sonuc.predict(sa.add_constant(girdi))\\n\\nanaliz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\\n\\n\\nprint(analiz.head())\\n\\nplt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\')\\n#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment=\\'top\\', horizontalalignment=\\'left\\')\\nplt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\\n#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\\nplt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8-) Her ilcenin Bildirimsiz+MHO(EWMA) kesinti grafiği (runtime: 18s)\n",
    "\n",
    "\"\"\"if not os.path.exists(\"graphs\"):\n",
    "    os.makedirs(\"graphs\")\n",
    "    print(\"images klasörü olustu\")\n",
    "if not os.path.exists(\"./graphs/bildirimsiz_detailed\"):\n",
    "    os.makedirs(\"./graphs/bildirimsiz_detailed\")\n",
    "    print(\"bildirimsiz_detailed klasoru olustu\")\n",
    "\n",
    "for name in merged_all_week.keys():\n",
    "\n",
    "    plt.figure(figsize=(17,8))\n",
    "    plt.plot(merged_all_week[name].index,merged_all_week[name][\"Bildirimsiz_sum\"], label=\"Bildirimsiz\")\n",
    "    plt.title(\"{} - Bildirimsiz (Haftalik)\".format(name), fontweight=\"bold\", fontsize=15)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(\"Tarih\", fontsize=13)\n",
    "    plt.ylabel(\"Kesinti Sayisi\", fontsize=13)\n",
    "\n",
    "    window_size = 3  # Hareketli ortalama penceresi\n",
    "    merged_all_week[name]['Moving_Average'] = merged_all_week[name][\"Bildirimsiz_sum\"].rolling(window=window_size, center=True).mean()\n",
    "    #plt.plot(merged_all_week[name]['Moving_Average'], label=\"MHO\", color=\"black\")\n",
    "\n",
    "    ortalama = merged_all_week[name][\"Bildirimsiz_sum\"].mean()\n",
    "    plt.axhline(y=ortalama, color='orange', linestyle='--', label='Ortalama %{:.1f}'.format(ortalama),linewidth=2.2)\n",
    "    alpha = 0.2  # Yumuşatma parametresi \n",
    "    # formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "    merged_all_week[name]['EWMA'] = merged_all_week[name][\"Bildirimsiz_sum\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    plt.plot(merged_all_week[name]['EWMA'], label=\"EWMA\", color=\"red\", lw=2.9)\n",
    "\n",
    "\n",
    "    plt.margins(0.01)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig(\"./graphs/bildirimsiz_detailed/{}.png\".format(name))\n",
    "    #plt.show()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "ayristirma2 = sm(merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], model=\"mul\", period=4)\n",
    "\n",
    "analiz = pd.concat([\n",
    "    ayristirma2.observed,\n",
    "    ayristirma2.trend,\n",
    "    ayristirma2.seasonal,\n",
    "    ayristirma2.observed/ayristirma2.seasonal # orijinal veri / S = T * E, regr. da üzerine tahmin yapılacak sey\n",
    "], axis=1)\n",
    "analiz.columns = [\"Orijinal Gözlem\", \"Trend\", \"Mevsimsellik\", \"Mevsimsellik Düzeltme\"]\n",
    "\n",
    "indeks = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 1)\n",
    "\n",
    "\n",
    "X = sa.add_constant(indeks)\n",
    "model = sa.OLS(analiz[\"Mevsimsellik Düzeltme\"], X)\n",
    "sonuc = model.fit()\n",
    "rsquared_value = sonuc.rsquared\n",
    "y = pd.date_range(analiz.index[-1] + pd.DateOffset(weeks=4), periods=4,freq=\"W\") # 4 tane ekstra ay ekle\n",
    "\n",
    "yeni_satirlar = pd.DataFrame(index=y)\n",
    "analiz = pd.concat([analiz, yeni_satirlar])\n",
    "\n",
    "# not: bu degerleri ayarla\n",
    "mev = [\n",
    "    1.038656,\n",
    "    0.973940,\n",
    "    0.987404,\n",
    "    1.038656\n",
    "]\n",
    "\n",
    "nan_indices = analiz.index[analiz['Mevsimsellik'].isna()]\n",
    "for i, index in enumerate(nan_indices):\n",
    "    if i < len(mev):\n",
    "        analiz.at[index, 'Mevsimsellik'] = mev[i]\n",
    "#print(analiz[\"Mevsimsellik\"])\n",
    "\n",
    "girdi = np.arange(1,len(merged_all_week[\"izmir-aliaga\"].index) + 5)\n",
    "regmodel = sonuc.predict(sa.add_constant(girdi))\n",
    "\n",
    "analiz[\"Tahmin\"] = analiz[\"Mevsimsellik\"] * regmodel\n",
    "\n",
    "\n",
    "print(analiz.head())\n",
    "\n",
    "plt.text(analiz.index[0], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top')\n",
    "#plt.text(analiz.index[-1], max(analiz[\"Mevsimsellik Düzeltme\"]), \"R-squared value: {:.3f}\".format(rsquared_value), fontsize=10, verticalalignment='top', horizontalalignment='left')\n",
    "plt.scatter(analiz.index, analiz[\"Mevsimsellik Düzeltme\"], label=\"Mevsimsellik Düzeltme\", color=\"blue\")\n",
    "#plt.plot(analiz[\"Orijinal Gözlem\"], label=\"Orijinal Gözlem\", color=\"purple\")\n",
    "plt.plot(analiz.index, analiz[\"Tahmin\"], label=\"Trend\", color=\"red\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{5.0: 30.0, 6.0: 26.285714285714285, 7.0: 34.170212765957444, 8.2: 31.0, 8.5: 36.0, 8.6: 34.0, 8.9: 37.0, 10.3: 26.0, 11.6: 26.0, 12.7: 35.0, 16.6: 37.0, 18.2: 44.0, 18.9: 28.0, 20.7: 35.0, 22.1: 26.0, 23.7: 33.0, 27.7: 25.0, 28.8: 52.5, 29.0: 32.0, 41.3: 35.0, 44.0: 69.0, 44.3: 27.0, 52.6: 22.0, 54.3: 32.0, 54.5: 13.0, 55.0: 20.0, 56.5: 18.0, 57.2: 43.0, 58.5: 37.0, 60.0: 44.0, 60.1: 28.0, 61.6: 29.0, 67.1: 32.0, 69.3: 45.0, 73.1: 22.0, 73.7: 32.0, 77.0: 61.0, 80.7: 41.0, 85.5: 23.0, 88.3: 38.0, 88.4: 37.0, 91.3: 49.0, 98.6: 72.0, 99.9: 41.0, 100.8: 54.0, 101.0: 31.0, 101.3: 27.0, 102.4: 28.0, 102.9: 18.0, 104.2: 40.0, 105.8: 53.0, 106.6: 30.0, 107.8: 20.0, 108.7: 35.0, 120.2: 40.0, 120.4: 28.0, 122.3: 31.0, 125.6: 54.0, 127.6: 39.0, 139.2: 53.0, 141.70000000000002: 35.0, 143.8: 68.0, 154.6: 27.0, 157.2: 41.0, 157.60000000000002: 31.0, 159.3: 47.0, 162.0: 58.0, 164.8: 32.0, 174.4: 29.0, 175.5: 37.0, 175.7: 33.0, 178.0: 27.0, 178.5: 53.0, 178.6: 60.0, 179.1: 53.0, 180.0: 30.0, 180.6: 28.0, 184.0: 43.0, 186.3: 68.0, 190.4: 35.0, 191.2: 84.0, 191.5: 33.0, 191.6: 32.0, 197.8: 66.0, 202.89999999999998: 35.0, 209.8: 45.0, 212.5: 40.0, 215.6: 59.0, 216.9: 37.0, 219.0: 30.0, 223.6: 25.0, 233.6: 35.0, 237.3: 64.0, 241.2: 35.0, 244.1: 39.0, 246.5: 62.0, 252.1: 52.0, 263.8: 64.0, 272.5: 27.0, 280.1: 60.0, 300.6: 35.0, 308.1: 40.0, 323.4: 76.0, 327.6: 65.0, 335.0: 35.0, 352.9: 85.0, 393.6: 48.0, 413.7: 40.0}\n",
      "     Ort. Yagis  Ort. Kesinti\n",
      "0           5.0     30.000000\n",
      "1           6.0     26.285714\n",
      "2           7.0     34.170213\n",
      "3           8.2     31.000000\n",
      "4           8.5     36.000000\n",
      "..          ...           ...\n",
      "103       327.6     65.000000\n",
      "104       335.0     35.000000\n",
      "105       352.9     85.000000\n",
      "106       393.6     48.000000\n",
      "107       413.7     40.000000\n",
      "\n",
      "[108 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIAAAAIuCAYAAADHbnP+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAADpYElEQVR4nOzdd3xN9/8H8NfNkCEDIcuMoBWhqFrVGrVVtUqLplZpjf6smjWSoGbri2rtTWm1qFGpraUhCCpiCzWSBiEJSZCb8/vj05N7b+a9yd339Xw87iM3Jyfnfu5M7vu+h0KSJAlERERERERERGS17Ey9ACIiIiIiIiIiMiwGgIiIiIiIiIiIrBwDQEREREREREREVo4BICIiIiIiIiIiK8cAEBERERERERGRlWMAiIiIiIiIiIjIyjEARERERERERERk5RgAIiIiIiIiIiKycgwAERERERERERFZOQaAiIjIYq1ZswYKhQKnTp0yyeXevHlT78deuHAhFAoFgoOD891HoVAgLCws+/vDhw9DoVDg8OHDel+PPgwdOhSOjo6Ijo7O9bPnz5+jdu3aqFatGp4+farXy+3bty+qVKmi12NqIywsDAqFIvtkZ2cHPz8/dOzYEceOHdPY9+bNm1AoFFizZk32Nl0eXy1atECLFi0KPJ42azW2nOsursePH6Ns2bLYvHlz9jb5uj148CDP3wkODi7yGp4/f45BgwbBz88P9vb2qFu3Lu7du4ewsDCcPXu2SMcE8n4u53UfValSBW+//XaBx3r06BFKlSqF7du3F3k9RERkXRxMvQAiIiJL06lTJ0RGRsLPz0/vx161ahUA4MKFCzhx4gQaNWpU6O/Ur18fkZGRCAoK0vt69GHu3LnYt28f+vTpg9OnT6NEiRLZPwsLC0NsbCz++OMPlCxZUq+XO3nyZAwfPlyvx9RFREQEPD09kZWVhX/++Qdz5sxBixYtcOLECdSvXx8A4Ofnh8jISAQGBurlMnU93oABA9C+fXu9XLYuvv/+e70eLzw8HP7+/vjwww/1etz8LF68GEuXLsW3336LV199FW5ubrh37x7Cw8NRpUoV1K1bV2+XVdT7qHTp0hg5ciTGjBmDjh07ajzviIjINjEDiIiISEflypVD48aN4eTkVOB+aWlpOh331KlTOHfuHDp16gQAWLlypVa/5+HhgcaNG8PDw0OnyzMWV1dXrF27FhcvXkRoaGj29pMnT2LOnDkYPXo0Xn/9db1fbmBgIOrVq6f342rr1VdfRePGjdG0aVP06NEDP//8MzIzM/Hzzz9n7+Pk5ITGjRujXLlyerlMbY8nPzYrVKiAxo0b6+WydREUFKS3gGVSUhKWLl2KoUOHGi2bKSYmBi4uLvj888/RpEkT1K5d22CXVZz7aNCgQbh586bGY46IiGwXA0BERGQ15PKX/E6yFi1aIDg4GJGRkWjatClcXFxQpUoVrF69GgCwe/du1K9fH66urqhduzYiIiI0LievEh35mH/88QeaNm0KV1dX9O/fX6f1ywGfWbNmoWnTpti8ebNWQaS8ykZOnTqFHj16oEqVKtnXr2fPnrh161au3z969CiaNGkCZ2dnlC9fHpMnT8aKFStyXccff/wRbdu2hZ+fH1xcXFCzZk2MHz9eq9KtJk2aYMyYMZg7dy5OnDiBZ8+eoW/fvqhZsyamTp2Ka9euoV+/fqhevTpcXV1Rvnx5dO7cGefPn891rAsXLqBt27ZwdXVFuXLlMHToUOzevTvXbZBXCdiWLVvQqFEjeHp6wtXVFVWrVtX5fioqT09PAICjo2P2Nm1LtiRJwpw5c1C5cmU4Ozujfv362LNnT6798jqeXEIUHR2Nbt26oXTp0tkZQgWVF+3atQv16tXLvq937doFQDz+a9asiZIlS6Jhw4a5SjBv3LiBHj16wN/fH05OTvDx8cFbb72lURqVswSsb9+++T5v1csd87JmzRpkZmbqJfsnPDwcjRo1QpkyZeDh4YH69etj5cqVkCQpex+FQoEVK1YgPT09e41r1qzBa6+9BgDo169frrXr8nzMSdsyve+//x4ODg4aQVYfHx+0adMGS5Ys0fGWICIia8QSMCIishpy+Yu6+/fvIyQkBOXLl9fYnpCQgH79+mHs2LGoUKECvv32W/Tv3x+3b9/Gzz//jC+//BKenp6YOnUq3n33Xdy4cQP+/v4FXn58fDxCQkIwduxYzJgxA3Z22n/Okp6ejk2bNuG1115DcHAw+vfvjwEDBmDLli3o06eP9jfCf27evImXXnoJPXr0QJkyZRAfH4/FixfjtddeQ2xsLMqWLQsA+Pvvv9GmTRvUqFEDa9euhaurK5YsWYINGzbkOubVq1fRsWNHjBgxAiVLlsSlS5cwe/ZsREVF4eDBg4WuKTw8HL/99hv69u2Ldu3a4erVqzhx4gScnJxw7949eHl5YdasWShXrhySkpKwdu1aNGrUCGfOnMFLL70EQNzGzZs3R8mSJbF48WJ4e3tj06ZN+Pzzzwu9/MjISHz44Yf48MMPERYWBmdnZ9y6dUurtReFUqlEZmZmdgnYpEmT4OTkhG7duul8rPDwcISHh+OTTz5Bt27dcPv2bQwcOBBKpTL7tilM165d0aNHDwwaNKjQoN25c+cwYcIETJw4EZ6enggPD0fXrl0xYcIEHDhwADNmzIBCocC4cePw9ttvIy4uDi4uLgCAjh07QqlUYs6cOahUqRIePHiAv/76C48fP8738iZPnoxBgwZpbPvuu++wYcOGQjOFdu/ejXr16qFUqVJ5/ly+H7Rx8+ZNfPbZZ6hUqRIA4Pjx4/i///s/3L17F1OmTAEgHkfTpk3DoUOHsh87fn5+WL16Nfr164dJkyZlZ/FVqFAh+7jaPB+LQpIkjBkzBgsXLsSKFSvQt29fjZ+3aNECEyZMwOPHj/O9jYiIyEZIREREFmr16tUSAOnkyZN5/vzp06dSw4YNJT8/P+nmzZvZ25s3by4BkE6dOpW97eHDh5K9vb3k4uIi3b17N3v72bNnJQDSwoULc11uXFxcrmMeOHCgSNdl3bp1EgBpyZIlkiRJUmpqquTm5ia98cYbufYFIIWGhmZ/f+jQIQmAdOjQoXyPn5mZKT158kQqWbKktGDBguzt3bt3l0qWLCndv38/e5tSqZSCgoJyXUd1WVlZ0osXL6QjR45IAKRz585pdT3Pnj0rlShRQgIgTZs2rcD1Pn/+XKpevbo0cuTI7O1jxoyRFAqFdOHCBY3927Vrl+s26NOnj1S5cuXs77/++msJgPT48WOt1lpUoaGhEoBcJw8PD2nr1q0a+8bFxUkApNWrV2dvy/n4evTokeTs7Cy99957Gr977NgxCYDUvHnzAo8nr2fKlCn5rlVd5cqVJRcXF+nOnTvZ2+TngZ+fn/T06dPs7du3b5cASDt27JAkSZIePHggAZDmz59f4G3UvHlzjXXn9NNPP0kKhUL68ssvCzyOJEmSq6urNGjQoHyvW0GngtagVCqlFy9eSFOnTpW8vLykrKys7J/16dNHKlmypMb+J0+ezHXb5ye/52Nez+X87qNOnTpJaWlp0vvvvy95enpK+/fvz/Oy9u3bJwGQ9uzZU+i6iIjIurEEjIiIrJJSqcSHH36Iixcv4rfffkPlypU1fu7n54dXX301+/syZcrA29sbdevW1cj0qVmzJgBoVapRunRptGrVqkjrXblyJVxcXNCjRw8AgJubG7p3744///wTV69e1fl4T548wbhx41CtWjU4ODjAwcEBbm5uePr0KS5evJi935EjR9CqVSuNDAQ7Ozt88MEHuY5548YN9OrVC76+vrC3t4ejoyOaN28OABrHLMgrr7yCrl27wsXFBRMmTMjenpmZiRkzZiAoKAglSpSAg4MDSpQogatXr+Zab3BwcK6skJ49exZ62XKJzgcffICffvoJd+/e1WrNWVlZyMzMzD4plUqtfm///v04efIkoqKisGvXLrRu3Ro9evTAtm3btPp9WWRkJDIyMvDRRx9pbG/atGmux3VB3n//fa33rVu3rkbWnPw8aNGiBVxdXXNtl58fZcqUQWBgIObOnYt58+bhzJkzyMrK0vpyAXEff/zxxwgJCcFXX31V4L6PHz9GWloavL29891Hvh9ynvJqlH3w4EG0bt0anp6e2Y/xKVOm4OHDh0hMTNTpeqjT9vmoi4cPH6JVq1aIiorC0aNH8dZbb+W5n3zbaPt4JyIi68UAEBERWaVBgwYhIiICP//8c54TecqUKZNrW4kSJXJtlyfnZGRkFHqZRZ0Kdu3aNfzxxx/o1KkTJEnC48eP8fjx4+xSIXkymC569eqFRYsWYcCAAfj9998RFRWFkydPoly5ckhPT8/e7+HDh/Dx8cn1+zm3PXnyBG+88QZOnDiB6dOn4/Dhwzh58iS2bt0KABrHLIyTkxPs7Oxgb2+fvW3UqFGYPHky3n33XezcuRMnTpzAyZMn8corrxRpvXl58803sX37dmRmZqJ3796oUKECgoODsWnTpgJ/r3///nB0dMw+5fdGO6dXXnkFDRo0wGuvvYZOnTphy5YtqFatGoYOHarV78sePnwIAPD19c31s7y25UeXx2d+z4PCnh8KhQIHDhxAu3btMGfOHNSvXx/lypXDsGHDkJqaWujlXrhwAe+++y7eeOMNrZqgy48NZ2fnfPeR74ecp5y/ExUVhbZt2wIAli9fjmPHjuHkyZOYOHGixmUVhbbPR11cuXIFJ06cQIcOHRAcHJzvfvL1LM76iYjIOrAHEBERWZ2wsDCsWLECq1evzn5DZwxFnUC0atUqSJKEn3/+Oc9pPWvXrsX06dM1AiYFSU5Oxq5duxAaGorx48dnb3/27BmSkpI09vXy8sK///6b6xgJCQka3x88eBD37t3D4cOHs7N+ABTY10UXGzZsQO/evTFjxgyN7Q8ePNDoW6LtevPTpUsXdOnSBc+ePcPx48cxc+ZM9OrVC1WqVEGTJk3y/J2wsDCNHkPu7u5aXVZOdnZ2qFWrFrZs2YLExMQCs1bUeXl5Acj7OiYkJORqdJ0fY03Iqly5cnbw5sqVK/jpp58QFhaG58+fF9iM+M6dO2jfvj0qVaqEX375RaNZdn7k2ybn47ooNm/eDEdHR+zatUsjOLR9+/ZiHVeX56MumjRpgu7du+OTTz4BIEbT59V3TL6M4vQZIiIi68AMICIisiorV65EeHg4pk6dmqsZqjlSKpVYu3YtAgMDcejQoVynL774AvHx8XlOfMqPQqGAJEm5xtSvWLEiV/lS8+bNcfDgQTx48CB7W1ZWFrZs2ZLrmAByHXPp0qVar6uwNec89u7du3OVrTRv3hwxMTGIjY3V2L5582adLs/JyQnNmzfH7NmzAQBnzpzJd98qVapoZI5o23Q5J6VSifPnz8PJyQkeHh5a/17jxo3h7OyMjRs3amz/66+/tCpNNKUaNWpg0qRJqF27NqKjo/PdLzk5GR06dIBCocBvv/2m9e1TokQJVK1aFdevXy/2WhUKBRwcHDQCrenp6Vi/fr1Wvy8/fnNm2ujyfNRVnz59sHnzZqxevRq9e/fO83g3btwAgEKbaRMRkfVjBhAREVmNyMhIDBo0CK+//jratGmD48ePa/y8cePGJlpZ/vbs2YN79+5h9uzZGmOxZcHBwVi0aBFWrlyJt99+W6tjenh44M0338TcuXNRtmxZVKlSBUeOHMHKlStzTQGaOHEidu7cibfeegsTJ06Ei4sLlixZkj0lSs4oaNq0KUqXLo1BgwYhNDQUjo6O2LhxI86dO1es6y97++23sWbNGrz88suoU6cOTp8+jblz52ZPUZKNGDECq1atQocOHTB16lT4+Pjghx9+wKVLlzTWm5cpU6bgzp07eOutt1ChQgU8fvwYCxYs0OhlpE+nT5/OHv3+77//YtWqVbh06RJGjhxZYMlSTqVLl8bo0aMxffp0DBgwAN27d8ft27cRFhamUwmYMfz999/4/PPP0b17d1SvXh0lSpTAwYMH8ffff2tkv+TUq1cvxMbGYtmyZbh9+zZu376d/bMKFSrkehyoa9GihU4B0vx06tQJ8+bNQ69evfDpp5/i4cOH+Prrr3MFbvITGBgIFxcXbNy4ETVr1oSbmxv8/f3h7++v9fOxKLp16wZXV1d069Yte5qgXJoHiElmXl5eqF27drEvi4iILBszgIiIyGpcvnwZmZmZOHbsGJo0aZLrZI5WrlyJEiVKoF+/fnn+vGzZsnjvvfewa9euPEuf8vPDDz+gZcuWGDt2LLp27YpTp05h37592QEJ2SuvvIJ9+/bBxcUFvXv3xqeffopatWphyJAhAJC9v5eXF3bv3g1XV1eEhISgf//+cHNzw48//ljEa65pwYIFCAkJwcyZM9G5c2fs2LEDW7duzdWo19/fH0eOHEGNGjUwaNAgfPTRRyhRogSmTp0KAAW+oW7UqBESEhIwbtw4tG3bFp9++ilcXFxw8OBB1KpVSy/XQ1379u2zH3v9+/fPDgJ9/fXXOh9r6tSpmDlzJvbu3Yt33nkH3377LZYsWVLkbCRD8fX1RWBgIL7//nt069YNXbp0wc6dO/HNN99k30d5uXDhArKysjBgwIBcz9sVK1YUeJkfffQR4uPjcfLkyWKtvVWrVli1ahXOnz+Pzp07Y+LEiejWrVuBgSt1rq6uWLVqFR4+fIi2bdvitddew7JlywBo/3wsqo4dO+K3337D3r170aVLl+wsJEmSsGPHDvTq1ctoJYBERGS+FJIkSaZeBBEREZmXtm3b4ubNm7hy5Yqpl6KVTz/9FJs2bcLDhw81sh/INtSpUwevv/46Fi9ebOqlmJUDBw6gbdu2uHDhAl5++WVTL4eIiEyMJWBEREQ2btSoUahXrx4qVqyIpKQkbNy4Efv27dNqCpMpTJ06Ff7+/qhatSqePHmCXbt2YcWKFZg0aRKDPzZqzpw5eO+99zBx4sQCy8VszfTp09G/f38Gf4iICAADQERERAaTlZWFrKysAvdxcDD9n2KlUokpU6YgISEBCoUCQUFBWL9+PUJCQky9tDw5Ojpi7ty5uHPnDjIzM1G9enXMmzcPw4cPN/XSyETat2+PuXPnIi4ujgGg/zx69AjNmzfPLuckIiJiCRgREZGBhIWFITw8vMB94uLitB7jTURERERUVAwAERERGci9e/dw7969AvepU6cOy5aIiIiIyOAYACIiIiIiIiIisnIcA09EREREREREZOVM33nSwLKysnDv3j24u7tDoVCYejlERERERERERHohSRJSU1Ph7+8PO7uCc3ysPgB07949VKxY0dTLICIiIiIiIiIyiNu3bxc6CdPqA0Du7u4AxI3h4eFh4tUQERERERGRudsXm4BZey7h35Rn2dt8PJwwvsPLaBPka8KVEWlKSUlBxYoVs2MfBbH6AJBc9uXh4cEAEBERERERERUoIiYeo7dfhQR72Dm5Zm9/8AwYvf0qFru5o32wnwlXSJSbNi1v2ASaiIiIiIiICIAyS0L4zljkNSpb3ha+MxbKLA7TJsvDABARERERERERgKi4JMQnZ+T7cwlAfHIGouKSjLcoIj1hAIiIiIiIiIgIQGJq/sGfouxHZE6svgcQERERWQ5lloSouCQkpmbA290ZDQPKwN6u8Jp2IiIiffB2d9brfoaiVCrx4sULk66BjMPR0RH29vZ6ORYDQERERGQWImLiEb4zViP13s/TGaGdg9hsk4iIjKJhQBn4eTojITkjzz5ACgC+nuIDClOQJAkJCQl4/PixSS6fTKNUqVLw9fXVqtFzQRgAIiIiIpOLiInH4A3Ruf7ZTkjOwOAN0VgcUp9BICIiMjh7OwVCOwdh8IZoKACNv0vyW+/QzkEmy06Vgz/e3t5wdXUtdkCAzJskSUhLS0NiYiIAwM+veP8LMQBEREREJlXYxBUFxMSVNkG+LAcjIiKDax/sh8Uh9XNlpfqaOCtVqVRmB3+8vLxMsgYyPhcXFwBAYmIivL29i1UOxgAQERERmZQuE1eaBPIfXiIiMrz2wX5oE+RrVn3p5J4/rq6uJlsDmYZ8n7948YIBICIiIrJcnLhCRETmyN5OYZYfPLDsy/bo6z7nGHgiIiIyKUuZuEJERERkyRgAIiIiIpOSJ67k99mWAmIamKkmrhARERHlp0qVKpg/f36xj9OiRQuMGDGi2McpCANAREREZFLyxBUAuYJA5jBxhYiIiIrv9u3b+OSTT+Dv748SJUqgcuXKGD58OB4+fFjo7968eRMKhQJnz57V+XLzCqwsWLAATk5O+OGHH3Q+Xk4nT57Ep59+qvX+hw8fhkKhwOPHjzW2b926FdOmTSv2egrCABARERGZnDxxxddTs8zL19OZI+CJiIj0TJklIfL6Q/x69i4irz+EMiuvWZz6c+PGDTRo0ABXrlzBpk2bcO3aNSxZsgQHDhxAkyZNkJSUlO/vPn/+XK9rCQ0NxYQJE7Bt2zb06tWr2McrV66cXhpzlylTBu7u7sU+TkEYACIiIiKz0D7YD0fHtcKmgY2xoEddbBrYGEfHtWLwh4iISI8iYuLRbPZB9Fx+HMM3n0XP5cfRbPZBRMTEG+wyhw4dihIlSmDv3r1o3rw5KlWqhA4dOmD//v24e/cuJk6cmL1vlSpVMH36dPTt2xeenp4YOHAgAgICAAD16tWDQqFAixYtdF6DJEn4v//7PyxYsAB79+5Fx44ds3+2c+dOvPrqq3B2dkbVqlURHh6OzMzM7J+HhYWhUqVKcHJygr+/P4YNG6axXvUSMIVCgRUrVuC9996Dq6srqlevjh07dgAQmUwtW7YEAJQuXRoKhQJ9+/YFwBIwIiIisjHyxJUudcujSaAXy76IiIj0KCImHoM3RCM+WXOyZkJyBgZviDZIECgpKQm///47hgwZAhcXF42f+fr64qOPPsKPP/4ISVJlIc2dOxfBwcE4ffo0Jk+ejKioKADA/v37ER8fj61bt+q0hszMTHz88cfYsmULjhw5gmbNmmX/7Pfff0dISAiGDRuG2NhYLF26FGvWrMFXX30FAPj555/xv//9D0uXLsXVq1exfft21K5du8DLCw8PxwcffIC///4bHTt2xEcffYSkpCRUrFgRv/zyCwDg8uXLiI+Px4IFC3S6LsXBMfBEREREREREVk6ZJSF8ZyzyKvaSIPruhe+MRZsgX71+AHP16lVIkoSaNWvm+fOaNWvi0aNHuH//Pry9vQEArVq1wujRo7P3uXnzJgDAy8sLvr6+Oq9h+fLlAIBz587h5Zdf1vjZV199hfHjx6NPnz4AgKpVq2LatGkYO3YsQkND8c8//8DX1xetW7eGo6MjKlWqhIYNGxZ4eX379kXPnj0BADNmzMC3336LqKgotG/fHmXKiKEW3t7eKFWqlM7XpTiYAURERERERERk5aLiknJl/qiTAMQnZyAqLv9+PIYgZ/4oFKqgU4MGDfR6Gc2aNYObmxsmTZqkUdoFAKdPn8bUqVPh5uaWfRo4cCDi4+ORlpaG7t27Iz09HVWrVsXAgQOxbdu2XMfIqU6dOtnnS5YsCXd3dyQmJur1OhUFA0BEREREREREVi4xNf/gT1H201a1atWgUCgQGxub588vXbqE0qVLo2zZstnbSpYsqdc11K5dGwcOHMDhw4fxwQcf4MWLF9k/y8rKQnh4OM6ePZt9On/+PK5evQpnZ2dUrFgRly9fxnfffQcXFxcMGTIEb775psYxcnJ0dNT4XqFQICsrS6/XqSgYACIiIiIiIiKyct7uzoXvpMN+2vLy8kKbNm3w/fffIz09XeNnCQkJ2LhxIz788EONDKCcSpQoAQBQKpVFXkfdunVx8OBBHD16FN27d88O4NSvXx+XL19GtWrVcp3s7ETIxMXFBe+88w4WLlyIw4cPIzIyEufPny/SOvRxXYqKASAiIiIiIiIiK9cwoAz8PJ2RX5hFAcDP0xkNA8ro/bIXLVqEZ8+eoV27dvjjjz9w+/ZtREREoE2bNihfvnx2w+X8eHt7w8XFBREREfj333+RnJwMANi2bVuunj4FqVOnDg4dOoTIyEh069YNz58/x5QpU7Bu3TqEhYXhwoULuHjxIn788UdMmjQJALBmzRqsXLkSMTExuHHjBtavXw8XFxdUrly5SLdF5cqVoVAosGvXLty/fx9Pnjwp0nGKggEgIiIiIiIiIitnb6dAaOcgAMgVBJK/D+0cZJAJnNWrV8epU6cQGBiIDz/8EIGBgfj000/RsmVLREZGZjdGzo+DgwMWLlyIpUuXwt/fH126dAEAJCcn4/LlyzqtpVatWjh06BCioqLw/vvvo2XLlti1axf27duH1157DY0bN8a8efOyAzylSpXC8uXL8frrr6NOnTo4cOAAdu7cCS8vryLdFuXLl0d4eDjGjx8PHx8ffP7550U6TlEoJPVZa1YoJSUFnp6eSE5OhoeHh6mXQ0RERERERKSzjIwMxMXFISAgAM7ORS/TioiJR/jOWI2G0H6ezgjtHIT2wX76WCrpWUH3vS4xD46BJyIiIiIiIrIR7YP90CbIF1FxSUhMzYC3uyj7MkTmD5kXBoCIiIiIiIiIbIi9nQJNAotWwkSWiz2AiIiIiIiIiIisHANARERERERERERWjgEgIiIiIiIiIiIrxwAQEREREREREZGVYwCIiIiIiIiIiMjKmTQAlJmZiUmTJiEgIAAuLi6oWrUqpk6diqysrOx9JElCWFgY/P394eLighYtWuDChQsmXDURERERERERkWUxaQBo9uzZWLJkCRYtWoSLFy9izpw5mDt3Lr799tvsfebMmYN58+Zh0aJFOHnyJHx9fdGmTRukpqaacOVERERERERERJbDpAGgyMhIdOnSBZ06dUKVKlXQrVs3tG3bFqdOnQIgsn/mz5+PiRMnomvXrggODsbatWuRlpaGH374wZRLJyIiIiIiIiKyGCYNADVr1gwHDhzAlStXAADnzp3D0aNH0bFjRwBAXFwcEhIS0LZt2+zfcXJyQvPmzfHXX3/lecxnz54hJSVF40REREREREREptG3b18oFIpcp/bt26NHjx7o0KGDxv579uyBQqHA5MmTNbZPmzYN/v7+AICbN29CoVDAwcEBd+/e1dgvPj4eDg4OUCgUuHnzZq71tG3bFvb29jh+/Lh+r6iZM2kAaNy4cejZsydefvllODo6ol69ehgxYgR69uwJAEhISAAA+Pj4aPyej49P9s9ymjlzJjw9PbNPFStWNOyVICIiIiIiIqICtW/fHvHx8RqnTZs2oWXLljh69CgyMzOz9z18+DAqVqyIQ4cOaRzj8OHDaNmypcY2f39/rFu3TmPb2rVrUb58+TzX8c8//yAyMhKff/45Vq5cqadrZxlMGgD68ccfsWHDBvzwww+Ijo7G2rVr8fXXX2Pt2rUa+ykUCo3vJUnKtU02YcIEJCcnZ59u375tsPUTERERERERUeGcnJzg6+urcSpdujRatmyJJ0+eZLeCAUSgZ/z48Th58iTS0tIAAM+fP0dkZGSuAFCfPn2wevVqjW1r1qxBnz598lzH6tWr8fbbb2Pw4MH48ccf8fTpUz1fU/Nl0gDQmDFjMH78ePTo0QO1a9fGxx9/jJEjR2LmzJkAAF9fXwDIle2TmJiYKytI5uTkBA8PD40TERERERERkdWRJODpU9OcJEkvV6FGjRrw9/fPzvZJTU1FdHQ0unfvjsDAQBw7dgwAcPz4caSnp+cKAL3zzjt49OgRjh49CgA4evQokpKS0Llz5zxuLgmrV69GSEgIXn75ZdSoUQM//fSTXq6HJTBpACgtLQ12dppLsLe3zx4DHxAQAF9fX+zbty/758+fP8eRI0fQtGlTo66ViIiIiIiIyKykpQFubqY5/ZeZo61du3bBzc1N4zRt2jQAQIsWLXD48GEAwJ9//okaNWqgXLlyaN68efZ2uSwsMDBQ47iOjo4ICQnBqlWrAACrVq1CSEgIHB0dc61h//79SEtLQ7t27QAAISEhNlUG5mDKC+/cuTO++uorVKpUCbVq1cKZM2cwb9489O/fH4Ao/RoxYgRmzJiB6tWro3r16pgxYwZcXV3Rq1cvUy6diIiIiIiIiLTUsmVLLF68WGNbmTJlsn82YsQIvHjxAocPH0aLFi0AAM2bN8e3334LQASAWrVqleexP/nkEzRp0gQzZszAli1bEBkZqdFTSLZy5Up8+OGHcHAQoZCePXtizJgxuHz5Ml566SV9XVWzZdIA0LfffovJkydjyJAhSExMhL+/Pz777DNMmTIle5+xY8ciPT0dQ4YMwaNHj9CoUSPs3bsX7u7uJlw5ERERERERkYm5ugJPnpjusnVQsmRJVKtWLc+ftWzZEk+fPsXJkydx6NAhjBkzBoAIAPXu3RtJSUmIjIzMt69PcHAwXn75ZfTs2RM1a9ZEcHAwzp49q7FPUlIStm/fjhcvXmgEopRKJVatWoXZs2frdH0skUkDQO7u7pg/fz7mz5+f7z4KhQJhYWEICwsz2rqIiIiIiIiIzJ5CAZQsaepVFFtgYCAqVqyIHTt24OzZs2jevDkAwM/PD1WqVME333yDjIyMXP1/1PXv3x9DhgzJlWUk27hxIypUqIDt27drbD9w4ABmzpyJr776KjszyFpZ97UjIiIiIiIiIpN79uxZrgFPDg4OKFu2LACRBfT999+jWrVqGkOf5DKwqlWrolKlSvkef+DAgejevTtKlSqV589XrlyJbt26ITg4WGN75cqVMW7cOOzevRtdunQp4rWzDCZtAk1ERERERERE1i8iIgJ+fn4ap2bNmmX/vGXLlkhNTc3u/yNr3rw5UlNTC8z+AVTBpLyyeE6fPo1z587h/fffz/Uzd3d3tG3b1iaaQSskSU+z28xUSkoKPD09kZyczJHwREREREREZJEyMjIQFxeHgIAAODs7m3o5ZEQF3fe6xDyYAUREREREREREZOUYACIiIiIiIiIisnIMABERERERERERWTkGgIiIiIiIiIiIrBwDQEREREREREQWwsrnOFEe9HWfMwBEREREREREZOYcHR0BAGlpaSZeCRmbfJ/Lj4GictDHYoiIiIiIiIjIcOzt7VGqVCkkJiYCAFxdXaFQKEy8KjIkSZKQlpaGxMRElCpVCvb29sU6HgNARERERERERBbA19cXALKDQGQbSpUqlX3fFwcDQEREREREREQWQKFQwM/PD97e3njx4oWpl0NG4OjoWOzMHxkDQEREREREREQWxN7eXm9BAbIdDAARERERERFZOWWWhKi4JCSmZsDb3RkNA8rA3o79Y4hsCQNAREREREREViwiJh7hO2MRn5yRvc3P0xmhnYPQPtjPhCsjImPiGHgiIiIiIiIrFRETj8EbojWCPwCQkJyBwRuiERETb6KVEZGxMQBERERERERkhZRZEsJ3xkLK42fytvCdsVBm5bUHEVkbBoCIiIiIiIisUFRcUq7MH3USgPjkDETFJRlvUURkMgwAERERERERWaHE1PyDP0XZj4gsGwNAREREREREVsjb3Vmv+xGRZWMAiIiIiIiIyAo1DCgDP09n5DfsXQExDaxhQBljLouITIQBICIiIiIiIitkb6dAaOcgAMgVBJK/D+0cBHu7/EJERGRNGAAiIiIiIiKyUu2D/bA4pD58PTXLvHw9nbE4pD7aB/uZaGVEZGwOpl4AERERERERGU77YD+0CfJFVFwSElMz4O0uyr6Y+UNkWxgAIiIiIiIisnL2dgo0CfQy9TKIyIRYAkZEREREREREZOUYACIiIiIiIiIisnIMABERERERERERWTkGgIiIiIiIiIiIrBwDQEREREREREREVo4BICIiIiIiIiIiK8cAEBERERERERGRlWMAiIiIiIiIiIjIyjEARERERERERERk5RgAIiIiIiIiIiKycgwAERERERERERFZOQaAiIiIiIiIiIisHANARERERERERERWzsHUCyAi0idlloSouCQkpmbA290ZDQPKwN5OYeplERERERERmRQDQERkNSJi4hG+MxbxyRnZ2/w8nRHaOQjtg/1MuDIiIiIiIiLTYgkYEVmFiJh4DN4QrRH8AYCE5AwM3hCNiJh4E62MiIiIiIjI9BgAIiKLp8ySEL4zFlIeP5O3he+MhTIrrz2IiIiIiIisHwNARGTxouKScmX+qJMAxCdnICouyXiLIiIiIiIiMiMMABGRxUtMzT/4U5T9iIiIiIiIrA0DQERk8bzdnfW6HxERERERkbVhAIiILF7DgDLw83RGfsPeFRDTwBoGlDHmsoiIiIiIiMwGA0BEZPHs7RQI7RwEALmCQPL3oZ2DYG+XX4iIiIiIiIjIujEARERWoX2wHxaH1Ievp2aZl6+nMxaH1Ef7YD8TrYyIiIiIiMj0HEy9ACIifWkf7Ic2Qb6IiktCYmoGvN1F2Rczf4iIiIiIyNYxAEREVsXeToEmgV6mXgYREREREZFZYQkYEREREREREZGVYwCIiIiIiIiIiMjKmTQAVKVKFSgUilynoUOHAgAkSUJYWBj8/f3h4uKCFi1a4MKFC6ZcMhERERERERGRxTFpAOjkyZOIj4/PPu3btw8A0L17dwDAnDlzMG/ePCxatAgnT56Er68v2rRpg9TUVFMum4iIiIiIiIjIopg0AFSuXDn4+vpmn3bt2oXAwEA0b94ckiRh/vz5mDhxIrp27Yrg4GCsXbsWaWlp+OGHH0y5bCIiIiIiIiIii2I2PYCeP3+ODRs2oH///lAoFIiLi0NCQgLatm2bvY+TkxOaN2+Ov/76K9/jPHv2DCkpKRonIiIiIiIiIiJbZjYBoO3bt+Px48fo27cvACAhIQEA4OPjo7Gfj49P9s/yMnPmTHh6emafKlasaLA1ExERERERERFZArMJAK1cuRIdOnSAv7+/xnaFQqHxvSRJubapmzBhApKTk7NPt2/fNsh6yfCUWRIirz/Er2fvIvL6QyizJFMviYiIiIiIiMgiOZh6AQBw69Yt7N+/H1u3bs3e5uvrC0BkAvn5+WVvT0xMzJUVpM7JyQlOTk6GWywZRURMPMJ3xiI+OSN7m5+nM0I7B6F9sF8Bv0lEREREREREOZlFBtDq1avh7e2NTp06ZW8LCAiAr69v9mQwQPQJOnLkCJo2bWqKZZKRRMTEY/CGaI3gDwAkJGdg8IZoRMTEm2hlRERERERERJbJ5AGgrKwsrF69Gn369IGDgyohSaFQYMSIEZgxYwa2bduGmJgY9O3bF66urujVq5cJV0yGpMySEL4zFnkVe8nbwnfGshyMiIiIiIiISAcmLwHbv38//vnnH/Tv3z/Xz8aOHYv09HQMGTIEjx49QqNGjbB37164u7ubYKVkDFFxSbkyf9RJAOKTMxAVl4QmgV7GWxgRERERERGRBTN5AKht27aQpLyzORQKBcLCwhAWFmbcRZHJJKbmH/wpyn5EREREREREZAYlYETqvN2d9bofEREREREREZlBBhCRuoYBZeDn6YyE5Iw8+wApAPh6OqNhQBljL42ITEiZJSEqLgmJqRnwdhevAfZ2ClMvi7TA+46IiIjIPDAARGbF3k6B0M5BGLwhGgpAIwgkv10I7RzENw9ENiQiJh7hO2M1+oP5eTojtHMQ2gf7mXBlVBjed0RERETmgyVgZHbaB/thcUh9+Hpqlnn5ejpjcUh9vmkgsiERMfEYvCE6V3P4hOQMDN4QjYiYeBOtjArD+46IiIjIvCik/DowW4mUlBR4enoiOTkZHh4epl4O6YBlA0S2TZklodnsg/lOBpRLQo+Oa8XXBjPD+46IiIjIOHSJebAEjMyWvZ2Co96JbFhUXFK+AQRAlIjGJ2cgKi6JrxVmhvcdERERkflhCRgREZmlxNT8AwhF2Y+Mh/cdERERkflhAIiIiMySt7tz4TvpsB8ZD+87IiIiIvPDABAREZmlhgFl4OfpjPw6xCggJko1DChjzGWRFnjfEREREZkfBoCIiMgs2dspENo5CAByBRLk70M7B7GJsBnifUdERERkfhgAIiIis9U+2A+LQ+rD11OzVMjX0xmLQ+qjfbCfiVZGheF9R0RERGReOAaeiIjMnjJLQlRcEhJTM+DtLkqHmD1iGXjfERERERkOx8ATEZFVsbdTcFy4heJ9R0RERGQeGAAiIiIiq8KsIyIiIqLcGAAiIiIiqxERE4/wnbGIT87I3ubn6YzQzkHsO0REREQ2jU2giYiIyCpExMRj8IZojeAPACQkZ2DwhmhExMSbaGVEREREpscAEBEREVk8ZZaE8J2xyGuyhbwtfGcslFlWPfuCiIiIKF8MABEREZHFi4pLypX5o04CEJ+cgai4JOMtioiIiMiMMABEREREFi8xNf/gT1H2IyIiIrI2bAJNREREFs/b3Vmv+xER2TJOUySyTgwAERERkcVrGFAGfp7OSEjOyLMPkAKAr6d4E0NERPnjNEUi68USMCIiIrJ49nYKhHYOAiCCPerk70M7B/ETbCKiAnCaIpF1YwCIiIiIrEL7YD8sDqkPX0/NMi9fT2csDqnPT66JiArAaYpE1o8lYERERGQ12gf7oU2QL3tXEBHpSJdpik0CvYy3MCLSGwaAiIiIyKrY2yn45oSISEecpkhk/VgCRkREREREZOM4TZHI+jEDiIiIiIiMgqOlc+NtQuaC0xSJrB8DQERERERkcBwtnRtvEzIn8jTFwRuioQA0gkCcpkhkHVgCRkREREQGxdHSufE2IXPEaYpE1k0hSZJVz/FLSUmBp6cnkpOT4eHhYerlEBEREdkUZZaEZrMP5jtdSC4rOTqulc1kFvA2IXPH0kQiy6FLzIMlYERERERkMBwtnRtvEzJ3nKZIZJ1YAkZEREREBsPR0rnxNiEiIlNgAIiIiIiIDIajpXPjbUJERKbAABARERERGYw8Wjq/7iEKiMlXtjRamrcJERGZAgNARERERGQw8mhpALkCHrY6Wpq3CRERmYJWU8B27NiBDh06wNHRETt27Chw33feeUdvi9MHTgEjIiIiMr2ImHiE74zVaH7s5+mM0M5BNjtamrcJEREVly4xD60CQHZ2dkhISIC3tzfs7PJPGlIoFFAqlbqv2IAYACIiIiIyDxwtnRtvEyIiKg69j4HPysrK8zwRERERkbY4Wjo33iZERGQsWgWACvP48WOUKlVKH4ciIiIiIioSZtMQERHlT+cA0OzZs1GlShV8+OGHAIDu3bvjl19+gZ+fH3777Te88sorel8kEREREVFB2E+HiIioYDpPAVu6dCkqVqwIANi3bx/279+PiIgIdOjQAWPGjNH7AomIiIiIChIRE4/BG6I1gj8AkJCcgcEbohERE2+ilREREZkPnTOA4uPjswNAu3btwgcffIC2bduiSpUqaNSokd4XSERERESUH2WWhPCdschrqokEMVY9fGcs2gT5shyMiIhsms4ZQKVLl8bt27cBABEREWjdujUAQJIks5sARkRERETWLSouKVfmjzoJQHxyBqLikoy3KCIiIjOkcwZQ165d0atXL1SvXh0PHz5Ehw4dAABnz55FtWrV9L5AIiIiIqL8JKbmH/wpyn5ERETWSucA0P/+9z9UqVIFt2/fxpw5c+Dm5gZAlIYNGTJE7wskIiIiIsqPt7uzXvcjIiKyVgpJkvIqmbYaKSkp8PT0RHJyMjw8PEy9HCKT44hcIiKyJsosCc1mH0RCckaefYAUAHw9nXF0XCv+vSMiIqujS8xDqwygHTt2oEOHDnB0dMSOHTsK3Pedd97RfqVEZFQckUtERNbG3k6B0M5BGLwhGgpAIwgkh3tCOwcx+ENERDZPqwwgOzs7JCQkwNvbG3Z2+feNVigUZtcImhlARII8IjfnE17+d3hxSH0GgYiIyGLxQw4iIrJFes8AysrKyvM8EVkGjsglIiJr1z7YD22CfFnmTERElA+dx8CvW7cOz549y7X9+fPnWLdunV4WRUT6xRG5RERkC+ztFGgS6IUudcujSaAXgz9ERERqdA4A9evXD8nJybm2p6amol+/fnpZFBHpF0fkEhEREREVwZo1wCefAEuWAH//DZhZyxMiXeg8Bl6SJCgUuT9NuXPnDjw9PfWyKCLSL47IJSJtcEogERWErxFkcw4fBvr3ByQJWLVKbHN3Bxo3Bpo2FadGjQC+DyYLoXUAqF69elAoFFAoFHjrrbfg4KD6VaVSibi4OLRv317nBdy9exfjxo3Dnj17kJ6ejho1amDlypV49dVXAYiAU3h4OJYtW4ZHjx6hUaNG+O6771CrVi2dL4vIVjUMKAM/T+dCR+Q2DChj7KURkZlgA10iKghfI8jmPHoEfPyxCP60aAE4OgLHjwOpqcC+feIEAAoFEBysCgg1bQoEBortRGZG6wDQu+++CwA4e/Ys2rVrBzc3t+yflShRAlWqVMH777+v04U/evQIr7/+Olq2bIk9e/bA29sb169fR6lSpbL3mTNnDubNm4c1a9agRo0amD59Otq0aYPLly/D3d1dp8sjslUckUtEBclvSmBCcgYGb4jmlEAiG8fXCLI5kgR89hlw5w5QvTqwcyfg5ibKvy5cAP76S3W6fh04f16cli4Vv1+unGZA6NVXARcX014nImg5Bl7d2rVr8eGHH8LZufilIuPHj8exY8fw559/5vlzSZLg7++PESNGYNy4cQCAZ8+ewcfHB7Nnz8Znn31W6GVwDDyRCj+9I6KclFkSms0+mG+jeDlD8Oi4VgwSE9kgvkaQTVqzBujXD3BwEEGe117Lf99//wUiI4Fjx8S+p04Bz59r7uPoCNSvrxkU8vc36FUg26FLzEPnAJDs+fPnSExMzDUWvlKlSlofIygoCO3atcOdO3dw5MgRlC9fHkOGDMHAgQMBADdu3EBgYCCio6NRr1697N/r0qULSpUqhbVr1+Y65rNnzzSmlKWkpKBixYoMABH9h/X7RKQu8vpD9Fx+vND9Ng1sjCaBXkZYERGZE75GkM25dg2oVw948gSYMQOYMEG333/2DIiO1swSSkjIvV/lypoBoTp1RMCJSEe6BIB0foRdvXoV/fv3x19//aWxXW4OrdShK/qNGzewePFijBo1Cl9++SWioqIwbNgwODk5oXfv3kj474ni4+Oj8Xs+Pj64detWnsecOXMmwsPDdbxWRLZDHpFLRARwSiARFYyvEWRTXrwAQkJE8OfNN4GxY3U/hpMT0KSJOH3xhSgnu3lTMyD099/ArVvitGmT+L2SJYGGDVUBoSZNgNKl9Xr1iHQOAPXt2xcODg7YtWsX/Pz88pwIpq2srCw0aNAAM2bMACAaTV+4cAGLFy9G7969s/fLeRn5TSIDgAkTJmDUqFHZ38sZQERERJQbpwSaF2ZpkrnhawTZlGnTgBMnxFSv9esBe/viH1OhAAICxOmjj8S21FQgKkoVEIqMBJKTgUOHxEkWFKSZJVSjBptLU7HoHAA6e/YsTp8+jZdffrnYF+7n54egoCCNbTVr1sQvv/wCAPD19QUAJCQkwM9P1Z8kMTExV1aQzMnJCU5OTsVeGxERkS3glEDzwT5tZI74GkE2488/ga++EueXLgV0aG2iM3d34K23xAkAsrKAixc1s4SuXAFiY8VpxQqxX5kymgGh114DXF0Nt06yOna6/kJQUBAePHiglwt//fXXcfnyZY1tV65cQeXKlQEAAQEB8PX1xT55xB5E76EjR46gadOmelkDERGRLZOnBAKqqYAyTgk0HnnKUs5Gu/KUpYiYeBOtjGwdXyPIJjx+LEq/srKAPn2ADz807uXb2QG1agEDBwKrVwOXLwOJicCOHcD48aIczdkZSEoCdu0CvvxSjKb38BBBoOHDgR9/BG7fNu66yeLo3AT64MGDmDRpEmbMmIHatWvD0dFR4+e6NFo+efIkmjZtivDwcHzwwQeIiorCwIEDsWzZMnz0X3rc7NmzMXPmTKxevRrVq1fHjBkzcPjwYa3HwHMKGBERUeGYfWI6nLJEloCvEWTVPvoI+OEHoGpV4MwZEVgxN8+fA+fOqTKEjh0D7t7NvV+FCppZQnXriilkZLUMOgXMzk4kDeXXl0eXJtAAsGvXLkyYMAFXr15FQEAARo0alT0FTD5ueHg4li5dikePHqFRo0b47rvvEBwcrNXxGQAiIiLSDvvPmAanLJGl4GsEWaWNG0X2j729KANr0sTUK9Le7duaZWNnzgA534+7uIgsIfXm0mXLmma9ZBAGDQAdOXKkwJ83b95cl8MZHANAREREZM5+PXsXwzefLXS/BT3qokvd8oZfEBGRrYiLExkyKSlAeDgwZYqpV1Q8T58Cp06J7CC5uXRSUu79atTQzBKqWVOUoZFFMugYeHML8BAREZkzfmJOheGUJSIiE8jMFJk/KSnA66+LvjqWrmRJoHlzcQJET6MrVzSzhC5eFNuuXAHWrBH7eXqKzCA5INSwoWhUTVZHqwDQ33//jeDgYNjZ2eHvv/8ucN86deroZWFERESWjj0zSBucskREZAIzZoiAiIcHsGED4KBzboT5s7MDXn5ZnPr3F9uSkoDjx1UBoRMnxAj6iAhxkn+vTh3NLKEqVTiC3gpoVQJmZ2eHhIQEeHt7w87ODgqFAnn9WlF6ABkaS8CIiMgU5KlOOf9ayv86LQ6pzyAQZZMfLwA0HjN8vBARGUBkJPDGG6JfzoYNogm0rcrMBP7+WzNL6Nat3Pv5+moGhOrXB5ycjL9eykXvPYBu3bqFSpUqQaFQ4FZeDwY18gh3c8EAEBERGRunOlFRMGOMiMgIUlJE35+4OKBXL9EEmjTdvSuCZHJAKDoaePFCc58SJYAGDTSDQj4+plmvjTNoE2hLwwAQEREZG6c6UVGxZxQRkYH16QOsWwdUrizGqnt6mnpF5i89HTh9WjNL6P793PtVraoZEAoOFtPVyKAM2gR67dq1KFu2LDp16gQAGDt2LJYtW4agoCBs2rTJ7DKAiIiIjC0xNe/Mn6LuR7bD3k7BoCARkaFs3iyCP3Z2ovSLwR/tuLgAzZqJEwBIEnD9umZAKCYGuHFDnDZsEPu5uwONGqkCQo0b8zY3MZ0zgF566SUsXrwYrVq1QmRkJN566y3Mnz8fu3btgoODA7Zu3WqotRYJM4CIiMjYmAFERERkZv75RzQ2Tk4GJk8Gpk419YqsS3KyaCgtB4SOHwdSUzX3USiAWrU0s4SqVWNz6WIyaAmYq6srLl26hEqVKmHcuHGIj4/HunXrcOHCBbRo0QL380oFMyEGgIiIyNjkHkCFTXViDyAiIiIjUCqBVq2AP/4QGSl//gk4Opp6VdZNqQQuXACOHVMFhW7cyL1fuXKaAaFXXxUZR6Q1g5aAubm54eHDh6hUqRL27t2LkSNHAgCcnZ2Rnp5etBUTERFZEXs7BUI7B2HwhmgokPdUp9DOQQz+EBERGcOcOSL44+Ymmj4z+GN49vYi46pOHWDwYLEtIUGzufSpU6KX0K+/ihMg7pv69TWDQv7+prseVkbnDKCPPvoIly5dQr169bBp0yb8888/8PLywo4dO/Dll18iJibGUGstEmYAERGRqXCqExERkYmdPCmCCJmZwOrVQN++pl4RyZ49ExPG5IDQsWPAv//m3q9yZc2AUJ06gIPOuSxWy6AlYI8fP8akSZNw+/ZtDB48GO3btwcAhIaGokSJEpg4cWLRV24ADAAREZEpcaoTERGRiTx5AtSrB1y7BnzwgWgCzX4z5kuSgJs3NZtL//03kJWluZ+ra+7m0mXKmGTJ5oBj4NUwAERERERERKSjy5eBgweBgQMtN9tiwABg5UqgQgURSChd2tQrIl2lpgJRUaqAUGSkaDidU82aQIcOwMSJNhcMMngA6M8//8TSpUtx48YNbNmyBeXLl8f69esREBCAZvJoODPBABAREREREZEOMjPFtKYrV4B584D/+r5alF9+Abp1Exk/Bw8CLVqYekWkD1lZwMWLmllCV66ofl62LDB3LtC7N2BnZ7p1GpEuMQ+db5FffvkF7dq1g4uLC6Kjo/Hs2TMAQGpqKmbMmFG0FRMREREREZF52LxZ9aZ63jzg+XPTrkdXd+6IzCUAGDeOwR9rYmcngpMDB4qeTpcvA4mJwM8/i+0PHgD9+gHNmwPnz5t6tWZH5wDQ9OnTsWTJEixfvhyOat3TmzZtiujoaL0ujoiIiIiIiIwoMxOYNk31/Z07wKZNpluPrrKygD59gEePxEjx8HBTr4gMrVw54P33gTNnRPZPyZLA0aOi/9Po0aKMjAAUIQB0+fJlvPnmm7m2e3h44PHjx/pYExEREREREZmCnP3j5QVMmiS2zZ6duxGvufrmG1Hy5eoK/PADUKKEqVdExuLoKAI+Fy8CXbsCSqV4PNSsKTKErLv9sVZ0DgD5+fnh2rVrubYfPXoUVatW1cuiiIiIyIbt2SNq91NSTL0SIiLbolSqsn9GjxYnDw/xhnrXLtOuTRvR0aIJMAAsWADUqGHa9ZBpVKwoekDt3g1UrQrcvQt07w507CgmwtkwnQNAn332GYYPH44TJ05AoVDg3r172LhxI0aPHo0hQ4YYYo1ERERkSzp2BNavB6ZPN/VKiIhsi3r2z9ChgKcnMHiw+NmsWeadQZGWBvTqBbx4Abz3HvDJJ6ZeEZlax45ATAwwZYrIBIuIAIKDRVlgRoapV2cSOgeAxo4di3fffRctW7bEkydP8Oabb2LAgAH47LPP8PnnnxtijURERGSL7twx9QqIiGyHUglMnSrOf/EF4O4uzg8fDjg5ifHbR4+abn2F+eIL0RDY3x9YvlxM/yJycREBn5gYoG1b4NkzICxMBIIiIky9OqPTOgAUExOTff6rr77CgwcPEBUVhePHj+P+/fuYNm0aZs2aZZBFEhERkQ2ytzf1CoiIbIec/VOmDKD+wb6fn2iqDIheQObo11+BJUvE+XXrRAYTkbrq1UXA56efRJDw+nWgQwdRGmZDHzhpHQBq164dbt68mf29q6srGjRogIYNG8LNzQ2zZ89GaGioIdZIREREtogBICIi41DP/hk9WpX9IxszRozf3r3b/EZrx8eryr1Gjwbeesu06yHzpVCIgM/Fi8DIkeL/jJ9/Fk2i580T5YNWTusA0BtvvIE2bdogMTEx18/mzp2LSZMmYcOGDXpdHBEREdkwBoCIiIwjv+wfWbVqYsw2AMyZY9y1FSQrC+jbF3j4EKhbl73jSDseHiLgc/o00LQp8OSJKCF89VXzLnPUA60DQBs2bEC1atXQtm1bJCcnZ2//5ptv8OWXX2L9+vXo3r27QRZJRERENshO51aFRESkq8Kyf2TjxomvmzYBapUhJrVwIbB3L+DsLEa+OzmZekVkSV55BfjzT2DlSlE2eP488MYbQP/+wP37pl6dQWj9n5WDgwO2bt0KNzc3vP3228jIyMD8+fMxfvx4rF27Fj169DDkOomIiMjWMABERGR4hWX/yF59FWjdWgSM5s0z3vry8/ffqqDUvHmijIdIV3Z2IuBz+TIwcKDYtno18NJLwLJlIsvMiuj0n5WLiwt2796N1NRUvPrqqxgzZgxWr16NXr16GWp9REREZKtYAkZEZFjaZv/I5IDLihXAgweGXVtB0tOBnj2B58+Bzp2BQYNMtxayDl5eIuATGSkygx49Aj77DGjSBDhzxtSr0xutA0A7duzAjh07cOTIEQwePBjXr1/He++9Bw8Pj+yf7dixw5BrJSIiIlvCDCAiIsPSNvtH9tZbIhMoPR349lvDry8/Y8cCsbGAj48o3+HId9KXxo2BU6eABQtEQDQqCmjQABg2DFBrhWOpFJIkSdrsaKfFP2EKhQJKpbLYi9KnlJQUeHp6Ijk5GR4eHqZeDhGZiDJLQlRcEhJTM+Dt7oyGAWVgb2db/yzwNiCLoFQCDg7i/LBh4h8wIiLSP6USqFVLlL589RXw5Zfa/d6WLcAHH4ig0a1bgJubYdeZ02+/AZ06ifN79gDt2xv38sl23LsnmkNv3iy+9/UFvvlGZJ+ZUdBRl5iHg7YHzbKy2jcish0RMfEI3xmL+OSM7G1+ns4I7RyE9sF+JlyZ8fA2IIvx7JnqPDOAiIgM58cfRfBH2+wfWdeuYirYtWuiFGzECIMtMZd//wX69RPnhw9n8IcMy99fND3/5BNg6FCRLffRRyLr7LvvgJdfNvUKdcb/rIjIqkXExGPwhmiNwAcAJCRnYPCGaETExJtoZcbD24AsSoba45Q9gIiIDEO9988XX4ix2NqytwfGjBHn580DXrzQ//ryIkmiWW9iIlC7NjBrlnEul6h1a9F0fPp0MXHu4EGgTh1g4kQgLc3Uq9MJA0BEZLWUWRLCd8YirzpXeVv4zlgos7SqhLVIvA3I4qhnAGlXpU5ERLoqavaPrHdv0X/n9m2RIWEM330nyr+cnMTId2dn41wuESAedxMnit5TnTqJwOeMGUBQELBzp6lXpzUGgIjIakXFJeXKelEnAYhPzkBUXJLxFmVkvA3I4qgHgDIzTbcOIiJrVZzsH5mzMzBypDg/e7bhR2VfuCCmlAHA3LlAcLBhL48oPwEBIuCzfTtQqZLog/XOO0CXLsDNm6ZeXaEYACIiq5WYmn/goyj7WSLeBmRx1ANAxiorICKyJcXN/pENGiSCR7GxwO7d+ltfThkZQK9e4u9D+/bFWzORPigUIuATGwuMHy+GV+zYIbKBZs4Enj839QrzpdcAkJYDxYiIjMLbXbvU4Pz2U2ZJiLz+EL+evYvI6w8tskyquLeBoVjDbWuN5PtlW/QdrPzzBradMcH9wwwgIiLD0Uf2j8zTUwSBAJEFZChffin6r5QrB6xZY1bTl8jGlSwpAj7nzgHNmwPp6eLx+sorok+QGdJ6Cphs5syZmDBhQq7tSqUSISEh2GSsGlAiokI0DCgDP09nJCRn5NkDRwHA11OMQ8/JWqZmFec2MBRruW2tTV73i8yo9w8zgIiIDEdf2T+yESOA+fOBY8eAo0eBZs2Kf0x1e/cC//ufOL96teg7RGRugoKAQ4eAjRtFYPXSJeCtt8TEsK+/FuPjzYTOGUDz58/HsmXLNLYplUr06NEDZ8+e1de6iIiKzd5OgdDOQQBEoEOd/H1o5yDY22n+1JqmZhX1NjAUa7ptrUl+94ss3pj3DzOAyBY9eQJcvGjqVZC1S04Gxo4V50eNKl72j8zPD+jTR5zXdxbQ/fuqYw8dKhrvEpkrhQIICREB1qFDxfcbNwIvvQQsWiSy78yAzgGg3377DePGjcNPP/0EAHjx4gW6d++OCxcu4NChQ3pfIBkWyzDI2rUP9sPikPrw9dQscfL1dMbikPq5MhqscWqWrreBoVjjbWsNCrpfcjLK/cMMILI1t24BdesCtWoBR46YejVkzcaOBe7eBapVUzVw1ocxY8Sb3V27gJgY/RxTkoABA4CEBKBmTdH4mcgSlColAj5RUUCDBkBKCvB//wc0bCi2mZjOJWCvvvoqtm3bhi5dusDJyQkrV67E9evXcejQIfgwJc+isAyDbEX7YD+0CfJFVFwSElMz4O0uSp7yynrRZWpWk0AvA65av3S5DQzFWm9bS1fY/SIz2v3DDCCyJVevijKB27fF9zNnij4SRPp28CAgV3GsWAG4uurv2NWrA++/D/z8MzBnDrBuXfGPuWyZaKpbooQY+e7iUvxjEhlTgwbA8ePisTxhAhAdDTRuDHz6qRgfX8Z47RfUFakJdIsWLbB+/Xp069YNN2/exJEjRxj8sTAswyBbY2+nQJNAL3SpWx5NAr3yDXxY89QsbW8DQ7Hm29aS6Xp7G/z+YQYQ2YoLF4A33xTBn8BAwM4O+P130UyUSJ+ePhXZNAAweLBhgozjxomvmzaJrLbiuHRJlaE0c6bIkCOyRPb24jl3+TLQu7fIbFu6VJSFrVkjvjcyrTKAunbtmuf2cuXKoVSpUvj000+zt23dulU/KyODKawMQwGR5t8myNfobxCJTM1cp2ZZA9625knX29vg9w8DQGQiyizJeFmSZ84AbdoADx8CtWsD+/YBw4YBP/0kGoauX2+YyyXbNGkSEBcHVKwIzJplmMto0EBksx04AMybByxYULTjPH8uRr6np4vnyIgRel0mkUn4+ABr1wL9+wNDhojx8f36AStXAosXA8HBRluKVhlAnp6eeZ7atWuHwMBAjW1k/nQpwyCyNfLUrPz+5VdAlEoac2qWteBta57k+6UwRrt/WAJGJhARE49msw+i5/LjGL75LHouP45msw8aJiM6MhJo2VIEfxo0AA4fFm8OxowRP9+8GfjnH/1fLtmmv/5SBWOWLdNP4+f8yFlAK1YADx4U7RiTJokAqZeXyJCwK1LBCpF5at4cOHtWlEq6uorJeXXritf/J0+MsgStMoBWr15t6HWQEbEMgyh/8tSswRuioQA0MuVMMTXLmvC2NU/q90thichGuX8y1P72MAOIjEAui8/5+JfL4vXaLP/wYeDtt0VJTrNmommu/AFqgwYiMHTokBirPW+efi6TbFdGBvDJJ6LMpE8foH17w15e69ZA/fqi18miRUBYmG6/f+CAqtnzihWAv7/el0hkco6OIuDz4Yciw23bNpH5uXmzeO3v2lU0VTcQhlRtEMswiApmLlOzrBFvW/Mk3y/5ZQL5GfP+YQYQGZFRpxNGRAAdOojgT+vW4vuc2fNyFtDy5cDjx8W/TLJt06aJfjo+PsYJKCoUqiygb78Vj3VtPXyoGvn+6afAu+/qfXlEZqVSJWDrVvFBQEAAcOcO0K0b0LEjcO2awS5WIUmFdx6qX78+Dhw4gNKlS6NevXpQFBCRio6O1usCiyslJQWenp5ITk6GhyFTHi2IMktCs9kHkZCckec/PAqIN2NHx7XiJ/Fk04zaD8LG8LY1T/L9kpCcjqSnz1HGzQm+Hka+f+bOFaOKAaBRIzFBg8hAIq8/RM/lhT/GNg1sXLzpd9u2iU97X7wQGUBbtgDOeQRcJQmoU0eM0p45Exg/vuiXSbYtOlqMnVYqxZvM994zzuUqlaLB7fXrovRs2LDCf0eSgO7dgV9+Eb97+jRQsqTh10pkLtLTxWv+7NmiD5aTk5gcNm5c3n8rctAl5qFVCZg88h0A3mU01uKxDINIO/LULNI/3rbmySzuF2YAkREZpSz+hx/E9BelUrzJ3bBBjLbOi0IhsoD69BFvnkeOFG8EiHTx4oUo/ZIfc8YK/gBi6tGYMcCgQcA334gJSI6OBf/O6tUi+OPoKJ4vDP6QrXFxAaZOBUJCgKFDgf37RQnl+vWiSXSbNnq7KK0CQKGhoQAApVKJFi1aoE6dOihdurTeFkHGJ6f7h++M1WgI7evpjNDOQSzDICIi0+AUMDIig5fFr1wJDBwoMhx69xbfOxTy73ePHsCXXwJ374pg0SefFO2yyXbNmSMazZYpI0qxjK1PHyA0VDQz37wZ+Pjj/Pe9elWVJTR9uughRGSratQA9u4VWaIjRohMutOn9RoA0qkHkL29Pdq1a4fHrEm2Cu2D/XB0XCtsGtgYC3rUxaaBjXF0XCsGf4iIyHSYAURGZNDphN9+CwwYIII/gwaJLIfCgj+AyA4aOVKc//prICtL98sm2xUbKzIJAGDhQtH/x9icnYHhw8X52bPzfwy/eCFGvj99Khqgjx5tvDUSmSuFAvjgA9G/a9o0YNQovR5e5ybQtWvXxo0bN/S6CDIdOd2/S93yaBLoxbIvIiIyLWYAkRHJZfEAcgWBilUWP2uWKqth1Cjg++91G2c9cKAY133pkmgQSqQNpRLo31/0EOnUSQRXTGXwYMDdHbhwAfjtt7z3CQsDTp0CSpcG1q3jyHcidR4ewKRJ+ZcMF5HOz7KvvvoKo0ePxq5duxAfH4+UlBSNExEREVGRMQBERqbX6YSSBEyZIpp3AsDkySKLR9eRvh4eImsIUI3FJirMwoXAiRPi8bNkiUFHSReqVCnVY3j27Nw//+MP0fQWEFPvKlQw2tKIbJlWU8DU2alFZtWngUmSBIVCAaVSqb/V6QGngBEREVmQfv2ANWvE+QoVgNu3Tbocsh3Fnk4oSaKERR63PWuWaiR2Udy7B1SpIgKhkZFA48ZFPxZZv2vXxAS59HRg2TKRRWZq9+6J8dbPnwNHjwKvvy62P34s1nr7tshYWrnSpMsksnR6nwKm7tChQ0VeGBEREVGBmAFEJlKsKXhZWWJyy5Il4vuFC4H/+7/iLcjfX0yEWb1aZAH98kvxjkfWKytLBHzS04FWrUTvKXPg7y+an69YIbKAduxQ9cS6fRuoVk1MuyMio9E5ABQQEICKFStqZP8AIgPoNj+lIyIiouLQVxNoSQJSUgBPz+KviaggmZliUte6daLkZvly/U3uGj1aBIC2bQOuXBETYohyWr4cOHwYcHUV501Z+pXTmDEiw2fnTiAmBoiOBn78UYyL37gRcHMz9QqJbIrOPYACAgJw//79XNuTkpIQEBCgl0URERGRjdJXBtB334keFOvWFXtJRPl6/lw02l23TvWGVp9j24OCgLffFgFNubSMSN3t2yLIAgBffQVUrWra9eRUowbQtas4P2qUyJQDgPBwoGFD062LyEbpHACSe/3k9OTJEzg7O+fxG/kLCwuDQqHQOPn6+mpcVlhYGPz9/eHi4oIWLVrgwoULui6ZiIiILEVGhup8UTOAlEpV49ywMI6TJ8PIyBBvbLdsARwdxdeePfV/OfKb+zVrgH//1f/xyXJJEvDZZ0BqKtCkSfHLDg1F7oW1bx/w5AnwxhvA+PGmXRORjdK6BGzUf/PnFQoFJk+eDFdX1+yfKZVKnDhxAnXr1tV5AbVq1cL+/fuzv7e3t88+P2fOHMybNw9r1qxBjRo1MH36dLRp0waXL1+Gu7u7zpdFREREZk4fGUD79wP//CPOx8WJ8pnu3Yu/NiLZ06fAu++Kx5qzs3iMtW9vmMt64w2RKREVBSxaBEybZpjLIcuzYQOwZ48YE71ypchCM0evvSZ6Ex08KMpy168337USWTmtM4DOnDmDM2fOQJIknD9/Pvv7M2fO4NKlS3jllVewRp7aoQMHBwf4+vpmn8qVKwdAZP/Mnz8fEydORNeuXREcHIy1a9ciLS0NP/zwg86XQ0RERBYgZwBIt2GlgjxRplQp8fWbb4p2HKK8pKSIYM/+/UDJkuINuKGCP4Do5zJ2rDj//fci+ESUkAAMHy7Oh4YCNWuadj2F+eYbkaX0ww9A5cqmXg2RzdI6A0ie/tWvXz8sWLBAbyPVr169Cn9/fzg5OaFRo0aYMWMGqlatiri4OCQkJKBt27bZ+zo5OaF58+b466+/8Nlnn+V5vGfPnuGZ2j+PKSkpelknERERGYF6AAgQ0210+aT4/n1g+3Zx/qefRP+UEyeAv/5SjSAmKqqkJKBdO+DUKZHJEBFhnPHs774rJiZduwasWmW+pT5kPJ9/Djx6BNSrpyoTNGd164rXYSIyKZ17AIWEhOQb/Fm0aJFOx2rUqBHWrVuH33//HcuXL0dCQgKaNm2Khw8fIiEhAQDg4+Oj8Ts+Pj7ZP8vLzJkz4enpmX2qWLGiTmsiIitw8aL4lDQry9QrISJd5QwA6VoGtn69+J0GDYA2bYCPPxbbv/lGP+sj2/Xvv0CLFiL44+UlylmMEfwBRBD0v3YMmDfPdH2t0tPFiPujR01z+ST88os4OTiIgKCjo6lXREQWQucAULdu3XDy5Mlc2+fPn48vv/xSp2N16NAB77//PmrXro3WrVtj9+7dAIC1a9dm75PXuPm8mlDLJkyYgOTk5OwTR9MTmT9lloTI6w/x69m7iLz+EMqsYpZq9Owppkxs26afBRKR8eQMAOnyRleSVOVfAwaIr/Kb5u3bRfYEUVHcuQM0bw6cPw/4+gJHjgD16xt3DX37AuXKATdvAj//bNzLln3+uSg7euMNUfZ2+rRp1mHLHj5UTdIaN05k1hARaUnnANC8efPQsWNHxMbGZm/7+uuvERoamh3AKaqSJUuidu3auHr1avY0sJzZPomJibmygtQ5OTnBw8ND40RE5isiJh7NZh9Ez+XHMXzzWfRcfhzNZh9EREx80Q54/Tpw7pw4Hxmpv4USkXEUJwPo+HEgNhZwcQF69BDbgoKADh1EcOh//9PfOsl2xMUBb74JXL4MVKwI/PEHUKuW8dfh4iICMICYcmfsvlY//yyyTRQKkXny++8i0+799wFO6TWekSNFNlrNmsDkyaZeDRFZGJ0DQP369cO4cePQtm1b3Lx5E7Nnz8a0adOwZ88evPHGG8VazLNnz3Dx4kX4+fkhICAAvr6+2LdvX/bPnz9/jiNHjqBp06bFuhwiMg8RMfEYvCEa8ckZGtsTkjMweEN00YJA6lk/ObMVJUmMhyYi81WcDCA5++eDD0R/Ftno0eLr6tXi03MibV2+LLJd4uKAwEDgzz+B6tVNt54hQ0QgKDpalKAZy+3bwMCB4vy4ccClS6K8UqEAtm4FatcGevcGbtww3pps0W+/iTJXhUIE45ycTL0iIrIwOgeAAGD06NH4+OOP0aBBA8yaNQt79+4tUlBm9OjROHLkCOLi4nDixAl069YNKSkp6NOnDxQKBUaMGIEZM2Zg27ZtiImJQd++feHq6opevXoVZdlEZEaUWRLCd8Yir88v5W3hO2N1LwdTDwBFR6sCPkql+KTytdfYG4jInBU1Ayg1Fdi8WZyXy79kLVuKMon0dGDJkmIvkWzE+fMi8+fuXZFt8ccfpp9eVLYs0L+/OD93rnEuU6kUwZ7Hj8Xf0fBwEQxbt07cRl27ig9Y1q8HXnoJGDxY3GakXykpgDwEZ8QI4/WfIiKrotUUsIULF+ba5ufnB1dXV7z55ps4ceIETpw4AQAYNmyY1hd+584d9OzZEw8ePEC5cuXQuHFjHD9+HJX/++M6duxYpKenY8iQIXj06BEaNWqEvXv3wt3dXevLICLzFBWXlCvzR50EID45A1FxSWgS6KXdQRMSVGVfTk7Akyfi09ugINH7Izpa/OzJE4DloZZHLncooA8cWYGcASBts/Z+/FGMx37ppdzTvhQK4IsvxJvYb78VGUH85JwKcuqUmPaVlCSCh3v3iv475mDUKGDxYlGC9fffQJ06hr28uXNFz6OSJcUI7xIlVD+rVUs0Iz51Cpg0SaxpyRJgzRqRrTR+vPncbpZu7FjRiyowEJg+3dSrISILpZCkwguIAwICtDuYQoEbZpb6mZKSAk9PTyQnJ7MfEJEZ+fXsXQzffLbQ/Rb0qIsudctrd9ClS4FBg4CGDcU/qEePAmvXirT0LVtEWQggSkDKlCn64sn4MjKAV18VbyR+/51v3q1VXiPfb97ULuuiSRPRA2jOnLxHIr94AQQEiMyElStVWRREOR07BnTsKDIuGjUC9uwBSpc29ao0ffgh8NNPIqi5bp3hLufkSaBpU1GKqc3z5o8/gIkTVVPC3NxEz5ovvtAsyyTdHD4sMhkB4NAhMY2OiOg/usQ8tCoBi4uL0+pkbsEfIjJf3u7Oet0PgKr86733RKkXoOoD9Pffqv3YB8jyXLokmvseOQKEhZl6NWQoObN/AO1KNmNiRPDHwUEEfPPi6CimFwFijLaxG+iSZThwAGjbVgR/3nwT2LfP/II/gCrIuWmT6M9jCE+eAB99JII/3boB/foV/jtvvimCQHv2iClpT54A06aJ4Ovs2SJLj3Tz9CnwySfi/GefMfhDlAe9TxS2YkXqAQSIhsyXL19Gpi7NGYmI/tMwoAz8PJ2RXzGPAoCfpzMaBmiZqZOcrGqIqR4AOnVKfFUPAPF1y/Ko95OYMwf46y/TrYUMJ0OtLNT5v+CvNgEguflz585AAZNCMXCgyEi4cEFkkpkjSQLmzxdZKGQ8L14AM2aIzJ+0NBEE2rMHMNe2Aw0aiIyQzEzxeDGE4cOBq1eBChWAZcu0L79VKMSI+FOnxOSwmjWBR49EOVhgoCjDzCvYS3mbPFk0165QQfz9IyINep8obOV0DgClpaXhk08+gaurK2rVqoV//vkHgOj9M2vWLL0vkIisk72dAqGdgwAgVxBI/j60cxDs7bT8h3P3bvEPfM2aogdIgwZi+9mzYrs8Gh5gAMgSqQeAsrKAPn34SbI1kt8UKhSqPiOFBYCePRPNZ4HczZ9zKlVKtc833xR5mQa1c6comZE/8SfDi4oSJaYTJwLPn4ux5jt2AK6upl5ZweQsoGXLRINmfVIf+b5hQ9GyoBQKcVuePy/KsQMCxPjyYcOAGjXE8fn3uGDHj6sCfMuWsX8hUQ4GmShs5XQOAE2YMAHnzp3D4cOH4eysKs1o3bo1fvzxR70ujoisW/tgPywOqQ9fT80yL19PZywOqY/2wX7aH0y9/AsAqlUT/QYyMsQn6bduqfblP5yWRw4AffghUL68aOo9YYJp10TaycwUzdm1Kb2UA0BOTqpeQIUFgH79VfT1Kl9eNO0tzPDh4tj794sAsbn57Tfx9epVzYwo0r8nT1TTlM6fB7y8RDBxyxbL6DPWvj0QHCyuhz6n26mPfB8/HmjevHjHs7cXpZmXLonm1f7+wD//iCBnrVqigTunc+b27JnouSRJotdThw6mXhGRWTHYRGErp3MAaPv27Vi0aBGaNWsGhVoqaFBQEK5fv67XxRGR9Wsf7Iej41ph08DGWNCjLjYNbIyj41rpFvxJT1e9aZIDQAqFKgto9WrN/RkAsjxyAKhWLfGpMSDKCA4cMN2aSDsrV4omsuPGFb6vegDI7r9/UQp7Y7hihfjar1/uBtJ5qVJF9DMBRC8gcyJJouwIENf72jXTrsea7dkjXk8WLBC3e0gIcPGi+GopkwYVClUW0IIF+imrUh/5/tprYuS7vpQoIQY1XLsGfP21CLhduQL06CH6Be3axd5c6qZNE49JHx/DlfkRWTBdJgqTis4BoPv378Pb2zvX9qdPn2oEhIiItGVvp0CTQC90qVseTQK9tC/7ku3bJ3o2VKwo0vhlch+gLVs092cAyPLIAaDy5UVvjsGDxff9+4v+T6R/p08DDx4U/zhyH64lSwq/r3QNAN28KTJ5AN2men3xhfi6aZNmeaGpXbwoMiNkly6Zbi3WKjFRNDbu2FHc1pUrAxERIvPHEseV9+ghXhcTEoCNG4t/vJwj3x0di3/MnFxcxHPwxg0RYPLwEGXanTuLYPGhQ/q/TEtz9iwgt9b47jtOLiXKQ2Kqdlmy2u5nK3QOAL322mvYvXt39vdy0Gf58uVo0qSJ/lZGRKQtufzr3Xc1P7mVM4DS0zX3ZwDI8qgHgADRCLNqVfEGbuRI063LWl28KJ4/3bsX/1h37oivT5+qsrfyo2sAaPVqkTHw1luiv4i2XnsNeOMN8Vrw7bfa/56hydk/MgaA9EeSRB+amjVFYMPOTrx2xMRoVzporkqUEGVsgAjeFKeU6uRJ0XAYEM+LatWKvbwCeXgAU6aIQNC4cSIwdPw40KoV0Lo1cOKEYS/fXL14IQLaSqXoofT++6ZeEZFZMshEYRugcwBo5syZmDhxIgYPHozMzEwsWLAAbdq0wZo1a/DVV18ZYo1ERPnLzBRNUwFV+ZdMzgDK63fIsshBBDkA5OYm3swpFCIIID8GTMTqxo9euCC+XrlS/GOpj6hetKjgXkC6BICUSlV5Z2HNn/MyerT4umQJkJqq++8bQkSE+FqhgvjKAJB+XL8uMgf79gWSkoBXXhGBhnnzxGuJpfv0UxFMuXRJDEQoCvWR7927i9vKWLy8RLbL9evA55+LrKMDB0Rvpi5dNKd42oKvvwbOnBFZP4sWmXo1RGZL7xOFbYTOAaCmTZvi2LFjSEtLQ2BgIPbu3QsfHx9ERkbiVfXSCyIiY/jzT9EA1stLfKKvrmLFvFP6tWlGS+YjPV2MEAZUASAAaNZMVcozcKB+ypWKwCrHj/77r/iapIe6eTl4Z28vPumX+3XlRZcA0L59IrhUpozI/tPV22+LSUTJyYVnJhnDkyfAH3+I859/Lr4yAFQ8mZkiK6Z2bVEq6OwMzJwpMl3y+4DAEnl4iN46gLi+RSGPfK9YEVi61DR9kPz8RObRlSuip5ednZjGVrcu0LOnfgLS5u7iRSAsTJyfPx/w9TXlaojMmt4nCtsInQNAAFC7dm2sXbsWMTExiI2NxYYNG1C7dm19r42IqHBy+VfnzoCDg+bPFArVP/klSoh/bAFmAFkaufzLxUWM8VY3bRoQFCQCFkOHGn1pVjt+NCFBfM3IyF1CqYvUVFXfn08/FV8XLMh//7wCQPkFbOXmzyEh4o29ruQSIEC80TL168KhQ2IEeUAA8M47YtulS2yKW1TR0UDDhsDYseIx3LKlmPQ1frxh+tqY2vDh4nr9+afIbtKF+sj39euLNvJdn6pUEeu5cAH44APxHNi8WbzWDxig2SfLmiiVYjLa8+di4ldIiKlXRGT29DpR2EZoFQBKSUnR+kREZDSSBGzfLs7nLP+SyQGgWrVUbxJN/UaPdCMHgCpUyP2ptLMzsG6dyC756SfxJsFIrHr8qBwAAoqXBSRn/3h6ih4fdnaitEMuMctJ2wygxESRGQCIN0xF1bu3yB68eVMVTDYVuf9Phw6i94qDg+ibZE5Nqi3FihUi+HPmjAhmrFolHneG7mljSv7+qoCBLllA+h75rk8vvyxGxJ85A3TqJAIkK1cC1auLgJecqWgtFi0CIiMBd3fTZWERWSC9TBS2IVoFgEqVKoXSpUsXeJL3ISIymtOnxT+vJUsCbdrkvU/PnmLKyyefqDKEGACyLDkbQOf06qvApEni/JAhQLxxsm6sevyo+hsrufyuKOT+PxUqiOehHKhduDDv/eUAkLNzwQGg9etFo9SGDYE6dYq+PldXVebY11+bLttGffx7+/YikyMwUHzPMjDdTZumaqAbGyvKiWzhzbTc12rbNlHOVRhDjnzXp7p1xYj4Y8eAFi1EhszChWIQwJdfFu81ylzcuCGuCyACeHLGMhFppdgThW2IVgGgQ4cO4eDBgwWe5H2IiIxG/sS+QwdRHpSXl14Sn+4PHcoAkKUqLAAEABMnAvXrizcCAwca5Y28VY8f1VcGkBwAkt/MDBsmvq5fn/dx1TOA7O3F+ZwBIElSlX8VJ/tHNnSouLyoKPEG0xSuXBGvUyVKiAlIgMh+ABgA0tXTp6oSoaVLbauHSlCQyJSRJNHgujBz5hh+5Ls+NW0KHDwo+n81bAikpYmeTgEBwPTp5tPMXVeSJP5upaWJAJeckUVEZABaBYCaN2+u9YmIyGjkAFB+5V85MQBkmbQJADk6ilIwJycxBccITX2tevyovkvA5KlWb7whJjClp6uCOOoy/guWFVQCFhkpgiKurkCPHkVfm8zbW2RBAMA33xT/eEUhZ/+8+aZ4Mw4wAFRUcqNgLy9xsjVjx4qvq1eLUsn8nDwpRrADxhn5ri8KhRgRf/w48OuvQHCw6DM2ebLImvvf/1SvI5ZixQoR2HJxEeftitSilYhIK1q/wsyZMwfpao0g//jjDzyTP6kDkJqaiiFDhuh3dURE+bl8WUzLcHQUn3hqgwEgy6RNAAgQfZ6mTxfnR4wQGRUGZLXjRyVJswRMnxlACoXo3QEA332X+7moTQ8gOXD0wQdi+pE+jBolvv76q3alM/qm3v9HxgBQ0Vy+LL7Kt5+teeMNkR3z7Fn+I8RNOfJdXxQK0Sz93DmRvVStGnD/vnguV68OLFsmykTN3Z07qmmWX32lKv0kIjIQrQNAEyZMQKpaauXbb7+Nu2qNCdPS0rB06VL9ro6IKD9y9k+rVqLBrDYYALJM2gaAADHV6fXXxRucfv3yHyGuBzqNH330SHzibgkePxY9NmT6yABS72fRsydQtqwo0/n1V839CwsApaSIprCAmAakLzVrqkpn/vc//R1XG2lpogwHYABIH+Tby1YDQAoFMGaMOP/dd6IkLidzGPmuL3Z24jUlNhZYvlxcpzt3gM8+E8/rjRvznyRoapIEDBokStcaN1aVyBIRGZDWASApRz+FnN8TERmVruVfgCoAZK7/DFLe5CCCNgEge3tgzRpRHnT4cP6fgOuJ1uNHBw4Un8rv32/Q9eiFevkXoJ8MILkEDBANnj/7TJzP2Qy6sADQjz+KgMnLL4t+IPokfwq/Zg3w8KF+j12Qw4fF9a5USTNo8dJL4uvdu5bb28QU5ACQfPvZovfeE5kkSUm5y2HNbeS7vjg6iqDwlSvA/PmitPP6dTEZ7ZVXxP8M5vbe5YcfRMlyiRJiupnc94yIyIBYZEpElufuXdGwVaEAunTR/veYAWR5srJUU720CQABohTg66/F+XHjVCUhBlLo+FGlEvj9d3F+61aDrkUv9BkAyisDCAAGDxbPxz/+AM6eVW0vLACk3vxZ31kLLVoA9eqJ/kQ9e4qJSMuWATt3AqdOAffuGea1Q738S/06lS4N+PiI8wZ+DFsVW88AAkQgQQ5ozpunetyqj3yfMMG8Rr7ri7OzyHC6fl2UVJUqBVy4AHTtCjRqBOzdax6BoH//VWX8TJkiGngTERkBA0BEZHm2bxdfmzTRbcILA0CWJzFR3F8KhW739aBBQJs2ohlonz4Gv88LHD964YIoSQPE9Bpzp97/Byj6iOWUFHECNDOAABHM69ZNnFfPAiooAHT+vAj8OjgAvXsXbU0FUS+d2bcPCAsTmUrvvCNGZJcvLz6p9/UVgaKOHUUgavJk4PvvRYbB8ePArVuq66GNvPr/yFgGppusLFUTaFsOAAGir0/ZsqIX2i+/aI58b9hQPL6tmZubGKseFyemRJYsKcpw27UDWrY03cQ/2f/9nwiu162ratxNRGQEDrrsvGLFCri5uQEAMjMzsWbNGpQtWxYANPoDEZH+KLMkRMUlITE1A97uoqGsxptLW1SU8i+AASBLJPf/8fHRbUSxQiFS6mvXBk6cAObOFZ94m8Lx46rz164BN24AVauaZi3akDOA7OzEG+qiZgDJ2T+lSok3YzkNGwZs3izKIGbPBsqVKzgAtHKl+NqliyjvMIQePcTlxsaKzDP107//ijfR//4rTuqZS3l4UaYsYgd9gbT+A/N/3b52TWQqODqqxr+re/ll0R+IASDt3L4tMrgcHcVocFvm4iKCDKGhYtz7jRviseTmJvrimPvId30pVUoMBxg2TIyM//57cTs0ayaCrtOnA/XrG3dNW7cCW7aITK1Vq2znviAis6B1AKhSpUpYvnx59ve+vr5Yv359rn2I9E6SLLtBYTFExMQjfGcs4pNVI039PJ0R2jlIVV5ia5KSRM8MQPcAkFxfzwCQ5dClAXROFSuK7JI+fcSboI4dRS8IY4uM1Px+3z5VDxxzJGcAVa0qAhRFDQDl1f9HXePGQIMGorxq2TLxKX1+AaBnz0S/EkBk3RiKQgF8+GHeP1MqgQcPRClYzuCQfLp3D1nxCbB78RyOSQ9Qa+ZEvHvXBQ9fCs77dVvO/mnWDHB3z32ZzADSjXw7VaumCvjbsiFDgFmzgOho4MwZsc2SRr7rk7e3aPA+ahQwbZoIvOzZI07dugFTp4qm0YaWlCTuF0CUKNerZ/jLJCJSo3UJ2M2bNxEXF1foiUivBg8WjQzlMgIbEhETj8EbojWCPwCQkJyBwRuiERETb6KVmdiuXeKNWO3auo9LZQaQ5ZEDQPkFEQrz8cfAu++KccC9e2tOtzIWOQOoUSPx1dzLwOQgjHybFzcAlLP/j0x9JPz334v7KL8A0PbtYh0VKgBt2xZtPcVlby8y0dTLvyZNEpOWtm4FIiMRsfs4Akf+grrDfsCul5rBQcrC3N/m42FSat6v2wWVfwEMAOnK1kfA51S2LNC/vzgvScAHH4iAuC2rWFEEnC9dAnr1Eq9DP/8MBAeLsrkbNwx7+aNGiSD7yy+L8lEiIiNjDyAybz/+KOq3L1ww9UqMSpklIXxnLPJqUyhvC98ZC2WWGTQyNLailn8BDABZouJkAAHin/slS8Qbob//Fo19jSkpSfXmfcoU8fXAAfOeRCeXXHl5ia/FLQHLLwAEAN27i6DKvXuiT4kcAHJ2VgWAlEpV8+f+/c12Uk7267ZCgccuHpjSdjAeunig5v2bGBL5E4Acr9vp6cChQ+J8YQGgq1f5uqUNNoDO7YsvRP+bgADxWmijGdW5VKsmSuHOnRMfEmRlAWvXiulxn32mCmDrU0SEuAyFQmQgOTsX/jtERHrGABCZr6dPVc1HX7ww7VqMLCouKVfmjzoJQHxyBqLiijGdxxKlpammKTEAZBuKGwACRIBhyRJxftYs0RPIWKKixNfq1UXz0VKlRBPWU6eMtwZdyRNy5ABQSkrRXoMLKwEDRKbP4MHi/MKFeWcAXb8O7N8v3jT166f7Oowk5+t2kqsnwlqLUr+hkT/hpcQ4zdftP/4QTcorVABq1cr7oJUqiTeJz5+LD0OoYAwA5RYQIAKIZ85Yz8h3fapdW3ywdOKEeI3OzBQZQtWqib5B8XrKtk5JAT79VJwfPlwMsSAiMgEGgMh8yZ8eA6Yp2zChxNT8gz9F2c9q/P67+NS8SpWi9XKRA0DmnH1BmvQRAAKA998HPvpIfMrbu7cIJhqD3P+ncWORufLWW+L7vXuNc/lFkTMDCBBBK11pkwEEiE/bHR3FbSUH59QDQHLz59atxXPfTOX1eryz5pv4vXpjOGYpMfe3+XBQZqr2y2/8uzo7O9V46PPnDbBqKyOXgL30kmnXYW78/ABPT1Ovwrw1bCgydP78E2jeXPzf+e23otR8zBjg/v3iHX/8eBEUr1pVNJ4mIjIRBoDIfNlwAMjbXbu0YG33sxrq5V9FSWNnBpDl0VcACBD/zPv7izHRX35Z/ONpQw4AyZ/2yv1rLCEA5OgIeHiI80UpA9MmAwgQY9V79BDn5dd99QBQbKz4asjmz3qQ5+uxQoFJbYfgsbMbav97HZ9GbVXtJweA2rcv+MByk1i5iS/lLSVFlBICDABR0TVrJkoz9+8Xr9vp6cDXX4tMqkmTVJnpujhyBFi8WJxfvlyU5BERmQgDQGZGmSUh8vpD/Hr2LiKvP7TNHi8yCwkAGeI+axhQBn6ezsgvxKGAmAbWMKBMsS/LYrx4AezcKc4XpfwLYADIEsmvA/oIAJUurcomWbBA1X/FULKyVBktjRuLr23aiK/Hj5tvc3s5AGRnB5T57zWmKG96tM0AAsS4anXqASBArOPdd3VfgxHl97p9360Mpr41EAAw4tgmNExPEI1mr1wRr0mtWxd8YAaAtCNn//j6ilJLoqJSKES25rFjwO7dYkz806fAV1+JQNC0adq/fqelAQMGiPOffgq0amW4dRMRaaFYAaD09HSkpKRonKjoImLi0Wz2QfRcfhzDN59Fz+XH0Wz2Qdud9qQeADLTHkCGus/s7RQI7SzS/nO+mZC/D+0cBHs7G2rmeOSIKEMpVw5o2rRox2AAyLI8eaL6J1sfASBAZFvIfRj69TNsEObiRXH8kiVFnwlAvHmoVk08Bg8fNtxlF4fcA0g9AKRrBlByMpCaKs5rM8Httdc0e2I4OWk2e+7dW2wzYwW9bm+r1QoHqzZACeUL2A/4RLypBIDXX1dlWeVHDgBFR+t3wdaG/X9I3xQKMfHv1CmRgRwcLF7bpkwRr+Vz5ojAUEGmTAGuXRN/w+bMMc66iYgKoHMAKC0tDZ9//jm8vb3h5uaG0qVLa5yoaDjyOw/qExjMMAPI0PdZ+2A/LA6pD19PzbICX09nLA6pj/bBfsU6vsWRy7+6dCn6FCAGgCyLXP7l5lb4m2RdyOn8t26JCTmGIo9/f+011WMPMP8yMDkDSKEoegBIfv0uXVr7cgd5JDyQOwPIzMu/ZPm+bpdygWLpEvE4PnECmDhR/CC/6V/qXnlF3Bfx8WJ8tDV4+lS8KdYn9v8hQ1EoRAbiuXPA5s3iMZaUBIwbJ3r6zJ8vGrrnFBUF/O9/4vzSpezDRERmQecA0JgxY3Dw4EF8//33cHJywooVKxAeHg5/f3+sW7fOEGu0ehz5nQ8zLgEz1n3WPtgPR8e1wqaBjbGgR11sGtgYR8e1sr3gT1YWsH27OF/U8i+AASBLo8/+P+rc3YHVq8U/9StWqLIx9E29AbQ6uQxs3z7DXG5x5VUCpmsASJfyL1nXrqpsIV9fVQCoUSPxybuFyO91u2XrV4FvvhE7ydlR2gSASpZUBTWspQxswACgRg3g11/1d0xmAJGh2dkBH34IxMSIce5VqwKJicDIkSKzc/Fi1f+rz54B/fuL19OQEKBTJ9OunYjoPzoHgHbu3Invv/8e3bp1g4ODA9544w1MmjQJM2bMwMaNGw2xRqvHkd/5MOMAkDHvM3s7BZoEeqFL3fJoEuhlW2VfspMnRXNPd3fVFKWikDOHGACyDIYKAAFiysuIEeL8gAHAw4f6vww5AyjnuN+WLcVj8coVkYVkbtRLwOTM3qJmAGlT/iVzdBSNV3/+GahTR3W/y2PiLUi+r9uffKLq+ePvryoNLIw1lYFJkrifJUlkfelrIh8DQGQsDg6iLPXSJTEyvmJF8fdqyBAR2Fy1Cpg6FbhwAfD2FhlCRERmQucAUFJSEgICAgAAHh4eSPrvn8JmzZrhjz/+0O/qbARHfufDjHsA8T4zMrn8q2PH4vUBYQaQZZEDQLoEEXTx1VfizWJCAvD55/o9dnKyanpVzgwgT0+R1QKYZxaQPkrAipIBBIhMl/ffF+dnzxaBgt69dTuGOVMoxJvDNm3EKGhtpxnWry++WkMG0L//Ag8eiPO3bumnL4pSCVy9Ks6zBIyMxdERGDhQPPa+/VZkLt66JQK9M2aIfRYtAry8TLtOIiI1OgeAqlatips3bwIAgoKC8NNPPwEQmUGlOHWhSDjyOw9paZqfyJs4AyjnpK+ybtoFIWzqPjMUSdIc/14cjo7iq5kFFCkfhswAAgAXF2DdOpGNs3kz8N/fM704cUI8dqtWFZ8A52TOfYCKWAKm/jqZGPtff5fiBO/KlhUZf9oGSSxFxYrifu/XT/vfsaYMoL//Fl9LlBBfZ88G/vu/sshu3hT/Jzg7A5UqFe9YRLpychIfIly/LnrMlS0rtnftCnTrZtq1ERHl4FD4Lpr69euHc+fOoXnz5pgwYQI6deqEb7/9FpmZmZg3b54h1mj15NGxCckZefaUUUA0/rWpkd/yGz+ZCQNAETHxCN8Zq1Hy5evhhFKujkhOe8H7zNAuXhSlMiVKaNcvoyDyGw4zKymkfBg6AASIBs1ffinG+g4ZArz5pvgUt7jk8q+c2T+yNm2AsDCR4aJUFr2xuSGoB4C0LAHL+Tq57tRFeAP4W+GOOgZcqs2QA0A3bojsMktuJisHgLp0EZlAhw4Bo0YBW7cW/Zhy+VeNGub1XCLb4uoqBgt89hnw559i5Lu1BbCJyOLpnAE0cuRIDBs2DADQsmVLXLp0CZs2bUJ0dDSGq0/wIK1x5Hce1Mu/AJO9Yc9v0te/Kc/w+L/gD+8zA5Ozf1q3Lv4kKAaALIsxAkAAMGkSULeuyDr89FNVD5zikBtA5+z/I2vYUDyeHz0yv6yOvMbAP3qU7+55vU76p9wHAMw+/8Q2p1jqW5kyQOXK4vzZsyZdSrHJAaBXXgEWLhQBm23bilcOyf4/ZE7c3MQHVsUpWSciMhCdA0A5VapUCV27dsUrr7yij/XYLI78ziFnAMgEJTuFTfpSACjl6ggfD95nBqWv8i9A9c8YA0Cm9/y5CLwsWpT/PvLrgKEDQCVKiFKwEiWAnTvFdJfiyMoSJWBA/hlADg6qhubmVgamQw+gPF8nJQl+qaLHS7xHOducYmkI1lIGJgeA6tQR092GDhXfDxtW9NdmjoAnIiLSis4lYAAQFRWFw4cPIzExEVnyP4r/YRlY0bUP9kObIF9ExSUhMTUD3u6ihMgms0jkCTIyE7xh12bS1+O0F9j4SX3Y2Sl4nxnCP/8Ap0+LTIR33in+8ZgBZB6Sk0VA79Ah8X2bNrnfuGVmiubMgOEDQICYxjR1KjB+vJhM1KpV0XuJXLkiMmZcXESWQ37atFFlPkycWLTLMgQdegDl9Trp8ewpSr4Q2+65eyHjv4mITQLZCLVY6tUDtm+37EbQL16omqPX+a84MDwc2LRJZPF8+60oodEVM4CIiIi0onMAaMaMGZg0aRJeeukl+Pj4QKFW26pgnWuxyaNjbZ4ZlIBpO8HrwdNn6FLXCG9QbdH27eLr66/n3UhXV3IA6Nmz4h+LiubOHTHN7fx51bbvvhOlIOr+/VcEIuztAR8f46xt9Gjg119F+Va/fiIwY1eERFm5/KtBA1Xj8bzIjaD/+gtITQXc3XW/LEPIrwdQVlau2yOv10k5++eRszsyHJ3z3Y90ZA2TwC5dEkEgDw9VgLVUKWDmTGDAABEM6tUL8NMxg5YBICIiIq3o/J/tggULsGrVKly8eBGHDx/GoUOHsk8HDx40xBrJFskBILkZqwkCQJzOZgb0Wf4FsATM1GJiRE+c8+fFc3vuXLF9zRogJUVzX7n/j6+v8Zq62tuL8i8XF+DgQeD774t2nMIaQMsCA4GAAPGG+MiRol2WIcg9gNRLwLKyRJAqh7xe//xS5PKvsgXuRzqSS8AuXgTS0027lqJSL/9S/9CwXz/RkD01VWTh6SIpCbgvek6hRg39rJOIiMhK6RwAsrOzw+uvv26ItRCpyAGgqlXFVxP0AJKns+WX16YA4MdJX4bz4AHwxx/ivL4CQCwBM53Dh4FmzcRz++WXRZDkiy+AmjXFm76cfXeM1QA6p+rVgTlzxPmxY4GrV3U/RmENoNXJWUDFaYCrb+oZQC4uYrQ2kGcZWF6vk9n9f9zL8nVSn/z9gXLlxNQ49Qw6S6IeAFJnZyfKvwDRj+uvv7Q/ptz/p0IF0XyXiIiI8lWkKWDfffedIdZCpJIzAGSCN+yczmZiO3eKN6J16wJVqujnmCwBM43Nm4F27UTvn9dfB44dExONFArg88/FPosWqQIPgOkCQIAYB//WWyLLomtX4Mcftc+4SE0VmU5A4RlAgCoAZE6NoNUDQICqDCyPSWB5vU76qgWAAL5O6o1CYfllYPkFgACgUSORCQSIhtBKpXbHZPkXERGR1nQOAI0ePRqXL19GYGAgOnfujK5du2qciIotI0OVzm3CABDA6Wwmpe/yL4AlYMYmScA33wA9e4rbvGtXkelSRi0bpHdv0Q/kyhXNLBg5AFShgnHXDIjAx6pVojdJTAzQo4coRfvkE5HJlGP4gYaoKHG9K1fWro9Jq1bi8i5dyt383lTUS8AAwNNTfE1OznP3nK+T/v+VgD3x9uPrpL5Z+iSwggJAgOgF5OEhmv+vWqXdMRkAIiIi0prOAaD/+7//w6FDh1CjRg14eXnB09NT40RUbPIbPxcXVfNXE75hbx/sh6PjWmHTwMZY0KMuNg1sjKPjWvFNjSE9eaLKiNBnAIglYMajVAIjR4rGyoD4RP+nn8TzWp2bm+pTf7kEBDBtBhAgGtSeOQN8+aU4n5Ii3pC2bCn69nz5pWqN6rTt/yMrVQpo2FCcN5csoJwZQPLf9px9mtSov042Lyky7D7t+SZfJ/VNDgBZYgbQgwfAvXvifHBw3vv4+IhG0IB4juWRdZaLXALGABAREVGhdJ4Ctm7dOvzyyy/o1KmTIdZDpCr/qlDBbN6wczqbkUVEiDKtwMD83ygUBUvAjCMjAwgJAX75RXw/d67o95PfpMihQ4EFC4DffgOuXQOqVTN9AAgQpYdffQVMmwYcPQqsXw9s2QL884/IVPjjD7FdnS79f2Tt2onA0cSJ4veCgvR2FYokZwDIw0N8zScDSJb9Ovk4Ufx65UqGWqHtkgNAf/8teuMVNGXO3Mh9i6pWLXji3dChwPLlYlz8lCmageG8yBlAL72kn3USERFZMZ0zgMqUKYPAwEBDrIVIkANAFSuq3rCboAm0rVNmSYi8/hC/nr2LyOsPocySjHfh6uVf+QUNtKR+Pc7f/28UNTOADCcpCWjTRgR/HB2BH34QWUAF3Y/Vq4vR8JIkRsIDqtcBUwaAZHZ2wJtvijelCQnI+t//AABp129qPjckSfcMIEBkR9WpA/z7L9CihapMxlSKkAGUTZJUpWymKN+zdoGBInjy7Jkq8GEpCiv/kjk6qoI+339f8PPhxQvg+nVxnhlAREREhdI5ABQWFobQ0FCkpaUZYj1Emm8ezCQDyNZExMSj2eyD6Ln8OIZvPouey4+j2eyDiIiJN/yFP38O7Nolzhez/Cvn9Ri/S7xhyniaUdxVUl5u3RKTvo4eFUGD338X/X+08X//J76uWiVKAM0hAygPEdceodctERB5/jhZ87lx7Rrw8KHoNSVnamijTBkxdr5+fdH/rGVL05b46NgDSMPjx4D8/wEDQPpnZyca4wOWVwambQAIEL2xunUTwcj/+z/VYzKnGzeAzEygZEmze60gIiIyRzoHgBYuXIg9e/bAx8cHtWvXRv369TVORMVmhiVgtiQiJh6DN0QjPlkzSJKQnIHBG6INHwQ6eFBkGvj66pZFkUNe1+O5nah6fZr61DjBLFty9qy4vy5eFG/E/vxTBDK01batyARKSRFZQE+fiu1m9KZOfkzFPRd/Oks+TwckKfu58feWPWLHV19VvXZpy8sLOHBA9ANKShJvgE+e1PM10FIRS8AAqAL4Xl65+z2RflhqHyBdAkAA8PXX4jH0xx/AoEF59wNSL/8qZrYoERGRLdC5B9C7775rgGUQqVEPAMn9DRgAMgplloTwnbHI67NWCWLMc/jOWLQJ8jXcWGe5/KtLF9UbUB3ldz2eO4jHk6My0/DXw5bs2ycmfD15Ino27dmje/aHnZ0YCT98ODBrltjm6Sk+2TcD6o+ppyVcAQCOWUo4KV/gmUMJKABc3bEfdYCiBy5LlRKNoDt2BP76C2jdWvTD0qWfkD4UpwRMvYSXDEP+sM2SJoEplWKiHqB9AKhyZdGDa9QoYNkyYOtW8drQr5/qsckJYERERDrROQAUGhpqiHUQqeSVAcQeQEYRFZeUK/NHnQQgPjkDUXFJhmmKrVQCv/4qzhej/Cu/6/HCXrzklcjKNOz1sCXr1onx6JmZon/Ntm0ikFEUffuKRsiPH4vvzSj7R/0xlebolL295PN0PHMoAQlArev/ZTgUJ2Dj6SmCPm+/LTIf2rYVAbVmzYqxeh0VpwRMzgBiAMhw5Aygs2dFsK6IgXKjunZNNId3dRVNoLU1cqQoefv8c9EUesAA0Ytr0SKgQQMGgIiIiHRkAf81kM1RfwPBEjCjSkzVrjeOtvvp7Phx0QjX01O38qEc8lvfCzuRAVQi8wUgSYa7HrZAkoAZM4A+fUTwp0cPEbgoavAHEKVGffuqvjejAJD6YyXLzj47CFTyeToAwCf1AV5+cAuSQlGsxy4A0eT3t99EGdiTJ2JK2OHDxTumLvIrAdMlA4j9fwynZk3RZyolRfTAsQRy+VdwMGBvr9vvtmwpgl3ffCOeGydOiFLJQYNUWVAMABEREWlF5wCQUqnE119/jYYNG8LX1xdlypTROBEVy7NnQKIYIcweQEb09Clw4waqXj2Ptlci0evsHgw7tgmTDixHrX+v59rd293ZMOuQy786ddK9h4qa/Nb37L8SMDtIsJeyDHc9rF1mJjBkiMjWAYAxY4CNG8Wb0uL6/HPVeTMKAOV8rDwtIfrbuD0XDY/fjBNvRJ/UqSf63xRXyZKiGXrbtqKpcseOotTOGPIrAWMGkHlwdARq1xbnLaUPkK79f3JydBSlYJcvAyEhIgC9dKnquBwBT0REpBWdA0Dh4eGYN28ePvjgAyQnJ2PUqFHo2rUr7OzsEBYWVuSFzJw5EwqFAiNGjMjeJkkSwsLC4O/vDxcXF7Ro0QIXLlwo8mWQBbh3T3x1chJvohgAMowFC4DXXweqVROfqLq5AYGBqP1BByzb9hVm/P4dRh3diAGnfsXXu/+XXRKiAODn6YyGAQYI9kqS5vj3YmgYUAZ+ns7I2d3nhZ2q6rViSftc10N9ZLzGeG8zYvI1pqWJfj9LlogSoQULgDlz9FeG8tJLIugBAAEB+jmmHuR8TD35LwAkZwC1uCECQCXf6aS/C3VxESWRnToB6elA586iHMzQ5ABQcUrAmAGUL708hy2tEXRxA0AyPz9g/XpRHikHwZycRAN5IjNh8r/TRGTxDPk6onMPoI0bN2L58uXo1KkTwsPD0bNnTwQGBqJOnTo4fvw4hg0bpvMiTp48iWXLlqFOjn8M5syZg3nz5mHNmjWoUaMGpk+fjjZt2uDy5ctwd3fX+XLIAqiXDygUqibQ7AGkP+np4pNU+U2ezNkZ8PHBY48yOJleAg9KlkLXmIOoef8maiXeQKxPIAAgtHOQYRonnz8vyhmcnID27Yt1KHs7BUI7B2HwhmgogOxm0HIPIACY1LqqxvWIiIlH+M5Yjd5Bfp7OCO0chPbBfsVaj76YfI3374sgxIkT4n7auBF4/339X87y5cD33wODB+v/2EWU8zElN4J2e5YOhywlmt0Ub8TtOnTQ7wU7O4vmtx98IIJB774LbNkCvPOOfi9HndwDqDglYMwAypPensMNGojnyd69ohTT3OkrACR74w1R/rVxIyfOkVkx+d9pIrJ4hn4d0fkj24SEBNT+71MXNzc3JP/3ieDbb7+N3bt367yAJ0+e4KOPPsLy5ctRunTp7O2SJGH+/PmYOHEiunbtiuDgYKxduxZpaWn44YcfdL4cshA53zwwA0j/rl4VwR9PTzGq+8oV8cl+Whpw8yZK/R0N5bZtWPjhGOytLqYZdTu/H76ezlgcUt9w/8DI2T9t24qMpGJqH+yHxSH14eupKt3JtLNH1n9ZDa2rqbJ/8hoZDyB7vLc5jIw3+RqvXweaNhXBn9Klgf37DRP8AYBKlcS0n3LlDHP8IlJ/TMklYK4vMtAy9RY8nz0Vt8trr+n/gkuUEEGfbt3Ea+H774ugkKEUtQRMkpgBVAC9Poffe088Lk6fBk6d0vNK9Sw5Gbh5U5yXs3b0wcFB9CB7+239HZOoGEz+d5qILJ4xXkd0DgBVqFAB8fHigqtVq4a9e/cCEFk8TkXo/zB06FB06tQJrVu31tgeFxeHhIQEtJVLAQA4OTmhefPm+Ouvv3S+HLIQOd88MACkf/LUlKAgMVmoenXxCb9ClQ3TPtgPR8e1QrUxoh9LyI1jODqymWE/vdJT+Zc6+XpsGtgYC3rUxaZPm0CR4zGV38h4QJU5FL4z1qQp3CZf48mTYrLVtWtiNPOxY8adSmVG5MdUjWriuTCqsR+WlrsvftimjXhTagiOjsCmTUDPnqIH0wcfAD/+aJjLyq8ELCUld+agukePRIYhwABQDnp/DpcrB3TvLs4vXqyPJRqOPP69QgWAvSLJSpn87zQRWTxjvY7oHAB67733cODAAQDA8OHDMXnyZFSvXh29e/dG//79dTrW5s2bER0djZkzZ+b6WUJCAgDAx8dHY7uPj0/2z/Ly7NkzpKSkaJzIguScIMMAkP5pOTbX3k6Bmn3eB/z84PgoCfZ7fjPcmuLigHPnxHQYPZe22Nsp0CTQC13qlkeTQK9cAaD8RsbLJCB7ZLypmHSNu3eL8e7374txzJGRYgqRDbO3U6C0j2j0XM1Fgt3vv4sftGtn2At2cBD9Tz7+GFAqgV69gA0b9H85+ZWASZJoGJ8fOYBfrpwoXaNsBnkODxokvm7aBDx+XKz1GZS+y7+IzJAl/C9BRObNWK8jOgeAZs2ahS+//BIA0K1bN/z5558YPHgwtmzZglmzZml9nNu3b2P48OHYsGEDnAv4R1Gh0Ow1IklSrm3qZs6cCU9Pz+xTRfYhsCw5A0ByXX9GhggSUPFpGQACIAIyH38szq9ZY7AlZWf/vPmmfiYoFUTOVHz2DID2I+1NOTLeZGtcsQLo0kWUB7ZpAxw5IpqwkmieDojSlpMnxXlDB4AA8ZxcvRr45BORjdO7t/6fmzlLwFxcVJlNBZWBcQR8vgzyHH79dTFWPT0dWLeuiCszAgaAyAZYwv8SRGTejPU6UuyxLY0bN8aoUaPwjo6f2p8+fRqJiYl49dVX4eDgAAcHBxw5cgQLFy6Eg4NDduZPzmyfxMTEXFlB6iZMmIDk5OTs0235E0myDDnfQJQrJ7IPAPFpp8TU2WK7fFl81XZsbp8+4utvvwGJiYZZk9zPpGtXwxxfXY4MIG1HwZtyZLwh15jnlAFJAkJDgYEDRaZJ794iE0jOBCFVAGjbNnF71a5tvLH19vbAsmWq18R+/cT3+pIzAKRQaJaB5eeff8RXfvCSi0GewwqFqlH6kiXm+/fRQAEgTloic2IJ/0sQkXkz1uuIVs0KduzYofUBtQ0EvfXWWzh//rzGtn79+uHll1/GuHHjULVqVfj6+mLfvn2o99+40+fPn+PIkSOYPXt2vsd1cnIqUi8iMhNywE79DcTSpeIfx717RbmDnJFCusvK0i0DCBC9gho2BKKigB9+AEaM0O+a/v0XkPt6vfuufo+dlxwBIHm8d0JyRp41twoAvp7OuUbGG5Oh1pjXlIEKbg7YfHo1Kmz/r7/MxInAtGkaPaIIqgDQrVviazEn1+nMzk5MSitRAli4EPjsMzEtcejQ4h87Zw8gQAT/Hj7MPwPo6lVg6lRx3sZLBPNisNeZkBBg7Fjg4kUxGr15c30sV3+yssSER0CvASBOWiJzYwn/SxCReTPW64hWAaB3tXxTplAooFQqtdrX3d0dwcHBGttKliwJLy+v7O0jRozAjBkzUL16dVSvXh0zZsyAq6srevXqpdVlkIV5/lwEAwDNEoIaNYCwMGDCBBF8aNcO8PY2xQot3927opzHwQGoWlX73+vbVwSA1qzRfwDo11/FJ9evvWac0pEcJWD5jYwHxAstAIR2DtIYGW9shlijPGVA/Viuz9MxfdUsVIg7DcnODorvvxeBBcpNDgDJjB0AAkSAZv580SD6m2+Azz8Xr6MjRxbvuDkzgICCJ4HdvAm89RaQkCDe5I8dW7zLt0IGe53x8AA++khkgC1ebH4BoFu3gNRUEaisUUMvh8zrtQtQTUgx6LRKonxYwv8SRGTejPU6olUJWFZWllYnbYM/2ho7dixGjBiBIUOGoEGDBrh79y727t0L95z/eJN1iI8XgYASJYCyZTV/9sUXwCuvAElJxX9zY8vk7J9q1cSbRm316CHul3PngLNn9bsmY5Z/AXk2Fs9rZDwgouwHHkSg/ZpvjLO2AhS0Rl3f8OQ1ZaDck0f48YfxaBF3GukOThj7UTiUAz/V0+qtkJub6ryrq+jHYgoKBTB3rgiQA8CoUcCcOcU75n/BUahn0+ZXAnb3rgj+3L4tsgr37eOkp3zo8zmsQW4GvXWr6kMUcyGXfwUF6fY3Jx+ctETmzGDPcSKyGcZ4HTHQvNqiOXz4sMb3CoUCYWFhCAsLM8l6yMjk/j/ly2t+8gyIfxxXrAAaNRJlSL16AZ06GX+Nlk7X8i9Z6dKiPOunn0QW0Pz5+llPcjJw8KA4r8fx7wXKZ7Jc+2A/tAnyRVRcEhJTM+Dt7oyGpe1g79Va7DB6NODra5w15iPPNQaU0fmTgJxTBqo+vIO1W0JRMflfPHTxwCfdQnHW/yV0jUtCk0ADN+W2VOofRLRqpRksMTaFAvjqK/HYDg8Hxo0Tj+9Jk4p2PHmUu9yEH1D1f1LPAPr3XxH8uXEDCAwEDhxgdmYh9PUc1lCvnvjbeOIEsGqVKhhoDvTc/0eXCSl87SJTMMhznIhsiqFfR4oUAHr69CmOHDmCf/75B89zvIkaNmyYXhZGNiiv/j/qGjQQ2T/ffCMaX164kLsMgwpW1AAQIJpB//QTsHGjyDCQAynFsXu36FtSs6b2TamLK0cJmDp5ZHw29T5lN2+aPAAE5LFGXaSmAitWwCfqPBadvYayTx+h7NPHqJCSCOfM57hZyg99u4fhZhnRzJjTSgqg/tpjivKvnBQKUSrr6CgCP5Mni+dWWJju/ZvyCgDlLAFLShKT4S5fFq/ZBw4A/v7FvRY2oVjP4fwMHiwCQEuXihI8e3v9Hr+o9BwA4qQlsgQGeY4TkU0x5OuIzgGgM2fOoGPHjkhLS8PTp09RpkwZPHjwAK6urvD29mYAiIpOmxHC4eEizT0uTjSnXbjQOGuzFsUJALVtKwIgCQliIpgODZuVWVLeUWxjl38BqsBVhhZvEOTHJCB6WTRubJg1GcvChcCkSagKIGcHqDN+L2HA+5PxsGSp7G2cVlIAcwsAySZOFI/xsWNFU+YXL0R2kC5BoLQ08TWvAFBKiggCtWsnAqS+viL4U7my/q4D6e6DD8QHJLduAb//DnTsaOoVCXoOAHHSEhERUfHoHAAaOXIkOnfujMWLF6NUqVI4fvw4HB0dERISguHDhxtijWQrtAkAlSwpml22aQMsWgT07Ak0aWKc9VkDeQR8UQJADg5iAtvcuaIMTMsAUH7TWsLbBKDtnj1ig7HKvwDAx0d8vXu38H3lrDRANe3Jkh07BgCQOr+DBVnlccPODfdLlsL9kqVx3asCJIUoveS0Ei0EBIivdeqI8idzMmaMCAKNGAHMnCnKwebO1T4IVFAJ2L17Irhw6pTo1XbgAFC9ul6XT0Xg4iKa9f/vf6IZtDkEgNLSxHQ4QG8BIE5aIiIiKh6tmkCrO3v2LL744gvY29vD3t4ez549Q8WKFTFnzhx8+eWXhlgj2QptAkAA0Lq1+EdXkoABA3L1cqF8pKaqgh5FLbfq00d83b0bSEwsdHd5WkvOng0JyRnYMnONeINQqRJQv37R1lMU1aqJr9euFb5vzgwgSyZJYpIbAMWkiXh51mTsDGqO45VfwbWylTSCPwCnlRSqcmXREP333029krwNHw589504/8034ntJi8a4L14A8kAHV1fVdjkDaOVK4K+/gFKlgL17RXNfMg/yxL7du83j9erCBfGY8/ZWBd6LSZ6QAqheq2R87SIiIiqczgEgR0dHKP77FNHHxwf//PMPAMDT0zP7PFGRaBsAAoCvvxb/VMbGArNmGXZd1kLO/vHxEW/eiqJWLTGuPTNTNOMuQGHTWtpd+QsAkPXuu7r3KCkOXQJA1pQBdPMm8PCh6BHzyiucVqIPr7xiFn2h8jVkCLB8uXh+ffut+F4e8Z4fOfsHyLsEDBAT0CIiRPNhMh8vvSQakkuSuN9NTc/lXzK+dhERERWdziVg9erVw6lTp1CjRg20bNkSU6ZMwYMHD7B+/XrUrl3bEGskW1FYE2h1Xl6in0mPHsD06UC3bvwkujDF6f+jrm9f4ORJYO1aUWKSj4KmtdhnKfHWNZGNcrHRW6hVvBXpRi5XsbUMoP+yf/DKK9mNsDmtxAYMGCCCfv36AUuWiCzAFSvyn9alHgByVnuDLe/v4iIyTBo1MtyaqegGDxaTFVesAKZM0U+z/qIyUAAI4GsXERFRUemcATRjxgz4+YlPV6ZNmwYvLy8MHjwYiYmJWLp0qd4XSDbixQsgPl6c1yYDCBBNL9/+//buPLypMm0D+J22dN8LbVqWUpYipciOguxYRGSRGVQEBlBHHJAZUUccRQf4BBFmRHAUx1FZFAEXQEClCrLIvm+17BSo0FKgtGVrS5t8f7ycNEmTNPtJTu7fdfVKmpykb855sz193ufpL2775z/X/J9tX+esANDQoeJLxcGD4scMS11YOuZmIab0Oq6GROJUs9aOjcdWUgbQ+fMmO4EZMM4AsmYJjafas0ecduhgcLHUZWBQ67ro1DiOX6CUaNQoYPFi8bxdswZo2VKcmiIFgIKDDTPz+vUTRaU3bQK6dXP5kMlOgwaJrLRLl4BVq+QdiwsDQABfu4iIiOxhcwCoffv26NmzJwCgTp06+PHHH1FSUoL9+/ejdevWzh4f+Yr8fPHlOiDA/H+mjalUwLx5YjnCjh2i8CWZ56wAUGwsMHCgOL9okdnNLHVh6Xt3+de6pvcjPjrMsfHYKj5ezBmNRnSTM0erNcwAun4dKCpy+fBcRsoA6thR3nGQPIYNE0HA9HRRv2vgQFHA2ZipAtCACAi9+Sbnj6erVUv8QwSQ9z1Rq3V5AIiIiIhsZ3MAqFevXigy8SWopKQEvXr1csaYPFqlRosdp69i1cEL2HH6Kio1jmUEOPv+vJb0RbtuXcDPhmlZv35VDaB//MMwY8NFvPaYOdIBzNjo0eJ08WKzRbilbi3VCnVqNehzYicAYE/rbu7v1qJSWVcHqKgIuHlTnJc6IHnrMrCKCmDfPnHeKAOIfMi996Jy124UdRH/xMld9VP11y9zASCShV3vN88+K95HN26sCvy728WLQGEh4O8PNG8uzxiIiIioGptrAG3atAnlJr7wlZaWYsuWLU4ZlKcy18568oA0u4oOOvv+vJoUuLF2+Ze+sWNFQeLt20WR09WrXVZU2GuPWWUlcOKEOO+MANBDD4li0pcuAWvXimUHRqRuLWMX7ze4/N68k0i8cRU3AkOQMLifPGn7TZuK5WtSi2JTpKBkbCzQqJFoe33uHOCNmY5Hj4qOa+Hhzjn+5JWk16+Rd+IwFsDPO07g05kbDF+/GADyGHa/3zRoADzyiFjm9/HHojW8u0nZP82aGdaSIiIiIllZnWpx+PBhHL77hp6dna37/fDhwzhw4AA+++wz1K1b12UDlZuldtZjF+9HZlaerPfn9aQv29YUgDbm5yc6ngQGAt9/D3z1lXPHdpdXH7OzZ0WmTnCw+HLgqIAA4E9/EucXLjS7Wd/0RIzplmJ42YkdAICNjdrjo50X5dlv1mQA6RclT04W5701A0iq/9OunfiPPPkc/dev60GivXtE2c3qr18MAHkEh99vxo4VpwsXiuCvu3H5FxERkUeyOgDUunVrtGnTBiqVCr169ULr1q11P+3atcO0adPwz3/+05VjlU1N7awBYOqabKuXAjn7/hTBlhbwpqSlAZMmifN/+5tod+1EXn/MpGUAqam2LbGzZNQocfr998DlyyY3qdRosfqQ3hcVrRZ9TooA0E+pnQDItN+sCQDpz0mlBIBYv8UnGb9+lQSJulsRZbeqv34xACQ7p7zfPPQQ0LChWMr69dfOH2RNGAAiIiLySFZ/E8zJycHp06eh1Wqxe/du5OTk6H4uXLiAkpISPP30064cq2wstbMGxAeyvOJS7M4plOX+FMHRABAgagC1aCGCES+/7Jxx3eX1x8xZBaD1pacD7duL+jJLlpjcxHi/Nbmai8aFF1DmH4BNjdrLt998LQNIKgDN+j8+yfh5qJ8BBBi9fjEAJDunvN/4+QHPPSfOy1EMmgEgIiIij2R1ACg5ORl169bFyJEjERsbi+TkZN1PYmIi/BW8rMBSO2tP2M4hOTlA376mu8G4kyM1gCSBgWIpmEolulOtW+ecscHDjpk9XBEAAqqKQZtZBma8P6TuX1sbtsGNu19CTW3nck2bilNpaZwpSskAKi2t+jLGDCCfZPz8uq6XAVRtOwaAZOe095unnxZdwXbvBvbvt7ytM5WVVb3nMABERETkUWxaC1KrVi2sWrXKVWPxWJbaWXvCdg555hngp5+ABx90/d+yxJEaQPo6dQLGjxfnn3uuqouTgzzqmNnDVQGgoUPFF4yDB4FDh6pdbbw/Hrpb/+enpp0sbudyajUQGipawZ89a3obpWQAHTwosrTq1HFO/SfyOsbPL+MMIIPtGACSndPeb+LjgSFDxPn//tfBUdng2DHxmhMd7dg/dYiIiMjpbC4G8uijj+K7775zwVA8V8eUWESH1jJ7vQqiM4e17azNtceW9Du2FaNytrmnPbalJTDuUlEB5N2tE+OMD4vTp4sv7Tk5wOTJjt8faj5mts4Bt3NmC3h9cXHAwIHi/KJF1a7W3291iwvQ8tJpVKr8sL7pfQBk3G/WtII3lQF0+bI8BVUdoV//x0Xd8cizGb9+GWcAGTwPpQBQaGj1OyK3cOr7zV/+Ik6//BIoLnbWEC3TX/7F1xwiIiKPYnMAqEmTJnjrrbcwZMgQzJgxA++//77BjxKty85H0a07Zq/XApg8IM3qdtZSe2wA1T7gRZbdxNw1/8KUb2fCv7jIvgHborLS9X+jJpcuiXH4+4vW4o6KiBCtbwHR/lb6AuwAS8dM+t2WOeBWV69WFWlOTXX+/UvLwBYvBu4YPk/099t9uVkAgANJzVAYGiX/fpOWgZkKAGm1hhlA0dFiXgFVl3sL1v/xecavX1IAKLLsBvw14j1A9zxkBpDsnPp+07WrqI1365Z4jXYH1v8hIiLyWDYHgD799FNER0dj3759+N///of33ntP9zNnzhwXDFFeUjcOS6JDayEjTW3T/fZNT8RHI9pCHWWYwt31ei5qaSqh0mjcs9xEo3H936iJ9IU6Kcl5LaoffhgYNkw8vj//uVpgwh7mjpk6KhgfjWiLvumJDv8Nl5Cyf+rXB8LCnH//Dz0kAneXLwNr11a7um96Ihb0isdTh8V1R+NFW3jZ95uUAXTyZPXrioqqMn3q1hX/xU5KEr/nydC23hHsAEYwfP3Kj4hDYUgkgior8HDhCcPnIQNAHsFp7zcqVVUW0EcfieC2qzEARERE5LECbL1BTk6OK8bhsWrqxgEARbfuYHdOITo1jrPpvvumJyIjTY3dOYUouF6K+Ihg3Lf8t6oNcnOB1q3tGLUNPCEA5IwOYKbMmSPqGx0+DPz738Brrzl8l6aOWceUWM/M/JG4qv6PpFYtYMQI4N13RTFoaUkYIL5sLFiAHhMmANevozIkFA1eHoel998v/36ztARMCkrGxVUthUlKEsG0ixfdMz5nKCqqCgAyA8jn6b9+lZzuj9gVS/C+6jj89IMJDAB5DKe93/zpT8CrrwK//QZ89hnQpo3IaoyOBqKigACbPwpaxgAQERGRx7L7Xf/KlStQqVSIi7Mt6OFtXN39yd9PZRg42rev6vz583bdp008YQmYswpAG6tTRywBGzkSmDoV+OMfnbIEqtoxczWNRmQ03bolurUFBtp2e1cHgABg1CgRAFqzRmQC1akjMmWefRb44QexzQMPwH/hQnSXAi9ysxQAMhWUlDKAvCkAJL2epKQAtWvLOxbyCLrXr3FPAyuWwG/FCmDePBHIBRgA8jBOeb+JihIZsZ9+Kl6TjUVEVAWErP2JiRGnkZGGmbsFBUB+vsg8atHCsXETERGR09kUACoqKsKkSZPw1Vdf4dq1awCAmJgYDB06FNOmTUN0dLQrxigre7txVGq09v3Xbu/eqvNGtUbsvk9L9ANAd+5UfQlwk0qNFpeyTiIJwMXwOCRotM7NChkxQtQ9+PlnYMwYYMMGwM/mlY/yOnZMjB8AfvwRePRR228PuDYA1LIl0K6dCDgsWSKWhI0bB1y7JgJW06cDL77ovCV+ziDVAMrJqT739ev/SEwEgFzynHQmL6z/4/H7VCm6dxddogoKRGC5b19xuYwBIE849p4wBpeYNEn8U+nCBZEZWFRU1SXz+nXxY299s8hIXWBI6+cHFYAb9ZJx5FIZOoaG2b3/FHssyONwrrkH9zORZ7A6AFRYWIhOnTrhwoULGD58OJo3bw6tVoujR49i4cKF+OWXX7B9+3bExMS4crxuJ3XjyC8uhamV8yqINfn63Tgys/IwdU22wdKxxKhgTB6QZnndfmEhcOZM1e96GUB232dN9ANAJSViyYubSI/p9W1HkATgs7N38OPMDY4/Jn0qlSgI3aIFsHmz+A/omDHOuW932bat6vyCBZ4ZAAJEFtC+fWKpnfQlsm1b4PPPPfM/wYmJ4kvu7duiFbwUEAKsygBy2XPSmbys/o9X7FOlCAgQLcLnzQO++kr2AJAnHHtPGIPLNGwolkTru3NHdAaTAkKmfq5dM3+dVCetpET8nD+vK1L9U2QKXv5kp937T9HHgjwK55p7cD8TeQ6VVmtdRcAJEybgl19+wfr165Fg1KkpPz8fffr0Qe/evfHee++5ZKD2KikpQVRUFIqLixEZGWnXfWRm5WHs4v0AYBAEkj7o6BdklLY13qmmtq1m3TqgT5+q37t0AbZscew+LdFoxJcAaQqkpIgP/bVqictr1TL8MXWZndsezL+Bj7fn4o5fAP7+6+e458o5jBv0D6y9p4tjj8mcOXNEBkpkJHD0aNWXeWe5ckWk2bsig2r06KoW6/7+IjihtrLoeHm5qGFTWSn+8+vsx63vyhVx/3fuiOP8xhvA66+7PavMJo0bi6Dr1q3AAw9UXS7t8+nTxWMAgK+/Bp54AujaFZnzvnLNc9LZ6tUTx33zZqBbN7lHY5HLXufIvF9/FZlAUVGiG2NQkAgwr1olAuduCpZ7wrH3hDF4nfJyXQBp+75T+GjlPkSU3URQRTk2NWqHa3rdHm3ZfzwW5C6ca+7B/UzkerbEPKzOAPruu+/w8ccfVwv+AIBarcasWbPwl7/8xeMCQM4gdeMwjlyrjSLXUscwUxE1LcQL3dQ12chIU5tOeZSWfyUniw5g5887fp+WlJQYdgRxY4Hv1gA+MrosL6K244/JnL/+VSxN2rMHGD8eWLHCOfcLiEyt1FQgI0PUwHG2rVvFaViYSNlfvBj4+9+tu+3p0yL4ExEhMl5cqXZtYNYsYP16UXOpXTvX/j1nkLqiSf/JlpjKALq7/7R5ea57TjrTxYsi+OPnJzKxPJhLX+fIvC5dRND24kWRHTJwoNszgDzh2HvCGLxSYCBQpw4q42rj5W/PIy+lTbVNbN1/PBbkLpxr7sH9TOR5rA4A5eXloYWFZRzp6enIz893yqA8kTXdOGrqGKYFkFdcar5jmBQAGjxYZKxcuIDdpy47dp+W3K3jBADYsUNkblRUiFP9H1OX2bKt0WVFxbeQnVuIAE0FalVWIkBTgTOx9XA4sanjj8kcf3+x/KtdO2DlShEA+sMfnHPfW7cCZWWiw4qzXbokgjgqFTB5MjBxolgG9vLL4rKa6C//smZ7R02YIH68hdThyzgAZKEGkObCReQV3Ta7P10yf+0hLf9KSwPCw+UbhxUcfu0k+/j5AY89BsydK5aByRAA8oRj7wlj8GbO3H88FuQunGvuwf1M5HmsDgDVrl0bZ8+eRT0zrbpzcnIU1xHMVLEySy9ODncMkwJA/fsDH3wAVFSgJOecY/dpSWGhOE1KAu6/3/bb22nzwQt4YdnBGrezt7OaWffeK1rhTp8OPP880KuXKFzpqOxscXrnjuP3ZUyq/5OeLpZjTJ4s/t6ePdbVdZECQM2aOX9sSmAqAKTVWswA8r99CxHlt3A9KMziXTt9/urLywM2bgQGDarKYjLmRfV/XN1tkSwYOlQEgFavFsEf6bngpgCQJxx7TxiDN3Pm/uOxIHfhXHMP7mciz2N1O6S+ffti0qRJKC8vr3ZdWVkZ3nzzTfSVikgqQGZWHrrM3IAnP9mJF5YdxJOf7MQD7/yCuetPYNXBC9hx+ioqNYYJjfZ2DAMgOrFIRZ87dADq1gUAJJVcsf8+ayJlAMXGWt7OyRzaT4564w2xXCs/X2TTOIM7AkAPPCDqdEhZSwsWWHd7dxWA9lamAkDXrlX9rh8ACg3VBQzjrxfWeNcumb+nTgHPPScKug4fLjIFzfGiDmCyvib4uvvuE8uOb9wQXQbdnAHkCcfeE8bgzZy5/3gsyF0419yD+5nI81gdAJo6dSqOHz+Opk2bYtasWVi9ejVWr16Nd955B02bNsXRo0cxZcoUFw7VfaRiZcYpi/klZXhv/UldQKjLzA3IzMrTXS91DDO30EYFUfFev2OYzr594rRZM1GouEEDAECLimL777MmUgDIzZ3bHNpPjgoOBj75RJz/5BNRr8ZR0tIvVweAAOCpp8Tp0qVVX9QsYQDIMlM1gKTsn9q1q38JvrsMLE173b3z9+BBkanRrBnwv/+J4quACGSaotV6VQaQrK8Jvk6lAh5/XJz/6iu3B4A84dh7whi8mTP3H48FuQvnmntwPxN5HqsDQPXq1cOOHTuQlpaG1157DY8++igeffRRTJo0CWlpadi2bRvq69fL8FKWipUZyy8uxdjF+3VBIH8/FSYPSAOAai900u+TB6RZLgDdvr04vbsv/X7Ptf8+ayJTAMih/eQM3bqJLApAtEE+cMD++yorE1kZgPMDQLduVQUGpQBQz57iv/XFxcB331m+vVbLAFBNTGUAmVr+JbkbAHq2ifhPlUvnr1YrujQ9/DDQpo34cq7RAI88IoJBgCgKbsqpU6JNc1AQ0LKlY+NwA9lfE3zdE0+I0++/r3pfkJ4bLuYJx94TxuDNnLn/eCzIXTjX3IP7mcjzWB0AAoCUlBSsXbsWV65cwc6dO7Fz505cvnwZmZmZaNKkiavG6FY1FSvTJwWJpq7J1i0HkzqGqaMMUxnVUcGW2xxKX/Slzkl3M4CQm4u+6YmY3ycJU3d8gVWLXkT733+z7j5rItUAcnMACHBgPznL7NmiA05xMfDQQ8Dx4/bdz4kT4ks5IIpdO9OePeI+k5LEkh9AFG0dNUqcr2kZ2KVLotObnx+gkOen05kKAJkqAC25GwC61/+26+avRiO6yXXpIlp0Z2aKY/jkk8ChQ+JLeqdO1cetT8r+adMGqFXL/rG4keyvCb6sbVugcWOR/VNUJC5zUwYQ4BnH3hPG4M2cuf94LMhdONfcg/uZyLNYXQRaX0xMDDp6wbICe9RUhCyoohxz1/wL2xvci8/bDTBZvd6ajmHVmMkAwp49wNNPo+fixboMk/fzNuLc1Gdqvs+ayFQDSGLXfnKW0FDxRbpXL2D/fuDBB0U3r+Rk2+5Hqv8DOD8DSH/5l37HqdGjgf/7P7F87fz5qmChMSn7p1EjkQlC1dmaAXS3EDQuXnT+/K2oAJYtA2bOBLKyxGVBQWLZ3yuviONoadz6pPo/XvY6Letrgi9TqUQW0NtvV13mxgAQ4BnH3hPG4M2cuf94LMhdONfcg/uZyHPYFQBSspqKkLW9cBR9T+xA57OHsLhNP2j8/AFUDxz5+6msb2eYlwdcuCA+hLdpIy6TvtTv3Cl+ANHOOTsbSUf2IalRrONtvWVaAqbPpv3kbFFRIruiWzcRLHnwQWDLFkCttv4+9ANAFRVi2Y6z2q0b1/+RpKQAPXoAmzYBn38uClubwuVfNbMzAwgXLwJw0vy9fRuYPx/497+Bs2fFZRERwLhxwIQJpuejNG5zS8CkDCAvKABtTNbXBF8mcwAI8Ixj7wlj8GbO3H88FuQunGvuwf1M5BlsWgLmC2oqVpZwQyybiiy/heaXz+oud6h6vbT8q3lzIDxcnG/RQpyqVMAf/wjs2CEyVYKCgMuXgZMn7f97Eg8IAMmuTh1g3TqR+XPqlNjXttAPAAHOWwam0QDbt4vzxgEgoKoY9MKFIuhkClvA18zOGkBSAMghRUXiC3dyMjB+vAj+1KkjLjt/HnjnHfPBSFPFqyV37ojXCsDrMoBIRi1bGgaLZQgAEREREZFrMQBkxFKxMgCIv1HV/rljbpZzqtcbL/8CRJbHgQPA6dPAt98C998vgj/SF7otWwzuolKjxY7TV822qDdJxhpAHqVePeCHH8T57dtFYWdrSR3AJM5aBpadLQIEoaFAq1bVr//jH0WWyOnT1eaCDjOAauZgBpBd8vOBf/xDBH4mTRIB3eRk4IMPgHPngNde07Wbr3HcpjKAfvsNKC0VGW6s/UQmmHy/kJaBSWQOANn1nkZEREREFnEJmAlSsbKpa7KrFYROMAgA/YaF7Qc5Xr3eVAAIAFq3rr5tly7iC//WrcAzzwAQbeuNx5oYFYzJA9IsF1aTuQaQR9Gv/VNZad1tysurZ2I5KwAkLf+6/37TRXzDwkTr5s8+E8uHunWrvg0DQDUzDgBptdZnANm63O/MGeBf/xLFu6UgY4sWIhj0xBO2FWu2lAGkv/zLjzF+MmTx/WLoUFFfrE4dWeeO3e9pRERERGQRvx2Y0Tc9EVtf7YWlz96PuUNb48UHU6GODDbIALrvQjY+Gt7GsQ+kWq35AJApXbuK061bAYgPymMX768WqDJuUW8Sl4BV8fevOm9tAOjUKbHkS/oyDjg/AGRq+ZdEWgb2zTfA9euG1926JbJJAAaALDEOAF27VnXeUhHosrKqbkk1OXwYGDYMaNoU+O9/xW07dQJWrxbXjRhhe6cuS0WgpQLQXlj/h1yrxveLiijgp5/E3JSJQ+9pRERERGQRA0AWSMXKBrWuixcebIpt/+iF7hFVNV5ibxahb60Sx/7Inj2iXbe/v+mlPsY6dRJZB6dOofKi+C+pqcR4Uy3qq2EAqIp+AMjaOj5S/Z/09KpMEHcGgDp3BlJTRRDgm28Mr5Myk+LigNq1nTMmJTIOpEhBs9q1gWATdb2Cgqoy5mpaBrZ1K/DII+J5vXSpqOvUty+webM4vgMG2J9lIQUdTS0BkzKAWP+H9FRqtNa9X/R+ELjvPncOTcfqMXI5GBEREZFdGACygb+fCpFFV8Qv0hdHc/VXrJGVBfTrJ84//HDVfVoSHS2KdQI4tfKnav8l1affor6aykqguFicZwAICNBbDWltBpAUAEpLAyIjxfkjRxwfS16eWC6kUoklYOaoVKIlPCCWFenj8i/rGC+lktqvN29u/jaW6gBptaKeVJcuIlvvxx9FkOeJJ0RNr7VrxXI9RzvFmcsAunWr6jEwA4j07M4ptP/9wk28YYxERERE3owBIFvl3U0/799fnP76q333c+wY0Ls3cPWqWPq1eLH1t+3SBQAQsH2rVZsbt6gHYLh8hQEgw0wMewJAo0aJ81OmmO/KZS0p++fee0UhX0tGjhRj37rVsB4RA0DWMQ6kHDokTi1l45kKAFVUAEuWiNv17y+OYWAgMGYMcPw4sGyZ6Zpe9pICVxUVwI0bVZcfOCDmb1ISULeu8/4eeT2T7wMObOcK3jBGIiIiIm/GAJAtbt+u+rL12GPi1J4MoFOnRPCnoEB8Yfzpp5q/6Ou7GwBKOLzXqs1Ntqi/u/xLGx6OHedLfL7TSqVGC83dLKB9py5btx+kDmBpaaJzU0gIsGMHkJlp9xh2nL6K09/9BADQdO5c843q1gX69BHnFy7U3cfvOw6I+0hNtWssPsO4m9bBg+LU2gCQRiPq+qSmAsOHiwyw8HDglVeAnBzg449d04krPLxqiVqHDmLeAaz/46Xc0fHK5PuAA9u5gjeMkYiIiMibsQuYLa7cXf5Vq5ao5eHvL2qGnD8PNGhg3X2cOyeCPxcvig5A69bZ3oXrbiHosOwjaPxHDc6U+pmsmaACoDbXov5uAOiSfwie/GSn7mJf7LQidZzZpFUhCMBfv9wD7a+XLO+HigqR2QGIAJBaDTz/PPDvfwP//KeYHzYs89HvevPdhs0AgKlXo9EpK6/mY/HUU0BmJko/XYDeod1x4fod/HDgCOoBeDWrHL2tuQ9fpZ8BpNXangH0wQfACy+I32vXFueff971WXUBAcCKFcDTT4tsrwceAF56CTh9WlzP+j9ew10drzqmxCIxKhj5xaW2v1+4iTRGS8vAEmUeIxEREZE3YwaQLS5fFqe1a4v/wLdtK363Ngvo99+Bnj1FwCg1FVi/XrTbtVW9ekByMlQaDd6pK5auGIcapN/Ntajfu/8UAOBaYJjB5b7WaUW/40yFnygE7a/R1LwfTp8WBZ9DQ6uCfxMniqU5e/fa1EVHfwwh5aVocekMAOCX2KbWHYuBA1EeGY3ggjw0PrQTKq0GjQovAAD2Bsf71PG0mX4AKC9PBHn9/ERhb3OkAFBeHvDZZ+L8Sy+J4O4bb7hvSeXDD4sstJEjRfDq3XeB774T1zEDyCu4s+OVv58KkwekAbD9/cJd/P1UGNjKctBrYKtEWcdIRERE5M0YALKFlAEUFydOu3UTp9bUAcrLA3r1EstCGjUCNmwQWSP2ursMrMOFo/hoRFvUD/PDM7tX4oNVM1G3uADqqGB8NKKtyf8gV2q0WLVRLF8qCokwuM6XOq0Yd5ypVImng7+msub9INX/ad68qn5QnTrA3/4mzv/zn2J5kI1jaJ13ArU0lcgLj8PvkXUsj0G6j8AgrE7rDgB47Mh6JJVcQUhFGcr9AnA+Wm3VffgsKQB05w6wb584n5oqlvOZIwWAfv1VtHGvVUsEfqwp4u5ssbHAokXA999XjQsQdcXIo8nR8apveiI+GtEW6ijDJVSW3i/cqVKjxepDloNeqw/l8bWMiIiIyE5cAmaLq1fFqZS107Wr+K97TRlAly8DDz4oivQ2aCCCP44WaO3SBfjyS2DLFvRt1QoPLXoFqrvLP3r4FyNk9074h5r+Ers7pxDau0vAioPDq12v32mlU+M4x8bpwYw7zmjuBnL8tSJwY3E/6BeA1vf3vwMffigCA8uXi1pRGo0IMJSXV53ePX/kVAGiT2ajTmUFAior0e+4KOy9t14aoFJZdSx25xRiQbOeGLJzFfqc3ImfUjsBAM7GJKHyblaTLxxPu+gHbaQ6OpaWfwFA4t0vyVJGYL9+8hdSf+QRkQ00bZoIBMk9HqqRLR2vnPm87ZueiIw0NXbnFKLgeiniI8SSKk/IqqlpnwB8LSMiIiJyBANAtpAygGrXFqd3s3Bw9Kj4MmhqOVdhIZCRIQIGdeuK4E9ysuNjkf72hg3Ahg0ihV+tBsrLEf7bYWDCC8D//mfypgXXSxFZKopZFwdVDwDpb6dkxo+vaglYpcXtAFQFgFq0MLw8NhZ48UVg6lRg6FDgySctdhVrDWCticv31jMMLFk6FgXXS/FbQmMcrdMQzS+fxQvblgIATsfVs/o+fFZQkKjVpNVaHwDSz7QBxDH2BNHRogYVeQU5O175+6k8MoDCLmBERERErsUlYFaq1GiRe+I8ACC/VphIQY+LqwoAbDXRkr24WHRoOnQISEgAfvkFaNzYOQNKS6sqHh0cDEyaJDKMli0TX2g/+QRYsMDkTeMjghF9+7oYookMIP3tlMz48WlUhhlA5rYDYNgBzNiLL4pgn0ZjOvijUonAQ3g47kTHoCAsBhci6uBsdCJOxtXH1uRWWNO8W81j0L9OpcI3LTMAAE2v5gIATtROrr6dj6vWbUmLqiygPXvEaU0BIP2lm2FhwIABLhmrrdzRSYqqOLq/2fGqOu4TIiIiItdiBpAVpC4tY7cdxUgAX+fcxtKZG0SXlu7dRTDg55+BwYOrbnT7tijSum+fyBj65RegWTPnDcrPT9T+2LoVGDu2KqsoIwP4v/8D3nwTGDcOaNMGaN3a4KYdU2JxRXMbQPUaQIBndINxB+OuOPpFoAEL+6GyUnReAkwHgKKiRDDu0iUgMFDUiAkMrDrv76/rEOan0WLQzA0OdeaRHsd3LXrgL7u+RcidUqxK64EF7QdYfR++wFy3pU1BwQi6ebOqFXxNAaDAQJHtd/myeM7LUfvHiLs6SZHgjP3tDV253I37hIiIiMi1mAFUA/0uLTG3SwAA10IjdV1a9rZ8QGz43XeGRX8XLxZLSmJiRLcv46VCztC/P/DOO9WXlL3+uqgJUloK/PGPupbvEn8/FTpEiwBEiVEGkKd0g3EH4644Gr0i0Bb3w6lTQFmZyLxq2ND0nYeEiOuSkkSwICpKXBYQYNAe3hmdeaT7uBYahW7PfYp2f12CNx56HkUhkT51PC2x1G2poFIvDh4XV32JlynS83nUKCeO0j7u7CRFztvf3tCVy924T4iIiIhciwEgC4y7tEgBoMKQSN1lL16OhTYyEsjPB3burLqx1Ab85ZdrzihwNj8/4IsvRADizBnxJdWoI5W64m77eKNisZ7SDcZd9LviVOoVgba4H6RaMe3bi2weJ45Bny3HQrqPmNpRKA+oZdd9KFVN3ZZuBwRVXdCqlUGAzqxFi0RW34MPOmuYdpGjk5Qvc/b+9vSuXHLgPiEiIiJyHS4Bs8C4I0ns3QBQUUgkAPGBP/dmJa70yECd1cuBlSuBzp2BW7dE1g8gX32QmBjRhapzZ2DNGmDmTOC116quv5sVNGVUF/RL7eBx3WDcSeqKU/5JGFAE/N8j96D5473M74ft28Vp585OH4MjnXk8ubuPnGrqLHS7llEAyBoNGogfmcnVScpXuWJ/83lbHfcJERERkWswAGSBcacRqXBy4d0AkCSnax8RAFqxApg1S3TmKi0VXxBbtnTbeKtp21a0JP/zn4E33gA6dgR69xbX3Q0A+deO4xdDiKUHIcGBAIB0dThg6YuGFADq1MnpY3D0WHhqdx851dQxqNSeAJCHYNck93LV/ubztjruEyIiIiLn4xIwC/Q7jdSqvKPLALoWahgA0j7UV9SDOXMGOHJEZNwAIvvHmuUkrvTMM8DTT4slYEOHAr//Li6X6gIZLQHzaQF346EVFea3KSqqagHv5AAQuUZNHYOqLQHzIuya5F7c30RERETkzWQNAH300Ue49957ERkZicjISHTq1Alr167VXa/VajFlyhQkJSUhJCQEPXr0wG9S+20nM9XSt2NKLAYW/Ib3vn8X+/4zAsEV5QCqMoBUEJ1f2reoDzz0kLij5cuB778X5/v3d8lYbfbBB6Ib2JUrwGOPiW5HN26I6xgAqiLV8zHVul2yaxeg1QKNGwMJCe4ZFzlE6ixkLhQrLQHTBgQAzZub3MZTW6zX9Nik1yh2TXIOX9zfnjr3fR2PCxEREdlD1iVg9erVwzvvvIMmTZoAABYtWoRBgwbhwIEDaNGiBWbNmoXZs2dj4cKFSE1NxbRp05CRkYHjx48jIqJ6+3J7mWvpO7NlEN5f8Krussth0VjYdgBuBYZU70gyeDCwahXwn/+I7JqwMKBHD6eN0SEhIcC33wLt2olC1X/+c9V10dGyDcvjWBMAckH9H3ItqbPQ2MX7oQIMCviqULUETNW8ORAUVO32ntxivabHBrBrkjP52v725Lnvy3hciIiIyF6yZgANGDAA/fr1Q2pqKlJTUzF9+nSEh4dj586d0Gq1mDNnDiZNmoQ//OEPSE9Px6JFi3Dr1i0sWbLEaWOw1NJ345wvAAAlqWl47rk5uG/cInzY+QkAJjqSDBggAgjS0qo+fcSyME/RqJHoDAYAy5aJ06gop3SxUgxrloAxAOSVLHUWat+8rvjFxPIvb2ixzq5J7uUr+9sb5r4v4nEhIiIiR3hMEejKykp88803uHnzJjp16oScnBzk5+ejT58+um2CgoLQvXt3bN++Hc8995zjf1OvpW/srWIMzN6MnNi62NyoHbQAOp0/DAAIHz0C8179m+WOJLGxIuPnl1/E756y/Etf//7ApEnA9Onidy7/MlRTBlBlpVgCBjAA5IXMdhb6PBdYsQQYONBg+5pafqsgWn5npKllz/hg1yT3Uvr+9qa570t4XIiIiMhRsgeAjhw5gk6dOqG0tBTh4eFYuXIl0tLSsP1upkWCUZ2VhIQEnDt3zuz9lZWVoaysTPd7SUmJ2W31W/r+af8PeHHbEmxJbo3NjdrBX1OJ+88fAQBk3dMe91rTkeQPfxABIJUKeOQRy9vKZepUEcRYvx6IY4cVAzVlAP32G3D9OhARAbRo4b5xkdOY7Cw0ejTwxBNiqaQeb2uxzq5J7qXk/e1tc99X8LgQERGRo2TvAtasWTMcPHgQO3fuxNixYzFq1ChkS12WAKiMumhptdpql+mbMWMGoqKidD/169c3u61+q97l6b0AAA+cO4TEkstIzz+FyLKbKA4KQ06DZtY9mMcfF0uthg/33ALB/v7A0qXAU08Bb74p92g8S00ZQNLyr/vu49I5pTEK/gBssU6+i3PfM/G4EBERkaNkzwAKDAzUFYFu37499uzZg7lz5+LVV0Xx5fz8fCQmVtVUKCgoqJYVpO+1117DSy+9pPu9pKTEbBBIv1Xv79Fq7KqfjvtyszD4t426y3c2aIn46DDrHkzt2sDp09ZtK6fatYH58+UeheexNgBUw/KvSo1WsUtDfAlbfpOS2PK6xLnvmXhciIiIyFGyB4CMabValJWVISUlBWq1GuvWrUObNm0AAOXl5di8eTNmzpxp9vZBQUEIMtHJxxSppW9+cSm0AL5N74X7crPwx6wNyIsQ6dNZ97THBAW19CULaloCZkUAiN1ZlMP49cGYCqLwr5JafpMy2fq6xLnvmXhciIiIyFGyLgF7/fXXsWXLFpw9exZHjhzBpEmTsGnTJgwfPhwqlQoTJkzA22+/jZUrVyIrKwujR49GaGgohg0b5pS/L7X0BcQHp7XNuuB2QBAaF/6OzudEAeiOzwxh9oavsJQBVFAgsrtUKrEEzAR2Z1EW49cHfUps+U3KZM/rEue+Z+JxISIiIkfJGgC6dOkS/vSnP6FZs2bo3bs3du3ahczMTGRkZAAAJk6ciAkTJmDcuHFo3749Lly4gJ9//hkRERFOG4N+S98bQaFY20xkd/hBi9I6Ceg6sJvT/hZ5OEsZQDt2iNO0NCA6utrVNXVnAUR3lkqNqS3IU/lKy29SJkdelzj3PROPCxERETlC1iVgn332mcXrVSoVpkyZgilTprh0HPotfbUNxwBjRQ2g4D4PiowP8g2WMoD27xenZrJ/2J1FuZTe8puUy9HXJc59z8TjQkRERPbyuBpActG19G34GDD9FeD334HeveUeFrmTpQBQUZE4VatN3pTdWZRNyS2/Sbmc8brEue+ZeFyIiIjIHgwAGfP3B774Ali7VrRzlwk7ScnA0hKw69fFaXh4tasqNVpcuV5m1Z9gdxbP4c3PMW8eO7mPM7tGcc4REREReT8GgEzp0UP8yISdpGRiKQPoxg1xahQAMnWsTGF3Fs/izc8xbx47uZezukZxzhEREREpg6xFoKk6dpKSkaUMICkApFeA3NyxMsbuLJ7Fm59j3jx2cj9ndI3inCMiIiJSDgaAPAg7ScnMUgaQ0RIwS8fKGLuzeA5vfo5589hJPo50jeKcIyIiIlIWLgHzIOwkJTNrloDdzQCq6VhJ3nykOUY/kMLMHw/hzc8xbx47ycverlGcc0RERETKwgCQB2EnKZnZUATa2mNQOyKIwR8P4s3PMW8eO8nPnq5RnHNEREREysIAkAdxZscWpXBr5xkbikDzWHmn2uFBVm3niceNc869fLHrlfFj9ubnCxEJvvhaRkRE5jEA5EGc1bFFKdzeecaGItA8Vt4nMysPU1b/ZnEbTz5unHPu44tdr0w9ZnVkEKJDa6H41h3OOSIv5IuvZUREZBmLQHsQZ3RsUQpZOs+EhIjTW7cML9dogJs3xfm7GUA8Vt5Fmk/5JWVmt/H048Y55x6+2PXK3GO+VFKGorvBH845Iu/ii69lRERUMwaAPIwjHVuUQrbOM/Hx4vTSJcPLpeAPYNAGnsfKO1jbsc0bjhvnnGv5Ytermh6zCkB0aC0kRHLOEXkLX3wtIyIi63AJmAeyt2OLUsjWeUatFqf5+YaXS8u//PyAYMMvQb5+rLyBtR3b/j2kFR5oWtsNI3IM55zr+GLXK2sec9GtO/jymbbw81NxzhF5AV98LSMiIuswAGTEU4rl2dOxRSlk6zyTkCBOjTOA9DuAqarPBV8+Vt7A2nly5ab55WGehnPONXyx65Utz49Breu6eDRE5Ay++FpGRETWYQBID4vleQbZuh3VlAGkt/yLvAe7Z5G1fHGu+OJjJlI6Pq+JiMgc1gC6y55ieZUaLXacvopVBy9gx+mrKK/QGPzuyrXVxn9bSeu4pW5H5vKuVBCBOad3npEygIqLgVK9eaCfAUReR7b5RF7HF+eKLz5mIqXj85qIiMxhBhCsK4I5dU02MtLUuuVgprKF/FSAfhzGVdlDSs9UkrodjV28HyrA4Li4tPNMdDQQGAiUl4tlYMnJ4nJmAHk12eYTeR1fnCu++JiJlI7PayIiMocZQLCtWB5gPlvIOAnHFa02faWtpyzdjlQq03WApAAQM4C8FrtnkbV8ca744mMmUjo+r4mIyBRmAMG2YnnWtpQGzGcP2cueTCVvJku3o4QEIDfXsA4Ql4ApArtnkbV8ca744mMmUjo+r4mIyBgDQLCtWJ61LaUlzmy16YttPd3e7UgqBG0qA4hLwLweu2eRtXxxrvjiYyZSOj6viYhIH5eAwbZiefa2zHRGq0229XQDaQkYM4CIiIiIiIhIQRgAQlWxPADVgkDGxfLsbZnpjFabzmrrqeQOYg5jBhARESkM3/eJiIgI4BIwHalYnnF3LbVRdy0pWyi/uNSqOkCqu/fhjFabNf1ta/6W0juIOcxUBhCLQBMRkZfi+z4RERFJGADSY02xPEutNY05u9Wmo209pQ5ixmOWOoixKwRMZwBxCRgREXkhvu8TERGRPi4BMyIVyxvUui46NY4zGUwx11rTeFNXtNq0t61nTR3EANFBzOfTwuvWFacnTgAajTjPJWBERORl+L5PRERExpgBZCdT2ULtkmOw79w1l7fatKetpy92ELNLu3ZAWBhQUAAcPAi0bcslYERE5HX4vk9ERETGGABygKnWmu76EGVrW092ELNSUBDQuzewejWwdq0IAElLwJgBREREXoLv+0RERGSMS8B8hLM6iPmEhx8Wp2vXilNmABERkZfh+z4REREZYwDIR0gdxMwtElNBdAVxRrcyrycFgHbsAK5dYxFoIiLyOnzfJyIiImMMAPkIqYMYgGofBp3drczrJScDzZuLItDr1rEINBEReR2+7xMREZExBoB8iL0dxHyS/jIwLgEjIiIvxPd9IiIi0qfSarWK7v9ZUlKCqKgoFBcXIzIyUu7heIRKjdamDmI+af16ICMDiI8XHcEAoLAQiImRd1xEREQ24vs+ERGRctkS82AXMB9kawcxn9S1KxAaWhX8AUR7eCIiIi/D930iIiICuASMyLSgIKBXr6rfAwPFDxEREREREZEXYgCIyBypDhDAAtBWqNRoseP0Vaw6eAE7Tl9FpUbRq0uJiIiIiIi8CpeAEZmjHwBiAWiLMrPyMHVNNvKKS3WXJUYFY/KANBYZJSIiIiIi8gDMACIyJyUFaNZMnGcGkFmZWXkYu3i/QfAHAPKLSzF28X5kZuXJNDIiIiIiIiKSMABEZImUBcQMIJMqNVpMXZMNU4u9pMumrsnmcjAiIiIiIiKZMQBEZMmwYYCfH9Cundwj8Ui7cwqrZf7o0wLIKy7F7pxC9w2KiIiIiIiIqmENICJLOnQALl8GoqPlHolHKrhuPvhjz3ZERERERETkGgwAkU+q1GixO6cQBddLER8RjI4psfD3U5neODbWvYOzk02PyUniI4Kduh0RERERERG5BgNA5HOU2LFKrsfUMSUWiVHByC8uNVkHSAVAHSWCUURERERERCQf1gAin6LEjlVyPiZ/PxUmD0gDIII9+qTfJw9Ic3kmEhEREREREVnGABD5DCV2rPKEx9Q3PREfjWgLdZThMi91VDA+GtHWa7OqiIiIiIiIlIRLwMhn2NKxqlPjOPcNzAGe8pj6piciI03t9hpEREREREREZB0GgMhnKLFjlSc9Jn8/ldcEzoiIiIiIiHwNl4CRz1BixyolPiYiIiIiIiJyPgaAyGdIHavMLUpSQXTO8qaOVUp8TEREREREROR8DACRz1BixyolPiYiIiIiIiJyPgaAyKcosWOVEh8TEREREREROZdKq9V6T89rO5SUlCAqKgrFxcWIjIyUezjkISo1WsV1rFLiYyIiIiIiIiLzbIl5sAsY+SQldqxS4mMiIiIiIiIi5+ASMCIiIiIiIiIihZM1ADRjxgx06NABERERiI+Px6OPPorjx48bbKPVajFlyhQkJSUhJCQEPXr0wG+//SbTiEnpKjVa7Dh9FasOXsCO01dRqVH0CkmyA+cIERERERF5I1mXgG3evBnPP/88OnTogIqKCkyaNAl9+vRBdnY2wsLCAACzZs3C7NmzsXDhQqSmpmLatGnIyMjA8ePHERERIefwSWEys/IwdU028opLdZclRgVj8oA0FlImAJwjRERERETkvTyqCPTly5cRHx+PzZs3o1u3btBqtUhKSsKECRPw6quvAgDKysqQkJCAmTNn4rnnnqvxPlkEmqyRmZWHsYv3w/jJIJVQZjct4hwhIiIiIiJPY0vMw6NqABUXFwMAYmNjAQA5OTnIz89Hnz59dNsEBQWhe/fu2L59uyxjJOWp1GgxdU12tS/2AHSXTV2TzaU+PoxzhIiIiIiIvJ3HBIC0Wi1eeukldOnSBenp6QCA/Px8AEBCQoLBtgkJCbrrjJWVlaGkpMTgh8iS3TmFBkt6jGkB5BWXYndOofsGRR6Fc4SIiIiIiLydxwSAxo8fj8OHD2Pp0qXVrlOpVAa/a7XaapdJZsyYgaioKN1P/fr1XTJeUo6C6+a/2NuzHSkP5wgREREREXk7jwgA/fWvf8Xq1auxceNG1KtXT3e5Wq0GgGrZPgUFBdWygiSvvfYaiouLdT+5ubmuGzgpQnxEsFO3I+XhHCEiIiIiIm8nawBIq9Vi/PjxWLFiBTZs2ICUlBSD61NSUqBWq7Fu3TrdZeXl5di8eTM6d+5s8j6DgoIQGRlp8ENkSceUWCRGBcN0Tpko8psYFYyOKbHuHBZ5EM4RIiIiIiLydrIGgJ5//nksXrwYS5YsQUREBPLz85Gfn4/bt28DEEu/JkyYgLfffhsrV65EVlYWRo8ejdDQUAwbNkzOoZOC+PupMHlAGgBU+4Iv/T55QBr8/cx9/Sel4xwhIiIiIiJvJ2sbeHN1fBYsWIDRo0cDEFlCU6dOxccff4xr167hvvvuw4cffqgrFF0TtoEna2Vm5WHqmmyDYr+JUcGYPCCN7b0JAOcIERERERF5FltiHrIGgNyBASCyRaVGi905hSi4Xor4CLGkh1kdpI9zhIiIiIiIPIUtMY8AN42JyCv4+6nQqXGc3MMgD8Y5QkRERERE3ogBICILmO2hbDy+RERERETkKxgAIjKD9V6UjceXiIiIiIh8iaxdwIg8VWZWHsYu3m8QHACA/OJSjF28H5lZeTKNjJyBx5eIiIiIiHwNA0BERio1Wkxdkw1T1dGly6auyUalRtH10xWLx5eIiIiIiHwRA0BERnbnFFbLDNGnBZBXXIrdOYXuGxQ5DY8vERERERH5IgaAiIwUXDcfHLBnO/IsPL5EREREROSLGAAiMhIfEezU7ciz8PgSEREREZEvYgCIyEjHlFgkRgXDXDNwFUS3qI4pse4cFjkJjy8REREREfkiBoCIjPj7qTB5QBoAVAsSSL9PHpAGfz9zIQTyZDy+RERERETkixgAIjKhb3oiPhrRFuoow2VA6qhgfDSiLfqmJ8o0MnIGHl8iIiIiIvI1Kq1Wq+hexyUlJYiKikJxcTEiIyPlHg55mUqNFrtzClFwvRTxEWJZEDNDlIPHl4iIiIiIvJktMY8AN42JyCv5+6nQqXGc3MMgF+HxJSIiIiIiX8ElYERERERERERECscAEBERERERERGRwjEARERERERERESkcAwAEREREREREREpHANAREREREREREQKxwAQEREREREREZHCMQBERERERERERKRwDAARERERERERESkcA0BERERERERERArHABARERERERERkcIxAEREREREREREpHAMABERERERERERKRwDQERERERERERECscAEBERERERERGRwjEARERERERERESkcAwAEREREREREREpHANAREREREREREQKxwAQEREREREREZHCMQBERERERERERKRwDAARERERERERESlcgNwDIPIElRotducUouB6KeIjgtExJRb+fiq5h0VERERERETkFAwAkc/LzMrD1DXZyCsu1V2WGBWMyQPS0Dc9UcaRERERERERETkHl4CRT8vMysPYxfsNgj8AkF9cirGL9yMzK0+mkRERERERERE5DwNA5LMqNVpMXZMNrYnrpMumrslGpcbUFkRERERERETegwEg8lm7cwqrZf7o0wLIKy7F7pxC9w2KiIiIiIiIyAUYACKfVXDdfPDHnu2IiIiIiIiIPBUDQOSz4iOCnbodERERERERkadiAIh8VseUWCRGBcNcs3cVRDewjimx7hwWERERERERkdMpvg28VisK+JaUlMg8EvJEf+9ZHy99dQgADIpBq+7+/veeTXHzxnU5hkZERERERERkkRTrkGIflqi01mzlxX7//XfUr19f7mEQEREREREREblEbm4u6tWrZ3EbxQeANBoNLl68iIiICKhU5hb7kFKVlJSgfv36yM3NRWRkpNzDIZlwHhDAeUAC5wEBnAckcB6QhHOBAO+dB1qtFtevX0dSUhL8/CxX+VH8EjA/P78ao2CkfJGRkV71JCbX4DwggPOABM4DAjgPSOA8IAnnAgHeOQ+ioqKs2o5FoImIiIiIiIiIFI4BICIiIiIiIiIihWMAiBQtKCgIkydPRlBQkNxDIRlxHhDAeUAC5wEBnAckcB6QhHOBAN+YB4ovAk1ERERERERE5OuYAUREREREREREpHAMABERERERERERKRwDQERERERERERECscAEBERERERERGRwjEARERERERERESkcAwAEREREREREREpXIDcAyAiIiJyFa1Wi/Xr12P79u3Iz8+HSqVCQkICHnjgAfTu3RsqlUruIZIbcB4QEZE+X31fUGm1Wq3cgyBylps3b2LJkiUmn8hPPvkkwsLC5B4iuYGvvqBTdZwLvu3ChQvo378/jhw5gvT0dCQkJECr1aKgoABZWVlo1aoVVq9ejbp168o9VHIhzgPSx/cFAjgPfJ0vvy8wAESKkZ2djYyMDNy6dQvdu3c3eCJv3rwZYWFh+Pnnn5GWlib3UMmFfPkFnQxxLtCgQYNw48YNLF68GImJiQbX5eXlYcSIEYiIiMB3330nzwDJLTgPSML3BQI4D8i33xcYACLF6NmzJ9RqNRYtWoTAwECD68rLyzF69Gjk5eVh48aNMo2Q3MGXX9DJEOcChYeHY9u2bWjVqpXJ6w8cOICuXbvixo0bbh4ZuRPnAUn4vkAA5wH59vsCawCRYuzatQt79+6tFvwBgMDAQLz++uvo2LGjDCMjd/rll1+wbdu2am/oAJCYmIh///vf6Nq1qwwjI3fjXKCQkBAUFhaavf7atWsICQlx44hIDpwHJOH7AgGcB+Tb7wvsAkaKERMTg5MnT5q9/tSpU4iJiXHjiEgOvvyCToY4F2jo0KEYNWoUvv32WxQXF+suLy4uxrfffounnnoKw4YNk3GE5A6cByTh+wIBnAfk2+8LzAAixXj22WcxatQovPHGG8jIyEBCQgJUKhXy8/Oxbt06vP3225gwYYLcwyQXk17QZ8+ejYyMDERFRQEQL+jr1q3Dyy+/rNgXdDLEuUDvvvsuKioqMHz4cFRUVOgyRMvLyxEQEIBnnnkG//rXv2QeJbka5wFJ+L5AAOcB+fb7AmsAkaLMnDkTc+fO1VXzB0SVf7VajQkTJmDixIkyj5Bcrby8HC+88ALmz59v9gV9zpw5JpcKkrJwLpCkpKQEe/fuxaVLlwAAarUa7dq1Q2RkpMwjI3cqKSnBvn37kJ+fD4DzwBfxfYEAzgOq4oufDxgAIkXKyckx+ICXkpIi84jI3XzxBZ1M45c+IiLSx88IBPDzAfkmBoBIsa5du4ZFixbh5MmTSEpKwsiRI1G/fn25h0VERG508+ZNLFmyBNu3b9dlhyYkJOCBBx7Ak08+ibCwMLmHSG52584d/PDDDzh58iQSExMxePBgzgMiIh/jq58PGAAixUhKSsKRI0cQFxeHnJwcPPDAA9BqtWjZsiWOHj2K69evY+fOnbjnnnvkHiq5mK++oJNl/NLne7Kzs5GRkYFbt26he/fuSEhIgFarRUFBATZv3oywsDD8/PPPSEtLk3uo5EKdO3fGjz/+iOjoaFy+fBm9evXCiRMnkJycjNzcXMTHx2P79u2oW7eu3EMlN+BnBDLGzwe+x5c/HzAARIrh5+eH/Px8xMfH48knn0R+fj5++OEHhIaGoqysDEOGDEFwcDC++eYbuYdKLuTLL+hkiF/6qGfPnlCr1Vi0aFG1Wg7l5eUYPXo08vLysHHjRplGSO6g//lgzJgx2LNnD9auXQu1Wo2rV69i4MCBuOeee/DZZ5/JPVRyMX5GIICfD8i3Px8wAESKof8Br1GjRvj000/Rq1cv3fW7du3CkCFDkJubK+MoydV8+QWdDPFLH4WGhmLv3r1mv8xlZWWhY8eOuHXrlptHRu6k/1rQrFkzzJ49G4888oju+k2bNuGpp55CTk6OjKMkd+BnBAL4+YB8+/MB28CTokidv8rKypCQkGBwXUJCAi5fvizHsMiNdu3ahb1795rs3BAYGIjXX38dHTt2lGFkJKfNmzdj9uzZUKvVAIC4uDhMnz4dTz31lMwjI1eKiYnByZMnzX7AO3XqFGJiYtw8KpKD9PmgqKioWmOIlJQU5OXlyTEscjN+RiBj/Hzgm3z58wEDQKQovXv3RkBAAEpKSnDixAm0aNFCd9358+dRu3ZtGUdH7uDLL+hUHb/0+bZnn30Wo0aNwhtvvIGMjAwkJCRApVIhPz8f69atw9tvv40JEybIPUxyg9GjRyMoKAh37tzBuXPnDN4j8vLyEB0dLd/gyG34GYEk/Hzg23z58wEDQKQYkydPNvg9NDTU4Pc1a9aga9eu7hwSycCXX9CpOn7p821TpkxBSEgIZs+ejYkTJ+o+8Gu1WqjVavzjH//AxIkTZR4ludqoUaN05wcNGoQbN24YXL98+XK0bt3azaMiOfAzAkn4+cC3+fLnA9YAIiLFmTlzJubOnavr7gFUvaBPmDBBsS/oZMg4fbtfv3547LHHdL+/8sorOHLkCDIzM909NJJBTk4O8vPzAQBqtbraf3zJd928eRP+/v4IDg6WeyjkBvyMQPx8QPr0Px8kJCSgUaNGMo/ItRgAIiLF4hc+soRf+oiIfBc/I5A5/HzguwIDA3Ho0CE0b95c7qG4DJeAEZFipaSkVPtAl5ubi8mTJ2P+/PkyjYo8RWFhIeeCD7h9+zb27duH2NjYanU/SktL8fXXX2PkyJEyjY7chfOAJEePHsXOnTvRuXNndOrUCceOHcOsWbNQVlaGESNGGHSQJeXSnwfNmjXDsWPHMHfuXM4DH/HSSy+ZvLyyshLvvPMO4uLiAACzZ89257DcghlARORTDh06hLZt26KyslLuoZDMOBeU78SJE+jTpw/Onz8PlUqFrl27YunSpUhMTAQAXLp0CUlJSZwDCsd5QJLMzEwMGjQI4eHhuHXrFlauXImRI0eiVatW0Gq12Lx5M3766Sd++Vc4zgPy8/NDq1atqtV62rx5M9q3b4+wsDCoVCps2LBBngG6EANARKQoq1evtnj9mTNn8PLLL/ODvg/gXKDBgwejoqICCxYsQFFREV566SVkZWVh06ZNaNCgAb/4+wjOA5J07twZvXr1wrRp07Bs2TKMGzcOY8eOxfTp0wEAkyZNwp49e/Dzzz/LPFJyJc4DmjFjBj755BN8+umnBoG+WrVq4dChQ2Y7BSoBA0BEpCh+fn5QqVSw9NKmUqn4Qd8HcC5QQkIC1q9fj5YtW+oue/755/H9999j48aNCAsL4xd/H8B5QJKoqCjs27cPTZo0gUajQVBQEHbt2oW2bdsCALKysvDggw/qagORMnEeEADs2bMHI0aMwIABAzBjxgzUqlXLJwJAfnIPgIjImRITE7F8+XJoNBqTP/v375d7iOQmnAt0+/ZtBAQYljv88MMPMXDgQHTv3h0nTpyQaWTkTpwHZIqfnx+Cg4MNloBERESguLhYvkGR23Ee+K4OHTpg3759uHz5Mtq3b48jR47oOgMqGQNARKQo7dq1s/jFvqaMEFIOzgW65557sHfv3mqX/+c//8GgQYMwcOBAGUZF7sZ5QJKGDRvi1KlTut937NiBBg0a6H7Pzc3V1YYi5eI8IEl4eDgWLVqE1157DRkZGT6RCcoAEBEpyiuvvILOnTubvb5JkybYuHGjG0dEcuFcoMGDB2Pp0qUmr/vggw/w5JNPMgjoAzgPSDJ27FiDL3jp6ekG2WFr165l4V8fwHlAxoYOHYq9e/dixYoVSE5Olns4LsUaQERERERERERECscMICIiIiIiIiIihWMAiIiIiIiIiIhI4RgAIiIiIiIiIiJSOAaAiIiIyCOpVCp89913cg9DFu587KNHj8ajjz6q+71Hjx6YMGGCW/62HH+PiIjIVzEARERERBbl5ubimWeeQVJSEgIDA5GcnIwXXngBV69erfG2Z8+ehUqlwsGDB10/UDe4cuUK1Go13n777WrXPf744+jQoQMqKioc/jt5eXl4+OGH7brtlClToFKp0Ldv32rXzZo1CyqVCj169NBdNnfuXCxcuNDq+zcOGDlqxYoVeOutt5x2f0RERGQaA0BERERk1pkzZ9C+fXucOHECS5cuxalTp/Df//4Xv/zyCzp16oTCwkKzty0vL3fjSN2jdu3a+N///oepU6fiyJEjusu//fZbrFmzBp9//rlBO2F7qdVqBAUF2X37xMREbNy4Eb///rvB5QsWLECDBg0MLouKikJ0dLTdf8ted+7cAQDExsYiIiLC7X+fiIjI1zAARERERGY9//zzCAwMxM8//4zu3bujQYMGePjhh7F+/XpcuHABkyZN0m3bsGFDTJs2DaNHj0ZUVBSeffZZpKSkAADatGljkHmyZ88eZGRkoHbt2oiKikL37t2xf/9+i2N59dVXkZqaitDQUDRq1AhvvvmmLogAiMyX1q1bY/78+WjQoAHCw8MxduxYVFZWYtasWVCr1YiPj8f06dMN7nf27Nlo2bIlwsLCUL9+fYwbNw43btwwO46BAwdi2LBhGDlyJO7cuYPLly9j3LhxmDFjBgIDAzFo0CAkJCQgPDwcHTp0wPr16w1un5eXh0ceeQQhISFISUnBkiVL0LBhQ8yZM0e3jf4SsPLycowfPx6JiYkIDg5Gw4YNMWPGDIv7Kj4+Hn369MGiRYt0l23fvh1XrlzBI488YrBtTRk9mZmZiIqKwueff44pU6Zg0aJFWLVqFVQqFVQqFTZt2gTAtuPTqFEjBAUFQavVcgkYERGRmzj+LyoiIiJSpMLCQvz000+YPn06QkJCDK5Tq9UYPnw4vvrqK8ybNw8qlQoA8K9//Qtvvvkm3njjDQDA+PHj0bFjR6xfvx4tWrRAYGAgAOD69esYNWoU3n//fQDAu+++i379+uHkyZNms0EiIiKwcOFCJCUl4ciRI3j22WcRERGBiRMn6rY5ffo01q5di8zMTJw+fRpDhgxBTk4OUlNTsXnzZmzfvh1PP/00evfujfvvvx8A4Ofnh/fffx8NGzZETk4Oxo0bh4kTJ2LevHlm983cuXPRsmVLvPXWWzh69CjS09Pxwgsv4PDhw+jXrx+mTZuG4OBgLFq0CAMGDMDx48d1mTcjR47ElStXsGnTJtSqVQsvvfQSCgoKzP6t999/H6tXr8bXX3+NBg0aIDc3F7m5uRaPHQA8/fTTmDhxoi5IN3/+fAwfPrzG2+lbtmwZxowZgy+++AKDBg3CjRs3cPToUZSUlGDBggUARAYPYN3xOXXqFL7++mssX74c/v7+No2FiIiIHMMAEBEREZl08uRJaLVaNG/e3OT1zZs3x7Vr13D58mXEx8cDAHr16oW///3vum3Onj0LAIiLi4NardZd3qtXL4P7+vjjjxETE4PNmzejf//+Jv+eFFQCRLbRyy+/jK+++sogwKDRaDB//nxEREQgLS0NPXv2xPHjx/Hjjz/Cz88PzZo1w8yZM7Fp0yZdAEg/+yQlJQVvvfUWxo4dazEAFBkZiQULFqBPnz4ICwvD4cOHoVKp0KpVK7Rq1Uq33bRp07By5UqsXr0a48ePx7Fjx7B+/Xrs2bMH7du3BwB8+umnaNq0qdm/df78eTRt2hRdunSBSqVCcnKy2W319e/fH3/5y1/w66+/ol27dvj666+xdetWzJ8/36rbz5s3D6+//jpWrVqFnj17AgDCw8MREhKCsrIyg+MJWHd8ysvL8cUXX6BOnTpWjYGIiIichwEgIiIisotWqwUAXfYPAF1QoyYFBQX45z//iQ0bNuDSpUuorKzErVu3cP78ebO3+fbbbzFnzhycOnUKN27cQEVFBSIjIw22adiwoUEGUUJCAvz9/eHn52dwmX7GzcaNG/H2228jOzsbJSUlqKioQGlpKW7evImwsDCz4+nVqxfuv/9+tG7dWheUuXnzJqZOnYrvv/8eFy9eREVFBW7fvq17XMePH0dAQADatm2ru58mTZogJibG7N8ZPXo0MjIy0KxZM/Tt2xf9+/dHnz59zG4vqVWrFkaMGIEFCxbgzJkzSE1Nxb333lvj7QBg+fLluHTpErZu3YqOHTtadRtrjk9ycjKDP0RERDJhDSAiIiIyqUmTJlCpVMjOzjZ5/bFjxxATE4PatWvrLrMUMNE3evRo7Nu3D3PmzMH27dtx8OBBxMXFmS0cvXPnTgwdOhQPP/wwvv/+exw4cACTJk2qtn2tWrUMflepVCYv02g0AIBz586hX79+SE9Px/Lly7Fv3z58+OGHAGBQv8acgIAAg6LPr7zyCpYvX47p06djy5YtOHjwIFq2bKkbpxQ0M2bucgBo27YtcnJy8NZbb+H27dt4/PHHMWTIkBrHBohlYN988w0+/PBDPP3001bdBgBat26NOnXqYMGCBRbHJrH2+Fg7P4iIiMj5mAFEREREJsXFxSEjIwPz5s3Diy++aFAHKD8/H19++SVGjhxpkAFkTKr5U1lZaXD5li1bMG/ePPTr1w+AaDV/5coVs/ezbds2JCcnGxSdPnfunF2PS9/evXtRUVGBd999V5cl9PXXX9t9f1u2bMHo0aMxePBgAMCNGzd0y+AA4J577kFFRQUOHDiAdu3aARB1cYqKiizeb2RkJJ544gk88cQTGDJkCPr27YvCwkJd/R1zWrRogRYtWuDw4cMYNmyY1Y+jcePGePfdd9GjRw/4+/vjgw8+0F0XGBhY7Xi66vgQERGR8zADiIiIiMz64IMPUFZWhoceegi//vorcnNzkZmZiYyMDNStW7daRy1j8fHxCAkJQWZmJi5duoTi4mIAIrvoiy++wNGjR7Fr1y4MHz68WqFpfU2aNMH58+exbNkynD59Gu+//z5Wrlzp8ONr3LgxKioq8J///AdnzpzBF198gf/+979231+TJk2wYsUKHDx4EIcOHcKwYcN02UaACAA9+OCDGDNmDHbv3o0DBw5gzJgxCAkJMRtIe++997Bs2TIcO3YMJ06cwDfffAO1Wm116/YNGzYgLy/P5lbvqamp2LhxI5YvX25QJ6lhw4Y4fPgwjh8/jitXruDOnTsuOz5ERETkPAwAERERkVlNmzbF3r170bhxYzzxxBNo3LgxxowZg549e2LHjh01ZqAEBATg/fffx8cff4ykpCQMGjQIgOhIde3aNbRp0wZ/+tOf8Le//U1XSNqUQYMG4cUXX8T48ePRunVrbN++HW+++abDj69169aYPXs2Zs6cifT0dHz55Zc1tli35L333kNMTAw6d+6MAQMG4KGHHjKo9wMAn3/+ORISEtCtWzcMHjxY1y0rODjY5H2Gh4dj5syZaN++PTp06ICzZ8/qilpbIywszObgj6RZs2bYsGEDli5dipdffhkA8Oyzz6JZs2Zo37496tSpg23btrns+BAREZHzqLTWLOwmIiIiIpf4/fffUb9+faxfvx69e/eWezhERESkUAwAEREREbnRhg0bcOPGDbRs2RJ5eXmYOHEiLly4gBMnTlQrWE1ERETkLCwCTURERORGd+7cweuvv44zZ84gIiICnTt3xpdffsngDxEREbkUM4CIiIiIiIiIiBSORaCJiIiIiIiIiBSOASAiIiIiIiIiIoVjAIiIiIiIiIiISOEYACIiIiIiIiIiUjgGgIiIiIiIiIiIFI4BICIiIiIiIiIihWMAiIiIiIiIiIhI4RgAIiIiIiIiIiJSOAaAiIiIiIiIiIgU7v8BFbeKzool8RsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 9-) ort. yagis miktarlari icin ort. kesinti sayisi grafigi (cok mantikli, gerekli degil)\n",
    "\n",
    "yagis_dict = {}\n",
    "for label,group in merged_all_week[\"izmir-aliaga\"].groupby(\"Yagis_max\"):\n",
    "    yagis_dict[label] = group\n",
    "\n",
    "yagis_dict_toplamlari = {}\n",
    "for deger in yagis_dict.keys():\n",
    "    yagis_dict_toplamlari[deger] = yagis_dict[deger][\"Bildirimsiz_sum\"].mean()\n",
    "print(yagis_dict_toplamlari)\n",
    "\n",
    "hesaplamalar = pd.DataFrame(list(yagis_dict_toplamlari.items()), columns=['Ort. Yagis', 'Ort. Kesinti'])\n",
    "print(hesaplamalar)\n",
    "window_size = 3  # Hareketli ortalama penceresi\n",
    "hesaplamalar['Moving_Average'] = hesaplamalar[\"Ort. Kesinti\"].rolling(window=window_size, center=True).mean()\n",
    "\n",
    "alpha = 0.3  # Yumuşatma parametresi \n",
    "# formul : EMA_t = α × X_t + (1 - α) × EMA_{t-1}\n",
    "hesaplamalar['EWMA'] = hesaplamalar[\"Ort. Kesinti\"].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "#plt.bar(merged_all_week[\"izmir-aliaga\"][\"Yagis\"],merged_all_week[\"izmir-aliaga\"][\"Bildirimsiz_sum\"], width=0.05, label=\"Bildirimsiz\")\n",
    "plt.scatter(yagis_dict_toplamlari.keys(), yagis_dict_toplamlari.values(), label=\"Ort. Kesinti\")\n",
    "#plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['Moving_Average'], label=\"MHO\", color=\"red\")\n",
    "plt.plot(hesaplamalar[\"Ort. Yagis\"], hesaplamalar['EWMA'], label=\"EWMA\", color=\"red\")\n",
    "plt.margins(0.01)\n",
    "plt.title(\"Izmir_Aliaga Yagis - Bildirimsiz (Haftalik)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Ortalama Yagis Miktari\")\n",
    "plt.ylabel(\"Ortalama Elektrik Kesintisi\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tarih         ilce  bildirimli_sum\n",
      "18   2024-02-01  izmir-konak               4\n",
      "65   2024-02-02  izmir-konak               1\n",
      "112  2024-02-03  izmir-konak               1\n",
      "159  2024-02-04  izmir-konak               0\n",
      "206  2024-02-05  izmir-konak               2\n",
      "Index(['Tarih', 'Ilce', 'Bildirimli_sum'], dtype='object')\n",
      "                Tarih         Ilce  Bildirimli_sum Bayram_Flag  Sicaklik_max  \\\n",
      "tarih                                                                          \n",
      "2024-02-01 2024-02-01  izmir-konak               4         NaN          11.9   \n",
      "2024-02-02 2024-02-02  izmir-konak               1         NaN          12.8   \n",
      "2024-02-03 2024-02-03  izmir-konak               1         NaN          12.5   \n",
      "2024-02-04 2024-02-04  izmir-konak               0         NaN          13.4   \n",
      "2024-02-05 2024-02-05  izmir-konak               2         NaN          17.6   \n",
      "2024-02-06 2024-02-06  izmir-konak               1         NaN          20.2   \n",
      "2024-02-07 2024-02-07  izmir-konak               0         NaN          18.3   \n",
      "2024-02-08 2024-02-08  izmir-konak               3         NaN          18.4   \n",
      "2024-02-09 2024-02-09  izmir-konak               1         NaN          17.4   \n",
      "2024-02-10 2024-02-10  izmir-konak               4         NaN          18.2   \n",
      "2024-02-11 2024-02-11  izmir-konak               0         NaN          18.5   \n",
      "2024-02-12 2024-02-12  izmir-konak               0         NaN          17.2   \n",
      "2024-02-13 2024-02-13  izmir-konak               0         NaN          16.6   \n",
      "2024-02-14 2024-02-14  izmir-konak               0         NaN          15.8   \n",
      "2024-02-15 2024-02-15  izmir-konak               0         NaN          12.6   \n",
      "2024-02-16 2024-02-16  izmir-konak               0         NaN          14.5   \n",
      "2024-02-17 2024-02-17  izmir-konak               4         NaN          14.4   \n",
      "2024-02-18 2024-02-18  izmir-konak               0         NaN          15.0   \n",
      "2024-02-19 2024-02-19  izmir-konak               1         NaN          14.0   \n",
      "2024-02-20 2024-02-20  izmir-konak               0         NaN          13.9   \n",
      "2024-02-21 2024-02-21  izmir-konak               0         NaN          14.6   \n",
      "2024-02-22 2024-02-22  izmir-konak               2         NaN          16.0   \n",
      "2024-02-23 2024-02-23  izmir-konak               3         NaN          16.9   \n",
      "2024-02-24 2024-02-24  izmir-konak               1         NaN          19.0   \n",
      "2024-02-25 2024-02-25  izmir-konak               1         NaN          19.2   \n",
      "2024-02-26 2024-02-26  izmir-konak               0         NaN          18.5   \n",
      "2024-02-27 2024-02-27  izmir-konak               3         NaN          19.5   \n",
      "2024-02-28 2024-02-28  izmir-konak               0         NaN          21.0   \n",
      "2024-02-29 2024-02-29  izmir-konak               0         NaN          22.1   \n",
      "\n",
      "            Sicaklik_min  Bagil_nem_max  Bagil_nem_min  Ruzgar_hizi_max  \\\n",
      "tarih                                                                     \n",
      "2024-02-01           4.6           89.5           50.8              2.9   \n",
      "2024-02-02           3.7           92.1           48.5              3.3   \n",
      "2024-02-03           5.6           88.7           49.0              4.7   \n",
      "2024-02-04           5.9           86.1           56.0              1.6   \n",
      "2024-02-05           7.1           95.7           59.0              2.7   \n",
      "2024-02-06           9.3           95.3           60.8              3.0   \n",
      "2024-02-07          12.0           95.7           67.3              6.9   \n",
      "2024-02-08          12.7           89.8           62.3              6.5   \n",
      "2024-02-09          10.8           94.2           60.1              3.5   \n",
      "2024-02-10          11.0           93.2           66.3              6.2   \n",
      "2024-02-11          13.8           82.0           54.5              8.0   \n",
      "2024-02-12          12.1           90.0           65.2              7.9   \n",
      "2024-02-13           8.9           96.1           52.5              3.9   \n",
      "2024-02-14           8.0           99.9           47.6              5.6   \n",
      "2024-02-15           8.2           80.6           59.9              5.3   \n",
      "2024-02-16           7.0           84.0           53.4              4.2   \n",
      "2024-02-17           7.4           87.0           53.2              4.0   \n",
      "2024-02-18           6.8           87.2           53.2              4.0   \n",
      "2024-02-19           6.3           84.7           49.8              3.7   \n",
      "2024-02-20           5.1           89.9           52.6              2.9   \n",
      "2024-02-21           6.1           88.8           44.0              3.4   \n",
      "2024-02-22           8.0           75.4           40.9              2.5   \n",
      "2024-02-23           8.6           95.3           62.4              4.3   \n",
      "2024-02-24           9.7           97.4           55.6              2.8   \n",
      "2024-02-25          10.6           90.8           48.1              3.1   \n",
      "2024-02-26          10.9           84.1           47.7              5.4   \n",
      "2024-02-27          11.8           87.6           50.0              2.6   \n",
      "2024-02-28          10.7           91.7           44.1              2.4   \n",
      "2024-02-29           9.9           92.8           41.2              2.6   \n",
      "\n",
      "            Ruzgar_hizi_min  Yagis_max  Yagis_min   Sicaklik  Bagil_nem  \\\n",
      "tarih                                                                     \n",
      "2024-02-01              2.9        1.0        1.0   7.416667  76.075000   \n",
      "2024-02-02              3.3        1.0        1.0   7.537500  76.600000   \n",
      "2024-02-03              4.7        1.0        1.0   8.354167  70.429167   \n",
      "2024-02-04              1.6        8.7        1.0   9.300000  72.108333   \n",
      "2024-02-05              2.7        1.0        1.0  11.412500  81.879167   \n",
      "2024-02-06              3.0        1.0        1.0  13.341667  82.562500   \n",
      "2024-02-07              6.9        4.6        1.0  14.958333  84.025000   \n",
      "2024-02-08              6.5        1.0        1.0  14.925000  80.550000   \n",
      "2024-02-09              3.5       57.1        1.0  13.379167  83.950000   \n",
      "2024-02-10              6.2       14.1        1.0  14.108333  79.545833   \n",
      "2024-02-11              8.0       94.2        1.0  15.645833  70.191667   \n",
      "2024-02-12              7.9       95.0        1.0  14.679167  78.145833   \n",
      "2024-02-13              3.9       31.7        1.0  11.904167  82.216667   \n",
      "2024-02-14              5.6        1.0        1.0  11.454167  76.887500   \n",
      "2024-02-15              5.3       68.7        1.0   9.841667  72.545833   \n",
      "2024-02-16              4.2        1.0        1.0   9.875000  72.495833   \n",
      "2024-02-17              4.0        1.0        1.0  10.062500  73.662500   \n",
      "2024-02-18              4.0        1.0        1.0  10.137500  73.129167   \n",
      "2024-02-19              3.7        1.0        1.0   9.700000  70.825000   \n",
      "2024-02-20              2.9        1.0        1.0   9.154167  75.720833   \n",
      "2024-02-21              3.4        8.3        1.0   9.941667  69.854167   \n",
      "2024-02-22              2.5        1.0        1.0  11.979167  60.583333   \n",
      "2024-02-23              4.3        1.0        1.0  12.425000  79.912500   \n",
      "2024-02-24              2.8        1.0        1.0  13.795833  81.883333   \n",
      "2024-02-25              3.1       25.7        1.0  14.058333  71.991667   \n",
      "2024-02-26              5.4       31.1        1.0  14.300000  67.937500   \n",
      "2024-02-27              2.6        1.0        1.0  14.954167  75.770833   \n",
      "2024-02-28              2.4        1.0        1.0  15.362500  73.537500   \n",
      "2024-02-29              2.6       85.1        1.0  15.991667  68.441667   \n",
      "\n",
      "            Ruzgar_hizi      Yagis  Gün  \n",
      "tarih                                    \n",
      "2024-02-01     2.054167   1.000000    1  \n",
      "2024-02-02     1.691667   1.000000    2  \n",
      "2024-02-03     2.483333   1.000000    3  \n",
      "2024-02-04     1.083333   1.616667    4  \n",
      "2024-02-05     2.004167   1.000000    5  \n",
      "2024-02-06     2.337500   1.000000    6  \n",
      "2024-02-07     4.529167   1.245833    7  \n",
      "2024-02-08     4.483333   1.000000    8  \n",
      "2024-02-09     2.491667   5.058333    9  \n",
      "2024-02-10     4.133333   2.512500   10  \n",
      "2024-02-11     6.191667  15.691667   11  \n",
      "2024-02-12     6.387500  37.950000   12  \n",
      "2024-02-13     2.108333   3.087500   13  \n",
      "2024-02-14     2.820833   1.000000   14  \n",
      "2024-02-15     4.150000   7.166667   15  \n",
      "2024-02-16     2.641667   1.000000   16  \n",
      "2024-02-17     2.487500   1.000000   17  \n",
      "2024-02-18     2.637500   1.000000   18  \n",
      "2024-02-19     2.166667   1.000000   19  \n",
      "2024-02-20     1.504167   1.000000   20  \n",
      "2024-02-21     1.695833   1.562500   21  \n",
      "2024-02-22     1.137500   1.000000   22  \n",
      "2024-02-23     2.583333   1.000000   23  \n",
      "2024-02-24     2.112500   1.000000   24  \n",
      "2024-02-25     2.170833   2.216667   25  \n",
      "2024-02-26     2.712500   5.658333   26  \n",
      "2024-02-27     1.112500   1.000000   27  \n",
      "2024-02-28     1.566667   1.000000   28  \n",
      "2024-02-29     1.633333  14.195833   29  \n"
     ]
    }
   ],
   "source": [
    "# 10-) test icin birlestirme islemleri\n",
    "\n",
    "test = pd.read_csv(\"./test.csv\", low_memory=False) # 47 ilce icin 28 gunluk veriler var. (tarih, ilce, bildirimli_sum)\n",
    "#print(test)\n",
    "\n",
    "dict_test :{str, pd.DataFrame} = {} # key olarak ilceleri, value olarak ilcelerin 4 ocak - 31 ocak arasi verilerini df olarak tutar\n",
    "for label, group in test.groupby(\"ilce\"):\n",
    "    dict_test[label] = group\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].head())\n",
    "\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    tarihler = [] # duzgun tarihleri tutacak\n",
    "    for date in dict_test[name][\"tarih\"]:\n",
    "        tarihler.append(datetime.strptime(date, \"%Y-%m-%d\")) \n",
    "\n",
    "    dict_test[name][\"tarih\"] = tarihler # duzeltilmis tarihleri ata\n",
    "\n",
    "    dict_test[name].set_index(\"tarih\", inplace=True) # tarih kolonunu index'e ata\n",
    "    dict_test[name][\"Tarih\"] = dict_test[name].index # tarih kolonunu yeniden olustur\n",
    "    dict_test[name] = dict_test[name].iloc[:, [2, 0, 1]] # kolon siralarini duzenle\n",
    "    dict_test[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\"] # kolon isimlerini duzenle\n",
    "\n",
    "print(dict_test[\"izmir-konak\"].columns)\n",
    "\n",
    "\n",
    "dict_test_merged = {} # birlestirilenleri tutacak dict\n",
    "for name in dict_test.keys():\n",
    "\n",
    "    gecici = merge_holiday(dict_test[name], holiday) # test'e holiday ekle\n",
    "    dict_test_merged[name] = merge_weather(gecici, weather_last[name]) # sonra weather'i ekle\n",
    "\n",
    "    dict_test_merged[name].columns = [\"Tarih\", \"Ilce\", \"Bildirimli_sum\", # tekrar isimlendir\n",
    "    \"Bayram_Flag\", \"Sicaklik_max\", \"Sicaklik_min\",\"Bagil_nem_max\",\"Bagil_nem_min\",\n",
    "    \"Ruzgar_hizi_max\", \"Ruzgar_hizi_min\",\"Yagis_max\", \"Yagis_min\",\"Sicaklik\",\"Bagil_nem\",\"Ruzgar_hizi\",\"Yagis\"]\n",
    "\n",
    "    dict_test_merged[name]['Gün'] = range(1, len(dict_test_merged[name]) + 1) # gun kolonu ekle (1-28 arasi oluyor)\n",
    "\n",
    "print(dict_test_merged[\"izmir-konak\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAKINE OGRENMESI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpus = tf.config.list_physical_devices('GPU')\n",
    "#logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "#print((gpus), \"Physical GPU,\\n\", (logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.config islemleri\n",
    "\n",
    "\n",
    "#tf.config.set_soft_device_placement(True) # otomatik dogru cihaz kullanimi\n",
    "#strategy = tf.distribute.MirroredStrategy() # birden fazla gpu kullanimi\n",
    "#tf.debugging.set_log_device_placement(True) # hangi cihazlarin kullanildigi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 11ms/step - loss: 46.5620 - mae: 5.4055 - val_loss: 43.4506 - val_mae: 5.1020\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 39.1944 - mae: 4.7062 - val_loss: 34.9535 - val_mae: 4.2447\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 29.1826 - mae: 3.7123 - val_loss: 23.5066 - val_mae: 3.0746\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.2004 - mae: 2.8504 - val_loss: 15.9256 - val_mae: 2.6430\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.3750 - mae: 2.8821 - val_loss: 15.7300 - val_mae: 2.8094\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.3164 - mae: 2.8467 - val_loss: 15.6282 - val_mae: 2.6588\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.1305 - mae: 2.7872 - val_loss: 15.4634 - val_mae: 2.6880\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.1154 - mae: 2.8311 - val_loss: 15.3866 - val_mae: 2.6791\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.0104 - mae: 2.7928 - val_loss: 15.3895 - val_mae: 2.6528\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.0844 - mae: 2.8222 - val_loss: 15.2570 - val_mae: 2.6737\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.0093 - mae: 2.7704 - val_loss: 15.3395 - val_mae: 2.6316\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.9242 - mae: 2.7937 - val_loss: 15.1791 - val_mae: 2.6636\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.8921 - mae: 2.7969 - val_loss: 15.1910 - val_mae: 2.6519\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.8767 - mae: 2.8069 - val_loss: 15.1428 - val_mae: 2.6508\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.8493 - mae: 2.7568 - val_loss: 15.2256 - val_mae: 2.6120\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.8009 - mae: 2.7830 - val_loss: 15.0657 - val_mae: 2.6577\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.7743 - mae: 2.7705 - val_loss: 15.0985 - val_mae: 2.6279\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.7865 - mae: 2.8117 - val_loss: 14.9733 - val_mae: 2.6610\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.6978 - mae: 2.7634 - val_loss: 15.0891 - val_mae: 2.6071\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.6611 - mae: 2.7663 - val_loss: 14.9543 - val_mae: 2.6378\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.7493 - mae: 2.7562 - val_loss: 15.0099 - val_mae: 2.6226\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.6286 - mae: 2.7883 - val_loss: 14.9116 - val_mae: 2.6373\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.5906 - mae: 2.7769 - val_loss: 14.9150 - val_mae: 2.6277\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.5759 - mae: 2.7482 - val_loss: 14.9083 - val_mae: 2.6150\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.5136 - mae: 2.7514 - val_loss: 14.8827 - val_mae: 2.6173\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 15.5170 - mae: 2.7696 - val_loss: 14.8537 - val_mae: 2.6158\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.5921 - mae: 2.7348 - val_loss: 14.9056 - val_mae: 2.5997\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.4350 - mae: 2.7621 - val_loss: 14.7540 - val_mae: 2.6591\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.4933 - mae: 2.7662 - val_loss: 14.8246 - val_mae: 2.6035\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.4277 - mae: 2.7647 - val_loss: 14.7877 - val_mae: 2.6126\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.4995 - mae: 2.7254 - val_loss: 14.8308 - val_mae: 2.5946\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.4345 - mae: 2.7800 - val_loss: 14.7216 - val_mae: 2.6209\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.3463 - mae: 2.7427 - val_loss: 14.7594 - val_mae: 2.5973\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.3210 - mae: 2.7287 - val_loss: 14.7460 - val_mae: 2.5966\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2636 - mae: 2.7467 - val_loss: 14.6410 - val_mae: 2.6202\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.2561 - mae: 2.7526 - val_loss: 14.6900 - val_mae: 2.5933\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2411 - mae: 2.7324 - val_loss: 14.7262 - val_mae: 2.5813\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2146 - mae: 2.7287 - val_loss: 14.6069 - val_mae: 2.6102\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2065 - mae: 2.7518 - val_loss: 14.6094 - val_mae: 2.5944\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1833 - mae: 2.7159 - val_loss: 14.6090 - val_mae: 2.5869\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1812 - mae: 2.7230 - val_loss: 14.6106 - val_mae: 2.5867\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2800 - mae: 2.7642 - val_loss: 14.6570 - val_mae: 2.5647\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1623 - mae: 2.7029 - val_loss: 14.5069 - val_mae: 2.5990\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1338 - mae: 2.7512 - val_loss: 14.5341 - val_mae: 2.5750\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1053 - mae: 2.6992 - val_loss: 14.5149 - val_mae: 2.5773\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.1107 - mae: 2.7620 - val_loss: 14.4389 - val_mae: 2.5863\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 15.2115 - mae: 2.6896 - val_loss: 14.6975 - val_mae: 2.5397\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.0227 - mae: 2.7476 - val_loss: 14.3766 - val_mae: 2.6091\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 14.9494 - mae: 2.7153 - val_loss: 14.4980 - val_mae: 2.5577\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 14.9175 - mae: 2.6865 - val_loss: 14.4131 - val_mae: 2.5676\n",
      "11/11 [==============================] - 0s 835us/step - loss: 11.8078 - mae: 2.6286\n",
      "Mean Absolute Error on Test Data: 2.628602981567383\n",
      "11/11 [==============================] - 0s 878us/step\n",
      "R-squared: 0.06521128552985322\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 0s 10ms/step - loss: 4.4215 - mae: 1.5643 - val_loss: 1.9993 - val_mae: 1.0111\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.8743 - mae: 1.1215 - val_loss: 1.2147 - val_mae: 0.8046\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1406 - mae: 1.0683 - val_loss: 1.3320 - val_mae: 0.9705\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1563 - mae: 1.1322 - val_loss: 1.3567 - val_mae: 0.9872\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.1067 - mae: 1.0841 - val_loss: 1.1891 - val_mae: 0.8879\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0790 - mae: 1.0588 - val_loss: 1.1939 - val_mae: 0.8936\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0723 - mae: 1.0639 - val_loss: 1.2206 - val_mae: 0.9136\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.0571 - mae: 1.0600 - val_loss: 1.1995 - val_mae: 0.9015\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 2.0490 - mae: 1.0545 - val_loss: 1.1886 - val_mae: 0.8950\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0428 - mae: 1.0536 - val_loss: 1.1953 - val_mae: 0.9011\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0380 - mae: 1.0544 - val_loss: 1.1950 - val_mae: 0.9012\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0373 - mae: 1.0574 - val_loss: 1.2083 - val_mae: 0.9112\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0353 - mae: 1.0477 - val_loss: 1.1738 - val_mae: 0.8891\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0269 - mae: 1.0518 - val_loss: 1.2060 - val_mae: 0.9126\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0270 - mae: 1.0491 - val_loss: 1.1731 - val_mae: 0.8902\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0300 - mae: 1.0595 - val_loss: 1.2333 - val_mae: 0.9327\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0154 - mae: 1.0483 - val_loss: 1.1594 - val_mae: 0.8805\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0103 - mae: 1.0451 - val_loss: 1.1858 - val_mae: 0.9024\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 2.0056 - mae: 1.0531 - val_loss: 1.1859 - val_mae: 0.9023\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 2.0091 - mae: 1.0418 - val_loss: 1.1630 - val_mae: 0.8854\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9960 - mae: 1.0443 - val_loss: 1.2006 - val_mae: 0.9143\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9963 - mae: 1.0539 - val_loss: 1.1859 - val_mae: 0.9044\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9934 - mae: 1.0492 - val_loss: 1.1911 - val_mae: 0.9082\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9885 - mae: 1.0408 - val_loss: 1.1672 - val_mae: 0.8903\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9901 - mae: 1.0389 - val_loss: 1.1896 - val_mae: 0.9076\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9833 - mae: 1.0485 - val_loss: 1.1899 - val_mae: 0.9086\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9818 - mae: 1.0446 - val_loss: 1.1872 - val_mae: 0.9060\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9803 - mae: 1.0506 - val_loss: 1.1734 - val_mae: 0.8962\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9759 - mae: 1.0367 - val_loss: 1.1675 - val_mae: 0.8911\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9762 - mae: 1.0319 - val_loss: 1.1648 - val_mae: 0.8881\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9762 - mae: 1.0371 - val_loss: 1.1983 - val_mae: 0.9138\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9795 - mae: 1.0417 - val_loss: 1.1713 - val_mae: 0.8931\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9679 - mae: 1.0524 - val_loss: 1.2146 - val_mae: 0.9251\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9644 - mae: 1.0449 - val_loss: 1.1652 - val_mae: 0.8901\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9719 - mae: 1.0273 - val_loss: 1.1602 - val_mae: 0.8856\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9574 - mae: 1.0334 - val_loss: 1.2258 - val_mae: 0.9336\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9624 - mae: 1.0575 - val_loss: 1.1953 - val_mae: 0.9122\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9520 - mae: 1.0378 - val_loss: 1.1762 - val_mae: 0.8978\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9674 - mae: 1.0236 - val_loss: 1.1479 - val_mae: 0.8728\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9513 - mae: 1.0402 - val_loss: 1.2269 - val_mae: 0.9339\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9467 - mae: 1.0424 - val_loss: 1.1678 - val_mae: 0.8917\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9457 - mae: 1.0277 - val_loss: 1.1711 - val_mae: 0.8939\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9400 - mae: 1.0358 - val_loss: 1.1875 - val_mae: 0.9053\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9390 - mae: 1.0268 - val_loss: 1.1622 - val_mae: 0.8861\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 1.9336 - mae: 1.0307 - val_loss: 1.1868 - val_mae: 0.9043\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9310 - mae: 1.0298 - val_loss: 1.1832 - val_mae: 0.9006\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 1.9300 - mae: 1.0312 - val_loss: 1.1832 - val_mae: 0.9007\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9289 - mae: 1.0395 - val_loss: 1.1910 - val_mae: 0.9082\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9221 - mae: 1.0328 - val_loss: 1.1714 - val_mae: 0.8917\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 1.9229 - mae: 1.0219 - val_loss: 1.1761 - val_mae: 0.8950\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.4071 - mae: 0.9435\n",
      "Mean Absolute Error on Test Data: 0.9435117244720459\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.06347483638440998\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 37.0583 - mae: 4.7146 - val_loss: 37.7932 - val_mae: 4.8118\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 28.9384 - mae: 3.8575 - val_loss: 27.5592 - val_mae: 3.8137\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.7401 - mae: 2.9360 - val_loss: 17.5316 - val_mae: 2.9120\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.9786 - mae: 2.7175 - val_loss: 14.7116 - val_mae: 2.8510\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.5964 - mae: 2.7992 - val_loss: 14.6011 - val_mae: 2.8327\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.3884 - mae: 2.7273 - val_loss: 14.5857 - val_mae: 2.8040\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2412 - mae: 2.6969 - val_loss: 14.5346 - val_mae: 2.7817\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 14.1746 - mae: 2.6703 - val_loss: 14.4902 - val_mae: 2.7689\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0517 - mae: 2.6855 - val_loss: 14.2266 - val_mae: 2.7844\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0435 - mae: 2.6938 - val_loss: 14.2069 - val_mae: 2.7740\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9722 - mae: 2.6735 - val_loss: 14.2144 - val_mae: 2.7665\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.9528 - mae: 2.6763 - val_loss: 14.1631 - val_mae: 2.7657\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.8928 - mae: 2.6703 - val_loss: 14.0910 - val_mae: 2.7707\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.8821 - mae: 2.6473 - val_loss: 14.2387 - val_mae: 2.7572\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8603 - mae: 2.6572 - val_loss: 13.9874 - val_mae: 2.7737\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8693 - mae: 2.6541 - val_loss: 14.1437 - val_mae: 2.7583\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8587 - mae: 2.6847 - val_loss: 13.9719 - val_mae: 2.7732\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7783 - mae: 2.6461 - val_loss: 14.1525 - val_mae: 2.7550\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.7993 - mae: 2.6264 - val_loss: 14.1061 - val_mae: 2.7569\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9255 - mae: 2.7084 - val_loss: 13.9348 - val_mae: 2.7701\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.8068 - mae: 2.6123 - val_loss: 14.2235 - val_mae: 2.7505\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7683 - mae: 2.6404 - val_loss: 13.9658 - val_mae: 2.7629\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.7681 - mae: 2.6835 - val_loss: 13.9635 - val_mae: 2.7634\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7421 - mae: 2.6207 - val_loss: 14.1138 - val_mae: 2.7510\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7671 - mae: 2.6595 - val_loss: 14.0101 - val_mae: 2.7580\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7324 - mae: 2.6427 - val_loss: 14.0359 - val_mae: 2.7554\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6832 - mae: 2.6413 - val_loss: 13.9497 - val_mae: 2.7607\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.7037 - mae: 2.6589 - val_loss: 13.9719 - val_mae: 2.7598\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.7015 - mae: 2.6290 - val_loss: 14.0374 - val_mae: 2.7535\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6762 - mae: 2.6456 - val_loss: 13.9226 - val_mae: 2.7611\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6883 - mae: 2.6440 - val_loss: 14.0128 - val_mae: 2.7541\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6761 - mae: 2.6429 - val_loss: 13.9951 - val_mae: 2.7552\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6668 - mae: 2.6608 - val_loss: 13.8583 - val_mae: 2.7712\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.6450 - mae: 2.6548 - val_loss: 13.9910 - val_mae: 2.7540\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6794 - mae: 2.6154 - val_loss: 14.0748 - val_mae: 2.7494\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6452 - mae: 2.6442 - val_loss: 13.8547 - val_mae: 2.7709\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6243 - mae: 2.6472 - val_loss: 14.0344 - val_mae: 2.7512\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6174 - mae: 2.6412 - val_loss: 13.9300 - val_mae: 2.7608\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6340 - mae: 2.6357 - val_loss: 14.1337 - val_mae: 2.7458\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6973 - mae: 2.6726 - val_loss: 14.0156 - val_mae: 2.7537\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6878 - mae: 2.6147 - val_loss: 14.2139 - val_mae: 2.7425\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.7464 - mae: 2.6870 - val_loss: 13.8638 - val_mae: 2.7743\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6474 - mae: 2.6137 - val_loss: 14.1116 - val_mae: 2.7460\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.5907 - mae: 2.6333 - val_loss: 13.9179 - val_mae: 2.7645\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.5795 - mae: 2.6519 - val_loss: 14.0246 - val_mae: 2.7521\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6720 - mae: 2.6716 - val_loss: 14.1471 - val_mae: 2.7462\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.6816 - mae: 2.6227 - val_loss: 13.9246 - val_mae: 2.7670\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.5698 - mae: 2.6534 - val_loss: 14.0411 - val_mae: 2.7527\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6123 - mae: 2.6130 - val_loss: 14.0060 - val_mae: 2.7547\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 13.5957 - mae: 2.6813 - val_loss: 13.8728 - val_mae: 2.7816\n",
      "11/11 [==============================] - 0s 906us/step - loss: 14.7181 - mae: 2.7270\n",
      "Mean Absolute Error on Test Data: 2.7269742488861084\n",
      "11/11 [==============================] - 0s 905us/step\n",
      "R-squared: 0.017618792511195913\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 35.4448 - mae: 4.0269 - val_loss: 41.5787 - val_mae: 3.7302\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 27.6500 - mae: 3.1726 - val_loss: 33.3485 - val_mae: 2.9426\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 21.2056 - mae: 2.6251 - val_loss: 28.6693 - val_mae: 2.8035\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.3835 - mae: 2.7538 - val_loss: 28.4582 - val_mae: 2.9119\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.3401 - mae: 2.7909 - val_loss: 28.4129 - val_mae: 2.8533\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.1855 - mae: 2.6872 - val_loss: 28.4799 - val_mae: 2.8133\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.1286 - mae: 2.6578 - val_loss: 28.4108 - val_mae: 2.8240\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.0185 - mae: 2.7000 - val_loss: 28.3396 - val_mae: 2.8619\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.1148 - mae: 2.7711 - val_loss: 28.3538 - val_mae: 2.8505\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.9795 - mae: 2.6314 - val_loss: 28.4984 - val_mae: 2.7907\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.9017 - mae: 2.6504 - val_loss: 28.2535 - val_mae: 2.8406\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.8896 - mae: 2.7149 - val_loss: 28.2569 - val_mae: 2.8295\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.8154 - mae: 2.6493 - val_loss: 28.2561 - val_mae: 2.8094\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.7591 - mae: 2.6611 - val_loss: 28.1264 - val_mae: 2.8285\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.7998 - mae: 2.6446 - val_loss: 28.1198 - val_mae: 2.8265\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.7166 - mae: 2.6948 - val_loss: 28.0763 - val_mae: 2.8269\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.7052 - mae: 2.6434 - val_loss: 28.0965 - val_mae: 2.8061\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.6379 - mae: 2.6505 - val_loss: 28.0087 - val_mae: 2.8121\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.6115 - mae: 2.6310 - val_loss: 27.9641 - val_mae: 2.7959\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.5769 - mae: 2.6374 - val_loss: 27.8983 - val_mae: 2.8071\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.5727 - mae: 2.6307 - val_loss: 27.8058 - val_mae: 2.8111\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.5166 - mae: 2.6434 - val_loss: 27.8252 - val_mae: 2.7987\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.4992 - mae: 2.6212 - val_loss: 27.7563 - val_mae: 2.7943\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.4933 - mae: 2.6518 - val_loss: 27.7160 - val_mae: 2.8057\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.5329 - mae: 2.6198 - val_loss: 27.7239 - val_mae: 2.7720\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.4446 - mae: 2.6620 - val_loss: 27.5349 - val_mae: 2.8222\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.3934 - mae: 2.6237 - val_loss: 27.5494 - val_mae: 2.7786\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 18.4036 - mae: 2.5777 - val_loss: 27.4628 - val_mae: 2.7799\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.4184 - mae: 2.6232 - val_loss: 27.4390 - val_mae: 2.7703\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.3905 - mae: 2.6525 - val_loss: 27.3752 - val_mae: 2.7842\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.3107 - mae: 2.6088 - val_loss: 27.3214 - val_mae: 2.7711\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.3386 - mae: 2.5715 - val_loss: 27.3165 - val_mae: 2.7577\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.4049 - mae: 2.6951 - val_loss: 27.2034 - val_mae: 2.8056\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.1016 - mae: 2.6021 - val_loss: 27.3278 - val_mae: 2.7453\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.2089 - mae: 2.5406 - val_loss: 27.2202 - val_mae: 2.7476\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.1175 - mae: 2.6104 - val_loss: 27.0294 - val_mae: 2.7766\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.1951 - mae: 2.6873 - val_loss: 27.0024 - val_mae: 2.7924\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.0061 - mae: 2.5777 - val_loss: 27.1396 - val_mae: 2.7318\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.0596 - mae: 2.5367 - val_loss: 26.9417 - val_mae: 2.7487\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.0572 - mae: 2.6253 - val_loss: 26.8608 - val_mae: 2.7616\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9864 - mae: 2.5883 - val_loss: 26.9062 - val_mae: 2.7394\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.9230 - mae: 2.5694 - val_loss: 26.7723 - val_mae: 2.7522\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.0599 - mae: 2.6160 - val_loss: 26.7923 - val_mae: 2.7383\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9174 - mae: 2.5899 - val_loss: 26.6583 - val_mae: 2.7561\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8558 - mae: 2.5431 - val_loss: 26.6501 - val_mae: 2.7365\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9361 - mae: 2.6057 - val_loss: 26.5286 - val_mae: 2.7461\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.7857 - mae: 2.5475 - val_loss: 26.5020 - val_mae: 2.7404\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.7742 - mae: 2.5681 - val_loss: 26.4000 - val_mae: 2.7428\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.8052 - mae: 2.5456 - val_loss: 26.3341 - val_mae: 2.7588\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.7383 - mae: 2.5626 - val_loss: 26.3038 - val_mae: 2.7509\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 13.8677 - mae: 2.6371\n",
      "Mean Absolute Error on Test Data: 2.6371402740478516\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03812812739358318\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 13ms/step - loss: 69.2720 - mae: 6.6647 - val_loss: 63.3451 - val_mae: 6.4431\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 60.1957 - mae: 5.9560 - val_loss: 52.0043 - val_mae: 5.5556\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 46.6237 - mae: 4.8374 - val_loss: 34.4486 - val_mae: 4.0468\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.2049 - mae: 3.5645 - val_loss: 21.5027 - val_mae: 3.2801\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.5314 - mae: 3.4249 - val_loss: 21.2366 - val_mae: 3.4492\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1191 - mae: 3.3327 - val_loss: 21.0335 - val_mae: 3.2795\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.8580 - mae: 3.2917 - val_loss: 20.7422 - val_mae: 3.3116\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.7512 - mae: 3.3066 - val_loss: 20.6098 - val_mae: 3.3018\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6693 - mae: 3.2885 - val_loss: 20.5135 - val_mae: 3.2752\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.7877 - mae: 3.3630 - val_loss: 20.4280 - val_mae: 3.3520\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5280 - mae: 3.3169 - val_loss: 20.3545 - val_mae: 3.2733\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5429 - mae: 3.2757 - val_loss: 20.3384 - val_mae: 3.2602\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4377 - mae: 3.2985 - val_loss: 20.2475 - val_mae: 3.3155\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4583 - mae: 3.3187 - val_loss: 20.2596 - val_mae: 3.2638\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3865 - mae: 3.2740 - val_loss: 20.2173 - val_mae: 3.2760\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4004 - mae: 3.3146 - val_loss: 20.1548 - val_mae: 3.2955\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4329 - mae: 3.2737 - val_loss: 20.1929 - val_mae: 3.2533\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2912 - mae: 3.3027 - val_loss: 20.0607 - val_mae: 3.3069\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.2345 - mae: 3.2984 - val_loss: 20.0440 - val_mae: 3.2652\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.2712 - mae: 3.2680 - val_loss: 20.0206 - val_mae: 3.2688\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2424 - mae: 3.3090 - val_loss: 19.9841 - val_mae: 3.2760\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2384 - mae: 3.2757 - val_loss: 20.0191 - val_mae: 3.2612\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2018 - mae: 3.3318 - val_loss: 19.9650 - val_mae: 3.3109\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3496 - mae: 3.2650 - val_loss: 20.0089 - val_mae: 3.2546\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.1107 - mae: 3.2956 - val_loss: 19.9278 - val_mae: 3.3189\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1587 - mae: 3.3369 - val_loss: 19.9096 - val_mae: 3.2706\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.0952 - mae: 3.2931 - val_loss: 19.9083 - val_mae: 3.2679\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.1002 - mae: 3.2658 - val_loss: 19.9440 - val_mae: 3.2528\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.0601 - mae: 3.2858 - val_loss: 19.8404 - val_mae: 3.2960\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1931 - mae: 3.2773 - val_loss: 19.8506 - val_mae: 3.2794\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.0823 - mae: 3.3485 - val_loss: 19.8006 - val_mae: 3.3257\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1914 - mae: 3.3139 - val_loss: 19.7655 - val_mae: 3.2784\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9735 - mae: 3.2850 - val_loss: 19.8303 - val_mae: 3.2557\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9539 - mae: 3.2661 - val_loss: 19.8242 - val_mae: 3.2740\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9161 - mae: 3.2837 - val_loss: 19.7777 - val_mae: 3.2822\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8652 - mae: 3.2741 - val_loss: 19.8610 - val_mae: 3.2533\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9322 - mae: 3.2545 - val_loss: 19.8518 - val_mae: 3.2506\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.9229 - mae: 3.2871 - val_loss: 19.7710 - val_mae: 3.2841\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.8163 - mae: 3.2726 - val_loss: 19.7786 - val_mae: 3.2561\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8348 - mae: 3.2633 - val_loss: 19.7209 - val_mae: 3.2795\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9562 - mae: 3.2596 - val_loss: 19.7550 - val_mae: 3.2604\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.7427 - mae: 3.2681 - val_loss: 19.7132 - val_mae: 3.2975\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.7454 - mae: 3.2958 - val_loss: 19.6920 - val_mae: 3.2703\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.7270 - mae: 3.2836 - val_loss: 19.6469 - val_mae: 3.2822\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8544 - mae: 3.2417 - val_loss: 19.7802 - val_mae: 3.2423\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.6831 - mae: 3.2876 - val_loss: 19.5786 - val_mae: 3.2857\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.6410 - mae: 3.2920 - val_loss: 19.6225 - val_mae: 3.2431\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5646 - mae: 3.2701 - val_loss: 19.5350 - val_mae: 3.2745\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5155 - mae: 3.2498 - val_loss: 19.6256 - val_mae: 3.2352\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5025 - mae: 3.2437 - val_loss: 19.4798 - val_mae: 3.2672\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 22.9910 - mae: 3.2825\n",
      "Mean Absolute Error on Test Data: 3.2824761867523193\n",
      "11/11 [==============================] - 0s 854us/step\n",
      "R-squared: 0.11885625887038209\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "11/11 [==============================] - 0s 18ms/step - loss: 7.5085 - mae: 1.8090 - val_loss: 3.2053 - val_mae: 1.2363\n",
      "Epoch 2/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 5.6718 - mae: 1.3610 - val_loss: 2.0746 - val_mae: 0.9808\n",
      "Epoch 3/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.3903 - mae: 1.2537 - val_loss: 1.9856 - val_mae: 1.1430\n",
      "Epoch 4/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2778 - mae: 1.3738 - val_loss: 2.1806 - val_mae: 1.2657\n",
      "Epoch 5/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.3004 - mae: 1.4142 - val_loss: 2.1201 - val_mae: 1.2344\n",
      "Epoch 6/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.2695 - mae: 1.3524 - val_loss: 1.9490 - val_mae: 1.1251\n",
      "Epoch 7/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2313 - mae: 1.3067 - val_loss: 1.9676 - val_mae: 1.1453\n",
      "Epoch 8/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2110 - mae: 1.3282 - val_loss: 2.0015 - val_mae: 1.1728\n",
      "Epoch 9/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2064 - mae: 1.3316 - val_loss: 1.9843 - val_mae: 1.1632\n",
      "Epoch 10/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1945 - mae: 1.3283 - val_loss: 1.9609 - val_mae: 1.1504\n",
      "Epoch 11/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1898 - mae: 1.3218 - val_loss: 1.9691 - val_mae: 1.1596\n",
      "Epoch 12/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2029 - mae: 1.3424 - val_loss: 1.9745 - val_mae: 1.1654\n",
      "Epoch 13/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.1581 - mae: 1.3145 - val_loss: 1.9124 - val_mae: 1.1216\n",
      "Epoch 14/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.2007 - mae: 1.2832 - val_loss: 1.8806 - val_mae: 1.0905\n",
      "Epoch 15/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1713 - mae: 1.3077 - val_loss: 1.9721 - val_mae: 1.1662\n",
      "Epoch 16/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.1392 - mae: 1.3346 - val_loss: 1.9946 - val_mae: 1.1830\n",
      "Epoch 17/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1329 - mae: 1.3337 - val_loss: 1.9499 - val_mae: 1.1559\n",
      "Epoch 18/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.1264 - mae: 1.3336 - val_loss: 1.9621 - val_mae: 1.1651\n",
      "Epoch 19/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1346 - mae: 1.3111 - val_loss: 1.9030 - val_mae: 1.1235\n",
      "Epoch 20/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1113 - mae: 1.3080 - val_loss: 1.9397 - val_mae: 1.1512\n",
      "Epoch 21/50\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 4.1163 - mae: 1.3310 - val_loss: 1.9687 - val_mae: 1.1707\n",
      "Epoch 22/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0983 - mae: 1.3276 - val_loss: 1.9418 - val_mae: 1.1535\n",
      "Epoch 23/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0954 - mae: 1.3232 - val_loss: 1.9578 - val_mae: 1.1629\n",
      "Epoch 24/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1139 - mae: 1.2993 - val_loss: 1.8898 - val_mae: 1.1147\n",
      "Epoch 25/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0775 - mae: 1.3178 - val_loss: 1.9815 - val_mae: 1.1777\n",
      "Epoch 26/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0761 - mae: 1.3293 - val_loss: 1.9375 - val_mae: 1.1497\n",
      "Epoch 27/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0899 - mae: 1.3000 - val_loss: 1.9069 - val_mae: 1.1303\n",
      "Epoch 28/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0690 - mae: 1.3095 - val_loss: 1.9152 - val_mae: 1.1367\n",
      "Epoch 29/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0674 - mae: 1.3181 - val_loss: 1.9408 - val_mae: 1.1537\n",
      "Epoch 30/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0538 - mae: 1.3138 - val_loss: 1.9281 - val_mae: 1.1457\n",
      "Epoch 31/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0543 - mae: 1.3106 - val_loss: 1.9317 - val_mae: 1.1490\n",
      "Epoch 32/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0768 - mae: 1.2954 - val_loss: 1.9211 - val_mae: 1.1414\n",
      "Epoch 33/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0360 - mae: 1.3309 - val_loss: 2.0291 - val_mae: 1.2075\n",
      "Epoch 34/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0468 - mae: 1.3188 - val_loss: 1.9170 - val_mae: 1.1388\n",
      "Epoch 35/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0801 - mae: 1.2795 - val_loss: 1.8875 - val_mae: 1.1173\n",
      "Epoch 36/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0164 - mae: 1.3354 - val_loss: 2.1427 - val_mae: 1.2638\n",
      "Epoch 37/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.1020 - mae: 1.4012 - val_loss: 2.0093 - val_mae: 1.1922\n",
      "Epoch 38/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.1567 - mae: 1.2808 - val_loss: 1.8635 - val_mae: 1.0755\n",
      "Epoch 39/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0247 - mae: 1.3169 - val_loss: 2.0956 - val_mae: 1.2393\n",
      "Epoch 40/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0630 - mae: 1.3818 - val_loss: 2.0640 - val_mae: 1.2218\n",
      "Epoch 41/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0108 - mae: 1.3251 - val_loss: 1.9118 - val_mae: 1.1259\n",
      "Epoch 42/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0325 - mae: 1.3203 - val_loss: 1.9997 - val_mae: 1.1858\n",
      "Epoch 43/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0288 - mae: 1.2981 - val_loss: 1.9031 - val_mae: 1.1224\n",
      "Epoch 44/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0157 - mae: 1.2878 - val_loss: 1.9530 - val_mae: 1.1575\n",
      "Epoch 45/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 4.0027 - mae: 1.3202 - val_loss: 1.9873 - val_mae: 1.1794\n",
      "Epoch 46/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.9956 - mae: 1.3057 - val_loss: 1.9190 - val_mae: 1.1346\n",
      "Epoch 47/50\n",
      "11/11 [==============================] - 0s 3ms/step - loss: 4.0041 - mae: 1.2859 - val_loss: 1.9333 - val_mae: 1.1451\n",
      "Epoch 48/50\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 3.9999 - mae: 1.3045 - val_loss: 1.9772 - val_mae: 1.1728\n",
      "Epoch 49/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 3.9961 - mae: 1.3204 - val_loss: 2.0149 - val_mae: 1.1956\n",
      "Epoch 50/50\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 4.0145 - mae: 1.3336 - val_loss: 1.9814 - val_mae: 1.1752\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 5.9308 - mae: 1.4821\n",
      "Mean Absolute Error on Test Data: 1.4821407794952393\n",
      "7/7 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.13689159344605628\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 127.2050 - mae: 9.1146 - val_loss: 106.6068 - val_mae: 8.3160\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 105.8915 - mae: 7.8922 - val_loss: 82.9496 - val_mae: 6.8195\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 78.7219 - mae: 6.1505 - val_loss: 55.2160 - val_mae: 4.9571\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 52.6547 - mae: 4.5803 - val_loss: 38.2625 - val_mae: 4.1499\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.7760 - mae: 4.4210 - val_loss: 37.2305 - val_mae: 4.3982\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.5195 - mae: 4.5632 - val_loss: 36.9694 - val_mae: 4.3482\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.2564 - mae: 4.4644 - val_loss: 36.7918 - val_mae: 4.2851\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.3093 - mae: 4.4232 - val_loss: 36.7173 - val_mae: 4.2728\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.1285 - mae: 4.4728 - val_loss: 36.6479 - val_mae: 4.2910\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.9321 - mae: 4.4108 - val_loss: 36.5799 - val_mae: 4.2462\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.7665 - mae: 4.4375 - val_loss: 36.5524 - val_mae: 4.3102\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.7554 - mae: 4.4611 - val_loss: 36.4149 - val_mae: 4.2877\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.6626 - mae: 4.3903 - val_loss: 36.3878 - val_mae: 4.2246\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.5850 - mae: 4.4036 - val_loss: 36.2762 - val_mae: 4.2534\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5173 - mae: 4.4375 - val_loss: 36.2406 - val_mae: 4.2716\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4064 - mae: 4.4264 - val_loss: 36.2009 - val_mae: 4.2684\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5588 - mae: 4.3832 - val_loss: 36.1875 - val_mae: 4.2277\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6553 - mae: 4.4776 - val_loss: 36.1699 - val_mae: 4.2952\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1443 - mae: 4.4133 - val_loss: 36.0984 - val_mae: 4.2299\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1787 - mae: 4.3940 - val_loss: 36.0280 - val_mae: 4.2469\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.2657 - mae: 4.4255 - val_loss: 36.0291 - val_mae: 4.2752\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0842 - mae: 4.4034 - val_loss: 35.9655 - val_mae: 4.2597\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0858 - mae: 4.4294 - val_loss: 35.9954 - val_mae: 4.2854\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0165 - mae: 4.4072 - val_loss: 35.9245 - val_mae: 4.2458\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.0080 - mae: 4.4193 - val_loss: 35.9310 - val_mae: 4.2753\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9549 - mae: 4.4411 - val_loss: 35.8599 - val_mae: 4.2471\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.8384 - mae: 4.3707 - val_loss: 35.8754 - val_mae: 4.2015\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.7301 - mae: 4.3765 - val_loss: 35.8057 - val_mae: 4.2554\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.7435 - mae: 4.4084 - val_loss: 35.7730 - val_mae: 4.2382\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6422 - mae: 4.3806 - val_loss: 35.7569 - val_mae: 4.2404\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6198 - mae: 4.3784 - val_loss: 35.7467 - val_mae: 4.2248\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6574 - mae: 4.3404 - val_loss: 35.7294 - val_mae: 4.2111\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.4819 - mae: 4.3710 - val_loss: 35.7324 - val_mae: 4.2421\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.5026 - mae: 4.3768 - val_loss: 35.6639 - val_mae: 4.2283\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6575 - mae: 4.4611 - val_loss: 35.8712 - val_mae: 4.2947\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.4621 - mae: 4.4122 - val_loss: 35.6975 - val_mae: 4.2356\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.3557 - mae: 4.3770 - val_loss: 35.6454 - val_mae: 4.2152\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2367 - mae: 4.3431 - val_loss: 35.6530 - val_mae: 4.1976\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.5040 - mae: 4.3203 - val_loss: 35.5795 - val_mae: 4.2163\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.2151 - mae: 4.3715 - val_loss: 35.5725 - val_mae: 4.2335\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.1804 - mae: 4.3684 - val_loss: 35.5614 - val_mae: 4.2364\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.1029 - mae: 4.3556 - val_loss: 35.5397 - val_mae: 4.2003\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2487 - mae: 4.3931 - val_loss: 35.4575 - val_mae: 4.2116\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.1801 - mae: 4.3134 - val_loss: 35.4624 - val_mae: 4.1858\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.1877 - mae: 4.3788 - val_loss: 35.4209 - val_mae: 4.2130\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.9655 - mae: 4.3577 - val_loss: 35.4112 - val_mae: 4.2144\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8522 - mae: 4.3548 - val_loss: 35.3659 - val_mae: 4.1802\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8896 - mae: 4.2968 - val_loss: 35.3669 - val_mae: 4.1584\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8430 - mae: 4.3455 - val_loss: 35.3476 - val_mae: 4.2279\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8094 - mae: 4.3630 - val_loss: 35.3059 - val_mae: 4.1845\n",
      "11/11 [==============================] - 0s 849us/step - loss: 40.5972 - mae: 4.2818\n",
      "Mean Absolute Error on Test Data: 4.2817702293396\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.04598667971971404\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 76.6956 - mae: 7.0895 - val_loss: 64.6967 - val_mae: 6.5734\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.5728 - mae: 6.1676 - val_loss: 51.8589 - val_mae: 5.5629\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 50.3932 - mae: 5.0096 - val_loss: 36.2695 - val_mae: 4.1833\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 34.4579 - mae: 3.6438 - val_loss: 23.4190 - val_mae: 3.2439\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.6515 - mae: 3.2691 - val_loss: 21.7567 - val_mae: 3.4371\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.5909 - mae: 3.4084 - val_loss: 21.4761 - val_mae: 3.3520\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.4307 - mae: 3.3541 - val_loss: 21.3772 - val_mae: 3.3293\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.4218 - mae: 3.2670 - val_loss: 21.4087 - val_mae: 3.2926\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.3273 - mae: 3.2919 - val_loss: 21.3471 - val_mae: 3.3323\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.3259 - mae: 3.2930 - val_loss: 21.3099 - val_mae: 3.3118\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.2277 - mae: 3.3051 - val_loss: 21.2754 - val_mae: 3.3239\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.3361 - mae: 3.2630 - val_loss: 21.2585 - val_mae: 3.2921\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.2081 - mae: 3.2817 - val_loss: 21.1963 - val_mae: 3.3165\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.1601 - mae: 3.3432 - val_loss: 21.1463 - val_mae: 3.3450\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.1668 - mae: 3.3267 - val_loss: 21.1020 - val_mae: 3.3341\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.1555 - mae: 3.4009 - val_loss: 21.1498 - val_mae: 3.3718\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 26.1656 - mae: 3.2711 - val_loss: 21.1127 - val_mae: 3.2736\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9681 - mae: 3.2831 - val_loss: 21.0487 - val_mae: 3.3279\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9978 - mae: 3.3035 - val_loss: 21.0405 - val_mae: 3.2958\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9517 - mae: 3.2681 - val_loss: 21.0268 - val_mae: 3.3126\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9450 - mae: 3.3207 - val_loss: 21.0175 - val_mae: 3.3243\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9220 - mae: 3.2501 - val_loss: 21.0258 - val_mae: 3.2725\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.8802 - mae: 3.2744 - val_loss: 20.9665 - val_mae: 3.3232\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.8740 - mae: 3.3488 - val_loss: 21.0010 - val_mae: 3.3472\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.8819 - mae: 3.2828 - val_loss: 20.9768 - val_mae: 3.2914\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.8095 - mae: 3.2852 - val_loss: 20.9889 - val_mae: 3.3473\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.5597 - mae: 3.5024 - val_loss: 21.0371 - val_mae: 3.3702\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.9325 - mae: 3.2568 - val_loss: 20.9997 - val_mae: 3.2723\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.7801 - mae: 3.2635 - val_loss: 20.9448 - val_mae: 3.2977\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.9594 - mae: 3.2418 - val_loss: 20.9592 - val_mae: 3.2869\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.7114 - mae: 3.3027 - val_loss: 20.9567 - val_mae: 3.3524\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.7504 - mae: 3.3371 - val_loss: 20.9327 - val_mae: 3.3405\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.7478 - mae: 3.3294 - val_loss: 20.9180 - val_mae: 3.2937\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6806 - mae: 3.2237 - val_loss: 21.0302 - val_mae: 3.2627\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 25.6779 - mae: 3.2532 - val_loss: 20.9422 - val_mae: 3.3150\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6965 - mae: 3.2797 - val_loss: 20.9306 - val_mae: 3.2834\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6291 - mae: 3.2698 - val_loss: 20.9030 - val_mae: 3.3134\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6034 - mae: 3.3118 - val_loss: 20.9330 - val_mae: 3.3343\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.5789 - mae: 3.2792 - val_loss: 20.9050 - val_mae: 3.3103\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.5982 - mae: 3.3153 - val_loss: 20.9092 - val_mae: 3.3128\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6132 - mae: 3.2261 - val_loss: 20.9298 - val_mae: 3.2751\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.5424 - mae: 3.2685 - val_loss: 20.9237 - val_mae: 3.3476\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6918 - mae: 3.3551 - val_loss: 20.8798 - val_mae: 3.3000\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.5040 - mae: 3.2625 - val_loss: 20.8960 - val_mae: 3.2921\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.6098 - mae: 3.2221 - val_loss: 20.8976 - val_mae: 3.2910\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.4185 - mae: 3.2748 - val_loss: 20.8889 - val_mae: 3.3309\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.4728 - mae: 3.2598 - val_loss: 20.8671 - val_mae: 3.3017\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.4267 - mae: 3.2300 - val_loss: 20.8520 - val_mae: 3.2888\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.3977 - mae: 3.3019 - val_loss: 20.8710 - val_mae: 3.3602\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 25.4545 - mae: 3.3284 - val_loss: 20.8534 - val_mae: 3.3238\n",
      "11/11 [==============================] - 0s 851us/step - loss: 21.0613 - mae: 3.4353\n",
      "Mean Absolute Error on Test Data: 3.4352598190307617\n",
      "11/11 [==============================] - 0s 915us/step\n",
      "R-squared: 0.026471215641352797\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 196.1249 - mae: 12.2536 - val_loss: 167.7319 - val_mae: 11.0309\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 160.4592 - mae: 10.7272 - val_loss: 125.7633 - val_mae: 9.0668\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 109.5444 - mae: 8.2466 - val_loss: 74.9189 - val_mae: 6.5299\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 60.3767 - mae: 5.7616 - val_loss: 48.6012 - val_mae: 5.2867\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 48.0757 - mae: 5.3939 - val_loss: 50.5477 - val_mae: 5.5096\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 47.5426 - mae: 5.4226 - val_loss: 48.3547 - val_mae: 5.3142\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 46.6793 - mae: 5.2891 - val_loss: 48.0443 - val_mae: 5.2757\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 46.6270 - mae: 5.2494 - val_loss: 48.0058 - val_mae: 5.2782\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 46.2408 - mae: 5.2718 - val_loss: 47.9563 - val_mae: 5.2780\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 46.0236 - mae: 5.2310 - val_loss: 47.6591 - val_mae: 5.2423\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.7931 - mae: 5.2207 - val_loss: 47.7368 - val_mae: 5.2575\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.6036 - mae: 5.2237 - val_loss: 47.6386 - val_mae: 5.2475\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.3210 - mae: 5.1904 - val_loss: 47.3221 - val_mae: 5.2050\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.2467 - mae: 5.1694 - val_loss: 47.4572 - val_mae: 5.2296\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.9907 - mae: 5.1873 - val_loss: 47.3825 - val_mae: 5.2256\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.9596 - mae: 5.1414 - val_loss: 47.0627 - val_mae: 5.1748\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.7974 - mae: 5.1798 - val_loss: 47.4731 - val_mae: 5.2464\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.5355 - mae: 5.1617 - val_loss: 47.1517 - val_mae: 5.2059\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.4191 - mae: 5.1335 - val_loss: 46.9920 - val_mae: 5.1886\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3923 - mae: 5.0936 - val_loss: 46.9370 - val_mae: 5.1838\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.1565 - mae: 5.1118 - val_loss: 47.2849 - val_mae: 5.2468\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.1442 - mae: 5.1486 - val_loss: 47.0676 - val_mae: 5.2197\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.9450 - mae: 5.1106 - val_loss: 47.0052 - val_mae: 5.2110\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.9512 - mae: 5.0640 - val_loss: 46.8596 - val_mae: 5.1882\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.7224 - mae: 5.1236 - val_loss: 47.2996 - val_mae: 5.2607\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.7412 - mae: 5.1120 - val_loss: 46.9826 - val_mae: 5.2167\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.6414 - mae: 5.0968 - val_loss: 47.0104 - val_mae: 5.2154\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.5074 - mae: 5.0570 - val_loss: 46.8315 - val_mae: 5.1870\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.5102 - mae: 5.0764 - val_loss: 47.2111 - val_mae: 5.2508\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.5254 - mae: 5.0627 - val_loss: 46.9779 - val_mae: 5.2188\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.3778 - mae: 5.0770 - val_loss: 46.9416 - val_mae: 5.2105\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.4586 - mae: 5.0773 - val_loss: 46.8083 - val_mae: 5.1857\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.2600 - mae: 5.0474 - val_loss: 47.1109 - val_mae: 5.2401\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.2758 - mae: 5.0781 - val_loss: 47.0326 - val_mae: 5.2317\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.3000 - mae: 5.0718 - val_loss: 46.8584 - val_mae: 5.2117\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.1611 - mae: 5.0575 - val_loss: 46.8119 - val_mae: 5.2090\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.0933 - mae: 5.0382 - val_loss: 46.8565 - val_mae: 5.2270\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 43.1294 - mae: 5.0469 - val_loss: 46.8775 - val_mae: 5.2401\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 43.2981 - mae: 5.1334 - val_loss: 46.9682 - val_mae: 5.2505\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.1056 - mae: 5.0199 - val_loss: 46.5375 - val_mae: 5.1788\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.1670 - mae: 5.0867 - val_loss: 46.7913 - val_mae: 5.2376\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.9148 - mae: 5.0365 - val_loss: 46.7059 - val_mae: 5.2217\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.0249 - mae: 5.0200 - val_loss: 46.4204 - val_mae: 5.1818\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 43.2089 - mae: 5.0909 - val_loss: 46.4740 - val_mae: 5.1992\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.8234 - mae: 5.0071 - val_loss: 46.4051 - val_mae: 5.1926\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.8262 - mae: 5.0472 - val_loss: 46.5496 - val_mae: 5.2298\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.8289 - mae: 5.0614 - val_loss: 46.2991 - val_mae: 5.2002\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.7172 - mae: 5.0076 - val_loss: 46.2224 - val_mae: 5.1890\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6876 - mae: 5.0001 - val_loss: 46.2869 - val_mae: 5.2141\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6750 - mae: 5.0614 - val_loss: 46.1752 - val_mae: 5.1955\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 51.2473 - mae: 5.2454\n",
      "Mean Absolute Error on Test Data: 5.24537992477417\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0441662228902564\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 13ms/step - loss: 25.6715 - mae: 3.9058 - val_loss: 17.2558 - val_mae: 3.0657\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8714 - mae: 2.9826 - val_loss: 10.9010 - val_mae: 2.3672\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 11.8780 - mae: 2.3889 - val_loss: 8.9046 - val_mae: 2.3855\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4227 - mae: 2.4400 - val_loss: 9.4147 - val_mae: 2.5325\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5020 - mae: 2.4493 - val_loss: 9.0400 - val_mae: 2.4387\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.4235 - mae: 2.4111 - val_loss: 8.9572 - val_mae: 2.4129\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4210 - mae: 2.4020 - val_loss: 8.9532 - val_mae: 2.4092\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4149 - mae: 2.3761 - val_loss: 8.9227 - val_mae: 2.4023\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4883 - mae: 2.4404 - val_loss: 9.1394 - val_mae: 2.4620\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.4156 - mae: 2.3817 - val_loss: 8.9057 - val_mae: 2.3924\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3760 - mae: 2.3778 - val_loss: 8.9491 - val_mae: 2.4106\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3484 - mae: 2.4003 - val_loss: 9.0251 - val_mae: 2.4321\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 10.4017 - mae: 2.3794 - val_loss: 8.9018 - val_mae: 2.3956\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.3271 - mae: 2.3966 - val_loss: 9.0181 - val_mae: 2.4336\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3322 - mae: 2.4021 - val_loss: 8.9591 - val_mae: 2.4186\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3467 - mae: 2.3752 - val_loss: 8.9047 - val_mae: 2.3997\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3505 - mae: 2.3918 - val_loss: 8.9339 - val_mae: 2.4112\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3286 - mae: 2.4070 - val_loss: 8.9861 - val_mae: 2.4303\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3224 - mae: 2.4126 - val_loss: 8.9633 - val_mae: 2.4238\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.3060 - mae: 2.3921 - val_loss: 8.8963 - val_mae: 2.3969\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.2894 - mae: 2.3822 - val_loss: 8.8786 - val_mae: 2.3970\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2781 - mae: 2.3742 - val_loss: 8.8615 - val_mae: 2.3919\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2773 - mae: 2.3656 - val_loss: 8.8665 - val_mae: 2.3974\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2813 - mae: 2.4028 - val_loss: 8.9682 - val_mae: 2.4305\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2650 - mae: 2.3974 - val_loss: 8.8285 - val_mae: 2.3844\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.2643 - mae: 2.3537 - val_loss: 8.8381 - val_mae: 2.3922\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.3380 - mae: 2.4266 - val_loss: 9.0372 - val_mae: 2.4527\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.3467 - mae: 2.3579 - val_loss: 8.7544 - val_mae: 2.3532\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 10.3115 - mae: 2.4091 - val_loss: 9.0002 - val_mae: 2.4423\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4114 - mae: 2.3592 - val_loss: 8.7284 - val_mae: 2.3499\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2320 - mae: 2.4008 - val_loss: 8.9993 - val_mae: 2.4508\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.2390 - mae: 2.4070 - val_loss: 8.8173 - val_mae: 2.3847\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.2185 - mae: 2.3476 - val_loss: 8.7378 - val_mae: 2.3642\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.2228 - mae: 2.3669 - val_loss: 8.7873 - val_mae: 2.3921\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1828 - mae: 2.3972 - val_loss: 9.0132 - val_mae: 2.4551\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.2097 - mae: 2.3895 - val_loss: 8.7446 - val_mae: 2.3678\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1812 - mae: 2.3535 - val_loss: 8.7646 - val_mae: 2.3736\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1647 - mae: 2.3728 - val_loss: 8.8213 - val_mae: 2.4041\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1878 - mae: 2.3598 - val_loss: 8.7747 - val_mae: 2.3889\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1764 - mae: 2.3797 - val_loss: 8.8945 - val_mae: 2.4271\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1658 - mae: 2.3682 - val_loss: 8.7343 - val_mae: 2.3717\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.1522 - mae: 2.3705 - val_loss: 8.7647 - val_mae: 2.3864\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.1223 - mae: 2.3686 - val_loss: 8.7878 - val_mae: 2.3943\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.1157 - mae: 2.3478 - val_loss: 8.7070 - val_mae: 2.3674\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.1064 - mae: 2.3614 - val_loss: 8.7707 - val_mae: 2.3968\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1124 - mae: 2.3505 - val_loss: 8.7306 - val_mae: 2.3850\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1225 - mae: 2.3510 - val_loss: 8.7915 - val_mae: 2.4080\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.1390 - mae: 2.3876 - val_loss: 8.6560 - val_mae: 2.3510\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 10.1456 - mae: 2.3196 - val_loss: 8.6452 - val_mae: 2.3587\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.1800 - mae: 2.4232 - val_loss: 8.8019 - val_mae: 2.4107\n",
      "11/11 [==============================] - 0s 969us/step - loss: 8.9860 - mae: 2.2630\n",
      "Mean Absolute Error on Test Data: 2.2629506587982178\n",
      "11/11 [==============================] - 0s 836us/step\n",
      "R-squared: 0.010107757833114461\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 43.9393 - mae: 5.6645 - val_loss: 40.8712 - val_mae: 5.0807\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 32.4372 - mae: 4.5666 - val_loss: 29.1118 - val_mae: 3.9133\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5342 - mae: 3.3433 - val_loss: 18.0571 - val_mae: 2.8034\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 12.5598 - mae: 2.5576 - val_loss: 14.8348 - val_mae: 2.7709\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.7058 - mae: 2.6209 - val_loss: 14.8458 - val_mae: 2.7784\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.6335 - mae: 2.6194 - val_loss: 14.7958 - val_mae: 2.7725\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.5468 - mae: 2.5787 - val_loss: 14.7283 - val_mae: 2.7307\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.5562 - mae: 2.5425 - val_loss: 14.7086 - val_mae: 2.7180\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.5413 - mae: 2.5739 - val_loss: 14.6629 - val_mae: 2.7444\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.4804 - mae: 2.5584 - val_loss: 14.6394 - val_mae: 2.7294\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.4853 - mae: 2.5519 - val_loss: 14.6187 - val_mae: 2.7390\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.4920 - mae: 2.5781 - val_loss: 14.6093 - val_mae: 2.7402\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.4937 - mae: 2.5942 - val_loss: 14.5732 - val_mae: 2.7489\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.4024 - mae: 2.5509 - val_loss: 14.5399 - val_mae: 2.7171\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.4301 - mae: 2.5623 - val_loss: 14.5203 - val_mae: 2.7287\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3898 - mae: 2.5437 - val_loss: 14.5117 - val_mae: 2.7203\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.4108 - mae: 2.5358 - val_loss: 14.5050 - val_mae: 2.7208\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3637 - mae: 2.5624 - val_loss: 14.5043 - val_mae: 2.7451\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3789 - mae: 2.5580 - val_loss: 14.4858 - val_mae: 2.7178\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3366 - mae: 2.5471 - val_loss: 14.4758 - val_mae: 2.7211\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3364 - mae: 2.5419 - val_loss: 14.4652 - val_mae: 2.7219\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3318 - mae: 2.5333 - val_loss: 14.4435 - val_mae: 2.7156\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3358 - mae: 2.5602 - val_loss: 14.4316 - val_mae: 2.7251\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3228 - mae: 2.5389 - val_loss: 14.4436 - val_mae: 2.7072\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3185 - mae: 2.5248 - val_loss: 14.4239 - val_mae: 2.7202\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3469 - mae: 2.5863 - val_loss: 14.4383 - val_mae: 2.7452\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3362 - mae: 2.5423 - val_loss: 14.3981 - val_mae: 2.7122\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.3302 - mae: 2.5662 - val_loss: 14.3715 - val_mae: 2.7153\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2972 - mae: 2.5362 - val_loss: 14.3660 - val_mae: 2.7178\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2825 - mae: 2.5746 - val_loss: 14.4179 - val_mae: 2.7618\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3452 - mae: 2.5783 - val_loss: 14.3522 - val_mae: 2.7010\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2907 - mae: 2.5213 - val_loss: 14.3762 - val_mae: 2.7040\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2922 - mae: 2.5718 - val_loss: 14.3666 - val_mae: 2.7367\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2963 - mae: 2.5327 - val_loss: 14.3476 - val_mae: 2.7040\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2598 - mae: 2.5338 - val_loss: 14.3336 - val_mae: 2.7301\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.3033 - mae: 2.5979 - val_loss: 14.3707 - val_mae: 2.7493\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2405 - mae: 2.5354 - val_loss: 14.3834 - val_mae: 2.6963\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2384 - mae: 2.5294 - val_loss: 14.3682 - val_mae: 2.7297\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2188 - mae: 2.5563 - val_loss: 14.3604 - val_mae: 2.7159\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2130 - mae: 2.5572 - val_loss: 14.3558 - val_mae: 2.7194\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2925 - mae: 2.5222 - val_loss: 14.3775 - val_mae: 2.6909\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2075 - mae: 2.5385 - val_loss: 14.3269 - val_mae: 2.7193\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2227 - mae: 2.5390 - val_loss: 14.3112 - val_mae: 2.7178\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2014 - mae: 2.5494 - val_loss: 14.2867 - val_mae: 2.7184\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 11.1979 - mae: 2.5368 - val_loss: 14.2901 - val_mae: 2.7039\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.1998 - mae: 2.5228 - val_loss: 14.2818 - val_mae: 2.6924\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.1706 - mae: 2.5295 - val_loss: 14.2772 - val_mae: 2.7413\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.2819 - mae: 2.6008 - val_loss: 14.2480 - val_mae: 2.7235\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.1827 - mae: 2.5616 - val_loss: 14.2253 - val_mae: 2.6964\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 11.1810 - mae: 2.5290 - val_loss: 14.2152 - val_mae: 2.6994\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 16.9831 - mae: 2.9572\n",
      "Mean Absolute Error on Test Data: 2.9571642875671387\n",
      "11/11 [==============================] - 0s 929us/step\n",
      "R-squared: 0.06343728849598806\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 25.0267 - mae: 4.1021 - val_loss: 23.9227 - val_mae: 3.8380\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.1783 - mae: 3.3787 - val_loss: 17.3103 - val_mae: 3.0625\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 12.6004 - mae: 2.5537 - val_loss: 10.8940 - val_mae: 2.4047\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7312 - mae: 2.1859 - val_loss: 9.2803 - val_mae: 2.3898\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.3373 - mae: 2.2245 - val_loss: 9.2456 - val_mae: 2.3640\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.2485 - mae: 2.1658 - val_loss: 9.2887 - val_mae: 2.3405\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.2258 - mae: 2.1601 - val_loss: 9.2536 - val_mae: 2.3470\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.2151 - mae: 2.1617 - val_loss: 9.2489 - val_mae: 2.3463\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.1938 - mae: 2.1691 - val_loss: 9.2480 - val_mae: 2.3441\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.1950 - mae: 2.1525 - val_loss: 9.2689 - val_mae: 2.3366\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.2183 - mae: 2.1758 - val_loss: 9.2352 - val_mae: 2.3486\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1706 - mae: 2.1495 - val_loss: 9.2931 - val_mae: 2.3307\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.1639 - mae: 2.1523 - val_loss: 9.2528 - val_mae: 2.3364\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1552 - mae: 2.1627 - val_loss: 9.2450 - val_mae: 2.3400\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.1570 - mae: 2.1539 - val_loss: 9.2567 - val_mae: 2.3345\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1362 - mae: 2.1572 - val_loss: 9.2456 - val_mae: 2.3376\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1356 - mae: 2.1550 - val_loss: 9.2692 - val_mae: 2.3311\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1290 - mae: 2.1516 - val_loss: 9.2607 - val_mae: 2.3329\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1340 - mae: 2.1579 - val_loss: 9.2815 - val_mae: 2.3276\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1295 - mae: 2.1380 - val_loss: 9.2684 - val_mae: 2.3319\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1215 - mae: 2.1550 - val_loss: 9.2681 - val_mae: 2.3331\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1112 - mae: 2.1570 - val_loss: 9.2660 - val_mae: 2.3338\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1101 - mae: 2.1474 - val_loss: 9.2817 - val_mae: 2.3275\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.1070 - mae: 2.1507 - val_loss: 9.2703 - val_mae: 2.3337\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0958 - mae: 2.1366 - val_loss: 9.3296 - val_mae: 2.3202\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 8.1412 - mae: 2.1592 - val_loss: 9.2720 - val_mae: 2.3324\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0865 - mae: 2.1368 - val_loss: 9.3065 - val_mae: 2.3231\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0900 - mae: 2.1497 - val_loss: 9.2712 - val_mae: 2.3362\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0831 - mae: 2.1410 - val_loss: 9.3025 - val_mae: 2.3258\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0727 - mae: 2.1441 - val_loss: 9.2890 - val_mae: 2.3304\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0983 - mae: 2.1390 - val_loss: 9.2811 - val_mae: 2.3330\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0711 - mae: 2.1463 - val_loss: 9.3022 - val_mae: 2.3274\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0636 - mae: 2.1360 - val_loss: 9.3066 - val_mae: 2.3267\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0678 - mae: 2.1399 - val_loss: 9.2922 - val_mae: 2.3303\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0725 - mae: 2.1449 - val_loss: 9.3307 - val_mae: 2.3229\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0591 - mae: 2.1242 - val_loss: 9.3185 - val_mae: 2.3253\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0488 - mae: 2.1422 - val_loss: 9.2879 - val_mae: 2.3373\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0491 - mae: 2.1411 - val_loss: 9.3055 - val_mae: 2.3289\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0620 - mae: 2.1461 - val_loss: 9.3257 - val_mae: 2.3230\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0521 - mae: 2.1287 - val_loss: 9.3151 - val_mae: 2.3273\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0479 - mae: 2.1466 - val_loss: 9.2930 - val_mae: 2.3346\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0304 - mae: 2.1399 - val_loss: 9.3160 - val_mae: 2.3251\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0430 - mae: 2.1401 - val_loss: 9.3190 - val_mae: 2.3276\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.0503 - mae: 2.1257 - val_loss: 9.3341 - val_mae: 2.3202\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0899 - mae: 2.1588 - val_loss: 9.3128 - val_mae: 2.3283\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0281 - mae: 2.1252 - val_loss: 9.3404 - val_mae: 2.3184\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0154 - mae: 2.1350 - val_loss: 9.3125 - val_mae: 2.3354\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0419 - mae: 2.1379 - val_loss: 9.3172 - val_mae: 2.3331\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0077 - mae: 2.1327 - val_loss: 9.3416 - val_mae: 2.3210\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.0160 - mae: 2.1312 - val_loss: 9.3157 - val_mae: 2.3308\n",
      "11/11 [==============================] - 0s 2ms/step - loss: 9.3209 - mae: 2.1772\n",
      "Mean Absolute Error on Test Data: 2.1771843433380127\n",
      "11/11 [==============================] - 0s 859us/step\n",
      "R-squared: 0.02698664922745364\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 0s 12ms/step - loss: 8.9595 - mae: 2.1798 - val_loss: 7.9899 - val_mae: 2.0140\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.7921 - mae: 1.5200 - val_loss: 5.2100 - val_mae: 1.5573\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.4113 - mae: 1.4220 - val_loss: 4.3619 - val_mae: 1.5795\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2951 - mae: 1.4933 - val_loss: 4.4118 - val_mae: 1.5599\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2734 - mae: 1.4635 - val_loss: 4.3980 - val_mae: 1.5661\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.3271 - mae: 1.5375 - val_loss: 4.3662 - val_mae: 1.6091\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.3242 - mae: 1.5307 - val_loss: 4.4010 - val_mae: 1.5754\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.2675 - mae: 1.4559 - val_loss: 4.5142 - val_mae: 1.5539\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2530 - mae: 1.4443 - val_loss: 4.4845 - val_mae: 1.5645\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2489 - mae: 1.4494 - val_loss: 4.4994 - val_mae: 1.5665\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.2412 - mae: 1.4533 - val_loss: 4.5330 - val_mae: 1.5641\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2490 - mae: 1.4264 - val_loss: 4.5749 - val_mae: 1.5587\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2522 - mae: 1.4173 - val_loss: 4.5817 - val_mae: 1.5617\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2347 - mae: 1.4474 - val_loss: 4.4972 - val_mae: 1.5813\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2317 - mae: 1.4804 - val_loss: 4.4822 - val_mae: 1.5863\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2304 - mae: 1.4807 - val_loss: 4.4906 - val_mae: 1.5820\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2192 - mae: 1.4523 - val_loss: 4.5731 - val_mae: 1.5682\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 4.2432 - mae: 1.4234 - val_loss: 4.5784 - val_mae: 1.5664\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 4.2368 - mae: 1.4775 - val_loss: 4.4536 - val_mae: 1.5956\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2286 - mae: 1.4743 - val_loss: 4.5562 - val_mae: 1.5724\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.2129 - mae: 1.4309 - val_loss: 4.6219 - val_mae: 1.5573\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.2443 - mae: 1.4084 - val_loss: 4.6013 - val_mae: 1.5577\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.2157 - mae: 1.4290 - val_loss: 4.4987 - val_mae: 1.5762\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.2072 - mae: 1.4759 - val_loss: 4.4828 - val_mae: 1.5861\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1919 - mae: 1.4550 - val_loss: 4.5891 - val_mae: 1.5699\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1995 - mae: 1.4250 - val_loss: 4.6131 - val_mae: 1.5654\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.2171 - mae: 1.4306 - val_loss: 4.5664 - val_mae: 1.5742\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.1990 - mae: 1.4288 - val_loss: 4.6091 - val_mae: 1.5761\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1815 - mae: 1.4539 - val_loss: 4.5207 - val_mae: 1.5934\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1894 - mae: 1.4728 - val_loss: 4.5306 - val_mae: 1.5902\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1975 - mae: 1.4328 - val_loss: 4.6290 - val_mae: 1.5752\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1804 - mae: 1.4306 - val_loss: 4.5564 - val_mae: 1.5928\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1932 - mae: 1.4883 - val_loss: 4.5215 - val_mae: 1.6030\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1709 - mae: 1.4587 - val_loss: 4.6127 - val_mae: 1.5847\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1789 - mae: 1.4366 - val_loss: 4.6170 - val_mae: 1.5868\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 4.1767 - mae: 1.4443 - val_loss: 4.6255 - val_mae: 1.5887\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1665 - mae: 1.4338 - val_loss: 4.5778 - val_mae: 1.5898\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1624 - mae: 1.4472 - val_loss: 4.5724 - val_mae: 1.5917\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1663 - mae: 1.4342 - val_loss: 4.6226 - val_mae: 1.5865\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1636 - mae: 1.4238 - val_loss: 4.6340 - val_mae: 1.5876\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1696 - mae: 1.4651 - val_loss: 4.5519 - val_mae: 1.6022\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1440 - mae: 1.4470 - val_loss: 4.6402 - val_mae: 1.5834\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1651 - mae: 1.4425 - val_loss: 4.5699 - val_mae: 1.5926\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1560 - mae: 1.4765 - val_loss: 4.4938 - val_mae: 1.6063\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1862 - mae: 1.4790 - val_loss: 4.5974 - val_mae: 1.5848\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1525 - mae: 1.4262 - val_loss: 4.5940 - val_mae: 1.5954\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1779 - mae: 1.4931 - val_loss: 4.5822 - val_mae: 1.6199\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1697 - mae: 1.4395 - val_loss: 4.7265 - val_mae: 1.5891\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.1903 - mae: 1.4409 - val_loss: 4.6500 - val_mae: 1.5950\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 4.1907 - mae: 1.4098 - val_loss: 4.7497 - val_mae: 1.5857\n",
      "9/9 [==============================] - 0s 2ms/step - loss: 4.0024 - mae: 1.4585\n",
      "Mean Absolute Error on Test Data: 1.4584568738937378\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.018066989111944043\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 14ms/step - loss: 8.7522 - mae: 2.2674 - val_loss: 8.0727 - val_mae: 2.0225\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.2886 - mae: 1.7158 - val_loss: 5.9018 - val_mae: 1.5525\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.5930 - mae: 1.4058 - val_loss: 4.4725 - val_mae: 1.3824\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.7549 - mae: 1.3653 - val_loss: 4.0789 - val_mae: 1.4271\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.7851 - mae: 1.4709 - val_loss: 4.0714 - val_mae: 1.4622\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.7162 - mae: 1.4460 - val_loss: 4.0338 - val_mae: 1.4161\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.6479 - mae: 1.4050 - val_loss: 4.0035 - val_mae: 1.4021\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.6502 - mae: 1.4390 - val_loss: 3.9734 - val_mae: 1.4339\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.6059 - mae: 1.4267 - val_loss: 3.9580 - val_mae: 1.3942\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.5722 - mae: 1.3742 - val_loss: 3.9799 - val_mae: 1.3630\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.5628 - mae: 1.3575 - val_loss: 3.9713 - val_mae: 1.3659\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.5432 - mae: 1.3623 - val_loss: 3.9554 - val_mae: 1.3739\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.5300 - mae: 1.3585 - val_loss: 3.9605 - val_mae: 1.3682\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.5153 - mae: 1.3691 - val_loss: 3.9401 - val_mae: 1.3950\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.5141 - mae: 1.3963 - val_loss: 3.9286 - val_mae: 1.3995\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.5086 - mae: 1.3488 - val_loss: 3.9697 - val_mae: 1.3528\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.4902 - mae: 1.3577 - val_loss: 3.9298 - val_mae: 1.3994\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.4801 - mae: 1.3609 - val_loss: 3.9416 - val_mae: 1.3812\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4561 - mae: 1.3850 - val_loss: 3.9261 - val_mae: 1.4320\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4744 - mae: 1.4048 - val_loss: 3.9257 - val_mae: 1.4041\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4512 - mae: 1.3508 - val_loss: 3.9520 - val_mae: 1.3765\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4443 - mae: 1.3767 - val_loss: 3.9395 - val_mae: 1.4240\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4497 - mae: 1.3581 - val_loss: 3.9758 - val_mae: 1.3707\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4440 - mae: 1.3365 - val_loss: 3.9643 - val_mae: 1.3820\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4349 - mae: 1.3739 - val_loss: 3.9432 - val_mae: 1.4197\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.4305 - mae: 1.3791 - val_loss: 3.9628 - val_mae: 1.3873\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4424 - mae: 1.3282 - val_loss: 3.9755 - val_mae: 1.3713\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4235 - mae: 1.3830 - val_loss: 3.9545 - val_mae: 1.4483\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4194 - mae: 1.3723 - val_loss: 3.9683 - val_mae: 1.3741\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.4244 - mae: 1.3401 - val_loss: 3.9577 - val_mae: 1.3856\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4136 - mae: 1.3644 - val_loss: 3.9495 - val_mae: 1.4050\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.4066 - mae: 1.3539 - val_loss: 3.9614 - val_mae: 1.3882\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4061 - mae: 1.3407 - val_loss: 3.9787 - val_mae: 1.3787\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4117 - mae: 1.3518 - val_loss: 3.9616 - val_mae: 1.4139\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4103 - mae: 1.3431 - val_loss: 3.9712 - val_mae: 1.3969\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 3.4116 - mae: 1.3977 - val_loss: 3.9860 - val_mae: 1.4633\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4341 - mae: 1.4069 - val_loss: 3.9763 - val_mae: 1.3986\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.3992 - mae: 1.3333 - val_loss: 4.0123 - val_mae: 1.3648\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.4008 - mae: 1.3631 - val_loss: 3.9723 - val_mae: 1.4392\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4580 - mae: 1.4277 - val_loss: 3.9739 - val_mae: 1.4434\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3822 - mae: 1.3704 - val_loss: 3.9880 - val_mae: 1.3871\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4242 - mae: 1.3205 - val_loss: 4.0221 - val_mae: 1.3667\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.3911 - mae: 1.3323 - val_loss: 3.9723 - val_mae: 1.4156\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3937 - mae: 1.3727 - val_loss: 3.9731 - val_mae: 1.4218\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.4236 - mae: 1.4069 - val_loss: 3.9801 - val_mae: 1.4451\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3884 - mae: 1.3623 - val_loss: 3.9845 - val_mae: 1.3880\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 3.3914 - mae: 1.3603 - val_loss: 3.9663 - val_mae: 1.4035\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.3831 - mae: 1.3378 - val_loss: 3.9908 - val_mae: 1.3753\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.3890 - mae: 1.3471 - val_loss: 3.9631 - val_mae: 1.4088\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 3.3681 - mae: 1.3522 - val_loss: 3.9752 - val_mae: 1.3896\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.8327 - mae: 1.4531\n",
      "Mean Absolute Error on Test Data: 1.4530879259109497\n",
      "9/9 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03543427006225419\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 48.7796 - mae: 5.5689 - val_loss: 52.8235 - val_mae: 5.1176\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 39.8032 - mae: 4.7699 - val_loss: 42.2844 - val_mae: 4.1485\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 28.2666 - mae: 3.7120 - val_loss: 30.4367 - val_mae: 3.3209\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.3683 - mae: 3.1582 - val_loss: 26.3167 - val_mae: 3.3914\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.2390 - mae: 3.2692 - val_loss: 26.2372 - val_mae: 3.4012\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9733 - mae: 3.1671 - val_loss: 26.1076 - val_mae: 3.2971\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.9134 - mae: 3.1431 - val_loss: 25.9615 - val_mae: 3.3227\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8723 - mae: 3.1665 - val_loss: 25.8700 - val_mae: 3.3263\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8472 - mae: 3.1577 - val_loss: 25.7445 - val_mae: 3.3387\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8121 - mae: 3.1627 - val_loss: 25.7024 - val_mae: 3.3389\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.7614 - mae: 3.1529 - val_loss: 25.6166 - val_mae: 3.3171\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.7401 - mae: 3.1257 - val_loss: 25.5432 - val_mae: 3.2942\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.7047 - mae: 3.1352 - val_loss: 25.4863 - val_mae: 3.3075\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.7387 - mae: 3.1555 - val_loss: 25.4592 - val_mae: 3.2994\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.6707 - mae: 3.1176 - val_loss: 25.3865 - val_mae: 3.3024\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.6417 - mae: 3.1436 - val_loss: 25.3325 - val_mae: 3.3156\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.5390 - mae: 3.1186 - val_loss: 25.2930 - val_mae: 3.2770\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.5547 - mae: 3.1246 - val_loss: 25.2398 - val_mae: 3.2788\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4901 - mae: 3.1052 - val_loss: 25.1725 - val_mae: 3.2860\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4763 - mae: 3.1281 - val_loss: 25.1589 - val_mae: 3.3251\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4582 - mae: 3.1101 - val_loss: 25.1011 - val_mae: 3.2558\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.3919 - mae: 3.1051 - val_loss: 25.0244 - val_mae: 3.2883\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.3818 - mae: 3.1246 - val_loss: 25.0278 - val_mae: 3.2957\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.3401 - mae: 3.0872 - val_loss: 25.0212 - val_mae: 3.2507\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.2850 - mae: 3.0904 - val_loss: 24.9470 - val_mae: 3.2853\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.2625 - mae: 3.0953 - val_loss: 24.9333 - val_mae: 3.2915\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.2283 - mae: 3.1081 - val_loss: 24.8808 - val_mae: 3.2655\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1849 - mae: 3.0799 - val_loss: 24.8803 - val_mae: 3.2347\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1355 - mae: 3.0760 - val_loss: 24.8245 - val_mae: 3.2633\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1043 - mae: 3.0797 - val_loss: 24.7846 - val_mae: 3.2416\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1710 - mae: 3.0972 - val_loss: 24.7347 - val_mae: 3.2562\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1067 - mae: 3.0446 - val_loss: 24.7806 - val_mae: 3.2300\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.0705 - mae: 3.0939 - val_loss: 24.6724 - val_mae: 3.2554\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.0125 - mae: 3.0660 - val_loss: 24.6685 - val_mae: 3.2475\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.9558 - mae: 3.0421 - val_loss: 24.6765 - val_mae: 3.2267\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.9441 - mae: 3.0710 - val_loss: 24.7048 - val_mae: 3.2358\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8814 - mae: 3.0573 - val_loss: 24.6344 - val_mae: 3.2393\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8445 - mae: 3.0587 - val_loss: 24.6713 - val_mae: 3.2193\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.8740 - mae: 3.0271 - val_loss: 24.5828 - val_mae: 3.2451\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8471 - mae: 3.0524 - val_loss: 24.6186 - val_mae: 3.2551\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8038 - mae: 3.0647 - val_loss: 24.5727 - val_mae: 3.2382\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.7694 - mae: 3.0400 - val_loss: 24.6480 - val_mae: 3.2112\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 16.7187 - mae: 3.0565 - val_loss: 24.5916 - val_mae: 3.2530\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7021 - mae: 3.0414 - val_loss: 24.5561 - val_mae: 3.2343\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7102 - mae: 3.0130 - val_loss: 24.6134 - val_mae: 3.2062\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7335 - mae: 3.0658 - val_loss: 24.5697 - val_mae: 3.2062\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6108 - mae: 3.0225 - val_loss: 24.6000 - val_mae: 3.2273\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5736 - mae: 3.0389 - val_loss: 24.5634 - val_mae: 3.2035\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5784 - mae: 3.0276 - val_loss: 24.5218 - val_mae: 3.1855\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5745 - mae: 3.0234 - val_loss: 24.5098 - val_mae: 3.2356\n",
      "11/11 [==============================] - 0s 965us/step - loss: 17.8012 - mae: 3.2369\n",
      "Mean Absolute Error on Test Data: 3.236928939819336\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.0824820992384373\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 1s 12ms/step - loss: 38.8630 - mae: 4.3405 - val_loss: 44.1101 - val_mae: 4.3387\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 30.9811 - mae: 3.5233 - val_loss: 34.6698 - val_mae: 3.4964\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 23.2796 - mae: 2.8644 - val_loss: 26.9190 - val_mae: 3.1248\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 20.2267 - mae: 2.9336 - val_loss: 25.3709 - val_mae: 3.3144\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 19.8266 - mae: 2.9817 - val_loss: 25.2589 - val_mae: 3.2292\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.5269 - mae: 2.9214 - val_loss: 25.0280 - val_mae: 3.2141\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.3412 - mae: 2.8859 - val_loss: 24.8043 - val_mae: 3.1942\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 19.1646 - mae: 2.8608 - val_loss: 24.5526 - val_mae: 3.1905\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.9818 - mae: 2.8726 - val_loss: 24.3330 - val_mae: 3.1932\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.9322 - mae: 2.8308 - val_loss: 24.2395 - val_mae: 3.1610\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.7634 - mae: 2.8729 - val_loss: 23.9801 - val_mae: 3.1830\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.6237 - mae: 2.8400 - val_loss: 23.8606 - val_mae: 3.1770\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.5501 - mae: 2.8010 - val_loss: 23.8050 - val_mae: 3.1533\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.4293 - mae: 2.7876 - val_loss: 23.6772 - val_mae: 3.1510\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.3369 - mae: 2.8346 - val_loss: 23.5356 - val_mae: 3.1542\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.2191 - mae: 2.7975 - val_loss: 23.4821 - val_mae: 3.1439\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 18.1147 - mae: 2.7814 - val_loss: 23.4862 - val_mae: 3.1195\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.0497 - mae: 2.7687 - val_loss: 23.2937 - val_mae: 3.1444\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 18.0503 - mae: 2.7655 - val_loss: 23.3688 - val_mae: 3.1128\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.9432 - mae: 2.8072 - val_loss: 23.0515 - val_mae: 3.1957\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8123 - mae: 2.7759 - val_loss: 23.2314 - val_mae: 3.1188\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8064 - mae: 2.7526 - val_loss: 23.1414 - val_mae: 3.1297\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.7802 - mae: 2.7153 - val_loss: 23.1772 - val_mae: 3.1202\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.8185 - mae: 2.8239 - val_loss: 22.9853 - val_mae: 3.1540\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.6925 - mae: 2.7076 - val_loss: 23.2265 - val_mae: 3.0954\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.5798 - mae: 2.7643 - val_loss: 22.8503 - val_mae: 3.1876\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.5169 - mae: 2.7352 - val_loss: 22.9855 - val_mae: 3.1394\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.4484 - mae: 2.7337 - val_loss: 23.0064 - val_mae: 3.1294\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4106 - mae: 2.7064 - val_loss: 22.9569 - val_mae: 3.1442\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4235 - mae: 2.7582 - val_loss: 22.8989 - val_mae: 3.1508\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.3814 - mae: 2.6771 - val_loss: 23.0565 - val_mae: 3.1195\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.3330 - mae: 2.7455 - val_loss: 22.8514 - val_mae: 3.1727\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1779 - mae: 2.7001 - val_loss: 22.9745 - val_mae: 3.1436\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1450 - mae: 2.7097 - val_loss: 22.8938 - val_mae: 3.1713\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.1391 - mae: 2.7236 - val_loss: 22.9508 - val_mae: 3.1641\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.0805 - mae: 2.6565 - val_loss: 23.1207 - val_mae: 3.1322\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.0335 - mae: 2.7074 - val_loss: 22.9699 - val_mae: 3.1757\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 17.0773 - mae: 2.7402 - val_loss: 23.0967 - val_mae: 3.1481\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.0827 - mae: 2.6447 - val_loss: 23.0685 - val_mae: 3.1540\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.9317 - mae: 2.6735 - val_loss: 23.0091 - val_mae: 3.1798\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8796 - mae: 2.6981 - val_loss: 23.0245 - val_mae: 3.1995\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8757 - mae: 2.6761 - val_loss: 23.0598 - val_mae: 3.1859\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7875 - mae: 2.6787 - val_loss: 23.0618 - val_mae: 3.1854\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7485 - mae: 2.6636 - val_loss: 23.1979 - val_mae: 3.1666\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.7880 - mae: 2.6869 - val_loss: 23.1722 - val_mae: 3.1768\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7052 - mae: 2.6630 - val_loss: 23.1620 - val_mae: 3.1835\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6606 - mae: 2.6606 - val_loss: 23.1690 - val_mae: 3.1857\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6848 - mae: 2.6377 - val_loss: 23.2426 - val_mae: 3.1775\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6712 - mae: 2.7028 - val_loss: 23.0838 - val_mae: 3.2322\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6800 - mae: 2.6394 - val_loss: 23.4668 - val_mae: 3.1465\n",
      "11/11 [==============================] - 0s 991us/step - loss: 23.7851 - mae: 3.0138\n",
      "Mean Absolute Error on Test Data: 3.013789415359497\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.06379840401078318\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 13ms/step - loss: 41.2686 - mae: 5.1686 - val_loss: 55.7146 - val_mae: 5.3377\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 30.7962 - mae: 4.1466 - val_loss: 42.6221 - val_mae: 4.2099\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 20.5346 - mae: 3.1635 - val_loss: 31.1198 - val_mae: 3.3482\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.9780 - mae: 2.8193 - val_loss: 27.1839 - val_mae: 3.3930\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.7408 - mae: 2.9333 - val_loss: 27.2720 - val_mae: 3.3771\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.5778 - mae: 2.8573 - val_loss: 27.8219 - val_mae: 3.3037\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.5324 - mae: 2.8276 - val_loss: 27.5437 - val_mae: 3.3316\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.4706 - mae: 2.8650 - val_loss: 27.4063 - val_mae: 3.3574\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.4737 - mae: 2.8766 - val_loss: 27.5912 - val_mae: 3.3201\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.4461 - mae: 2.8191 - val_loss: 27.6771 - val_mae: 3.3129\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.3671 - mae: 2.8412 - val_loss: 27.4467 - val_mae: 3.3545\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.3342 - mae: 2.8503 - val_loss: 27.5177 - val_mae: 3.3427\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.3829 - mae: 2.8208 - val_loss: 27.5923 - val_mae: 3.3255\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2857 - mae: 2.8437 - val_loss: 27.5225 - val_mae: 3.3305\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2423 - mae: 2.8298 - val_loss: 27.5781 - val_mae: 3.3141\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2200 - mae: 2.8151 - val_loss: 27.4771 - val_mae: 3.3250\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2166 - mae: 2.8073 - val_loss: 27.4917 - val_mae: 3.3144\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2110 - mae: 2.8393 - val_loss: 27.3393 - val_mae: 3.3362\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.1795 - mae: 2.7984 - val_loss: 27.5868 - val_mae: 3.3004\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.1478 - mae: 2.8172 - val_loss: 27.3317 - val_mae: 3.3246\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.0817 - mae: 2.8015 - val_loss: 27.4150 - val_mae: 3.3029\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0980 - mae: 2.7946 - val_loss: 27.3759 - val_mae: 3.3095\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0955 - mae: 2.8314 - val_loss: 27.2494 - val_mae: 3.3153\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.1267 - mae: 2.7703 - val_loss: 27.3948 - val_mae: 3.2940\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9983 - mae: 2.8004 - val_loss: 27.2193 - val_mae: 3.3144\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0345 - mae: 2.7767 - val_loss: 27.2165 - val_mae: 3.3046\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0519 - mae: 2.8202 - val_loss: 27.2403 - val_mae: 3.3007\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9631 - mae: 2.7757 - val_loss: 27.2376 - val_mae: 3.2993\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.9800 - mae: 2.8325 - val_loss: 27.0354 - val_mae: 3.3155\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9014 - mae: 2.7587 - val_loss: 27.3682 - val_mae: 3.2613\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8766 - mae: 2.7787 - val_loss: 26.9036 - val_mae: 3.3176\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8522 - mae: 2.7768 - val_loss: 27.1768 - val_mae: 3.2825\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8757 - mae: 2.7795 - val_loss: 26.8390 - val_mae: 3.3233\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7984 - mae: 2.7754 - val_loss: 27.0331 - val_mae: 3.2796\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7991 - mae: 2.7781 - val_loss: 27.0147 - val_mae: 3.2733\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7448 - mae: 2.7644 - val_loss: 27.0146 - val_mae: 3.2704\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7731 - mae: 2.7491 - val_loss: 27.2715 - val_mae: 3.2475\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.6949 - mae: 2.7727 - val_loss: 26.7526 - val_mae: 3.3087\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6946 - mae: 2.7719 - val_loss: 26.7743 - val_mae: 3.2812\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6924 - mae: 2.7697 - val_loss: 27.0781 - val_mae: 3.2485\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6449 - mae: 2.7292 - val_loss: 26.5902 - val_mae: 3.2978\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6405 - mae: 2.7964 - val_loss: 26.6731 - val_mae: 3.2812\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6100 - mae: 2.7237 - val_loss: 26.9513 - val_mae: 3.2557\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.5817 - mae: 2.7597 - val_loss: 26.6090 - val_mae: 3.2807\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.5438 - mae: 2.7554 - val_loss: 26.9668 - val_mae: 3.2447\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.5738 - mae: 2.7432 - val_loss: 26.7130 - val_mae: 3.2656\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.5520 - mae: 2.7495 - val_loss: 26.6384 - val_mae: 3.2724\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.5173 - mae: 2.7173 - val_loss: 26.8186 - val_mae: 3.2486\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.6409 - mae: 2.7945 - val_loss: 26.5774 - val_mae: 3.2813\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.4944 - mae: 2.7231 - val_loss: 26.7392 - val_mae: 3.2518\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 12.6748 - mae: 2.7583\n",
      "Mean Absolute Error on Test Data: 2.758338212966919\n",
      "11/11 [==============================] - 0s 956us/step\n",
      "R-squared: 0.028507076819576427\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 11ms/step - loss: 70.7339 - mae: 6.7113 - val_loss: 51.0191 - val_mae: 5.7092\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 56.5279 - mae: 5.6237 - val_loss: 36.6417 - val_mae: 4.4279\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.1334 - mae: 4.1522 - val_loss: 21.7149 - val_mae: 3.0606\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 26.7380 - mae: 3.3522 - val_loss: 18.6227 - val_mae: 3.1945\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.8563 - mae: 3.5226 - val_loss: 18.5414 - val_mae: 3.1720\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.6551 - mae: 3.5029 - val_loss: 18.5613 - val_mae: 3.1741\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.5061 - mae: 3.4628 - val_loss: 18.3076 - val_mae: 3.0995\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.4640 - mae: 3.3819 - val_loss: 18.2330 - val_mae: 3.0713\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.5492 - mae: 3.4533 - val_loss: 18.3475 - val_mae: 3.1073\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.3346 - mae: 3.3892 - val_loss: 18.2590 - val_mae: 3.0697\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.3636 - mae: 3.4178 - val_loss: 18.4252 - val_mae: 3.1170\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.4443 - mae: 3.4820 - val_loss: 18.6009 - val_mae: 3.1600\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.3355 - mae: 3.4074 - val_loss: 18.3224 - val_mae: 3.0739\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2821 - mae: 3.3964 - val_loss: 18.3351 - val_mae: 3.0762\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.5108 - mae: 3.3389 - val_loss: 18.3994 - val_mae: 3.0953\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.5300 - mae: 3.5728 - val_loss: 19.1372 - val_mae: 3.2624\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.3964 - mae: 3.5378 - val_loss: 18.6052 - val_mae: 3.1408\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.3721 - mae: 3.3892 - val_loss: 18.4509 - val_mae: 3.0977\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1982 - mae: 3.3949 - val_loss: 18.4682 - val_mae: 3.1058\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 24.2609 - mae: 3.4679 - val_loss: 18.5705 - val_mae: 3.1289\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1795 - mae: 3.4239 - val_loss: 18.3905 - val_mae: 3.0573\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2947 - mae: 3.3440 - val_loss: 18.4737 - val_mae: 3.0893\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2412 - mae: 3.4720 - val_loss: 18.7107 - val_mae: 3.1573\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2306 - mae: 3.4729 - val_loss: 18.4967 - val_mae: 3.1026\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2167 - mae: 3.4211 - val_loss: 18.5553 - val_mae: 3.1193\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2483 - mae: 3.4759 - val_loss: 18.6243 - val_mae: 3.1377\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2137 - mae: 3.3970 - val_loss: 18.4919 - val_mae: 3.0954\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1573 - mae: 3.4182 - val_loss: 18.5539 - val_mae: 3.1166\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.3481 - mae: 3.4162 - val_loss: 18.5619 - val_mae: 3.1113\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.4423 - mae: 3.3238 - val_loss: 18.4226 - val_mae: 3.0278\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2055 - mae: 3.3660 - val_loss: 18.5738 - val_mae: 3.1089\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1412 - mae: 3.4096 - val_loss: 18.5312 - val_mae: 3.0904\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1530 - mae: 3.4008 - val_loss: 18.5556 - val_mae: 3.1026\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.3511 - mae: 3.5019 - val_loss: 18.8348 - val_mae: 3.1595\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1834 - mae: 3.4565 - val_loss: 18.6192 - val_mae: 3.1099\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1311 - mae: 3.3970 - val_loss: 18.5641 - val_mae: 3.1005\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2155 - mae: 3.3788 - val_loss: 18.5500 - val_mae: 3.1038\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.0997 - mae: 3.4084 - val_loss: 18.5267 - val_mae: 3.0934\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0904 - mae: 3.3988 - val_loss: 18.5206 - val_mae: 3.0913\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2109 - mae: 3.3678 - val_loss: 18.4628 - val_mae: 3.0798\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1002 - mae: 3.4073 - val_loss: 18.6648 - val_mae: 3.1431\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1736 - mae: 3.4795 - val_loss: 18.7827 - val_mae: 3.1449\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1195 - mae: 3.4368 - val_loss: 18.6172 - val_mae: 3.0987\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1105 - mae: 3.3839 - val_loss: 18.5615 - val_mae: 3.0841\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0901 - mae: 3.4103 - val_loss: 18.6829 - val_mae: 3.1291\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0868 - mae: 3.4135 - val_loss: 18.5751 - val_mae: 3.0940\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1490 - mae: 3.3732 - val_loss: 18.5610 - val_mae: 3.0879\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.1345 - mae: 3.3853 - val_loss: 18.6734 - val_mae: 3.1267\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.2628 - mae: 3.5229 - val_loss: 18.7853 - val_mae: 3.1592\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.2461 - mae: 3.3680 - val_loss: 18.5109 - val_mae: 3.0765\n",
      "11/11 [==============================] - 0s 974us/step - loss: 22.4783 - mae: 3.5872\n",
      "Mean Absolute Error on Test Data: 3.5871517658233643\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.06581462798242421\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 1s 14ms/step - loss: 7.4174 - mae: 2.1132 - val_loss: 5.7731 - val_mae: 1.7437\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 4.7814 - mae: 1.5134 - val_loss: 3.5719 - val_mae: 1.3202\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 6ms/step - loss: 3.0763 - mae: 1.2495 - val_loss: 3.1000 - val_mae: 1.3587\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.9184 - mae: 1.3125 - val_loss: 3.0510 - val_mae: 1.3402\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.8101 - mae: 1.2247 - val_loss: 2.9882 - val_mae: 1.2753\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7857 - mae: 1.2114 - val_loss: 2.9761 - val_mae: 1.2935\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7975 - mae: 1.2484 - val_loss: 2.9757 - val_mae: 1.3041\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7629 - mae: 1.2105 - val_loss: 2.9556 - val_mae: 1.2701\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7506 - mae: 1.2121 - val_loss: 2.9618 - val_mae: 1.3070\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7428 - mae: 1.2243 - val_loss: 2.9421 - val_mae: 1.2853\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7529 - mae: 1.2315 - val_loss: 2.9330 - val_mae: 1.2820\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.7560 - mae: 1.2053 - val_loss: 2.9257 - val_mae: 1.2744\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7385 - mae: 1.2323 - val_loss: 2.9464 - val_mae: 1.3105\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7156 - mae: 1.2160 - val_loss: 2.9152 - val_mae: 1.2702\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7211 - mae: 1.2034 - val_loss: 2.9106 - val_mae: 1.2733\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7221 - mae: 1.2241 - val_loss: 2.9221 - val_mae: 1.2986\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7206 - mae: 1.2127 - val_loss: 2.9022 - val_mae: 1.2717\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7121 - mae: 1.2090 - val_loss: 2.8998 - val_mae: 1.2791\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7102 - mae: 1.2206 - val_loss: 2.8972 - val_mae: 1.2825\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7116 - mae: 1.2055 - val_loss: 2.8906 - val_mae: 1.2670\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.7160 - mae: 1.2232 - val_loss: 2.8926 - val_mae: 1.2843\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7100 - mae: 1.2134 - val_loss: 2.8816 - val_mae: 1.2583\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7195 - mae: 1.2144 - val_loss: 2.8866 - val_mae: 1.2819\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6966 - mae: 1.2080 - val_loss: 2.8783 - val_mae: 1.2661\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7208 - mae: 1.2178 - val_loss: 2.9024 - val_mae: 1.3014\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6929 - mae: 1.2089 - val_loss: 2.8723 - val_mae: 1.2617\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6996 - mae: 1.2068 - val_loss: 2.8832 - val_mae: 1.2845\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.7109 - mae: 1.2079 - val_loss: 2.8733 - val_mae: 1.2741\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.7169 - mae: 1.2144 - val_loss: 2.8664 - val_mae: 1.2606\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6897 - mae: 1.2103 - val_loss: 2.8746 - val_mae: 1.2791\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6982 - mae: 1.2197 - val_loss: 2.8654 - val_mae: 1.2680\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6909 - mae: 1.1992 - val_loss: 2.8613 - val_mae: 1.2624\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6847 - mae: 1.2037 - val_loss: 2.8637 - val_mae: 1.2715\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6948 - mae: 1.2165 - val_loss: 2.8576 - val_mae: 1.2614\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6950 - mae: 1.2088 - val_loss: 2.8790 - val_mae: 1.2905\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6900 - mae: 1.2141 - val_loss: 2.8631 - val_mae: 1.2721\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6934 - mae: 1.1999 - val_loss: 2.8569 - val_mae: 1.2604\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6843 - mae: 1.2053 - val_loss: 2.8575 - val_mae: 1.2673\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.6809 - mae: 1.2098 - val_loss: 2.8586 - val_mae: 1.2702\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6875 - mae: 1.2025 - val_loss: 2.8552 - val_mae: 1.2609\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6767 - mae: 1.2047 - val_loss: 2.8691 - val_mae: 1.2842\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6787 - mae: 1.2083 - val_loss: 2.8538 - val_mae: 1.2645\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6733 - mae: 1.2007 - val_loss: 2.8539 - val_mae: 1.2682\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6749 - mae: 1.2101 - val_loss: 2.8507 - val_mae: 1.2676\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6874 - mae: 1.2158 - val_loss: 2.8478 - val_mae: 1.2551\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 2.6801 - mae: 1.1912 - val_loss: 2.8476 - val_mae: 1.2543\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6725 - mae: 1.2115 - val_loss: 2.8556 - val_mae: 1.2727\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 2.6847 - mae: 1.2018 - val_loss: 2.8501 - val_mae: 1.2671\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.6745 - mae: 1.2104 - val_loss: 2.8474 - val_mae: 1.2638\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 2.6726 - mae: 1.2037 - val_loss: 2.8448 - val_mae: 1.2623\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.0563 - mae: 1.3403\n",
      "Mean Absolute Error on Test Data: 1.3403193950653076\n",
      "9/9 [==============================] - 0s 943us/step\n",
      "R-squared: -0.03611441409261951\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 63.7399 - mae: 5.1969 - val_loss: 65.9677 - val_mae: 5.1981\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 53.4011 - mae: 4.3303 - val_loss: 53.7344 - val_mae: 4.3431\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 42.3925 - mae: 3.6564 - val_loss: 42.1524 - val_mae: 3.9542\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 36.8770 - mae: 3.8442 - val_loss: 39.0344 - val_mae: 4.2546\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.9542 - mae: 4.0744 - val_loss: 38.8150 - val_mae: 4.2134\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.5823 - mae: 3.8826 - val_loss: 38.9042 - val_mae: 4.1017\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.3579 - mae: 3.8559 - val_loss: 38.5817 - val_mae: 4.1258\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.2037 - mae: 3.9099 - val_loss: 38.2190 - val_mae: 4.1677\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.0340 - mae: 3.8914 - val_loss: 38.2844 - val_mae: 4.0923\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.8487 - mae: 3.8820 - val_loss: 37.9472 - val_mae: 4.1230\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.7517 - mae: 3.9199 - val_loss: 37.7905 - val_mae: 4.0985\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.5709 - mae: 3.8452 - val_loss: 37.5952 - val_mae: 4.0947\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.3722 - mae: 3.8453 - val_loss: 37.4166 - val_mae: 4.0798\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.2416 - mae: 3.8819 - val_loss: 37.2267 - val_mae: 4.0834\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.0823 - mae: 3.8829 - val_loss: 37.0644 - val_mae: 4.0595\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.0589 - mae: 3.8269 - val_loss: 37.1009 - val_mae: 4.0129\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.9491 - mae: 3.8686 - val_loss: 36.5859 - val_mae: 4.1016\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7119 - mae: 3.8878 - val_loss: 36.8762 - val_mae: 3.9909\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7424 - mae: 3.7758 - val_loss: 36.5595 - val_mae: 4.0224\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.5521 - mae: 3.7644 - val_loss: 36.5384 - val_mae: 3.9989\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.4916 - mae: 3.7514 - val_loss: 36.3931 - val_mae: 4.0035\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.3462 - mae: 3.7992 - val_loss: 36.1881 - val_mae: 4.0310\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.6010 - mae: 3.9097 - val_loss: 36.0786 - val_mae: 4.0036\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.1917 - mae: 3.7725 - val_loss: 36.1814 - val_mae: 3.9633\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.2204 - mae: 3.7191 - val_loss: 35.8819 - val_mae: 4.0047\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.0525 - mae: 3.7955 - val_loss: 35.7139 - val_mae: 4.0183\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.9798 - mae: 3.7816 - val_loss: 35.6777 - val_mae: 3.9800\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.0773 - mae: 3.8528 - val_loss: 35.5393 - val_mae: 4.0034\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.8396 - mae: 3.7632 - val_loss: 35.7998 - val_mae: 3.9218\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.8977 - mae: 3.7070 - val_loss: 35.6131 - val_mae: 3.9457\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.8279 - mae: 3.7396 - val_loss: 35.4859 - val_mae: 3.9479\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.9378 - mae: 3.8023 - val_loss: 35.4007 - val_mae: 3.9433\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.7921 - mae: 3.6784 - val_loss: 35.4868 - val_mae: 3.9267\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.6998 - mae: 3.7771 - val_loss: 35.1712 - val_mae: 3.9742\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.6473 - mae: 3.7108 - val_loss: 35.4581 - val_mae: 3.9107\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.6507 - mae: 3.7703 - val_loss: 35.1249 - val_mae: 3.9570\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.5459 - mae: 3.7564 - val_loss: 35.1814 - val_mae: 3.9210\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.4965 - mae: 3.6998 - val_loss: 35.0755 - val_mae: 3.9251\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.4827 - mae: 3.7326 - val_loss: 35.0513 - val_mae: 3.9319\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.4512 - mae: 3.6766 - val_loss: 35.0240 - val_mae: 3.9257\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.5427 - mae: 3.7939 - val_loss: 34.6788 - val_mae: 4.0306\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.2296 - mae: 3.7190 - val_loss: 35.1612 - val_mae: 3.8792\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.3606 - mae: 3.6210 - val_loss: 34.8754 - val_mae: 3.9225\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.2633 - mae: 3.7597 - val_loss: 34.6710 - val_mae: 4.0016\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.2706 - mae: 3.7224 - val_loss: 34.8820 - val_mae: 3.9055\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.1943 - mae: 3.6659 - val_loss: 34.7818 - val_mae: 3.9167\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.1848 - mae: 3.7504 - val_loss: 34.5670 - val_mae: 3.9794\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.2338 - mae: 3.6597 - val_loss: 34.8711 - val_mae: 3.8996\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 32.0617 - mae: 3.6790 - val_loss: 34.6476 - val_mae: 3.9364\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 32.3586 - mae: 3.7843 - val_loss: 34.7064 - val_mae: 3.8991\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 20.5365 - mae: 3.1889\n",
      "Mean Absolute Error on Test Data: 3.1888835430145264\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.1231254581642941\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 165.5509 - mae: 9.6044 - val_loss: 136.4137 - val_mae: 8.3148\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 145.1095 - mae: 8.4936 - val_loss: 112.9705 - val_mae: 6.8702\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 114.3733 - mae: 6.7869 - val_loss: 83.9625 - val_mae: 5.1999\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 83.4236 - mae: 5.4136 - val_loss: 68.2777 - val_mae: 5.1066\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 73.5193 - mae: 5.5867 - val_loss: 70.1376 - val_mae: 5.7687\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 72.5264 - mae: 5.6924 - val_loss: 67.8839 - val_mae: 5.4273\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 71.8755 - mae: 5.3997 - val_loss: 67.0779 - val_mae: 5.2789\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 71.1266 - mae: 5.4701 - val_loss: 67.3640 - val_mae: 5.4196\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 70.2887 - mae: 5.4420 - val_loss: 66.6387 - val_mae: 5.3020\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 69.8597 - mae: 5.3046 - val_loss: 66.0995 - val_mae: 5.1892\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 69.7298 - mae: 5.4071 - val_loss: 66.4742 - val_mae: 5.3585\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 68.7329 - mae: 5.3751 - val_loss: 65.8339 - val_mae: 5.2445\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 68.3231 - mae: 5.2684 - val_loss: 65.4770 - val_mae: 5.1762\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 67.9832 - mae: 5.2319 - val_loss: 65.3379 - val_mae: 5.1784\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 67.5794 - mae: 5.2602 - val_loss: 65.2980 - val_mae: 5.2002\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 67.2781 - mae: 5.2582 - val_loss: 65.0924 - val_mae: 5.1591\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 67.0341 - mae: 5.2482 - val_loss: 64.9062 - val_mae: 5.1441\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 66.8281 - mae: 5.2728 - val_loss: 65.2409 - val_mae: 5.2256\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 66.3762 - mae: 5.2064 - val_loss: 64.6197 - val_mae: 5.0847\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 66.2462 - mae: 5.1725 - val_loss: 64.4719 - val_mae: 5.0617\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 65.8780 - mae: 5.1920 - val_loss: 64.7148 - val_mae: 5.1327\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 65.7113 - mae: 5.2156 - val_loss: 64.5190 - val_mae: 5.0972\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 65.6766 - mae: 5.2174 - val_loss: 64.3197 - val_mae: 5.0501\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 65.3896 - mae: 5.1508 - val_loss: 64.4209 - val_mae: 5.0769\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 65.2635 - mae: 5.1815 - val_loss: 64.2365 - val_mae: 5.0321\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 65.0263 - mae: 5.1632 - val_loss: 64.2603 - val_mae: 5.0457\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.8660 - mae: 5.1534 - val_loss: 64.1889 - val_mae: 5.0343\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.8744 - mae: 5.1803 - val_loss: 64.3345 - val_mae: 5.0667\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 64.7629 - mae: 5.1201 - val_loss: 63.9445 - val_mae: 4.9371\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 64.6014 - mae: 5.0925 - val_loss: 64.1313 - val_mae: 5.0306\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.9220 - mae: 5.2450 - val_loss: 64.2110 - val_mae: 5.0528\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 64.4098 - mae: 5.0733 - val_loss: 63.8179 - val_mae: 4.9425\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.2739 - mae: 5.0541 - val_loss: 63.8471 - val_mae: 4.9795\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.0270 - mae: 5.1441 - val_loss: 64.2279 - val_mae: 5.0886\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.8590 - mae: 5.1233 - val_loss: 63.7560 - val_mae: 4.9802\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.9991 - mae: 5.0488 - val_loss: 63.7134 - val_mae: 4.9782\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 64.0125 - mae: 5.2527 - val_loss: 64.8638 - val_mae: 5.1795\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.7548 - mae: 5.0810 - val_loss: 63.6790 - val_mae: 4.8711\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.8481 - mae: 5.0570 - val_loss: 63.6948 - val_mae: 4.9372\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.6440 - mae: 5.0282 - val_loss: 63.7268 - val_mae: 4.9718\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.3031 - mae: 5.0731 - val_loss: 63.6669 - val_mae: 4.9720\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.4376 - mae: 5.1034 - val_loss: 63.5301 - val_mae: 4.9446\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.3886 - mae: 4.9974 - val_loss: 63.4427 - val_mae: 4.9321\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.1698 - mae: 5.0747 - val_loss: 63.5909 - val_mae: 4.9806\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 63.1118 - mae: 5.0816 - val_loss: 63.4821 - val_mae: 4.9565\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.9623 - mae: 5.0432 - val_loss: 63.3096 - val_mae: 4.9544\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 62.7993 - mae: 5.0637 - val_loss: 63.5676 - val_mae: 5.0166\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.1418 - mae: 5.1480 - val_loss: 63.5221 - val_mae: 4.9914\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.7031 - mae: 5.0131 - val_loss: 63.0906 - val_mae: 4.8860\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 62.6742 - mae: 4.9956 - val_loss: 63.1006 - val_mae: 4.9202\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 52.5732 - mae: 5.1519\n",
      "Mean Absolute Error on Test Data: 5.1518683433532715\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "R-squared: 0.11913022892962566\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 94.8673 - mae: 8.0904 - val_loss: 92.6880 - val_mae: 7.8631\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 77.6699 - mae: 7.0018 - val_loss: 71.7969 - val_mae: 6.5141\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 55.3198 - mae: 5.3652 - val_loss: 45.4355 - val_mae: 4.6546\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 34.6547 - mae: 4.0137 - val_loss: 30.4831 - val_mae: 3.9673\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.6760 - mae: 3.9306 - val_loss: 29.3630 - val_mae: 4.0486\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4718 - mae: 3.9565 - val_loss: 29.4791 - val_mae: 3.9928\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.3989 - mae: 3.8563 - val_loss: 29.4951 - val_mae: 3.9733\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.1843 - mae: 3.8893 - val_loss: 29.0235 - val_mae: 4.0111\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.2826 - mae: 3.8789 - val_loss: 29.2347 - val_mae: 3.9674\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.1193 - mae: 3.8822 - val_loss: 28.9664 - val_mae: 3.9765\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0837 - mae: 3.8980 - val_loss: 28.8169 - val_mae: 3.9757\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0298 - mae: 3.8879 - val_loss: 28.8043 - val_mae: 3.9575\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0041 - mae: 3.8684 - val_loss: 28.6937 - val_mae: 3.9582\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.0155 - mae: 3.9164 - val_loss: 28.5300 - val_mae: 3.9616\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.1576 - mae: 3.8447 - val_loss: 28.8300 - val_mae: 3.9302\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8929 - mae: 3.9046 - val_loss: 28.3211 - val_mae: 3.9801\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.9427 - mae: 3.8900 - val_loss: 28.6401 - val_mae: 3.9325\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9011 - mae: 3.8551 - val_loss: 28.3999 - val_mae: 3.9391\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8769 - mae: 3.8898 - val_loss: 28.3542 - val_mae: 3.9313\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8988 - mae: 3.8609 - val_loss: 28.2213 - val_mae: 3.9403\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.8266 - mae: 3.9053 - val_loss: 28.0432 - val_mae: 3.9529\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8114 - mae: 3.8922 - val_loss: 28.3314 - val_mae: 3.9139\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8200 - mae: 3.8628 - val_loss: 28.0944 - val_mae: 3.9266\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8191 - mae: 3.8578 - val_loss: 28.0762 - val_mae: 3.9249\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9792 - mae: 3.9734 - val_loss: 27.8278 - val_mae: 3.9406\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.7337 - mae: 3.8622 - val_loss: 28.1586 - val_mae: 3.8994\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6876 - mae: 3.8582 - val_loss: 27.9082 - val_mae: 3.9237\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0034 - mae: 3.9674 - val_loss: 27.9369 - val_mae: 3.9116\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8151 - mae: 3.8506 - val_loss: 28.0029 - val_mae: 3.9061\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7650 - mae: 3.8883 - val_loss: 27.7980 - val_mae: 3.9203\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.6848 - mae: 3.8979 - val_loss: 27.7935 - val_mae: 3.9104\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.7545 - mae: 3.8613 - val_loss: 27.9775 - val_mae: 3.8908\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6703 - mae: 3.8484 - val_loss: 27.7332 - val_mae: 3.8986\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7130 - mae: 3.9032 - val_loss: 27.7093 - val_mae: 3.8954\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6508 - mae: 3.8827 - val_loss: 27.7287 - val_mae: 3.8970\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6692 - mae: 3.8478 - val_loss: 27.7807 - val_mae: 3.8859\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6053 - mae: 3.8930 - val_loss: 27.5030 - val_mae: 3.9078\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.6870 - mae: 3.8577 - val_loss: 27.7076 - val_mae: 3.8882\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9898 - mae: 3.9730 - val_loss: 27.4866 - val_mae: 3.9152\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0282 - mae: 3.8394 - val_loss: 27.9870 - val_mae: 3.8753\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.6437 - mae: 3.8768 - val_loss: 27.5472 - val_mae: 3.8966\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.5214 - mae: 3.8547 - val_loss: 27.7693 - val_mae: 3.8805\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.5421 - mae: 3.8291 - val_loss: 27.5287 - val_mae: 3.8916\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.5965 - mae: 3.9115 - val_loss: 27.3680 - val_mae: 3.9141\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7085 - mae: 3.8490 - val_loss: 27.8718 - val_mae: 3.8737\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.5812 - mae: 3.8565 - val_loss: 27.6155 - val_mae: 3.8853\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.5126 - mae: 3.8506 - val_loss: 27.5258 - val_mae: 3.8887\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.4747 - mae: 3.8815 - val_loss: 27.4173 - val_mae: 3.8967\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.6105 - mae: 3.8206 - val_loss: 27.6455 - val_mae: 3.8731\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.4977 - mae: 3.8615 - val_loss: 27.2694 - val_mae: 3.9112\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 30.8606 - mae: 4.0368\n",
      "Mean Absolute Error on Test Data: 4.036815166473389\n",
      "11/11 [==============================] - 0s 828us/step\n",
      "R-squared: 0.044985119383052496\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 75.9999 - mae: 7.2530 - val_loss: 61.3793 - val_mae: 6.2509\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 56.1139 - mae: 5.8203 - val_loss: 40.8785 - val_mae: 4.5624\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 34.4518 - mae: 4.1073 - val_loss: 23.2511 - val_mae: 3.2658\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.8164 - mae: 3.5098 - val_loss: 22.4860 - val_mae: 3.5710\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.9474 - mae: 3.6603 - val_loss: 21.7998 - val_mae: 3.4154\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.4817 - mae: 3.5466 - val_loss: 21.6618 - val_mae: 3.3648\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.5698 - mae: 3.5550 - val_loss: 21.5975 - val_mae: 3.3566\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3762 - mae: 3.4935 - val_loss: 21.5258 - val_mae: 3.3087\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.4115 - mae: 3.4890 - val_loss: 21.5397 - val_mae: 3.3247\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.3838 - mae: 3.5027 - val_loss: 21.5327 - val_mae: 3.3272\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.4685 - mae: 3.5426 - val_loss: 21.5760 - val_mae: 3.3569\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.4105 - mae: 3.4856 - val_loss: 21.4835 - val_mae: 3.3138\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3835 - mae: 3.5525 - val_loss: 21.6724 - val_mae: 3.4025\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.6701 - mae: 3.4961 - val_loss: 21.4711 - val_mae: 3.2988\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3199 - mae: 3.5550 - val_loss: 21.6936 - val_mae: 3.4125\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.2999 - mae: 3.5358 - val_loss: 21.5477 - val_mae: 3.3641\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.3286 - mae: 3.4865 - val_loss: 21.4844 - val_mae: 3.3312\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2695 - mae: 3.5146 - val_loss: 21.4651 - val_mae: 3.3166\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2804 - mae: 3.4860 - val_loss: 21.5051 - val_mae: 3.3428\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2394 - mae: 3.5041 - val_loss: 21.4628 - val_mae: 3.3271\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.2330 - mae: 3.5136 - val_loss: 21.5360 - val_mae: 3.3668\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1892 - mae: 3.4896 - val_loss: 21.4409 - val_mae: 3.3135\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1632 - mae: 3.5097 - val_loss: 21.4481 - val_mae: 3.3385\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1254 - mae: 3.4972 - val_loss: 21.4606 - val_mae: 3.3477\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1101 - mae: 3.4973 - val_loss: 21.4140 - val_mae: 3.3156\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1342 - mae: 3.4998 - val_loss: 21.4324 - val_mae: 3.3405\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0597 - mae: 3.4920 - val_loss: 21.3870 - val_mae: 3.3178\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1210 - mae: 3.5132 - val_loss: 21.3844 - val_mae: 3.3180\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0588 - mae: 3.4664 - val_loss: 21.3494 - val_mae: 3.2954\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.1101 - mae: 3.4607 - val_loss: 21.4338 - val_mae: 3.3428\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9868 - mae: 3.5014 - val_loss: 21.4927 - val_mae: 3.3739\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0306 - mae: 3.5185 - val_loss: 21.3551 - val_mae: 3.3099\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 22.1839 - mae: 3.4429 - val_loss: 21.3578 - val_mae: 3.3206\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9893 - mae: 3.5001 - val_loss: 21.4786 - val_mae: 3.3722\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9963 - mae: 3.4678 - val_loss: 21.3632 - val_mae: 3.3105\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.9641 - mae: 3.4884 - val_loss: 21.4171 - val_mae: 3.3481\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.9147 - mae: 3.4903 - val_loss: 21.3818 - val_mae: 3.3362\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.0405 - mae: 3.4849 - val_loss: 21.3910 - val_mae: 3.3429\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 21.9981 - mae: 3.5370 - val_loss: 21.4592 - val_mae: 3.3714\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.8716 - mae: 3.4851 - val_loss: 21.3441 - val_mae: 3.3181\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9137 - mae: 3.4786 - val_loss: 21.2989 - val_mae: 3.2954\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 22.1010 - mae: 3.4310 - val_loss: 21.3261 - val_mae: 3.3326\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.8443 - mae: 3.5029 - val_loss: 21.6457 - val_mae: 3.4512\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9887 - mae: 3.5106 - val_loss: 21.1902 - val_mae: 3.2920\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8817 - mae: 3.5031 - val_loss: 21.2206 - val_mae: 3.3151\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.7838 - mae: 3.4696 - val_loss: 21.2183 - val_mae: 3.2898\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8779 - mae: 3.4705 - val_loss: 21.2361 - val_mae: 3.3266\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.9164 - mae: 3.4397 - val_loss: 21.2708 - val_mae: 3.3140\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.8655 - mae: 3.4964 - val_loss: 21.2702 - val_mae: 3.3345\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 21.7776 - mae: 3.4491 - val_loss: 21.2290 - val_mae: 3.3128\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 24.3228 - mae: 3.4513\n",
      "Mean Absolute Error on Test Data: 3.4512710571289062\n",
      "11/11 [==============================] - 0s 778us/step\n",
      "R-squared: 0.08617372852017213\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "12/12 [==============================] - 1s 17ms/step - loss: 6.8696 - mae: 1.7375 - val_loss: 8.6688 - val_mae: 1.7549\n",
      "Epoch 2/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 5.3609 - mae: 1.3327 - val_loss: 6.9594 - val_mae: 1.3862\n",
      "Epoch 3/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.3546 - mae: 1.2031 - val_loss: 5.9720 - val_mae: 1.3242\n",
      "Epoch 4/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.1387 - mae: 1.2594 - val_loss: 5.8094 - val_mae: 1.3607\n",
      "Epoch 5/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.1098 - mae: 1.2515 - val_loss: 5.8574 - val_mae: 1.3213\n",
      "Epoch 6/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0816 - mae: 1.2061 - val_loss: 5.8836 - val_mae: 1.3081\n",
      "Epoch 7/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0631 - mae: 1.2024 - val_loss: 5.8111 - val_mae: 1.3086\n",
      "Epoch 8/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0423 - mae: 1.2018 - val_loss: 5.7837 - val_mae: 1.3060\n",
      "Epoch 9/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0286 - mae: 1.1957 - val_loss: 5.7762 - val_mae: 1.2993\n",
      "Epoch 10/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0124 - mae: 1.1937 - val_loss: 5.7442 - val_mae: 1.3001\n",
      "Epoch 11/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0085 - mae: 1.1937 - val_loss: 5.7586 - val_mae: 1.2961\n",
      "Epoch 12/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 4.0032 - mae: 1.2016 - val_loss: 5.6997 - val_mae: 1.3062\n",
      "Epoch 13/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 4.0043 - mae: 1.1910 - val_loss: 5.7646 - val_mae: 1.2945\n",
      "Epoch 14/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9787 - mae: 1.1936 - val_loss: 5.6939 - val_mae: 1.3048\n",
      "Epoch 15/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.9837 - mae: 1.2118 - val_loss: 5.7105 - val_mae: 1.3007\n",
      "Epoch 16/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.9840 - mae: 1.1839 - val_loss: 5.7720 - val_mae: 1.2943\n",
      "Epoch 17/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9587 - mae: 1.1880 - val_loss: 5.6993 - val_mae: 1.3045\n",
      "Epoch 18/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9814 - mae: 1.2234 - val_loss: 5.6882 - val_mae: 1.3108\n",
      "Epoch 19/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9610 - mae: 1.1895 - val_loss: 5.7375 - val_mae: 1.2997\n",
      "Epoch 20/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9706 - mae: 1.2008 - val_loss: 5.6917 - val_mae: 1.3103\n",
      "Epoch 21/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9554 - mae: 1.1852 - val_loss: 5.7487 - val_mae: 1.2993\n",
      "Epoch 22/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.9433 - mae: 1.1860 - val_loss: 5.7005 - val_mae: 1.3086\n",
      "Epoch 23/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.9497 - mae: 1.2107 - val_loss: 5.6732 - val_mae: 1.3190\n",
      "Epoch 24/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9339 - mae: 1.1994 - val_loss: 5.7343 - val_mae: 1.3043\n",
      "Epoch 25/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9358 - mae: 1.1860 - val_loss: 5.7223 - val_mae: 1.3052\n",
      "Epoch 26/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9398 - mae: 1.1930 - val_loss: 5.6917 - val_mae: 1.3159\n",
      "Epoch 27/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9292 - mae: 1.1972 - val_loss: 5.7315 - val_mae: 1.3072\n",
      "Epoch 28/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.9340 - mae: 1.1933 - val_loss: 5.7454 - val_mae: 1.3085\n",
      "Epoch 29/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9217 - mae: 1.1942 - val_loss: 5.6936 - val_mae: 1.3194\n",
      "Epoch 30/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9244 - mae: 1.2002 - val_loss: 5.7099 - val_mae: 1.3158\n",
      "Epoch 31/50\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 3.9200 - mae: 1.2045 - val_loss: 5.7009 - val_mae: 1.3186\n",
      "Epoch 32/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9098 - mae: 1.1950 - val_loss: 5.7343 - val_mae: 1.3127\n",
      "Epoch 33/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9119 - mae: 1.1972 - val_loss: 5.7441 - val_mae: 1.3144\n",
      "Epoch 34/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9104 - mae: 1.1983 - val_loss: 5.7198 - val_mae: 1.3169\n",
      "Epoch 35/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9041 - mae: 1.1901 - val_loss: 5.7383 - val_mae: 1.3146\n",
      "Epoch 36/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9124 - mae: 1.2065 - val_loss: 5.6892 - val_mae: 1.3279\n",
      "Epoch 37/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8966 - mae: 1.2062 - val_loss: 5.7643 - val_mae: 1.3133\n",
      "Epoch 38/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9036 - mae: 1.1871 - val_loss: 5.7381 - val_mae: 1.3179\n",
      "Epoch 39/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.9112 - mae: 1.2046 - val_loss: 5.7084 - val_mae: 1.3257\n",
      "Epoch 40/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.8899 - mae: 1.2074 - val_loss: 5.7621 - val_mae: 1.3168\n",
      "Epoch 41/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8998 - mae: 1.1829 - val_loss: 5.7382 - val_mae: 1.3190\n",
      "Epoch 42/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8873 - mae: 1.1993 - val_loss: 5.7315 - val_mae: 1.3220\n",
      "Epoch 43/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8943 - mae: 1.1950 - val_loss: 5.7764 - val_mae: 1.3153\n",
      "Epoch 44/50\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 3.8909 - mae: 1.1946 - val_loss: 5.7018 - val_mae: 1.3373\n",
      "Epoch 45/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.8894 - mae: 1.2065 - val_loss: 5.7335 - val_mae: 1.3259\n",
      "Epoch 46/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.8857 - mae: 1.1910 - val_loss: 5.7591 - val_mae: 1.3209\n",
      "Epoch 47/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.8764 - mae: 1.2060 - val_loss: 5.7079 - val_mae: 1.3369\n",
      "Epoch 48/50\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 3.8749 - mae: 1.2053 - val_loss: 5.7599 - val_mae: 1.3241\n",
      "Epoch 49/50\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 3.8819 - mae: 1.2026 - val_loss: 5.7653 - val_mae: 1.3211\n",
      "Epoch 50/50\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 3.8708 - mae: 1.1842 - val_loss: 5.7643 - val_mae: 1.3261\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.4986 - mae: 1.1166\n",
      "Mean Absolute Error on Test Data: 1.1165518760681152\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "R-squared: 0.026255554906887024\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 11ms/step - loss: 144.7214 - mae: 9.8678 - val_loss: 150.6962 - val_mae: 9.3377\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 122.8350 - mae: 8.7317 - val_loss: 123.6225 - val_mae: 7.9145\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 91.5374 - mae: 6.9338 - val_loss: 87.8216 - val_mae: 6.0178\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 57.8817 - mae: 5.0567 - val_loss: 63.1450 - val_mae: 5.3241\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 46.4883 - mae: 4.8367 - val_loss: 61.3409 - val_mae: 5.8460\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.8202 - mae: 4.8655 - val_loss: 60.5684 - val_mae: 5.5936\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.2322 - mae: 4.7234 - val_loss: 60.4682 - val_mae: 5.5437\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.2224 - mae: 4.6832 - val_loss: 60.4097 - val_mae: 5.4916\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.1850 - mae: 4.7054 - val_loss: 60.1460 - val_mae: 5.5593\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.9978 - mae: 4.7090 - val_loss: 60.0675 - val_mae: 5.5198\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 45.1478 - mae: 4.6807 - val_loss: 59.8787 - val_mae: 5.5508\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.9867 - mae: 4.6985 - val_loss: 59.8095 - val_mae: 5.5408\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.0355 - mae: 4.6638 - val_loss: 59.7850 - val_mae: 5.4944\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 45.0199 - mae: 4.7708 - val_loss: 59.6142 - val_mae: 5.6002\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.7132 - mae: 4.7022 - val_loss: 59.5358 - val_mae: 5.4934\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.8806 - mae: 4.6415 - val_loss: 59.4949 - val_mae: 5.4782\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.7792 - mae: 4.7066 - val_loss: 59.3671 - val_mae: 5.5652\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.6760 - mae: 4.6765 - val_loss: 59.4244 - val_mae: 5.4569\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.6728 - mae: 4.6697 - val_loss: 59.2835 - val_mae: 5.4932\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.5762 - mae: 4.6899 - val_loss: 59.2061 - val_mae: 5.5526\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.6950 - mae: 4.7445 - val_loss: 59.1178 - val_mae: 5.5201\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.8222 - mae: 4.6399 - val_loss: 59.1783 - val_mae: 5.4364\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 44.5737 - mae: 4.6935 - val_loss: 59.0084 - val_mae: 5.5562\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.4881 - mae: 4.7046 - val_loss: 58.9360 - val_mae: 5.4719\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 8ms/step - loss: 44.5292 - mae: 4.6694 - val_loss: 58.9104 - val_mae: 5.4479\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.5877 - mae: 4.6784 - val_loss: 58.8011 - val_mae: 5.4918\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.5622 - mae: 4.6285 - val_loss: 58.9018 - val_mae: 5.4122\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4885 - mae: 4.6577 - val_loss: 58.6612 - val_mae: 5.4783\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4339 - mae: 4.6638 - val_loss: 58.6805 - val_mae: 5.4613\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4414 - mae: 4.6410 - val_loss: 58.6512 - val_mae: 5.4702\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.7548 - mae: 4.7523 - val_loss: 58.5546 - val_mae: 5.5252\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.2966 - mae: 4.6427 - val_loss: 58.7297 - val_mae: 5.3905\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.5713 - mae: 4.6096 - val_loss: 58.5716 - val_mae: 5.4495\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.4593 - mae: 4.7013 - val_loss: 58.5095 - val_mae: 5.4742\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4127 - mae: 4.6485 - val_loss: 58.5131 - val_mae: 5.4652\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.6126 - mae: 4.7358 - val_loss: 58.4221 - val_mae: 5.4751\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3957 - mae: 4.6522 - val_loss: 58.4034 - val_mae: 5.4552\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3823 - mae: 4.6924 - val_loss: 58.4033 - val_mae: 5.4668\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.4756 - mae: 4.7021 - val_loss: 58.4316 - val_mae: 5.4340\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4155 - mae: 4.6219 - val_loss: 58.4590 - val_mae: 5.4294\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.3111 - mae: 4.6792 - val_loss: 58.3629 - val_mae: 5.4656\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3088 - mae: 4.6823 - val_loss: 58.3218 - val_mae: 5.4554\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3762 - mae: 4.6215 - val_loss: 58.4674 - val_mae: 5.3974\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.3180 - mae: 4.6316 - val_loss: 58.3521 - val_mae: 5.4684\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.4488 - mae: 4.7146 - val_loss: 58.3297 - val_mae: 5.4681\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.5770 - mae: 4.6205 - val_loss: 58.5720 - val_mae: 5.3750\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.1614 - mae: 4.6470 - val_loss: 58.4019 - val_mae: 5.5351\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.5314 - mae: 4.7362 - val_loss: 58.3164 - val_mae: 5.4366\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 44.2140 - mae: 4.6362 - val_loss: 58.2617 - val_mae: 5.4722\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 44.4402 - mae: 4.7164 - val_loss: 58.2728 - val_mae: 5.4375\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 51.3481 - mae: 5.1942\n",
      "Mean Absolute Error on Test Data: 5.194201946258545\n",
      "11/11 [==============================] - 0s 826us/step\n",
      "R-squared: 0.05899433359950723\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 43.1803 - mae: 5.3488 - val_loss: 33.2732 - val_mae: 4.6897\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.2543 - mae: 4.5688 - val_loss: 24.8678 - val_mae: 3.7897\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 24.9732 - mae: 3.5281 - val_loss: 14.9514 - val_mae: 2.7105\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.4911 - mae: 2.8808 - val_loss: 11.2874 - val_mae: 2.6192\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.7659 - mae: 2.9171 - val_loss: 11.5528 - val_mae: 2.6881\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.6324 - mae: 2.8688 - val_loss: 11.2416 - val_mae: 2.6034\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.5643 - mae: 2.8472 - val_loss: 11.2381 - val_mae: 2.6047\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.5178 - mae: 2.8343 - val_loss: 11.2546 - val_mae: 2.6132\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.4449 - mae: 2.8457 - val_loss: 11.2748 - val_mae: 2.6204\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.4024 - mae: 2.8396 - val_loss: 11.2381 - val_mae: 2.6071\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.4254 - mae: 2.8511 - val_loss: 11.3124 - val_mae: 2.6332\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.3763 - mae: 2.8235 - val_loss: 11.2060 - val_mae: 2.5941\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.2982 - mae: 2.8231 - val_loss: 11.2107 - val_mae: 2.5972\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.3003 - mae: 2.8443 - val_loss: 11.2928 - val_mae: 2.6296\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.2203 - mae: 2.8146 - val_loss: 11.1757 - val_mae: 2.5768\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.2045 - mae: 2.8191 - val_loss: 11.2449 - val_mae: 2.6143\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.1718 - mae: 2.8202 - val_loss: 11.1869 - val_mae: 2.5887\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.1234 - mae: 2.8009 - val_loss: 11.1812 - val_mae: 2.5842\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.1205 - mae: 2.8147 - val_loss: 11.2555 - val_mae: 2.6180\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0772 - mae: 2.8218 - val_loss: 11.2111 - val_mae: 2.6026\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0431 - mae: 2.8021 - val_loss: 11.1705 - val_mae: 2.5848\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.1101 - mae: 2.7733 - val_loss: 11.1743 - val_mae: 2.5854\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0426 - mae: 2.8016 - val_loss: 11.2219 - val_mae: 2.6051\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 14.0967 - mae: 2.8505 - val_loss: 11.2898 - val_mae: 2.6244\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0567 - mae: 2.7714 - val_loss: 11.1474 - val_mae: 2.5660\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9551 - mae: 2.7911 - val_loss: 11.2463 - val_mae: 2.6120\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9618 - mae: 2.7920 - val_loss: 11.1981 - val_mae: 2.5966\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9616 - mae: 2.8014 - val_loss: 11.2759 - val_mae: 2.6188\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 13.8967 - mae: 2.8051 - val_loss: 11.1895 - val_mae: 2.5878\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.0069 - mae: 2.7520 - val_loss: 11.1967 - val_mae: 2.5910\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.9025 - mae: 2.8051 - val_loss: 11.2462 - val_mae: 2.6094\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8791 - mae: 2.7771 - val_loss: 11.2269 - val_mae: 2.6011\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8784 - mae: 2.7947 - val_loss: 11.2300 - val_mae: 2.6010\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8298 - mae: 2.7781 - val_loss: 11.2098 - val_mae: 2.5926\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8755 - mae: 2.7735 - val_loss: 11.1950 - val_mae: 2.5827\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8248 - mae: 2.7753 - val_loss: 11.2909 - val_mae: 2.6191\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8226 - mae: 2.7790 - val_loss: 11.2222 - val_mae: 2.5958\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8655 - mae: 2.8084 - val_loss: 11.2728 - val_mae: 2.6134\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7926 - mae: 2.7527 - val_loss: 11.2114 - val_mae: 2.5853\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.8590 - mae: 2.8025 - val_loss: 11.2771 - val_mae: 2.6142\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8471 - mae: 2.7460 - val_loss: 11.2212 - val_mae: 2.5812\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.8029 - mae: 2.7925 - val_loss: 11.3016 - val_mae: 2.6210\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7524 - mae: 2.7844 - val_loss: 11.2511 - val_mae: 2.5986\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7603 - mae: 2.7484 - val_loss: 11.2503 - val_mae: 2.5998\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7408 - mae: 2.7745 - val_loss: 11.2693 - val_mae: 2.6104\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7336 - mae: 2.7783 - val_loss: 11.3305 - val_mae: 2.6306\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7123 - mae: 2.7828 - val_loss: 11.2542 - val_mae: 2.6004\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7283 - mae: 2.7506 - val_loss: 11.2724 - val_mae: 2.6121\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 13.7173 - mae: 2.7750 - val_loss: 11.3066 - val_mae: 2.6284\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 13.7148 - mae: 2.7821 - val_loss: 11.2261 - val_mae: 2.5922\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 16.8629 - mae: 3.0039\n",
      "Mean Absolute Error on Test Data: 3.0039196014404297\n",
      "11/11 [==============================] - 0s 830us/step\n",
      "R-squared: 0.017301513912422184\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "14/14 [==============================] - 0s 13ms/step - loss: 10.9822 - mae: 2.2148 - val_loss: 8.9752 - val_mae: 2.0474\n",
      "Epoch 2/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 9.0452 - mae: 1.8406 - val_loss: 7.0646 - val_mae: 1.5807\n",
      "Epoch 3/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 7.2091 - mae: 1.4812 - val_loss: 5.4526 - val_mae: 1.3878\n",
      "Epoch 4/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 6.0375 - mae: 1.4357 - val_loss: 5.0181 - val_mae: 1.4588\n",
      "Epoch 5/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.9459 - mae: 1.5824 - val_loss: 5.0217 - val_mae: 1.5032\n",
      "Epoch 6/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.8734 - mae: 1.5504 - val_loss: 4.9847 - val_mae: 1.4342\n",
      "Epoch 7/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.8272 - mae: 1.5040 - val_loss: 4.9733 - val_mae: 1.4336\n",
      "Epoch 8/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.7860 - mae: 1.5097 - val_loss: 4.9636 - val_mae: 1.4460\n",
      "Epoch 9/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.7407 - mae: 1.5070 - val_loss: 4.9534 - val_mae: 1.4386\n",
      "Epoch 10/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.7079 - mae: 1.5075 - val_loss: 4.9473 - val_mae: 1.4522\n",
      "Epoch 11/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6753 - mae: 1.4783 - val_loss: 4.9421 - val_mae: 1.4436\n",
      "Epoch 12/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.6115 - mae: 1.5013 - val_loss: 4.9503 - val_mae: 1.4798\n",
      "Epoch 13/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.5765 - mae: 1.5013 - val_loss: 4.9431 - val_mae: 1.4561\n",
      "Epoch 14/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5381 - mae: 1.4620 - val_loss: 4.9558 - val_mae: 1.4384\n",
      "Epoch 15/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.5172 - mae: 1.4707 - val_loss: 4.9634 - val_mae: 1.4796\n",
      "Epoch 16/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4802 - mae: 1.4697 - val_loss: 4.9713 - val_mae: 1.4694\n",
      "Epoch 17/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4677 - mae: 1.4809 - val_loss: 4.9930 - val_mae: 1.4914\n",
      "Epoch 18/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4579 - mae: 1.4608 - val_loss: 4.9984 - val_mae: 1.4514\n",
      "Epoch 19/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4173 - mae: 1.4632 - val_loss: 5.0239 - val_mae: 1.5164\n",
      "Epoch 20/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4340 - mae: 1.5047 - val_loss: 5.0400 - val_mae: 1.5102\n",
      "Epoch 21/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.4015 - mae: 1.4661 - val_loss: 5.0376 - val_mae: 1.4892\n",
      "Epoch 22/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.4075 - mae: 1.4515 - val_loss: 5.0490 - val_mae: 1.4915\n",
      "Epoch 23/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3726 - mae: 1.4559 - val_loss: 5.0502 - val_mae: 1.4822\n",
      "Epoch 24/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3966 - mae: 1.4753 - val_loss: 5.0662 - val_mae: 1.5009\n",
      "Epoch 25/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3899 - mae: 1.4385 - val_loss: 5.0790 - val_mae: 1.4629\n",
      "Epoch 26/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3664 - mae: 1.4598 - val_loss: 5.0875 - val_mae: 1.5280\n",
      "Epoch 27/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3759 - mae: 1.5109 - val_loss: 5.1115 - val_mae: 1.5385\n",
      "Epoch 28/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3325 - mae: 1.4729 - val_loss: 5.0979 - val_mae: 1.5084\n",
      "Epoch 29/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3491 - mae: 1.4391 - val_loss: 5.0987 - val_mae: 1.4967\n",
      "Epoch 30/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3246 - mae: 1.4659 - val_loss: 5.1282 - val_mae: 1.5342\n",
      "Epoch 31/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3203 - mae: 1.4592 - val_loss: 5.1135 - val_mae: 1.5057\n",
      "Epoch 32/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3045 - mae: 1.4516 - val_loss: 5.1364 - val_mae: 1.5250\n",
      "Epoch 33/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3066 - mae: 1.4556 - val_loss: 5.1228 - val_mae: 1.5155\n",
      "Epoch 34/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.3027 - mae: 1.4663 - val_loss: 5.1396 - val_mae: 1.5260\n",
      "Epoch 35/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2885 - mae: 1.4569 - val_loss: 5.1335 - val_mae: 1.5156\n",
      "Epoch 36/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2944 - mae: 1.4548 - val_loss: 5.1413 - val_mae: 1.5306\n",
      "Epoch 37/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2747 - mae: 1.4624 - val_loss: 5.1451 - val_mae: 1.5149\n",
      "Epoch 38/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2748 - mae: 1.4519 - val_loss: 5.1434 - val_mae: 1.5090\n",
      "Epoch 39/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2711 - mae: 1.4560 - val_loss: 5.1497 - val_mae: 1.5395\n",
      "Epoch 40/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2674 - mae: 1.4615 - val_loss: 5.1239 - val_mae: 1.5176\n",
      "Epoch 41/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.3095 - mae: 1.4315 - val_loss: 5.1174 - val_mae: 1.4991\n",
      "Epoch 42/50\n",
      "14/14 [==============================] - 0s 5ms/step - loss: 5.2533 - mae: 1.4933 - val_loss: 5.2401 - val_mae: 1.5939\n",
      "Epoch 43/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.2698 - mae: 1.4622 - val_loss: 5.1379 - val_mae: 1.4974\n",
      "Epoch 44/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2784 - mae: 1.4553 - val_loss: 5.1540 - val_mae: 1.5287\n",
      "Epoch 45/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2479 - mae: 1.4381 - val_loss: 5.1429 - val_mae: 1.4996\n",
      "Epoch 46/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.2504 - mae: 1.4737 - val_loss: 5.1602 - val_mae: 1.5455\n",
      "Epoch 47/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.2394 - mae: 1.4477 - val_loss: 5.1238 - val_mae: 1.4938\n",
      "Epoch 48/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2315 - mae: 1.4599 - val_loss: 5.1724 - val_mae: 1.5424\n",
      "Epoch 49/50\n",
      "14/14 [==============================] - 0s 3ms/step - loss: 5.2132 - mae: 1.4708 - val_loss: 5.1527 - val_mae: 1.5101\n",
      "Epoch 50/50\n",
      "14/14 [==============================] - 0s 4ms/step - loss: 5.2378 - mae: 1.4269 - val_loss: 5.1150 - val_mae: 1.4902\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 4.1808 - mae: 1.3519\n",
      "Mean Absolute Error on Test Data: 1.3518956899642944\n",
      "9/9 [==============================] - 0s 975us/step\n",
      "R-squared: 0.12177389668730576\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 81.5316 - mae: 6.6686 - val_loss: 54.2750 - val_mae: 5.3200\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 71.4105 - mae: 5.8926 - val_loss: 44.3545 - val_mae: 4.4511\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 57.3163 - mae: 4.8443 - val_loss: 32.0217 - val_mae: 3.4167\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 42.1568 - mae: 3.9133 - val_loss: 25.1098 - val_mae: 3.3496\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 36.0575 - mae: 4.0086 - val_loss: 27.0670 - val_mae: 3.8492\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.8116 - mae: 4.1167 - val_loss: 25.7890 - val_mae: 3.6134\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 35.4770 - mae: 4.0495 - val_loss: 25.6696 - val_mae: 3.6020\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.3250 - mae: 4.0034 - val_loss: 25.5461 - val_mae: 3.5929\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.2442 - mae: 3.9785 - val_loss: 25.2698 - val_mae: 3.5508\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 35.0687 - mae: 4.0246 - val_loss: 25.6557 - val_mae: 3.6424\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.9666 - mae: 4.0176 - val_loss: 25.3881 - val_mae: 3.5999\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.8851 - mae: 4.0061 - val_loss: 25.0811 - val_mae: 3.5460\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.7538 - mae: 3.9842 - val_loss: 25.3194 - val_mae: 3.6054\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.7098 - mae: 4.0228 - val_loss: 25.2860 - val_mae: 3.6094\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.5870 - mae: 3.9691 - val_loss: 24.8812 - val_mae: 3.5344\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.5242 - mae: 3.9689 - val_loss: 25.2148 - val_mae: 3.6086\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.5763 - mae: 4.0234 - val_loss: 24.8119 - val_mae: 3.5333\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.4033 - mae: 3.9564 - val_loss: 24.9618 - val_mae: 3.5712\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.4185 - mae: 3.9760 - val_loss: 25.1151 - val_mae: 3.6040\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.3210 - mae: 3.9794 - val_loss: 25.0208 - val_mae: 3.5886\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.3070 - mae: 3.9754 - val_loss: 24.7840 - val_mae: 3.5450\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.2624 - mae: 3.9945 - val_loss: 25.0468 - val_mae: 3.5980\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.2426 - mae: 3.9919 - val_loss: 24.8912 - val_mae: 3.5729\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.2364 - mae: 3.9529 - val_loss: 24.7860 - val_mae: 3.5542\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.2133 - mae: 3.9265 - val_loss: 24.5981 - val_mae: 3.5165\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.0767 - mae: 3.9644 - val_loss: 25.1228 - val_mae: 3.6188\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.3313 - mae: 3.9461 - val_loss: 24.6527 - val_mae: 3.5309\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.0509 - mae: 4.0127 - val_loss: 25.2775 - val_mae: 3.6522\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 34.0588 - mae: 3.9919 - val_loss: 24.7152 - val_mae: 3.5496\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 34.0725 - mae: 3.9918 - val_loss: 25.0039 - val_mae: 3.6068\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.9174 - mae: 3.9444 - val_loss: 24.4819 - val_mae: 3.5076\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.9865 - mae: 3.9497 - val_loss: 24.6479 - val_mae: 3.5413\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.9719 - mae: 3.9641 - val_loss: 24.8588 - val_mae: 3.5859\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.9143 - mae: 3.9737 - val_loss: 24.5427 - val_mae: 3.5237\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.8886 - mae: 3.9453 - val_loss: 24.6469 - val_mae: 3.5509\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.9118 - mae: 3.9037 - val_loss: 24.5118 - val_mae: 3.5221\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.8776 - mae: 3.9728 - val_loss: 24.8617 - val_mae: 3.5966\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.8415 - mae: 3.9958 - val_loss: 24.8134 - val_mae: 3.5879\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7950 - mae: 3.9209 - val_loss: 24.4695 - val_mae: 3.5241\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7479 - mae: 3.9525 - val_loss: 24.7891 - val_mae: 3.5924\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.7826 - mae: 3.9208 - val_loss: 24.5265 - val_mae: 3.5398\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7701 - mae: 3.9699 - val_loss: 25.0992 - val_mae: 3.6555\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7563 - mae: 3.9310 - val_loss: 24.5085 - val_mae: 3.5444\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.6344 - mae: 3.9514 - val_loss: 24.8149 - val_mae: 3.6057\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.6196 - mae: 3.9367 - val_loss: 24.5827 - val_mae: 3.5665\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.6220 - mae: 3.9713 - val_loss: 24.5070 - val_mae: 3.5499\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 33.8484 - mae: 3.8685 - val_loss: 24.0804 - val_mae: 3.4592\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.7030 - mae: 4.0180 - val_loss: 25.2138 - val_mae: 3.6926\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.6633 - mae: 3.9353 - val_loss: 24.3086 - val_mae: 3.5206\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 33.5187 - mae: 3.9561 - val_loss: 24.8567 - val_mae: 3.6270\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 34.0141 - mae: 3.8160\n",
      "Mean Absolute Error on Test Data: 3.8159990310668945\n",
      "11/11 [==============================] - 0s 855us/step\n",
      "R-squared: 0.058596877442107176\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 107.4494 - mae: 8.0249 - val_loss: 84.0721 - val_mae: 7.3278\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 85.1258 - mae: 6.5886 - val_loss: 58.8755 - val_mae: 5.5677\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 58.9941 - mae: 4.7330 - val_loss: 34.3846 - val_mae: 4.0101\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.9369 - mae: 4.1901 - val_loss: 29.4654 - val_mae: 4.2058\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4119 - mae: 4.3517 - val_loss: 29.2895 - val_mae: 4.0925\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1344 - mae: 4.2236 - val_loss: 29.3251 - val_mae: 4.0475\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1861 - mae: 4.1739 - val_loss: 29.3258 - val_mae: 4.0412\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.0935 - mae: 4.2594 - val_loss: 29.2594 - val_mae: 4.0749\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0044 - mae: 4.2045 - val_loss: 29.2711 - val_mae: 4.0428\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.8698 - mae: 4.2146 - val_loss: 29.2549 - val_mae: 4.0450\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.8756 - mae: 4.1910 - val_loss: 29.2442 - val_mae: 4.0416\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.8821 - mae: 4.2410 - val_loss: 29.1674 - val_mae: 4.0748\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.7175 - mae: 4.1728 - val_loss: 29.2777 - val_mae: 4.0122\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6568 - mae: 4.1854 - val_loss: 29.1658 - val_mae: 4.0467\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9317 - mae: 4.2608 - val_loss: 29.1297 - val_mae: 4.1103\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.5148 - mae: 4.2434 - val_loss: 29.1934 - val_mae: 4.0291\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6337 - mae: 4.1657 - val_loss: 29.1035 - val_mae: 4.0411\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.5359 - mae: 4.2316 - val_loss: 29.1203 - val_mae: 4.0358\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.4753 - mae: 4.1991 - val_loss: 29.0658 - val_mae: 4.0769\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.4470 - mae: 4.2670 - val_loss: 29.0957 - val_mae: 4.0985\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.6001 - mae: 4.1700 - val_loss: 29.3042 - val_mae: 4.0071\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2772 - mae: 4.2165 - val_loss: 29.0744 - val_mae: 4.0975\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.3088 - mae: 4.2521 - val_loss: 29.0632 - val_mae: 4.0503\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.3766 - mae: 4.2761 - val_loss: 29.0385 - val_mae: 4.0800\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2779 - mae: 4.2920 - val_loss: 29.0311 - val_mae: 4.0914\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.1056 - mae: 4.2095 - val_loss: 29.0550 - val_mae: 4.0391\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.1499 - mae: 4.2196 - val_loss: 29.0061 - val_mae: 4.0606\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2689 - mae: 4.1865 - val_loss: 29.0024 - val_mae: 4.0700\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.1168 - mae: 4.2412 - val_loss: 29.1045 - val_mae: 4.0213\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.0417 - mae: 4.1462 - val_loss: 28.9654 - val_mae: 4.0562\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.0477 - mae: 4.2823 - val_loss: 28.9604 - val_mae: 4.1010\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.0900 - mae: 4.3236 - val_loss: 29.0932 - val_mae: 4.0678\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8824 - mae: 4.1828 - val_loss: 29.2532 - val_mae: 4.0138\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.9345 - mae: 4.1272 - val_loss: 29.1906 - val_mae: 4.0091\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.2596 - mae: 4.3177 - val_loss: 28.9038 - val_mae: 4.0986\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.7291 - mae: 4.2429 - val_loss: 29.0210 - val_mae: 4.0334\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.8399 - mae: 4.1457 - val_loss: 29.0154 - val_mae: 4.0281\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6446 - mae: 4.1739 - val_loss: 28.9210 - val_mae: 4.0421\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6129 - mae: 4.1864 - val_loss: 28.8372 - val_mae: 4.0580\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6559 - mae: 4.2447 - val_loss: 28.8209 - val_mae: 4.0738\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6955 - mae: 4.2022 - val_loss: 29.1375 - val_mae: 3.9848\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.5352 - mae: 4.1497 - val_loss: 28.8162 - val_mae: 4.0748\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.5942 - mae: 4.2835 - val_loss: 28.8020 - val_mae: 4.0750\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 40.5685 - mae: 4.1900 - val_loss: 28.8727 - val_mae: 4.0336\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6089 - mae: 4.1434 - val_loss: 28.8283 - val_mae: 4.0370\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 40.3581 - mae: 4.1875 - val_loss: 28.7748 - val_mae: 4.0483\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.3651 - mae: 4.2221 - val_loss: 28.8184 - val_mae: 4.0423\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.4450 - mae: 4.2434 - val_loss: 28.8379 - val_mae: 4.0305\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.2595 - mae: 4.1711 - val_loss: 28.8011 - val_mae: 4.0180\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.4536 - mae: 4.1674 - val_loss: 28.6915 - val_mae: 4.0606\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 43.3143 - mae: 4.2588\n",
      "Mean Absolute Error on Test Data: 4.258849620819092\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.03576121505841401\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 115.6667 - mae: 8.5929 - val_loss: 84.3839 - val_mae: 6.9182\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 87.5081 - mae: 6.8720 - val_loss: 57.2814 - val_mae: 5.0214\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 57.4569 - mae: 5.0401 - val_loss: 38.1257 - val_mae: 4.1342\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.4928 - mae: 4.5223 - val_loss: 37.9901 - val_mae: 4.6150\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.4426 - mae: 4.8046 - val_loss: 37.6354 - val_mae: 4.5810\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.7044 - mae: 4.6898 - val_loss: 36.9386 - val_mae: 4.4827\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 40.6807 - mae: 4.5111 - val_loss: 36.3081 - val_mae: 4.3434\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 40.2434 - mae: 4.5510 - val_loss: 36.7786 - val_mae: 4.4779\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.9688 - mae: 4.5846 - val_loss: 36.6136 - val_mae: 4.4574\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.8429 - mae: 4.5960 - val_loss: 36.6736 - val_mae: 4.4769\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.7126 - mae: 4.5470 - val_loss: 36.3622 - val_mae: 4.4216\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.5926 - mae: 4.5445 - val_loss: 36.3273 - val_mae: 4.4204\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.7411 - mae: 4.4934 - val_loss: 36.2583 - val_mae: 4.4066\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.5904 - mae: 4.5899 - val_loss: 36.5326 - val_mae: 4.4640\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.3690 - mae: 4.5253 - val_loss: 36.2323 - val_mae: 4.4035\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.3021 - mae: 4.5206 - val_loss: 36.3549 - val_mae: 4.4320\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.5638 - mae: 4.5932 - val_loss: 36.4757 - val_mae: 4.4551\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.3505 - mae: 4.4586 - val_loss: 35.8414 - val_mae: 4.3000\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.2987 - mae: 4.5092 - val_loss: 36.3860 - val_mae: 4.4398\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 39.3242 - mae: 4.5981 - val_loss: 36.5365 - val_mae: 4.4638\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.0774 - mae: 4.4759 - val_loss: 36.0485 - val_mae: 4.3626\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.0903 - mae: 4.6093 - val_loss: 37.0938 - val_mae: 4.5475\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.9941 - mae: 4.5525 - val_loss: 36.2034 - val_mae: 4.3888\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.0280 - mae: 4.4408 - val_loss: 36.1752 - val_mae: 4.3837\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.8127 - mae: 4.5530 - val_loss: 37.2709 - val_mae: 4.5647\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.9770 - mae: 4.5261 - val_loss: 36.1852 - val_mae: 4.3822\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.8177 - mae: 4.4805 - val_loss: 36.2898 - val_mae: 4.4061\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.7497 - mae: 4.5245 - val_loss: 36.7109 - val_mae: 4.4895\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.8938 - mae: 4.5394 - val_loss: 36.3909 - val_mae: 4.4317\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.7739 - mae: 4.5309 - val_loss: 36.3286 - val_mae: 4.4222\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.6648 - mae: 4.4808 - val_loss: 36.0414 - val_mae: 4.3625\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.7043 - mae: 4.4684 - val_loss: 36.0302 - val_mae: 4.3630\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.7431 - mae: 4.4332 - val_loss: 36.1079 - val_mae: 4.3828\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.6583 - mae: 4.5152 - val_loss: 36.4640 - val_mae: 4.4558\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.6758 - mae: 4.5267 - val_loss: 36.2039 - val_mae: 4.4093\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.7414 - mae: 4.4329 - val_loss: 35.8963 - val_mae: 4.3417\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.6510 - mae: 4.4797 - val_loss: 36.0580 - val_mae: 4.3803\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.6336 - mae: 4.4355 - val_loss: 35.9267 - val_mae: 4.3471\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.5147 - mae: 4.4808 - val_loss: 36.5258 - val_mae: 4.4760\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.6716 - mae: 4.5692 - val_loss: 35.9771 - val_mae: 4.3794\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.4854 - mae: 4.4281 - val_loss: 35.6835 - val_mae: 4.3036\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.5363 - mae: 4.4230 - val_loss: 36.2118 - val_mae: 4.4325\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.4761 - mae: 4.4782 - val_loss: 36.0454 - val_mae: 4.3964\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 38.4228 - mae: 4.4956 - val_loss: 36.5022 - val_mae: 4.4811\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.4720 - mae: 4.5358 - val_loss: 35.9105 - val_mae: 4.3675\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.3975 - mae: 4.4378 - val_loss: 35.7525 - val_mae: 4.3253\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 38.5623 - mae: 4.4628 - val_loss: 35.9913 - val_mae: 4.3813\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.4107 - mae: 4.4623 - val_loss: 35.9528 - val_mae: 4.3785\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.5454 - mae: 4.4270 - val_loss: 35.8540 - val_mae: 4.3607\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.5808 - mae: 4.5486 - val_loss: 36.4726 - val_mae: 4.4881\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 46.1338 - mae: 4.5132\n",
      "Mean Absolute Error on Test Data: 4.51318883895874\n",
      "11/11 [==============================] - 0s 908us/step\n",
      "R-squared: 0.08505486645877214\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Epoch 1/50\n",
      "10/10 [==============================] - 0s 17ms/step - loss: 4.0140 - mae: 1.5558 - val_loss: 3.3108 - val_mae: 1.2781\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 2.5169 - mae: 1.0697 - val_loss: 2.3224 - val_mae: 0.8786\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.7995 - mae: 0.8458 - val_loss: 1.9257 - val_mae: 0.9237\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5965 - mae: 0.8977 - val_loss: 1.9419 - val_mae: 0.9998\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.6079 - mae: 0.9312 - val_loss: 1.9233 - val_mae: 0.9851\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.5752 - mae: 0.9051 - val_loss: 1.9066 - val_mae: 0.9578\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5729 - mae: 0.8820 - val_loss: 1.9123 - val_mae: 0.9450\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5660 - mae: 0.8828 - val_loss: 1.9115 - val_mae: 0.9606\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5553 - mae: 0.8901 - val_loss: 1.9191 - val_mae: 0.9749\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5514 - mae: 0.8921 - val_loss: 1.9205 - val_mae: 0.9719\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.5443 - mae: 0.8878 - val_loss: 1.9223 - val_mae: 0.9688\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5374 - mae: 0.8740 - val_loss: 1.9267 - val_mae: 0.9560\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5332 - mae: 0.8722 - val_loss: 1.9296 - val_mae: 0.9696\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5267 - mae: 0.8787 - val_loss: 1.9297 - val_mae: 0.9741\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5274 - mae: 0.8862 - val_loss: 1.9331 - val_mae: 0.9804\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5131 - mae: 0.8732 - val_loss: 1.9331 - val_mae: 0.9691\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5127 - mae: 0.8635 - val_loss: 1.9367 - val_mae: 0.9669\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.5294 - mae: 0.8844 - val_loss: 1.9434 - val_mae: 0.9915\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4994 - mae: 0.8610 - val_loss: 1.9497 - val_mae: 0.9625\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5016 - mae: 0.8517 - val_loss: 1.9466 - val_mae: 0.9749\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.5069 - mae: 0.8607 - val_loss: 1.9491 - val_mae: 0.9757\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.5017 - mae: 0.8774 - val_loss: 1.9567 - val_mae: 1.0052\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4907 - mae: 0.8757 - val_loss: 1.9531 - val_mae: 0.9770\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4982 - mae: 0.8463 - val_loss: 1.9559 - val_mae: 0.9672\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4874 - mae: 0.8563 - val_loss: 1.9545 - val_mae: 0.9961\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4846 - mae: 0.8653 - val_loss: 1.9536 - val_mae: 0.9833\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4800 - mae: 0.8514 - val_loss: 1.9561 - val_mae: 0.9750\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4787 - mae: 0.8464 - val_loss: 1.9557 - val_mae: 0.9759\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4790 - mae: 0.8547 - val_loss: 1.9557 - val_mae: 0.9960\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4823 - mae: 0.8582 - val_loss: 1.9529 - val_mae: 0.9782\n",
      "Epoch 31/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4749 - mae: 0.8580 - val_loss: 1.9584 - val_mae: 0.9962\n",
      "Epoch 32/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4731 - mae: 0.8610 - val_loss: 1.9615 - val_mae: 0.9875\n",
      "Epoch 33/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4740 - mae: 0.8490 - val_loss: 1.9615 - val_mae: 0.9788\n",
      "Epoch 34/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4716 - mae: 0.8446 - val_loss: 1.9592 - val_mae: 0.9729\n",
      "Epoch 35/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4663 - mae: 0.8484 - val_loss: 1.9602 - val_mae: 0.9954\n",
      "Epoch 36/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4684 - mae: 0.8534 - val_loss: 1.9548 - val_mae: 0.9810\n",
      "Epoch 37/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4656 - mae: 0.8464 - val_loss: 1.9574 - val_mae: 0.9791\n",
      "Epoch 38/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4720 - mae: 0.8567 - val_loss: 1.9576 - val_mae: 0.9934\n",
      "Epoch 39/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4665 - mae: 0.8438 - val_loss: 1.9588 - val_mae: 0.9684\n",
      "Epoch 40/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4803 - mae: 0.8611 - val_loss: 1.9531 - val_mae: 0.9912\n",
      "Epoch 41/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4620 - mae: 0.8471 - val_loss: 1.9576 - val_mae: 0.9692\n",
      "Epoch 42/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4804 - mae: 0.8350 - val_loss: 1.9571 - val_mae: 0.9681\n",
      "Epoch 43/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4662 - mae: 0.8589 - val_loss: 1.9650 - val_mae: 1.0131\n",
      "Epoch 44/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4694 - mae: 0.8610 - val_loss: 1.9537 - val_mae: 0.9627\n",
      "Epoch 45/50\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 1.4613 - mae: 0.8435 - val_loss: 1.9443 - val_mae: 0.9731\n",
      "Epoch 46/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4687 - mae: 0.8588 - val_loss: 1.9488 - val_mae: 0.9856\n",
      "Epoch 47/50\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 1.4501 - mae: 0.8453 - val_loss: 1.9536 - val_mae: 0.9693\n",
      "Epoch 48/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4567 - mae: 0.8413 - val_loss: 1.9525 - val_mae: 0.9796\n",
      "Epoch 49/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4541 - mae: 0.8554 - val_loss: 1.9498 - val_mae: 0.9898\n",
      "Epoch 50/50\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 1.4686 - mae: 0.8688 - val_loss: 1.9507 - val_mae: 0.9910\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2791 - mae: 0.8349\n",
      "Mean Absolute Error on Test Data: 0.8348994255065918\n",
      "6/6 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.12553812996528546\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 10ms/step - loss: 83.9471 - mae: 7.6896 - val_loss: 103.3425 - val_mae: 7.7189\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 63.2250 - mae: 6.2566 - val_loss: 76.8982 - val_mae: 5.9760\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 39.2810 - mae: 4.4441 - val_loss: 50.6797 - val_mae: 4.2856\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.9921 - mae: 3.5449 - val_loss: 41.7478 - val_mae: 4.2784\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.5273 - mae: 3.7616 - val_loss: 41.6030 - val_mae: 4.2952\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0722 - mae: 3.5966 - val_loss: 42.6937 - val_mae: 4.1447\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 24.1861 - mae: 3.5539 - val_loss: 42.2819 - val_mae: 4.1701\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0243 - mae: 3.5698 - val_loss: 42.1954 - val_mae: 4.1692\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.9765 - mae: 3.5843 - val_loss: 41.9693 - val_mae: 4.1856\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 24.0182 - mae: 3.6258 - val_loss: 41.9579 - val_mae: 4.1870\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.9934 - mae: 3.5633 - val_loss: 42.5484 - val_mae: 4.1425\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.9421 - mae: 3.5711 - val_loss: 41.9311 - val_mae: 4.1825\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.8662 - mae: 3.5853 - val_loss: 42.0096 - val_mae: 4.1637\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.9264 - mae: 3.5498 - val_loss: 42.0393 - val_mae: 4.1509\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.8462 - mae: 3.5838 - val_loss: 41.8739 - val_mae: 4.1621\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.8220 - mae: 3.6094 - val_loss: 41.4678 - val_mae: 4.2015\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.8055 - mae: 3.5614 - val_loss: 42.1156 - val_mae: 4.1328\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.7389 - mae: 3.5669 - val_loss: 41.6722 - val_mae: 4.1632\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.7421 - mae: 3.5524 - val_loss: 41.9669 - val_mae: 4.1318\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.6738 - mae: 3.5743 - val_loss: 41.5128 - val_mae: 4.1800\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6978 - mae: 3.5674 - val_loss: 41.8616 - val_mae: 4.1469\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6976 - mae: 3.5507 - val_loss: 41.8819 - val_mae: 4.1318\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.8065 - mae: 3.6267 - val_loss: 41.4498 - val_mae: 4.1621\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6187 - mae: 3.5346 - val_loss: 41.8197 - val_mae: 4.1202\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 23.6273 - mae: 3.5617 - val_loss: 41.7087 - val_mae: 4.1280\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.6723 - mae: 3.5155 - val_loss: 41.9982 - val_mae: 4.1101\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5087 - mae: 3.5409 - val_loss: 41.5919 - val_mae: 4.1502\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5203 - mae: 3.5767 - val_loss: 41.7284 - val_mae: 4.1321\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4835 - mae: 3.5163 - val_loss: 42.0765 - val_mae: 4.1057\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5517 - mae: 3.5298 - val_loss: 41.3019 - val_mae: 4.1669\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.5139 - mae: 3.5832 - val_loss: 41.5656 - val_mae: 4.1342\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4237 - mae: 3.5346 - val_loss: 41.5575 - val_mae: 4.1468\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.4228 - mae: 3.5584 - val_loss: 41.7141 - val_mae: 4.1465\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.4475 - mae: 3.5087 - val_loss: 41.8427 - val_mae: 4.1175\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3826 - mae: 3.5423 - val_loss: 41.6254 - val_mae: 4.1438\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3528 - mae: 3.5449 - val_loss: 41.7109 - val_mae: 4.1328\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3322 - mae: 3.5381 - val_loss: 41.7144 - val_mae: 4.1356\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3169 - mae: 3.5307 - val_loss: 41.7186 - val_mae: 4.1408\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2809 - mae: 3.5355 - val_loss: 41.6355 - val_mae: 4.1458\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3108 - mae: 3.5402 - val_loss: 41.9340 - val_mae: 4.1263\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3193 - mae: 3.5033 - val_loss: 41.8266 - val_mae: 4.1212\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3499 - mae: 3.5698 - val_loss: 41.6339 - val_mae: 4.1472\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1590 - mae: 3.4974 - val_loss: 41.8756 - val_mae: 4.1146\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1801 - mae: 3.5129 - val_loss: 41.6963 - val_mae: 4.1257\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 23.1606 - mae: 3.4964 - val_loss: 41.7627 - val_mae: 4.1376\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1526 - mae: 3.5247 - val_loss: 41.7545 - val_mae: 4.1313\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1311 - mae: 3.5138 - val_loss: 41.6126 - val_mae: 4.1544\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.1283 - mae: 3.4954 - val_loss: 41.6392 - val_mae: 4.1328\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.0691 - mae: 3.5116 - val_loss: 41.6358 - val_mae: 4.1443\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.2439 - mae: 3.5108 - val_loss: 41.3687 - val_mae: 4.1842\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 45.5682 - mae: 4.2380\n",
      "Mean Absolute Error on Test Data: 4.238042831420898\n",
      "11/11 [==============================] - 0s 778us/step\n",
      "R-squared: 0.05704411437751766\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 86.0619 - mae: 7.3834 - val_loss: 67.6908 - val_mae: 6.4449\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 70.2965 - mae: 6.2792 - val_loss: 51.4582 - val_mae: 5.2013\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 52.2344 - mae: 4.9263 - val_loss: 33.9273 - val_mae: 3.9228\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 35.3042 - mae: 3.8403 - val_loss: 24.5449 - val_mae: 3.5328\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.4896 - mae: 3.7618 - val_loss: 24.7992 - val_mae: 3.8249\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.6054 - mae: 3.8717 - val_loss: 24.5742 - val_mae: 3.7999\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.3003 - mae: 3.8071 - val_loss: 24.0459 - val_mae: 3.6741\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 29.1254 - mae: 3.7185 - val_loss: 23.8651 - val_mae: 3.6176\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.9649 - mae: 3.7125 - val_loss: 23.9067 - val_mae: 3.6879\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.8514 - mae: 3.7625 - val_loss: 23.8099 - val_mae: 3.6757\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.7372 - mae: 3.7067 - val_loss: 23.6150 - val_mae: 3.6168\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.6455 - mae: 3.7082 - val_loss: 23.6651 - val_mae: 3.6675\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.5337 - mae: 3.7094 - val_loss: 23.5473 - val_mae: 3.6341\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.4238 - mae: 3.7360 - val_loss: 23.7509 - val_mae: 3.7177\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4055 - mae: 3.7164 - val_loss: 23.4606 - val_mae: 3.6181\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4295 - mae: 3.6628 - val_loss: 23.4611 - val_mae: 3.6387\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.6789 - mae: 3.8176 - val_loss: 23.6463 - val_mae: 3.7256\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.3276 - mae: 3.6989 - val_loss: 23.3183 - val_mae: 3.6256\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.2217 - mae: 3.6930 - val_loss: 23.2563 - val_mae: 3.6092\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.1693 - mae: 3.6795 - val_loss: 23.2261 - val_mae: 3.6067\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0753 - mae: 3.6657 - val_loss: 23.2722 - val_mae: 3.6308\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0520 - mae: 3.6738 - val_loss: 23.4139 - val_mae: 3.6882\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.1125 - mae: 3.7775 - val_loss: 23.5657 - val_mae: 3.7251\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.4470 - mae: 3.8327 - val_loss: 23.3008 - val_mae: 3.6550\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.8754 - mae: 3.6361 - val_loss: 23.1438 - val_mae: 3.5658\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9492 - mae: 3.6479 - val_loss: 23.3114 - val_mae: 3.6685\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9106 - mae: 3.6963 - val_loss: 23.2603 - val_mae: 3.6528\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.8422 - mae: 3.6877 - val_loss: 23.2920 - val_mae: 3.6683\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 28.0151 - mae: 3.7548 - val_loss: 23.4662 - val_mae: 3.7071\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.9438 - mae: 3.7279 - val_loss: 23.3634 - val_mae: 3.6767\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8476 - mae: 3.7159 - val_loss: 23.3018 - val_mae: 3.6653\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.8093 - mae: 3.7079 - val_loss: 23.2586 - val_mae: 3.6643\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.8785 - mae: 3.6487 - val_loss: 23.1172 - val_mae: 3.6272\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.9479 - mae: 3.7366 - val_loss: 23.2579 - val_mae: 3.6829\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 27.8846 - mae: 3.6616 - val_loss: 23.0643 - val_mae: 3.6220\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7346 - mae: 3.6679 - val_loss: 23.1772 - val_mae: 3.6673\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7068 - mae: 3.6952 - val_loss: 23.1972 - val_mae: 3.6800\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7312 - mae: 3.7019 - val_loss: 23.2428 - val_mae: 3.6931\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 28.0356 - mae: 3.8098 - val_loss: 23.4420 - val_mae: 3.7239\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6757 - mae: 3.7053 - val_loss: 23.1252 - val_mae: 3.6355\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.7860 - mae: 3.6344 - val_loss: 23.0463 - val_mae: 3.6194\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7137 - mae: 3.7219 - val_loss: 23.3027 - val_mae: 3.7110\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.5801 - mae: 3.6839 - val_loss: 22.9896 - val_mae: 3.6262\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6239 - mae: 3.6830 - val_loss: 22.9611 - val_mae: 3.6283\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.6632 - mae: 3.6428 - val_loss: 22.9438 - val_mae: 3.6273\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.5647 - mae: 3.6918 - val_loss: 23.0284 - val_mae: 3.6559\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 27.6217 - mae: 3.6500 - val_loss: 22.9149 - val_mae: 3.6213\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.5147 - mae: 3.6924 - val_loss: 23.2954 - val_mae: 3.7299\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.7439 - mae: 3.7859 - val_loss: 23.1623 - val_mae: 3.7085\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 27.5439 - mae: 3.7037 - val_loss: 23.0013 - val_mae: 3.6545\n",
      "11/11 [==============================] - 0s 941us/step - loss: 28.7148 - mae: 3.9687\n",
      "Mean Absolute Error on Test Data: 3.9687063694000244\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.01606468357414559\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 0s 13ms/step - loss: 8.0152 - mae: 2.0239 - val_loss: 6.1143 - val_mae: 1.6124\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.1765 - mae: 1.4576 - val_loss: 4.1325 - val_mae: 1.3505\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 4.0322 - mae: 1.4049 - val_loss: 3.9422 - val_mae: 1.4705\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.9924 - mae: 1.4764 - val_loss: 3.9105 - val_mae: 1.4177\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9725 - mae: 1.4077 - val_loss: 3.9156 - val_mae: 1.3946\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9337 - mae: 1.4102 - val_loss: 3.9109 - val_mae: 1.4071\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9280 - mae: 1.4338 - val_loss: 3.9136 - val_mae: 1.4023\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9097 - mae: 1.4091 - val_loss: 3.9140 - val_mae: 1.4025\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8973 - mae: 1.4096 - val_loss: 3.9110 - val_mae: 1.4111\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.9045 - mae: 1.4419 - val_loss: 3.9110 - val_mae: 1.4161\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8892 - mae: 1.4206 - val_loss: 3.9102 - val_mae: 1.4009\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8869 - mae: 1.4387 - val_loss: 3.9107 - val_mae: 1.4107\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8793 - mae: 1.3930 - val_loss: 3.9321 - val_mae: 1.3824\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8435 - mae: 1.3967 - val_loss: 3.9094 - val_mae: 1.4031\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8768 - mae: 1.4429 - val_loss: 3.9115 - val_mae: 1.4077\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8928 - mae: 1.3683 - val_loss: 3.9354 - val_mae: 1.3747\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8300 - mae: 1.3860 - val_loss: 3.9098 - val_mae: 1.4156\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8266 - mae: 1.4233 - val_loss: 3.9114 - val_mae: 1.4051\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8171 - mae: 1.3961 - val_loss: 3.9146 - val_mae: 1.4037\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8343 - mae: 1.3856 - val_loss: 3.9190 - val_mae: 1.3955\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8403 - mae: 1.4485 - val_loss: 3.9171 - val_mae: 1.4339\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8032 - mae: 1.4031 - val_loss: 3.9199 - val_mae: 1.3902\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.7912 - mae: 1.3982 - val_loss: 3.9044 - val_mae: 1.4032\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.8030 - mae: 1.3861 - val_loss: 3.9012 - val_mae: 1.4077\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.8012 - mae: 1.4038 - val_loss: 3.9052 - val_mae: 1.4001\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7813 - mae: 1.4156 - val_loss: 3.9022 - val_mae: 1.4154\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7768 - mae: 1.4136 - val_loss: 3.9117 - val_mae: 1.3991\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7798 - mae: 1.3791 - val_loss: 3.9165 - val_mae: 1.3953\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7669 - mae: 1.4098 - val_loss: 3.9049 - val_mae: 1.4243\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.7530 - mae: 1.4012 - val_loss: 3.9076 - val_mae: 1.3951\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7498 - mae: 1.3956 - val_loss: 3.8977 - val_mae: 1.4104\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7465 - mae: 1.4101 - val_loss: 3.9000 - val_mae: 1.4164\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7392 - mae: 1.3942 - val_loss: 3.8917 - val_mae: 1.4086\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7345 - mae: 1.3913 - val_loss: 3.8896 - val_mae: 1.4192\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7243 - mae: 1.3964 - val_loss: 3.8903 - val_mae: 1.4101\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7243 - mae: 1.4071 - val_loss: 3.8906 - val_mae: 1.4054\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3.7222 - mae: 1.3749 - val_loss: 3.8848 - val_mae: 1.4148\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.7344 - mae: 1.4322 - val_loss: 3.8839 - val_mae: 1.4101\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7262 - mae: 1.3962 - val_loss: 3.8870 - val_mae: 1.4168\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.7159 - mae: 1.4112 - val_loss: 3.8934 - val_mae: 1.4056\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6964 - mae: 1.3744 - val_loss: 3.8784 - val_mae: 1.4119\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6973 - mae: 1.4049 - val_loss: 3.8844 - val_mae: 1.4165\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6821 - mae: 1.4034 - val_loss: 3.8937 - val_mae: 1.4170\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6763 - mae: 1.3875 - val_loss: 3.8931 - val_mae: 1.4135\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3.6792 - mae: 1.3996 - val_loss: 3.9072 - val_mae: 1.4022\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.6968 - mae: 1.4074 - val_loss: 3.9076 - val_mae: 1.4038\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.6643 - mae: 1.3703 - val_loss: 3.9065 - val_mae: 1.4090\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6688 - mae: 1.3975 - val_loss: 3.9148 - val_mae: 1.4117\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 3.6607 - mae: 1.3756 - val_loss: 3.9092 - val_mae: 1.4026\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.6808 - mae: 1.4041 - val_loss: 3.9005 - val_mae: 1.4019\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 3.6601 - mae: 1.3839\n",
      "Mean Absolute Error on Test Data: 1.3839285373687744\n",
      "9/9 [==============================] - 0s 772us/step\n",
      "R-squared: 0.03685456995577541\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "9/9 [==============================] - 0s 20ms/step - loss: 3.1617 - mae: 1.4518 - val_loss: 2.7523 - val_mae: 1.2839\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 2.0685 - mae: 1.0644 - val_loss: 1.7876 - val_mae: 0.9047\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 1.3440 - mae: 0.7175 - val_loss: 1.2729 - val_mae: 0.7633\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 1.0597 - mae: 0.7260 - val_loss: 1.1632 - val_mae: 0.8487\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.0448 - mae: 0.7767 - val_loss: 1.1531 - val_mae: 0.8614\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.0336 - mae: 0.7717 - val_loss: 1.1349 - val_mae: 0.8419\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 1.0127 - mae: 0.7484 - val_loss: 1.1250 - val_mae: 0.8131\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 1.0042 - mae: 0.7289 - val_loss: 1.1179 - val_mae: 0.7998\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9962 - mae: 0.7250 - val_loss: 1.1022 - val_mae: 0.8044\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9926 - mae: 0.7334 - val_loss: 1.0898 - val_mae: 0.8091\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9864 - mae: 0.7235 - val_loss: 1.0843 - val_mae: 0.7948\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9842 - mae: 0.7309 - val_loss: 1.0694 - val_mae: 0.8053\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9733 - mae: 0.7263 - val_loss: 1.0638 - val_mae: 0.7982\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9706 - mae: 0.7263 - val_loss: 1.0546 - val_mae: 0.8007\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9664 - mae: 0.7151 - val_loss: 1.0536 - val_mae: 0.7859\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9611 - mae: 0.7101 - val_loss: 1.0448 - val_mae: 0.7837\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.9571 - mae: 0.7141 - val_loss: 1.0342 - val_mae: 0.7918\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9540 - mae: 0.7233 - val_loss: 1.0273 - val_mae: 0.7937\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9501 - mae: 0.7186 - val_loss: 1.0249 - val_mae: 0.7825\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9476 - mae: 0.7041 - val_loss: 1.0298 - val_mae: 0.7665\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9490 - mae: 0.7084 - val_loss: 1.0159 - val_mae: 0.7843\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9415 - mae: 0.7143 - val_loss: 1.0129 - val_mae: 0.7805\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9397 - mae: 0.7120 - val_loss: 1.0112 - val_mae: 0.7755\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 0s 7ms/step - loss: 0.9398 - mae: 0.7041 - val_loss: 1.0098 - val_mae: 0.7693\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9352 - mae: 0.7057 - val_loss: 1.0039 - val_mae: 0.7784\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9346 - mae: 0.7157 - val_loss: 0.9951 - val_mae: 0.7900\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9339 - mae: 0.7204 - val_loss: 0.9956 - val_mae: 0.7768\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9322 - mae: 0.7046 - val_loss: 0.9971 - val_mae: 0.7660\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9302 - mae: 0.7062 - val_loss: 0.9909 - val_mae: 0.7736\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9332 - mae: 0.7212 - val_loss: 0.9859 - val_mae: 0.7828\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9273 - mae: 0.7051 - val_loss: 0.9966 - val_mae: 0.7585\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9294 - mae: 0.7069 - val_loss: 0.9863 - val_mae: 0.7760\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9265 - mae: 0.7077 - val_loss: 0.9903 - val_mae: 0.7643\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9272 - mae: 0.7146 - val_loss: 0.9828 - val_mae: 0.7854\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9256 - mae: 0.7121 - val_loss: 0.9896 - val_mae: 0.7641\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9220 - mae: 0.6977 - val_loss: 0.9821 - val_mae: 0.7740\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9212 - mae: 0.7158 - val_loss: 0.9758 - val_mae: 0.7785\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9178 - mae: 0.7068 - val_loss: 0.9825 - val_mae: 0.7685\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9161 - mae: 0.7063 - val_loss: 0.9780 - val_mae: 0.7732\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9162 - mae: 0.7098 - val_loss: 0.9795 - val_mae: 0.7758\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9161 - mae: 0.7041 - val_loss: 0.9775 - val_mae: 0.7688\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9134 - mae: 0.7016 - val_loss: 0.9768 - val_mae: 0.7682\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9137 - mae: 0.7082 - val_loss: 0.9722 - val_mae: 0.7717\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9105 - mae: 0.7049 - val_loss: 0.9775 - val_mae: 0.7681\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9124 - mae: 0.7058 - val_loss: 0.9730 - val_mae: 0.7749\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 0s 5ms/step - loss: 0.9078 - mae: 0.7046 - val_loss: 0.9714 - val_mae: 0.7696\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9083 - mae: 0.7022 - val_loss: 0.9689 - val_mae: 0.7710\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9154 - mae: 0.6979 - val_loss: 0.9744 - val_mae: 0.7644\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 0s 4ms/step - loss: 0.9040 - mae: 0.7084 - val_loss: 0.9640 - val_mae: 0.7812\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 0s 6ms/step - loss: 0.9063 - mae: 0.7125 - val_loss: 0.9650 - val_mae: 0.7740\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.6385 - mae: 0.8249\n",
      "Mean Absolute Error on Test Data: 0.8249479532241821\n",
      "6/6 [==============================] - 0s 2ms/step\n",
      "R-squared: -0.045563807130022616\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 22.9758 - mae: 3.4439 - val_loss: 22.4764 - val_mae: 3.0118\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 15.7819 - mae: 2.5199 - val_loss: 15.9768 - val_mae: 2.3952\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 11.5743 - mae: 2.2249 - val_loss: 13.8274 - val_mae: 2.4096\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 11.2347 - mae: 2.3604 - val_loss: 13.7773 - val_mae: 2.4147\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 11.0939 - mae: 2.3100 - val_loss: 13.7525 - val_mae: 2.3895\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 11.0350 - mae: 2.2947 - val_loss: 13.6901 - val_mae: 2.3962\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.9913 - mae: 2.3028 - val_loss: 13.6445 - val_mae: 2.4038\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.9506 - mae: 2.3031 - val_loss: 13.6150 - val_mae: 2.3848\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.9524 - mae: 2.2403 - val_loss: 13.6809 - val_mae: 2.3451\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.8980 - mae: 2.2535 - val_loss: 13.5892 - val_mae: 2.3679\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.8683 - mae: 2.2476 - val_loss: 13.5775 - val_mae: 2.3616\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.8418 - mae: 2.2603 - val_loss: 13.5412 - val_mae: 2.3700\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.8343 - mae: 2.2675 - val_loss: 13.5779 - val_mae: 2.3509\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.8107 - mae: 2.2581 - val_loss: 13.5292 - val_mae: 2.3598\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.8026 - mae: 2.2746 - val_loss: 13.4904 - val_mae: 2.3867\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.8207 - mae: 2.2399 - val_loss: 13.5699 - val_mae: 2.3397\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.7709 - mae: 2.2409 - val_loss: 13.5024 - val_mae: 2.3536\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.7483 - mae: 2.2752 - val_loss: 13.4682 - val_mae: 2.3685\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.7260 - mae: 2.2584 - val_loss: 13.4858 - val_mae: 2.3497\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.7581 - mae: 2.2801 - val_loss: 13.4427 - val_mae: 2.3616\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.6919 - mae: 2.2530 - val_loss: 13.4411 - val_mae: 2.3504\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.7321 - mae: 2.2797 - val_loss: 13.4195 - val_mae: 2.3522\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.6870 - mae: 2.2551 - val_loss: 13.3988 - val_mae: 2.3557\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.6851 - mae: 2.2328 - val_loss: 13.4229 - val_mae: 2.3443\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.7235 - mae: 2.2947 - val_loss: 13.3634 - val_mae: 2.3847\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.7414 - mae: 2.2309 - val_loss: 13.4668 - val_mae: 2.3316\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.6449 - mae: 2.2420 - val_loss: 13.3537 - val_mae: 2.3654\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.6527 - mae: 2.2499 - val_loss: 13.4013 - val_mae: 2.3454\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.6069 - mae: 2.2454 - val_loss: 13.3750 - val_mae: 2.3557\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.6523 - mae: 2.2292 - val_loss: 13.3731 - val_mae: 2.3523\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 10.7208 - mae: 2.3265 - val_loss: 13.3315 - val_mae: 2.3686\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.6031 - mae: 2.2306 - val_loss: 13.4207 - val_mae: 2.3325\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5949 - mae: 2.2137 - val_loss: 13.3930 - val_mae: 2.3417\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5713 - mae: 2.2459 - val_loss: 13.3822 - val_mae: 2.3416\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.5751 - mae: 2.2290 - val_loss: 13.3977 - val_mae: 2.3441\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5755 - mae: 2.2884 - val_loss: 13.3741 - val_mae: 2.3894\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.5814 - mae: 2.2940 - val_loss: 13.3757 - val_mae: 2.3424\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5451 - mae: 2.2256 - val_loss: 13.4054 - val_mae: 2.3335\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5475 - mae: 2.2491 - val_loss: 13.3494 - val_mae: 2.3505\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.6185 - mae: 2.2967 - val_loss: 13.3184 - val_mae: 2.3546\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5079 - mae: 2.2295 - val_loss: 13.3934 - val_mae: 2.3268\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.5281 - mae: 2.2491 - val_loss: 13.3213 - val_mae: 2.3423\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5246 - mae: 2.2540 - val_loss: 13.3347 - val_mae: 2.3383\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.5111 - mae: 2.2133 - val_loss: 13.3225 - val_mae: 2.3422\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4897 - mae: 2.2516 - val_loss: 13.3300 - val_mae: 2.3369\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.5029 - mae: 2.2109 - val_loss: 13.3233 - val_mae: 2.3365\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4659 - mae: 2.2283 - val_loss: 13.2837 - val_mae: 2.3473\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.4998 - mae: 2.2255 - val_loss: 13.3249 - val_mae: 2.3350\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 10.5368 - mae: 2.2852 - val_loss: 13.2964 - val_mae: 2.3396\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 10.4425 - mae: 2.2258 - val_loss: 13.3016 - val_mae: 2.3317\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 11.0805 - mae: 2.3989\n",
      "Mean Absolute Error on Test Data: 2.3989274501800537\n",
      "10/10 [==============================] - 0s 655us/step\n",
      "R-squared: 0.06543257009446435\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 1s 13ms/step - loss: 9.6938 - mae: 2.0582 - val_loss: 5.3411 - val_mae: 1.6083\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 6.7267 - mae: 1.5487 - val_loss: 3.6758 - val_mae: 1.4538\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.7338 - mae: 1.5854 - val_loss: 3.7692 - val_mae: 1.6009\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.6583 - mae: 1.6388 - val_loss: 3.6697 - val_mae: 1.5588\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.5882 - mae: 1.5770 - val_loss: 3.6635 - val_mae: 1.5452\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.5313 - mae: 1.5744 - val_loss: 3.6857 - val_mae: 1.5511\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.5004 - mae: 1.5717 - val_loss: 3.7121 - val_mae: 1.5552\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.4785 - mae: 1.5761 - val_loss: 3.7594 - val_mae: 1.5703\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.4492 - mae: 1.5703 - val_loss: 3.7702 - val_mae: 1.5636\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.4254 - mae: 1.5600 - val_loss: 3.7986 - val_mae: 1.5645\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.4068 - mae: 1.5518 - val_loss: 3.8275 - val_mae: 1.5684\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.4003 - mae: 1.5528 - val_loss: 3.8975 - val_mae: 1.5975\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.3739 - mae: 1.5748 - val_loss: 3.8804 - val_mae: 1.5776\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.3801 - mae: 1.5241 - val_loss: 3.8855 - val_mae: 1.5706\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.3573 - mae: 1.5286 - val_loss: 3.8842 - val_mae: 1.5714\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3638 - mae: 1.5851 - val_loss: 4.0152 - val_mae: 1.6240\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3188 - mae: 1.5399 - val_loss: 3.9259 - val_mae: 1.5696\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.3173 - mae: 1.5356 - val_loss: 4.0076 - val_mae: 1.6020\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3236 - mae: 1.5226 - val_loss: 4.0100 - val_mae: 1.5952\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2986 - mae: 1.5507 - val_loss: 4.0486 - val_mae: 1.6131\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3092 - mae: 1.5195 - val_loss: 4.0202 - val_mae: 1.5939\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2878 - mae: 1.5435 - val_loss: 4.0502 - val_mae: 1.6130\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2864 - mae: 1.5269 - val_loss: 4.0355 - val_mae: 1.5943\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2830 - mae: 1.5396 - val_loss: 4.0636 - val_mae: 1.6186\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2575 - mae: 1.5321 - val_loss: 4.0015 - val_mae: 1.5798\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2773 - mae: 1.4993 - val_loss: 4.0514 - val_mae: 1.6085\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2690 - mae: 1.5506 - val_loss: 4.0603 - val_mae: 1.6115\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2467 - mae: 1.5338 - val_loss: 4.0655 - val_mae: 1.6026\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2885 - mae: 1.4915 - val_loss: 4.0278 - val_mae: 1.5913\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2977 - mae: 1.5757 - val_loss: 4.1548 - val_mae: 1.6439\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5.2033 - mae: 1.5191 - val_loss: 4.0135 - val_mae: 1.5766\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.3038 - mae: 1.4711 - val_loss: 4.0028 - val_mae: 1.5826\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2320 - mae: 1.5408 - val_loss: 4.1284 - val_mae: 1.6418\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2110 - mae: 1.5401 - val_loss: 4.0496 - val_mae: 1.6016\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2442 - mae: 1.4906 - val_loss: 4.0671 - val_mae: 1.6061\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2360 - mae: 1.5416 - val_loss: 4.1181 - val_mae: 1.6239\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.1906 - mae: 1.5064 - val_loss: 4.0392 - val_mae: 1.5880\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2497 - mae: 1.4723 - val_loss: 4.0483 - val_mae: 1.6020\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.2148 - mae: 1.5501 - val_loss: 4.1992 - val_mae: 1.6559\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.2018 - mae: 1.5076 - val_loss: 4.0456 - val_mae: 1.5912\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 5.1979 - mae: 1.4912 - val_loss: 4.0894 - val_mae: 1.6147\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1855 - mae: 1.5213 - val_loss: 4.1408 - val_mae: 1.6355\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.1818 - mae: 1.5138 - val_loss: 4.0600 - val_mae: 1.5981\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.1738 - mae: 1.4899 - val_loss: 4.1068 - val_mae: 1.6222\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1980 - mae: 1.5451 - val_loss: 4.1206 - val_mae: 1.6251\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 5.1639 - mae: 1.5073 - val_loss: 4.0912 - val_mae: 1.6035\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1565 - mae: 1.5037 - val_loss: 4.1685 - val_mae: 1.6375\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1720 - mae: 1.5104 - val_loss: 4.0787 - val_mae: 1.6055\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1518 - mae: 1.4918 - val_loss: 4.1312 - val_mae: 1.6258\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 5.1560 - mae: 1.5182 - val_loss: 4.1377 - val_mae: 1.6302\n",
      "9/9 [==============================] - 0s 796us/step - loss: 7.9212 - mae: 1.7160\n",
      "Mean Absolute Error on Test Data: 1.7159754037857056\n",
      "9/9 [==============================] - 0s 851us/step\n",
      "R-squared: 0.06489225323987036\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "13/13 [==============================] - 0s 12ms/step - loss: 4.8397 - mae: 1.5629 - val_loss: 2.7373 - val_mae: 1.1107\n",
      "Epoch 2/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 3.3138 - mae: 1.1561 - val_loss: 1.8036 - val_mae: 0.9494\n",
      "Epoch 3/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.5872 - mae: 1.0959 - val_loss: 1.8508 - val_mae: 1.0486\n",
      "Epoch 4/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.5833 - mae: 1.1444 - val_loss: 1.8168 - val_mae: 1.0298\n",
      "Epoch 5/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.5267 - mae: 1.1080 - val_loss: 1.7654 - val_mae: 0.9975\n",
      "Epoch 6/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.5163 - mae: 1.0880 - val_loss: 1.7498 - val_mae: 0.9878\n",
      "Epoch 7/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.4836 - mae: 1.0930 - val_loss: 1.7977 - val_mae: 1.0193\n",
      "Epoch 8/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4697 - mae: 1.0991 - val_loss: 1.7956 - val_mae: 1.0175\n",
      "Epoch 9/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.4663 - mae: 1.0891 - val_loss: 1.7712 - val_mae: 0.9999\n",
      "Epoch 10/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4370 - mae: 1.0796 - val_loss: 1.7859 - val_mae: 1.0111\n",
      "Epoch 11/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.4239 - mae: 1.0866 - val_loss: 1.7828 - val_mae: 1.0080\n",
      "Epoch 12/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.4322 - mae: 1.0663 - val_loss: 1.7501 - val_mae: 0.9833\n",
      "Epoch 13/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3979 - mae: 1.0739 - val_loss: 1.8037 - val_mae: 1.0205\n",
      "Epoch 14/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3971 - mae: 1.0923 - val_loss: 1.8123 - val_mae: 1.0264\n",
      "Epoch 15/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3845 - mae: 1.0651 - val_loss: 1.7605 - val_mae: 0.9905\n",
      "Epoch 16/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3625 - mae: 1.0715 - val_loss: 1.8103 - val_mae: 1.0262\n",
      "Epoch 17/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3839 - mae: 1.1034 - val_loss: 1.8073 - val_mae: 1.0212\n",
      "Epoch 18/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3624 - mae: 1.0447 - val_loss: 1.7395 - val_mae: 0.9675\n",
      "Epoch 19/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3429 - mae: 1.0493 - val_loss: 1.8009 - val_mae: 1.0169\n",
      "Epoch 20/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3545 - mae: 1.0973 - val_loss: 1.8101 - val_mae: 1.0198\n",
      "Epoch 21/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3199 - mae: 1.0569 - val_loss: 1.7688 - val_mae: 0.9873\n",
      "Epoch 22/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.3203 - mae: 1.0507 - val_loss: 1.7996 - val_mae: 1.0094\n",
      "Epoch 23/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.3258 - mae: 1.0856 - val_loss: 1.8148 - val_mae: 1.0166\n",
      "Epoch 24/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3084 - mae: 1.0425 - val_loss: 1.7634 - val_mae: 0.9770\n",
      "Epoch 25/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3008 - mae: 1.0583 - val_loss: 1.8396 - val_mae: 1.0292\n",
      "Epoch 26/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3033 - mae: 1.0713 - val_loss: 1.8652 - val_mae: 1.0425\n",
      "Epoch 27/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2913 - mae: 1.0605 - val_loss: 1.7914 - val_mae: 0.9942\n",
      "Epoch 28/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.3077 - mae: 1.0419 - val_loss: 1.8401 - val_mae: 1.0255\n",
      "Epoch 29/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2872 - mae: 1.0592 - val_loss: 1.8370 - val_mae: 1.0212\n",
      "Epoch 30/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2808 - mae: 1.0692 - val_loss: 1.8123 - val_mae: 1.0044\n",
      "Epoch 31/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2725 - mae: 1.0543 - val_loss: 1.7995 - val_mae: 0.9919\n",
      "Epoch 32/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2891 - mae: 1.0287 - val_loss: 1.8194 - val_mae: 1.0053\n",
      "Epoch 33/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2835 - mae: 1.0768 - val_loss: 1.8486 - val_mae: 1.0261\n",
      "Epoch 34/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2720 - mae: 1.0314 - val_loss: 1.7826 - val_mae: 0.9742\n",
      "Epoch 35/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2660 - mae: 1.0642 - val_loss: 1.9219 - val_mae: 1.0647\n",
      "Epoch 36/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2586 - mae: 1.0616 - val_loss: 1.7867 - val_mae: 0.9827\n",
      "Epoch 37/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2756 - mae: 1.0222 - val_loss: 1.8063 - val_mae: 0.9928\n",
      "Epoch 38/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2574 - mae: 1.0606 - val_loss: 1.9035 - val_mae: 1.0539\n",
      "Epoch 39/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2483 - mae: 1.0416 - val_loss: 1.7976 - val_mae: 0.9836\n",
      "Epoch 40/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2571 - mae: 1.0282 - val_loss: 1.8325 - val_mae: 1.0098\n",
      "Epoch 41/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2402 - mae: 1.0446 - val_loss: 1.8317 - val_mae: 1.0039\n",
      "Epoch 42/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2460 - mae: 1.0325 - val_loss: 1.8251 - val_mae: 1.0005\n",
      "Epoch 43/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2454 - mae: 1.0699 - val_loss: 1.8904 - val_mae: 1.0415\n",
      "Epoch 44/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2430 - mae: 1.0387 - val_loss: 1.8510 - val_mae: 1.0143\n",
      "Epoch 45/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2339 - mae: 1.0324 - val_loss: 1.8720 - val_mae: 1.0271\n",
      "Epoch 46/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2306 - mae: 1.0600 - val_loss: 1.8985 - val_mae: 1.0376\n",
      "Epoch 47/50\n",
      "13/13 [==============================] - 0s 5ms/step - loss: 2.2274 - mae: 1.0473 - val_loss: 1.8715 - val_mae: 1.0246\n",
      "Epoch 48/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2199 - mae: 1.0386 - val_loss: 1.8452 - val_mae: 1.0046\n",
      "Epoch 49/50\n",
      "13/13 [==============================] - 0s 4ms/step - loss: 2.2196 - mae: 1.0384 - val_loss: 1.8576 - val_mae: 1.0137\n",
      "Epoch 50/50\n",
      "13/13 [==============================] - 0s 3ms/step - loss: 2.2186 - mae: 1.0299 - val_loss: 1.8227 - val_mae: 0.9917\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 2.9780 - mae: 1.1859\n",
      "Mean Absolute Error on Test Data: 1.1858704090118408\n",
      "8/8 [==============================] - 0s 873us/step\n",
      "R-squared: 0.012866398188244976\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 16.9397 - mae: 2.9424 - val_loss: 19.3941 - val_mae: 2.8121\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 11.2342 - mae: 2.1952 - val_loss: 14.0622 - val_mae: 2.3095\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 8.4194 - mae: 1.9858 - val_loss: 12.1553 - val_mae: 2.2791\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1516 - mae: 2.0588 - val_loss: 12.1035 - val_mae: 2.2725\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1002 - mae: 2.0411 - val_loss: 12.1497 - val_mae: 2.2462\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0437 - mae: 1.9948 - val_loss: 12.1363 - val_mae: 2.2361\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.0218 - mae: 1.9886 - val_loss: 12.0769 - val_mae: 2.2319\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0140 - mae: 2.0122 - val_loss: 11.9894 - val_mae: 2.2332\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0138 - mae: 1.9829 - val_loss: 12.0171 - val_mae: 2.2206\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9736 - mae: 1.9960 - val_loss: 11.9037 - val_mae: 2.2279\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9606 - mae: 1.9949 - val_loss: 11.9870 - val_mae: 2.2128\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9525 - mae: 1.9845 - val_loss: 11.9371 - val_mae: 2.2147\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9459 - mae: 1.9799 - val_loss: 11.9160 - val_mae: 2.2100\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9322 - mae: 1.9939 - val_loss: 11.8647 - val_mae: 2.2138\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9257 - mae: 1.9937 - val_loss: 11.9288 - val_mae: 2.2002\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9382 - mae: 1.9556 - val_loss: 11.9183 - val_mae: 2.1969\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9362 - mae: 2.0065 - val_loss: 11.8089 - val_mae: 2.2073\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9126 - mae: 1.9755 - val_loss: 11.8351 - val_mae: 2.1992\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9132 - mae: 1.9799 - val_loss: 11.8347 - val_mae: 2.1964\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9187 - mae: 2.0099 - val_loss: 11.7766 - val_mae: 2.1976\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.8862 - mae: 1.9767 - val_loss: 11.8001 - val_mae: 2.1909\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8795 - mae: 1.9659 - val_loss: 11.7814 - val_mae: 2.1907\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9286 - mae: 2.0124 - val_loss: 11.7673 - val_mae: 2.1907\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8810 - mae: 1.9526 - val_loss: 11.8328 - val_mae: 2.1801\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8999 - mae: 1.9913 - val_loss: 11.6814 - val_mae: 2.1969\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8606 - mae: 1.9776 - val_loss: 11.7609 - val_mae: 2.1834\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8654 - mae: 1.9808 - val_loss: 11.7793 - val_mae: 2.1805\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8589 - mae: 1.9584 - val_loss: 11.7770 - val_mae: 2.1777\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8529 - mae: 1.9590 - val_loss: 11.6977 - val_mae: 2.1862\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8863 - mae: 2.0181 - val_loss: 11.6294 - val_mae: 2.1918\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8886 - mae: 1.9893 - val_loss: 11.6677 - val_mae: 2.1845\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8764 - mae: 1.9487 - val_loss: 11.7412 - val_mae: 2.1731\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8653 - mae: 1.9920 - val_loss: 11.6637 - val_mae: 2.1799\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.8422 - mae: 1.9617 - val_loss: 11.7536 - val_mae: 2.1667\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8593 - mae: 1.9892 - val_loss: 11.6200 - val_mae: 2.1806\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8540 - mae: 1.9599 - val_loss: 11.6939 - val_mae: 2.1711\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8186 - mae: 1.9719 - val_loss: 11.6063 - val_mae: 2.1818\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8352 - mae: 1.9851 - val_loss: 11.5849 - val_mae: 2.1821\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8273 - mae: 1.9860 - val_loss: 11.6493 - val_mae: 2.1680\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8205 - mae: 1.9759 - val_loss: 11.6410 - val_mae: 2.1687\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8369 - mae: 1.9883 - val_loss: 11.5938 - val_mae: 2.1733\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8552 - mae: 1.9450 - val_loss: 11.6517 - val_mae: 2.1648\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8538 - mae: 2.0036 - val_loss: 11.4939 - val_mae: 2.1878\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8010 - mae: 1.9775 - val_loss: 11.6674 - val_mae: 2.1621\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8107 - mae: 1.9484 - val_loss: 11.6691 - val_mae: 2.1633\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.7986 - mae: 1.9746 - val_loss: 11.5302 - val_mae: 2.1733\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8088 - mae: 1.9733 - val_loss: 11.5307 - val_mae: 2.1741\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8108 - mae: 1.9969 - val_loss: 11.5490 - val_mae: 2.1717\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8019 - mae: 1.9442 - val_loss: 11.7305 - val_mae: 2.1538\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8174 - mae: 1.9835 - val_loss: 11.4996 - val_mae: 2.1778\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 8.9474 - mae: 1.9135\n",
      "Mean Absolute Error on Test Data: 1.9134650230407715\n",
      "10/10 [==============================] - 0s 952us/step\n",
      "R-squared: 0.07177376553190584\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 10ms/step - loss: 109.6863 - mae: 8.7401 - val_loss: 127.4807 - val_mae: 9.0828\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 98.5630 - mae: 8.0877 - val_loss: 111.3039 - val_mae: 8.1922\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 79.0739 - mae: 6.8484 - val_loss: 84.4289 - val_mae: 6.6105\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 53.1990 - mae: 5.1385 - val_loss: 54.6956 - val_mae: 4.8543\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 34.9628 - mae: 4.1799 - val_loss: 41.8261 - val_mae: 4.5148\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 32.1821 - mae: 4.3030 - val_loss: 41.3786 - val_mae: 4.5620\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 32.1794 - mae: 4.2283 - val_loss: 41.9053 - val_mae: 4.4875\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 32.0409 - mae: 4.1850 - val_loss: 41.7958 - val_mae: 4.4827\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.9874 - mae: 4.2336 - val_loss: 41.4448 - val_mae: 4.5031\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.9416 - mae: 4.1973 - val_loss: 41.8648 - val_mae: 4.4558\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.9615 - mae: 4.2277 - val_loss: 41.1163 - val_mae: 4.5129\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.8911 - mae: 4.2392 - val_loss: 41.4185 - val_mae: 4.4639\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 32.0727 - mae: 4.1482 - val_loss: 41.7128 - val_mae: 4.4410\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.9891 - mae: 4.2704 - val_loss: 40.9040 - val_mae: 4.5192\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.9213 - mae: 4.2200 - val_loss: 41.2365 - val_mae: 4.4641\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.9409 - mae: 4.2804 - val_loss: 40.8374 - val_mae: 4.4889\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.8679 - mae: 4.1844 - val_loss: 41.3333 - val_mae: 4.4300\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.8577 - mae: 4.1473 - val_loss: 41.4877 - val_mae: 4.4163\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7265 - mae: 4.1630 - val_loss: 40.9190 - val_mae: 4.4602\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7670 - mae: 4.2268 - val_loss: 40.9918 - val_mae: 4.4451\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7704 - mae: 4.2187 - val_loss: 40.7916 - val_mae: 4.4593\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.8327 - mae: 4.1757 - val_loss: 41.0262 - val_mae: 4.4303\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7223 - mae: 4.1947 - val_loss: 40.9817 - val_mae: 4.4386\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7067 - mae: 4.1832 - val_loss: 40.6759 - val_mae: 4.4530\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.6988 - mae: 4.2033 - val_loss: 40.7564 - val_mae: 4.4576\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.6928 - mae: 4.2151 - val_loss: 41.0079 - val_mae: 4.4304\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7617 - mae: 4.2061 - val_loss: 40.8191 - val_mae: 4.4456\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7132 - mae: 4.1529 - val_loss: 41.1952 - val_mae: 4.4112\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7475 - mae: 4.2325 - val_loss: 40.7169 - val_mae: 4.4498\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5885 - mae: 4.1776 - val_loss: 41.1132 - val_mae: 4.4128\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5630 - mae: 4.1670 - val_loss: 40.7036 - val_mae: 4.4531\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.7270 - mae: 4.2484 - val_loss: 40.8822 - val_mae: 4.4287\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.6185 - mae: 4.1408 - val_loss: 41.0970 - val_mae: 4.4136\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.6906 - mae: 4.1419 - val_loss: 40.8146 - val_mae: 4.4317\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.5882 - mae: 4.2241 - val_loss: 40.6569 - val_mae: 4.4509\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.6461 - mae: 4.1563 - val_loss: 40.9052 - val_mae: 4.4195\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.8389 - mae: 4.2792 - val_loss: 40.5538 - val_mae: 4.4522\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5626 - mae: 4.2203 - val_loss: 40.9917 - val_mae: 4.4053\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5309 - mae: 4.1486 - val_loss: 40.8639 - val_mae: 4.4153\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.4867 - mae: 4.1823 - val_loss: 40.6878 - val_mae: 4.4252\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.5948 - mae: 4.1425 - val_loss: 40.7975 - val_mae: 4.4169\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5396 - mae: 4.2294 - val_loss: 40.3756 - val_mae: 4.4614\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.6294 - mae: 4.1349 - val_loss: 41.0610 - val_mae: 4.4035\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.4745 - mae: 4.1950 - val_loss: 40.5425 - val_mae: 4.4405\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.4679 - mae: 4.1707 - val_loss: 40.7148 - val_mae: 4.4268\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5271 - mae: 4.2281 - val_loss: 40.5760 - val_mae: 4.4310\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.5494 - mae: 4.2332 - val_loss: 40.8991 - val_mae: 4.4030\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 31.4899 - mae: 4.1256 - val_loss: 40.9269 - val_mae: 4.4126\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.5389 - mae: 4.2439 - val_loss: 40.4696 - val_mae: 4.4390\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 31.3621 - mae: 4.1729 - val_loss: 40.8686 - val_mae: 4.4042\n",
      "11/11 [==============================] - 0s 919us/step - loss: 51.5312 - mae: 4.5588\n",
      "Mean Absolute Error on Test Data: 4.558752059936523\n",
      "11/11 [==============================] - 0s 937us/step\n",
      "R-squared: -0.002155197653765928\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 17.1669 - mae: 2.9618 - val_loss: 15.2102 - val_mae: 2.6565\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 12.6452 - mae: 2.3077 - val_loss: 10.4577 - val_mae: 1.9917\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.9872 - mae: 1.9314 - val_loss: 8.0154 - val_mae: 1.8729\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.4181 - mae: 2.0523 - val_loss: 7.9103 - val_mae: 1.9200\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.2186 - mae: 1.9808 - val_loss: 7.9416 - val_mae: 1.8531\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1989 - mae: 1.9438 - val_loss: 7.8659 - val_mae: 1.8548\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1720 - mae: 1.9697 - val_loss: 7.7986 - val_mae: 1.8636\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1555 - mae: 1.9415 - val_loss: 7.8381 - val_mae: 1.8434\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.1035 - mae: 1.9582 - val_loss: 7.7543 - val_mae: 1.8579\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0855 - mae: 1.9727 - val_loss: 7.7257 - val_mae: 1.8555\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 8.0591 - mae: 1.9487 - val_loss: 7.7506 - val_mae: 1.8429\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0710 - mae: 1.9621 - val_loss: 7.7191 - val_mae: 1.8431\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0371 - mae: 1.9478 - val_loss: 7.7280 - val_mae: 1.8415\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0191 - mae: 1.9542 - val_loss: 7.6874 - val_mae: 1.8480\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.0003 - mae: 1.9498 - val_loss: 7.7079 - val_mae: 1.8399\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9943 - mae: 1.9485 - val_loss: 7.6903 - val_mae: 1.8427\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9788 - mae: 1.9578 - val_loss: 7.6630 - val_mae: 1.8542\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9735 - mae: 1.9431 - val_loss: 7.6922 - val_mae: 1.8367\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9683 - mae: 1.9496 - val_loss: 7.6464 - val_mae: 1.8597\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9463 - mae: 1.9545 - val_loss: 7.6873 - val_mae: 1.8405\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9541 - mae: 1.9419 - val_loss: 7.6909 - val_mae: 1.8368\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9449 - mae: 1.9553 - val_loss: 7.6415 - val_mae: 1.8485\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9583 - mae: 1.9647 - val_loss: 7.6662 - val_mae: 1.8312\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9593 - mae: 1.9367 - val_loss: 7.6378 - val_mae: 1.8599\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.9208 - mae: 1.9711 - val_loss: 7.6647 - val_mae: 1.8386\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9322 - mae: 1.9246 - val_loss: 7.6799 - val_mae: 1.8333\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8920 - mae: 1.9495 - val_loss: 7.6245 - val_mae: 1.8524\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9070 - mae: 1.9540 - val_loss: 7.6710 - val_mae: 1.8339\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8979 - mae: 1.9457 - val_loss: 7.6048 - val_mae: 1.8516\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9087 - mae: 1.9455 - val_loss: 7.6220 - val_mae: 1.8480\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8983 - mae: 1.9645 - val_loss: 7.6387 - val_mae: 1.8528\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.9028 - mae: 1.9265 - val_loss: 7.6638 - val_mae: 1.8393\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8856 - mae: 1.9562 - val_loss: 7.6198 - val_mae: 1.8501\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8642 - mae: 1.9499 - val_loss: 7.6832 - val_mae: 1.8347\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8729 - mae: 1.9249 - val_loss: 7.6787 - val_mae: 1.8554\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8715 - mae: 1.9441 - val_loss: 7.6332 - val_mae: 1.8468\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8766 - mae: 1.9755 - val_loss: 7.6088 - val_mae: 1.8740\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8318 - mae: 1.9364 - val_loss: 7.7169 - val_mae: 1.8324\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8457 - mae: 1.9353 - val_loss: 7.6850 - val_mae: 1.8525\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8441 - mae: 1.9272 - val_loss: 7.6614 - val_mae: 1.8436\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8636 - mae: 1.9698 - val_loss: 7.6806 - val_mae: 1.8589\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8230 - mae: 1.9297 - val_loss: 7.6999 - val_mae: 1.8385\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8475 - mae: 1.9567 - val_loss: 7.6769 - val_mae: 1.8438\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 7.8532 - mae: 1.9663 - val_loss: 7.6984 - val_mae: 1.8468\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8563 - mae: 1.9287 - val_loss: 7.6948 - val_mae: 1.8333\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8115 - mae: 1.9305 - val_loss: 7.7246 - val_mae: 1.8456\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.8006 - mae: 1.9209 - val_loss: 7.7239 - val_mae: 1.8404\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.8119 - mae: 1.9543 - val_loss: 7.7160 - val_mae: 1.8690\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 7.7885 - mae: 1.9234 - val_loss: 7.7377 - val_mae: 1.8384\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 7.7819 - mae: 1.9363 - val_loss: 7.6775 - val_mae: 1.8690\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 5.9014 - mae: 1.7965\n",
      "Mean Absolute Error on Test Data: 1.7964754104614258\n",
      "10/10 [==============================] - 0s 855us/step\n",
      "R-squared: -0.0015353412178904513\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 10ms/step - loss: 32.5529 - mae: 4.8030 - val_loss: 29.0991 - val_mae: 4.2382\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 23.4513 - mae: 3.8197 - val_loss: 19.9541 - val_mae: 3.2375\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 14.1160 - mae: 2.7365 - val_loss: 12.3959 - val_mae: 2.4619\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 9.4217 - mae: 2.2709 - val_loss: 11.5099 - val_mae: 2.5943\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 9.2118 - mae: 2.3285 - val_loss: 11.3133 - val_mae: 2.5224\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 9.0415 - mae: 2.2415 - val_loss: 11.2344 - val_mae: 2.4673\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.9968 - mae: 2.2310 - val_loss: 11.2241 - val_mae: 2.4888\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.9923 - mae: 2.2583 - val_loss: 11.2527 - val_mae: 2.5136\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.9277 - mae: 2.2340 - val_loss: 11.1930 - val_mae: 2.4723\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.9093 - mae: 2.2296 - val_loss: 11.1865 - val_mae: 2.4784\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8813 - mae: 2.2238 - val_loss: 11.2046 - val_mae: 2.4936\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.8568 - mae: 2.2371 - val_loss: 11.2038 - val_mae: 2.4915\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8425 - mae: 2.2276 - val_loss: 11.1928 - val_mae: 2.4868\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8394 - mae: 2.2146 - val_loss: 11.1864 - val_mae: 2.4767\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8145 - mae: 2.2319 - val_loss: 11.2431 - val_mae: 2.5093\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8102 - mae: 2.2387 - val_loss: 11.2045 - val_mae: 2.4870\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7889 - mae: 2.2200 - val_loss: 11.1919 - val_mae: 2.4745\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.8063 - mae: 2.2088 - val_loss: 11.1755 - val_mae: 2.4672\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8470 - mae: 2.2424 - val_loss: 11.2334 - val_mae: 2.4977\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8244 - mae: 2.2054 - val_loss: 11.1999 - val_mae: 2.4789\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.8171 - mae: 2.2403 - val_loss: 11.2047 - val_mae: 2.4845\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7740 - mae: 2.2145 - val_loss: 11.2041 - val_mae: 2.4782\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7642 - mae: 2.2098 - val_loss: 11.2236 - val_mae: 2.4904\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7804 - mae: 2.2258 - val_loss: 11.2539 - val_mae: 2.5005\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7650 - mae: 2.2225 - val_loss: 11.2363 - val_mae: 2.4938\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7620 - mae: 2.2269 - val_loss: 11.2355 - val_mae: 2.4920\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7448 - mae: 2.2108 - val_loss: 11.2190 - val_mae: 2.4846\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7423 - mae: 2.2153 - val_loss: 11.2141 - val_mae: 2.4854\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7546 - mae: 2.2187 - val_loss: 11.2111 - val_mae: 2.4777\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7548 - mae: 2.2203 - val_loss: 11.2462 - val_mae: 2.4991\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7269 - mae: 2.2205 - val_loss: 11.2137 - val_mae: 2.4814\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7361 - mae: 2.2048 - val_loss: 11.2146 - val_mae: 2.4757\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7341 - mae: 2.2170 - val_loss: 11.2250 - val_mae: 2.4922\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7560 - mae: 2.2004 - val_loss: 11.2189 - val_mae: 2.4896\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7296 - mae: 2.2279 - val_loss: 11.2309 - val_mae: 2.4941\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7478 - mae: 2.2149 - val_loss: 11.2291 - val_mae: 2.4934\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7572 - mae: 2.2342 - val_loss: 11.2008 - val_mae: 2.4750\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7324 - mae: 2.1893 - val_loss: 11.1780 - val_mae: 2.4693\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7376 - mae: 2.2293 - val_loss: 11.2831 - val_mae: 2.5157\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7530 - mae: 2.2053 - val_loss: 11.2046 - val_mae: 2.4862\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7004 - mae: 2.2083 - val_loss: 11.2019 - val_mae: 2.4876\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7222 - mae: 2.2156 - val_loss: 11.1603 - val_mae: 2.4748\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.6879 - mae: 2.2051 - val_loss: 11.2178 - val_mae: 2.4947\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7011 - mae: 2.2144 - val_loss: 11.2276 - val_mae: 2.4991\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7408 - mae: 2.2044 - val_loss: 11.1777 - val_mae: 2.4742\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7042 - mae: 2.2323 - val_loss: 11.2279 - val_mae: 2.5018\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.7411 - mae: 2.2126 - val_loss: 11.2385 - val_mae: 2.5026\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.6887 - mae: 2.2060 - val_loss: 11.1808 - val_mae: 2.4737\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 8.6698 - mae: 2.2077 - val_loss: 11.1956 - val_mae: 2.4911\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 8.7176 - mae: 2.2135 - val_loss: 11.1860 - val_mae: 2.4760\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 9.3673 - mae: 2.3639\n",
      "Mean Absolute Error on Test Data: 2.3639490604400635\n",
      "11/11 [==============================] - 0s 723us/step\n",
      "R-squared: 0.01691035783435546\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 1s 12ms/step - loss: 75.4762 - mae: 7.3346 - val_loss: 75.5362 - val_mae: 6.8137\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 58.3352 - mae: 6.0869 - val_loss: 56.3481 - val_mae: 5.3385\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 38.4923 - mae: 4.4536 - val_loss: 36.7060 - val_mae: 3.7494\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 23.3944 - mae: 3.3025 - val_loss: 28.8322 - val_mae: 3.4839\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.3248 - mae: 3.4475 - val_loss: 28.7722 - val_mae: 3.5785\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.9288 - mae: 3.3418 - val_loss: 28.8864 - val_mae: 3.4679\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8716 - mae: 3.3200 - val_loss: 28.7477 - val_mae: 3.5011\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8511 - mae: 3.3445 - val_loss: 28.7433 - val_mae: 3.5014\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8373 - mae: 3.3179 - val_loss: 28.7900 - val_mae: 3.4824\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8022 - mae: 3.3312 - val_loss: 28.7477 - val_mae: 3.4967\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8406 - mae: 3.3267 - val_loss: 28.8221 - val_mae: 3.4737\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8182 - mae: 3.3110 - val_loss: 28.7306 - val_mae: 3.4995\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7910 - mae: 3.3215 - val_loss: 28.7296 - val_mae: 3.5002\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7505 - mae: 3.3803 - val_loss: 28.7064 - val_mae: 3.5434\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6908 - mae: 3.3675 - val_loss: 28.7333 - val_mae: 3.4897\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.6727 - mae: 3.3103 - val_loss: 28.8172 - val_mae: 3.4682\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9123 - mae: 3.2843 - val_loss: 28.8122 - val_mae: 3.4689\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7147 - mae: 3.3609 - val_loss: 28.6968 - val_mae: 3.5206\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7057 - mae: 3.3098 - val_loss: 28.8648 - val_mae: 3.4619\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7908 - mae: 3.3560 - val_loss: 28.7174 - val_mae: 3.5017\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6235 - mae: 3.2874 - val_loss: 28.9127 - val_mae: 3.4546\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7548 - mae: 3.3400 - val_loss: 28.7393 - val_mae: 3.4911\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5762 - mae: 3.3127 - val_loss: 28.8252 - val_mae: 3.4643\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5686 - mae: 3.2915 - val_loss: 28.7697 - val_mae: 3.4769\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5595 - mae: 3.3354 - val_loss: 28.7108 - val_mae: 3.5052\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6701 - mae: 3.3736 - val_loss: 28.7317 - val_mae: 3.4868\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5139 - mae: 3.2977 - val_loss: 28.9295 - val_mae: 3.4503\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6075 - mae: 3.3072 - val_loss: 28.7107 - val_mae: 3.4987\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5208 - mae: 3.2944 - val_loss: 28.8459 - val_mae: 3.4589\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5000 - mae: 3.2871 - val_loss: 28.6999 - val_mae: 3.4956\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5303 - mae: 3.2995 - val_loss: 28.7280 - val_mae: 3.4804\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.4721 - mae: 3.3079 - val_loss: 28.7292 - val_mae: 3.4808\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4448 - mae: 3.3104 - val_loss: 28.6972 - val_mae: 3.4903\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4707 - mae: 3.2862 - val_loss: 28.7331 - val_mae: 3.4792\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4687 - mae: 3.3043 - val_loss: 28.7274 - val_mae: 3.4836\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4442 - mae: 3.3307 - val_loss: 28.6765 - val_mae: 3.5117\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5037 - mae: 3.2904 - val_loss: 28.7823 - val_mae: 3.4718\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4246 - mae: 3.3136 - val_loss: 28.7029 - val_mae: 3.4965\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4294 - mae: 3.3101 - val_loss: 28.7718 - val_mae: 3.4780\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5319 - mae: 3.3704 - val_loss: 28.6794 - val_mae: 3.5183\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5188 - mae: 3.2954 - val_loss: 28.7485 - val_mae: 3.4806\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3917 - mae: 3.3339 - val_loss: 28.7096 - val_mae: 3.4953\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5141 - mae: 3.2650 - val_loss: 28.8784 - val_mae: 3.4555\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3675 - mae: 3.3088 - val_loss: 28.6706 - val_mae: 3.5164\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3433 - mae: 3.3082 - val_loss: 28.7693 - val_mae: 3.4765\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3461 - mae: 3.3000 - val_loss: 28.7166 - val_mae: 3.4913\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3153 - mae: 3.2973 - val_loss: 28.6920 - val_mae: 3.4996\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3630 - mae: 3.3232 - val_loss: 28.7619 - val_mae: 3.4821\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.3213 - mae: 3.2839 - val_loss: 28.7151 - val_mae: 3.4990\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4106 - mae: 3.3522 - val_loss: 28.7535 - val_mae: 3.4864\n",
      "11/11 [==============================] - 0s 826us/step - loss: 31.0084 - mae: 3.6740\n",
      "Mean Absolute Error on Test Data: 3.6739795207977295\n",
      "11/11 [==============================] - 0s 858us/step\n",
      "R-squared: 0.03956380128661352\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 12.9945 - mae: 2.5004 - val_loss: 9.7515 - val_mae: 2.0191\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 8.2207 - mae: 1.9083 - val_loss: 6.7166 - val_mae: 1.7448\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8621 - mae: 1.9184 - val_loss: 6.6195 - val_mae: 1.8757\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.8902 - mae: 1.9510 - val_loss: 6.5432 - val_mae: 1.7975\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.7620 - mae: 1.8794 - val_loss: 6.5661 - val_mae: 1.7489\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.7459 - mae: 1.8835 - val_loss: 6.5048 - val_mae: 1.7627\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6859 - mae: 1.8709 - val_loss: 6.4904 - val_mae: 1.7489\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.6443 - mae: 1.8700 - val_loss: 6.4883 - val_mae: 1.7462\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5793 - mae: 1.8628 - val_loss: 6.4447 - val_mae: 1.7681\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5516 - mae: 1.8669 - val_loss: 6.4634 - val_mae: 1.7478\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5269 - mae: 1.8542 - val_loss: 6.4551 - val_mae: 1.7474\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.5371 - mae: 1.8658 - val_loss: 6.4853 - val_mae: 1.7356\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5066 - mae: 1.8304 - val_loss: 6.4678 - val_mae: 1.7395\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.5025 - mae: 1.8647 - val_loss: 6.4398 - val_mae: 1.7503\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4846 - mae: 1.8474 - val_loss: 6.4901 - val_mae: 1.7307\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4748 - mae: 1.8157 - val_loss: 6.4622 - val_mae: 1.7412\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4763 - mae: 1.8661 - val_loss: 6.4466 - val_mae: 1.7538\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4232 - mae: 1.8280 - val_loss: 6.5240 - val_mae: 1.7323\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4744 - mae: 1.8357 - val_loss: 6.4508 - val_mae: 1.7454\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4434 - mae: 1.8200 - val_loss: 6.4903 - val_mae: 1.7370\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4740 - mae: 1.8845 - val_loss: 6.4555 - val_mae: 1.7757\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4309 - mae: 1.8299 - val_loss: 6.4991 - val_mae: 1.7408\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4357 - mae: 1.8151 - val_loss: 6.4834 - val_mae: 1.7422\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4407 - mae: 1.8690 - val_loss: 6.4600 - val_mae: 1.7680\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4449 - mae: 1.8206 - val_loss: 6.4789 - val_mae: 1.7431\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.4058 - mae: 1.8199 - val_loss: 6.4598 - val_mae: 1.7518\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3924 - mae: 1.8398 - val_loss: 6.4726 - val_mae: 1.7489\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4106 - mae: 1.8123 - val_loss: 6.4813 - val_mae: 1.7396\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3852 - mae: 1.8357 - val_loss: 6.4506 - val_mae: 1.7502\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3851 - mae: 1.8231 - val_loss: 6.4691 - val_mae: 1.7602\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4000 - mae: 1.8587 - val_loss: 6.4702 - val_mae: 1.7541\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3641 - mae: 1.8296 - val_loss: 6.4466 - val_mae: 1.7554\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3721 - mae: 1.8406 - val_loss: 6.4520 - val_mae: 1.7452\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3540 - mae: 1.8210 - val_loss: 6.4404 - val_mae: 1.7502\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3459 - mae: 1.8377 - val_loss: 6.4221 - val_mae: 1.7555\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3742 - mae: 1.8467 - val_loss: 6.4537 - val_mae: 1.7420\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3528 - mae: 1.8295 - val_loss: 6.4222 - val_mae: 1.7484\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3338 - mae: 1.8317 - val_loss: 6.4165 - val_mae: 1.7633\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.4412 - mae: 1.9048 - val_loss: 6.4287 - val_mae: 1.7504\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3325 - mae: 1.8205 - val_loss: 6.4636 - val_mae: 1.7380\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3201 - mae: 1.8171 - val_loss: 6.4165 - val_mae: 1.7543\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3916 - mae: 1.8527 - val_loss: 6.4887 - val_mae: 1.7319\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3304 - mae: 1.8092 - val_loss: 6.4174 - val_mae: 1.7523\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3144 - mae: 1.8335 - val_loss: 6.4294 - val_mae: 1.7486\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 6.3127 - mae: 1.8294 - val_loss: 6.4199 - val_mae: 1.7528\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3272 - mae: 1.8098 - val_loss: 6.4388 - val_mae: 1.7476\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 5ms/step - loss: 6.2988 - mae: 1.8438 - val_loss: 6.4256 - val_mae: 1.7786\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3375 - mae: 1.8222 - val_loss: 6.4301 - val_mae: 1.7572\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3079 - mae: 1.8401 - val_loss: 6.4135 - val_mae: 1.7642\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 6.3330 - mae: 1.8407 - val_loss: 6.4384 - val_mae: 1.7582\n",
      "10/10 [==============================] - 0s 851us/step - loss: 7.6507 - mae: 1.9523\n",
      "Mean Absolute Error on Test Data: 1.9522805213928223\n",
      "10/10 [==============================] - 0s 1ms/step\n",
      "R-squared: -0.02253674318306098\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Epoch 1/50\n",
      "17/17 [==============================] - 0s 12ms/step - loss: 37.6204 - mae: 4.3776 - val_loss: 18.6066 - val_mae: 3.2152\n",
      "Epoch 2/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 29.8953 - mae: 3.5768 - val_loss: 12.9285 - val_mae: 2.5117\n",
      "Epoch 3/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 22.4821 - mae: 2.8949 - val_loss: 8.6341 - val_mae: 2.1298\n",
      "Epoch 4/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 18.2134 - mae: 2.7981 - val_loss: 9.3040 - val_mae: 2.4723\n",
      "Epoch 5/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.6574 - mae: 2.9065 - val_loss: 9.0430 - val_mae: 2.4186\n",
      "Epoch 6/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.4715 - mae: 2.8249 - val_loss: 8.6011 - val_mae: 2.3102\n",
      "Epoch 7/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.3010 - mae: 2.7846 - val_loss: 8.7711 - val_mae: 2.3561\n",
      "Epoch 8/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.2077 - mae: 2.8156 - val_loss: 8.9610 - val_mae: 2.3996\n",
      "Epoch 9/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 17.1354 - mae: 2.8194 - val_loss: 8.7880 - val_mae: 2.3580\n",
      "Epoch 10/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 17.0465 - mae: 2.7910 - val_loss: 8.7665 - val_mae: 2.3497\n",
      "Epoch 11/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.9877 - mae: 2.7555 - val_loss: 8.6517 - val_mae: 2.3173\n",
      "Epoch 12/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.9227 - mae: 2.7644 - val_loss: 8.7711 - val_mae: 2.3448\n",
      "Epoch 13/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.8658 - mae: 2.7727 - val_loss: 8.8191 - val_mae: 2.3519\n",
      "Epoch 14/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.8233 - mae: 2.7731 - val_loss: 8.7613 - val_mae: 2.3332\n",
      "Epoch 15/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7955 - mae: 2.7310 - val_loss: 8.7439 - val_mae: 2.3250\n",
      "Epoch 16/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7449 - mae: 2.7301 - val_loss: 8.8003 - val_mae: 2.3361\n",
      "Epoch 17/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7661 - mae: 2.7250 - val_loss: 8.6752 - val_mae: 2.3032\n",
      "Epoch 18/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.8342 - mae: 2.7972 - val_loss: 8.9886 - val_mae: 2.3743\n",
      "Epoch 19/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.7340 - mae: 2.7033 - val_loss: 8.6641 - val_mae: 2.2913\n",
      "Epoch 20/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.6778 - mae: 2.7210 - val_loss: 8.8479 - val_mae: 2.3343\n",
      "Epoch 21/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.6326 - mae: 2.7259 - val_loss: 8.7581 - val_mae: 2.3114\n",
      "Epoch 22/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6010 - mae: 2.7058 - val_loss: 8.7677 - val_mae: 2.3106\n",
      "Epoch 23/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.6466 - mae: 2.7347 - val_loss: 8.7180 - val_mae: 2.2952\n",
      "Epoch 24/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.6295 - mae: 2.6994 - val_loss: 8.8493 - val_mae: 2.3292\n",
      "Epoch 25/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5680 - mae: 2.7089 - val_loss: 8.7131 - val_mae: 2.2986\n",
      "Epoch 26/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5825 - mae: 2.6979 - val_loss: 8.8763 - val_mae: 2.3319\n",
      "Epoch 27/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5725 - mae: 2.7411 - val_loss: 8.9179 - val_mae: 2.3369\n",
      "Epoch 28/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5625 - mae: 2.6730 - val_loss: 8.6056 - val_mae: 2.2598\n",
      "Epoch 29/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6250 - mae: 2.7119 - val_loss: 8.6921 - val_mae: 2.2864\n",
      "Epoch 30/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5442 - mae: 2.6949 - val_loss: 8.9501 - val_mae: 2.3476\n",
      "Epoch 31/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5508 - mae: 2.6891 - val_loss: 8.6523 - val_mae: 2.2727\n",
      "Epoch 32/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.6722 - mae: 2.7599 - val_loss: 8.7765 - val_mae: 2.3039\n",
      "Epoch 33/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5358 - mae: 2.6697 - val_loss: 8.7467 - val_mae: 2.2959\n",
      "Epoch 34/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5178 - mae: 2.7051 - val_loss: 8.7281 - val_mae: 2.2866\n",
      "Epoch 35/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4645 - mae: 2.6834 - val_loss: 8.7564 - val_mae: 2.3000\n",
      "Epoch 36/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5139 - mae: 2.6810 - val_loss: 8.8109 - val_mae: 2.3109\n",
      "Epoch 37/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.4948 - mae: 2.6873 - val_loss: 8.8122 - val_mae: 2.3131\n",
      "Epoch 38/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4789 - mae: 2.7349 - val_loss: 8.9178 - val_mae: 2.3330\n",
      "Epoch 39/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.4329 - mae: 2.6963 - val_loss: 8.7247 - val_mae: 2.2865\n",
      "Epoch 40/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.5492 - mae: 2.6567 - val_loss: 8.6172 - val_mae: 2.2605\n",
      "Epoch 41/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.5079 - mae: 2.7405 - val_loss: 8.9439 - val_mae: 2.3379\n",
      "Epoch 42/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4537 - mae: 2.6712 - val_loss: 8.6892 - val_mae: 2.2781\n",
      "Epoch 43/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4697 - mae: 2.7148 - val_loss: 8.9196 - val_mae: 2.3269\n",
      "Epoch 44/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.4047 - mae: 2.6647 - val_loss: 8.5883 - val_mae: 2.2549\n",
      "Epoch 45/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4240 - mae: 2.6720 - val_loss: 8.6635 - val_mae: 2.2752\n",
      "Epoch 46/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4729 - mae: 2.7220 - val_loss: 8.8145 - val_mae: 2.3071\n",
      "Epoch 47/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4035 - mae: 2.6530 - val_loss: 8.6270 - val_mae: 2.2620\n",
      "Epoch 48/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4418 - mae: 2.6970 - val_loss: 8.7302 - val_mae: 2.2959\n",
      "Epoch 49/50\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 16.4260 - mae: 2.6559 - val_loss: 8.5808 - val_mae: 2.2517\n",
      "Epoch 50/50\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 16.4339 - mae: 2.7117 - val_loss: 8.8213 - val_mae: 2.3112\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 13.8599 - mae: 2.7692\n",
      "Mean Absolute Error on Test Data: 2.7692203521728516\n",
      "11/11 [==============================] - 0s 979us/step\n",
      "R-squared: 0.0573939518672647\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 64.3962 - mae: 6.5166 - val_loss: 63.1811 - val_mae: 6.2093\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 51.1503 - mae: 5.4999 - val_loss: 47.2366 - val_mae: 4.9761\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 34.5374 - mae: 4.0937 - val_loss: 29.4818 - val_mae: 3.5435\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 22.4681 - mae: 3.3592 - val_loss: 23.4223 - val_mae: 3.3565\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.3741 - mae: 3.5253 - val_loss: 23.3841 - val_mae: 3.3467\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.2028 - mae: 3.3994 - val_loss: 23.6155 - val_mae: 3.2756\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.1644 - mae: 3.4165 - val_loss: 23.3347 - val_mae: 3.3019\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.0908 - mae: 3.4299 - val_loss: 23.2115 - val_mae: 3.2954\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0619 - mae: 3.4086 - val_loss: 23.2788 - val_mae: 3.2701\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.0818 - mae: 3.4138 - val_loss: 23.1406 - val_mae: 3.2869\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0384 - mae: 3.3865 - val_loss: 23.2306 - val_mae: 3.2569\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 21.0000 - mae: 3.3928 - val_loss: 23.0819 - val_mae: 3.2691\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9584 - mae: 3.4108 - val_loss: 22.9781 - val_mae: 3.2785\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9895 - mae: 3.4131 - val_loss: 22.8776 - val_mae: 3.2789\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.9289 - mae: 3.4008 - val_loss: 23.0504 - val_mae: 3.2413\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0224 - mae: 3.4241 - val_loss: 22.8389 - val_mae: 3.2978\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.0595 - mae: 3.3712 - val_loss: 23.1827 - val_mae: 3.2316\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 21.1808 - mae: 3.4647 - val_loss: 22.7787 - val_mae: 3.2729\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8912 - mae: 3.3739 - val_loss: 22.9484 - val_mae: 3.2280\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.8433 - mae: 3.4133 - val_loss: 22.5891 - val_mae: 3.2889\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.8734 - mae: 3.4627 - val_loss: 22.6346 - val_mae: 3.2544\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7403 - mae: 3.3744 - val_loss: 22.8771 - val_mae: 3.2125\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.9561 - mae: 3.3458 - val_loss: 22.7097 - val_mae: 3.2257\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7265 - mae: 3.3909 - val_loss: 22.4956 - val_mae: 3.2588\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.7144 - mae: 3.3815 - val_loss: 22.7971 - val_mae: 3.2053\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7223 - mae: 3.3791 - val_loss: 22.4519 - val_mae: 3.2475\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6898 - mae: 3.3903 - val_loss: 22.4533 - val_mae: 3.2424\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7385 - mae: 3.4296 - val_loss: 22.5292 - val_mae: 3.2240\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6834 - mae: 3.3620 - val_loss: 22.5138 - val_mae: 3.2179\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7059 - mae: 3.3562 - val_loss: 22.4849 - val_mae: 3.2233\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7506 - mae: 3.4207 - val_loss: 22.3918 - val_mae: 3.2360\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6120 - mae: 3.3826 - val_loss: 22.5024 - val_mae: 3.2146\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5989 - mae: 3.3765 - val_loss: 22.3658 - val_mae: 3.2389\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6431 - mae: 3.4340 - val_loss: 22.3306 - val_mae: 3.2346\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.6528 - mae: 3.3476 - val_loss: 22.5464 - val_mae: 3.2036\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.7131 - mae: 3.4522 - val_loss: 22.2598 - val_mae: 3.2617\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5216 - mae: 3.3660 - val_loss: 22.5703 - val_mae: 3.2016\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5664 - mae: 3.3496 - val_loss: 22.2931 - val_mae: 3.2286\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5494 - mae: 3.3970 - val_loss: 22.2661 - val_mae: 3.2211\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5254 - mae: 3.3840 - val_loss: 22.2040 - val_mae: 3.2327\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.5075 - mae: 3.4058 - val_loss: 22.2671 - val_mae: 3.2327\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5937 - mae: 3.4307 - val_loss: 22.2044 - val_mae: 3.2541\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.4396 - mae: 3.3657 - val_loss: 22.4639 - val_mae: 3.1927\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5151 - mae: 3.3751 - val_loss: 22.2007 - val_mae: 3.2249\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.4427 - mae: 3.3903 - val_loss: 22.1880 - val_mae: 3.2244\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 20.4296 - mae: 3.3752 - val_loss: 22.1808 - val_mae: 3.2195\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.4915 - mae: 3.4304 - val_loss: 22.1677 - val_mae: 3.2378\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.5448 - mae: 3.3669 - val_loss: 22.2292 - val_mae: 3.2195\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 20.6003 - mae: 3.4513 - val_loss: 21.9894 - val_mae: 3.2593\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 20.5521 - mae: 3.3542 - val_loss: 22.4210 - val_mae: 3.1798\n",
      "11/11 [==============================] - 0s 858us/step - loss: 20.3762 - mae: 3.2735\n",
      "Mean Absolute Error on Test Data: 3.2735109329223633\n",
      "11/11 [==============================] - 0s 922us/step\n",
      "R-squared: 0.029858998440041473\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 1/50\n",
      "18/18 [==============================] - 0s 11ms/step - loss: 138.0907 - mae: 9.7299 - val_loss: 131.5430 - val_mae: 9.5994\n",
      "Epoch 2/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 115.7402 - mae: 8.5349 - val_loss: 102.9350 - val_mae: 8.0121\n",
      "Epoch 3/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 83.8123 - mae: 6.5978 - val_loss: 64.2466 - val_mae: 5.5756\n",
      "Epoch 4/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 52.2849 - mae: 4.5458 - val_loss: 39.2474 - val_mae: 4.4231\n",
      "Epoch 5/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.1698 - mae: 4.4102 - val_loss: 37.6409 - val_mae: 4.4865\n",
      "Epoch 6/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 43.0325 - mae: 4.3878 - val_loss: 38.1425 - val_mae: 4.4128\n",
      "Epoch 7/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.8693 - mae: 4.2509 - val_loss: 37.9209 - val_mae: 4.4109\n",
      "Epoch 8/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6376 - mae: 4.3585 - val_loss: 37.4860 - val_mae: 4.4260\n",
      "Epoch 9/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6115 - mae: 4.3542 - val_loss: 37.6543 - val_mae: 4.4096\n",
      "Epoch 10/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5662 - mae: 4.3390 - val_loss: 37.5884 - val_mae: 4.4078\n",
      "Epoch 11/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5100 - mae: 4.3103 - val_loss: 37.7964 - val_mae: 4.4001\n",
      "Epoch 12/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5596 - mae: 4.3297 - val_loss: 37.5465 - val_mae: 4.3981\n",
      "Epoch 13/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.6346 - mae: 4.2807 - val_loss: 37.6762 - val_mae: 4.3971\n",
      "Epoch 14/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.7507 - mae: 4.4117 - val_loss: 37.4158 - val_mae: 4.4112\n",
      "Epoch 15/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4720 - mae: 4.3280 - val_loss: 37.8093 - val_mae: 4.4039\n",
      "Epoch 16/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.5373 - mae: 4.2732 - val_loss: 37.7437 - val_mae: 4.3993\n",
      "Epoch 17/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4215 - mae: 4.3633 - val_loss: 37.3640 - val_mae: 4.4172\n",
      "Epoch 18/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4990 - mae: 4.4201 - val_loss: 37.4716 - val_mae: 4.4024\n",
      "Epoch 19/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4521 - mae: 4.3351 - val_loss: 37.4282 - val_mae: 4.4019\n",
      "Epoch 20/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.3090 - mae: 4.3498 - val_loss: 37.6311 - val_mae: 4.3908\n",
      "Epoch 21/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.4628 - mae: 4.2579 - val_loss: 37.8199 - val_mae: 4.3889\n",
      "Epoch 22/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.4271 - mae: 4.4134 - val_loss: 37.2316 - val_mae: 4.4254\n",
      "Epoch 23/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4415 - mae: 4.3152 - val_loss: 37.8662 - val_mae: 4.3944\n",
      "Epoch 24/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4012 - mae: 4.2793 - val_loss: 37.9506 - val_mae: 4.3895\n",
      "Epoch 25/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.4004 - mae: 4.4256 - val_loss: 37.2265 - val_mae: 4.4305\n",
      "Epoch 26/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.4949 - mae: 4.3369 - val_loss: 37.8207 - val_mae: 4.3926\n",
      "Epoch 27/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.3049 - mae: 4.3459 - val_loss: 37.5809 - val_mae: 4.4073\n",
      "Epoch 28/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.2359 - mae: 4.4035 - val_loss: 37.2106 - val_mae: 4.4049\n",
      "Epoch 29/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.1807 - mae: 4.3645 - val_loss: 37.4367 - val_mae: 4.3896\n",
      "Epoch 30/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.2414 - mae: 4.2859 - val_loss: 37.5554 - val_mae: 4.3895\n",
      "Epoch 31/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1710 - mae: 4.3470 - val_loss: 37.4732 - val_mae: 4.3936\n",
      "Epoch 32/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 42.1551 - mae: 4.3634 - val_loss: 37.5193 - val_mae: 4.3981\n",
      "Epoch 33/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1935 - mae: 4.2797 - val_loss: 37.9706 - val_mae: 4.3974\n",
      "Epoch 34/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0845 - mae: 4.2900 - val_loss: 37.6024 - val_mae: 4.4001\n",
      "Epoch 35/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0923 - mae: 4.3159 - val_loss: 37.5365 - val_mae: 4.3994\n",
      "Epoch 36/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0475 - mae: 4.3382 - val_loss: 37.6169 - val_mae: 4.3969\n",
      "Epoch 37/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0177 - mae: 4.3275 - val_loss: 37.6283 - val_mae: 4.3981\n",
      "Epoch 38/50\n",
      "18/18 [==============================] - 0s 5ms/step - loss: 42.0450 - mae: 4.3377 - val_loss: 37.3657 - val_mae: 4.4007\n",
      "Epoch 39/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1288 - mae: 4.2998 - val_loss: 37.8179 - val_mae: 4.3902\n",
      "Epoch 40/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0875 - mae: 4.3665 - val_loss: 37.3857 - val_mae: 4.4022\n",
      "Epoch 41/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.1441 - mae: 4.3468 - val_loss: 37.8157 - val_mae: 4.3961\n",
      "Epoch 42/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9563 - mae: 4.3182 - val_loss: 37.4719 - val_mae: 4.4029\n",
      "Epoch 43/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9808 - mae: 4.2942 - val_loss: 37.6059 - val_mae: 4.3898\n",
      "Epoch 44/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 42.0421 - mae: 4.2596 - val_loss: 37.7563 - val_mae: 4.3987\n",
      "Epoch 45/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9964 - mae: 4.3839 - val_loss: 37.3504 - val_mae: 4.4111\n",
      "Epoch 46/50\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 41.9372 - mae: 4.3760 - val_loss: 37.5607 - val_mae: 4.3983\n",
      "Epoch 47/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9875 - mae: 4.3051 - val_loss: 37.5941 - val_mae: 4.4048\n",
      "Epoch 48/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.8411 - mae: 4.2994 - val_loss: 37.9467 - val_mae: 4.4037\n",
      "Epoch 49/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.9317 - mae: 4.2961 - val_loss: 37.6962 - val_mae: 4.4128\n",
      "Epoch 50/50\n",
      "18/18 [==============================] - 0s 3ms/step - loss: 41.8645 - mae: 4.3342 - val_loss: 37.7360 - val_mae: 4.4036\n",
      "11/11 [==============================] - 0s 968us/step - loss: 37.2798 - mae: 4.2999\n",
      "Mean Absolute Error on Test Data: 4.299875736236572\n",
      "11/11 [==============================] - 0s 1ms/step\n",
      "R-squared: 0.035915551284582126\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "# all_in_one    (runtime: 6m 40s (auto), 2m 19s (no GPU), 5m59s (nothing))\n",
    "\n",
    "# gpu deactivate\n",
    "tf.config.set_visible_devices([],'GPU') # 2m 19s\n",
    "\n",
    "# gpu ayarlanmasi\n",
    "#tf.config.set_soft_device_placement(True) # otomatik dogru cihaz kullanimi\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "#tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[1], 'GPU') # 2. gpu ayari\n",
    "#tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "#tf.config.experimental.set_visible_devices(physical_devices[1], 'GPU')\n",
    "\n",
    "df = merged_all\n",
    "df_test = dict_test_merged\n",
    "features = ['Bildirimli_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "features_gun = ['Bildirimli_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max','Gün']\n",
    "features_bayramsiz = ['Bildirimli_sum','Sicaklik_max','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "features_output = ['Bildirimli_sum','Bildirimsiz_sum','Sicaklik_max','Bayram_Flag','Bagil_nem_max','Ruzgar_hizi_max','Yagis_max']\n",
    "output_var = df\n",
    "target = 'Bildirimsiz_sum'\n",
    "# ilceler = []\n",
    "\n",
    "# NN 3\n",
    "# ilceler = ['izmir-konak','izmir-kinik']\n",
    "all_submissions = []\n",
    "for ilce in ilceler:\n",
    "    df = merged_all[ilce]\n",
    "    df_test = dict_test_merged[ilce]\n",
    "    output_var = df['Bildirimsiz_sum']\n",
    "\n",
    "    # ilcelerin numerizasyonu\n",
    "    columns_tonumerate = ['Bayram_Flag']\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "\n",
    "    # test csv dosyasi numerizasyon\n",
    "    for column in columns_tonumerate:\n",
    "        encoder = LabelEncoder()\n",
    "        df_test[column] = encoder.fit_transform(df_test[column])\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_transform = scaler.fit_transform(df[features])\n",
    "    feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "    feature_transform_gun = scaler.fit_transform(df[features_gun])\n",
    "    feature_transform_gun = pd.DataFrame(columns=features_gun, data=feature_transform_gun, index=df.index)\n",
    "    scaler2 = MinMaxScaler()\n",
    "    feature_test = scaler2.fit_transform(df_test[features])\n",
    "    feature_test = pd.DataFrame(columns=features, data=feature_test, index=df_test.index)\n",
    "\n",
    "\n",
    "    #with strategy.scope(): # strategy ayarlamasi\n",
    "    X = feature_transform\n",
    "    y = output_var\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=53)\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation='relu', input_shape=(6,)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='mean_squared_error',\n",
    "                metrics=['mae'])\n",
    "\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.3)\n",
    "\n",
    "    loss, mae = model.evaluate(X_test, y_test)\n",
    "    print(\"Mean Absolute Error on Test Data:\", mae)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "    print(\"R-squared:\", r2)\n",
    "\n",
    "    predictions_new = model.predict(feature_test)\n",
    "    predictions_new = np.round(predictions_new).astype(int)\n",
    "    df_test['bildirimsiz_sum'] = predictions_new\n",
    "    df_test.to_csv('test_with_predictions.csv', index=False)\n",
    "\n",
    "    df_test.rename(columns={'Ilce': 'ilce'}, inplace=True)\n",
    "    df_test.rename(columns={'Tarih': 'tarih'}, inplace=True)\n",
    "    df_test.rename(columns={'Bildirimli_sum': 'bildirimli_sum'}, inplace=True)\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    all_submissions.append(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bildirimsiz_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bildirimsiz_sum'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df_test \u001b[38;5;241m=\u001b[39m test \u001b[38;5;66;03m#test csv birlestirilmis hazir dosyasi\u001b[39;00m\n\u001b[0;32m      4\u001b[0m features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimli_sum\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msicaklik vs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m#kalan ozellikler de yazilmali\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m output_var \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbildirimsiz_sum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbildirimsiz_sum\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m ilceler \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\frame.py:4090\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4088\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4090\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4092\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\okkes\\anaconda3\\envs\\pyokkes\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'bildirimsiz_sum'"
     ]
    }
   ],
   "source": [
    "#merged_all[]\n",
    "df = merged_all['izmir-konak'] #ml icin hazir csv dosyasi\n",
    "df_test = test #test csv birlestirilmis hazir dosyasi\n",
    "features = ['bildirimli_sum','sicaklik vs'] #kalan ozellikler de yazilmali\n",
    "output_var = df['bildirimsiz_sum']\n",
    "target = 'bildirimsiz_sum'\n",
    "ilceler = []\n",
    "\n",
    "dict = {}\n",
    "for label, group in train.groupby(\"ilce\"):\n",
    "    dict[label] = group\n",
    "ilceler = list(dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ilcelerin numerizasyonu -------- NEW olmasa da direkt sayilar olsa daha iyi olabilir\n",
    "columns_tonumerate = ['ilce','BAYRAM_FLAG','vs vs.']\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    encode = encoder.fit_transform(df[column])\n",
    "    df[column + '_NEW'] = encode #buraya _NEW eklemeli miyim tekrar bakmak gerek\n",
    "    df.drop(columns=[column], inplace=True)\n",
    "\n",
    "# csv dosyasi numerizasyon - sadece numarizasyon degil ayni zamanda weather ile birlestirme de yapilmali!!\n",
    "for column in columns_tonumerate:\n",
    "    encoder = LabelEncoder()\n",
    "    encode = encoder.fit_transform(df_test[column])\n",
    "    df_test[column + '_NEW'] = encode\n",
    "    df_test.drop(columns=[column], inplace=True)\n",
    "\n",
    "#Scaling\n",
    "scaler = MinMaxScaler()\n",
    "feature_transform = scaler.fit_transform(df[features])\n",
    "feature_transform = pd.DataFrame(columns=features, data=feature_transform, index=df.index)\n",
    "feature_transform.head()\n",
    "\n",
    "\n",
    "# test.csv numerizasyonu da gerekiyor ayni sekilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-y test-train elde edimi\n",
    "x = df[features]\n",
    "y = output_var # = df[\"target_var\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "#Splitting to Training set and Test set --- burasi timeseries icin split\n",
    "timesplit = TimeSeriesSplit(n_splits=15)\n",
    "for train_index, test_index in timesplit.split(feature_transform):\n",
    "        X_tr, X_te = feature_transform[:len(train_index)], feature_transform[len(train_index): (len(train_index)+len(test_index))]\n",
    "        y_tr, y_te = output_var[:len(train_index)].values.ravel(), output_var[len(train_index): (len(train_index)+len(test_index))].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "#Process the data for LSTM\n",
    "trainX = np.array(X_tr)\n",
    "testX = np.array(X_te)\n",
    "X_tr = trainX.reshape(X_tr.shape[0], 1, X_tr.shape[1])\n",
    "X_te = testX.reshape(X_te.shape[0], 1, X_te.shape[1])\n",
    "\n",
    "#Building the LSTM Model\n",
    "lstm = Sequential()\n",
    "lstm.add(LSTM(32, input_shape=(1, trainX.shape[1]), activation='relu', return_sequences=False))\n",
    "lstm.add(Dense(1))\n",
    "lstm.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "#Model Training\n",
    "history=lstm.fit(X_tr, y_tr, epochs=100, batch_size=8, verbose=1, shuffle=False)\n",
    "\n",
    "#LSTM Prediction\n",
    "y_pr= lstm.predict(X_te)\n",
    "\n",
    "# Predicted vs True Adj Close Value – LSTM  --burasi copy paste\n",
    "plt.plot(y_te, label='True Value')\n",
    "plt.plot(y_pr, label='LSTM Value')\n",
    "plt.title(\"Prediction by LSTM\")\n",
    "plt.xlabel('Time Scale')\n",
    "plt.ylabel('Scaled USD')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# test_pred = lstm.predict(gercek test)\n",
    "# csv ye yazdir vs vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "\n",
    "X = df[features].values()\n",
    "y = df[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=53, shuffle=True)\n",
    "\n",
    "k=17\n",
    "neigh = KNeighborsClassifier(n_neighbors=k, weights='distance')\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "y_hat = neigh.predict(X_test)\n",
    "\n",
    "test_accuracy = neigh.score(X_test, y_test)\n",
    "\n",
    "print(\"Test accuracy with class weights:\", test_accuracy)\n",
    "print(\"egitim verisi dogrulugu \", metrics.accuracy_score(y_train,neigh.predict(X_train)))\n",
    "print(\"test verisi dogrulugu \", metrics.accuracy_score(y_test,y_hat))\n",
    "\n",
    "# test tahmin\n",
    "y_hat = neigh.predict(isteburayatestdosyasi)\n",
    "submission = pd.read_csv(\"sample_submission.csv\", low_memory=False)\n",
    "submission.iloc[:, 1] = y_hat\n",
    "# submission.to_csv(\"knnsubmission.csv\", index=False)\n",
    "\n",
    "# optimal k degeri\n",
    "\n",
    "# # Define the range of k values to try\n",
    "# k_values = range(1, 21)\n",
    "\n",
    "# # Perform cross-validation for each value of k\n",
    "# cv_scores = []\n",
    "# for k in k_values:\n",
    "#     neigh = KNeighborsClassifier(n_neighbors=k)\n",
    "#     scores = cross_val_score(neigh, X_train, y_train, cv=5)\n",
    "#     cv_scores.append(scores.mean())\n",
    "\n",
    "# # Find the optimal value of k with the highest cross-validation score\n",
    "# optimal_k = k_values[cv_scores.index(max(cv_scores))]\n",
    "# print(\"Optimal k:\", optimal_k)\n",
    "\n",
    "# # Train the model with the optimal k value\n",
    "# neigh = KNeighborsClassifier(n_neighbors=optimal_k).fit(X_train, y_train)\n",
    "# test_accuracy = neigh.score(X_test, y_test)\n",
    "# print(\"Test accuracy with optimal k:\", test_accuracy)\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "# alinan kaynakta goruntu isleme icin kullaniliyordu bazi uyusmazliklar olabilir\n",
    "# Cast the records into float values \n",
    "# x_train = x_train.astype('float32') \n",
    "# x_test = x_test.astype('float32') \n",
    "\n",
    "print(\"Feature matrix:\", x_train.shape) \n",
    "print(\"Target matrix:\", x_test.shape) \n",
    "print(\"Feature matrix:\", y_train.shape) \n",
    "print(\"Target matrix:\", y_test.shape)  \n",
    "model = Sequential([ \n",
    "    Flatten(input_shape=(x_train.shape)), \n",
    "    \n",
    "    # dense layer 1 \n",
    "    Dense(256, activation='sigmoid'),   \n",
    "    \n",
    "    # dense layer 2 \n",
    "    Dense(128, activation='sigmoid'),  \n",
    "    \n",
    "    # output layer \n",
    "    Dense(10, activation='sigmoid'),   \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy']) \n",
    "\n",
    "model.fit(x_train, y_train, epochs=10,  \n",
    "          batch_size=2000,  \n",
    "          validation_split=0.2)\n",
    "\n",
    "results = model.evaluate(x_test,  y_test, verbose = 0) \n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential() specifies that the network is a linear stack of layers\n",
    "\n",
    "model.add() adds the hidden layer.\n",
    "\n",
    "Dense means that neurons between layers are fully connected\n",
    "\n",
    "input_dim defines the number of features in the training dataset\n",
    "\n",
    "activation defines the activation function\n",
    "\n",
    "loss selects the cost function\n",
    "\n",
    "optimizer selects the learning algorithm\n",
    "\n",
    "metrics selects the performance metrics to be saved for further analysis\n",
    "\n",
    "model.fit() initialize the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN2\n",
    "\n",
    "X = df[features] #features\n",
    "y = df['target_var'] #expected values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_dim=2, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy', 'mean_squared_error'])\n",
    "\n",
    "history = model.fit(X, y, epochs=3000, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X).round()\n",
    "num_correct_predictions = (y_pred == y).sum()\n",
    "accuracy = (num_correct_predictions / y.shape[0]) * 100\n",
    "print('Multi-layer perceptron accuracy: %.2f%%' % accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
